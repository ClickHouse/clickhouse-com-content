[{"title":"ClickHouse Cloud Service","type":0,"sectionRef":"#","url":"docs/about-us/cloud","content":"ClickHouse Cloud Service We are building a serverless hosted ClickHouse offering that is: Cloud-agnosticNo infrastructure to manageAutomatic scalingConsumption-based pricingTurnkey data migration servicesWorld-class security and privacy guaranteesReduce total cost of ownership and let us take the worry out of operating ClickHouse, at any scale.","keywords":""},{"title":"Distinctive Features of ClickHouse","type":0,"sectionRef":"#","url":"docs/about-us/distinctive-features","content":"","keywords":""},{"title":"True Column-Oriented Database Management System​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"docs/about-us/distinctive-features#true-column-oriented-database-management-system","content":"In a real column-oriented DBMS, no extra data is stored with the values. Among other things, this means that constant-length values must be supported, to avoid storing their length “number” next to the values. For example, a billion UInt8-type values should consume around 1 GB uncompressed, or this strongly affects the CPU use. It is essential to store data compactly (without any “garbage”) even when uncompressed since the speed of decompression (CPU usage) depends mainly on the volume of uncompressed data. It is worth noting because there are systems that can store values of different columns separately, but that can’t effectively process analytical queries due to their optimization for other scenarios. Examples are HBase, BigTable, Cassandra, and HyperTable. You would get throughput around a hundred thousand rows per second in these systems, but not hundreds of millions of rows per second. It’s also worth noting that ClickHouse is a database management system, not a single database. ClickHouse allows creating tables and databases in runtime, loading data, and running queries without reconfiguring and restarting the server. "},{"title":"Data Compression​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"docs/about-us/distinctive-features#data-compression","content":"Some column-oriented DBMSs do not use data compression. However, data compression does play a key role in achieving excellent performance. In addition to efficient general-purpose compression codecs with different trade-offs between disk space and CPU consumption, ClickHouse provides specialized codecs for specific kinds of data, which allow ClickHouse to compete with and outperform more niche databases, like time-series ones. "},{"title":"Disk Storage of Data​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"docs/about-us/distinctive-features#disk-storage-of-data","content":"Keeping data physically sorted by primary key makes it possible to extract data for its specific values or value ranges with low latency, less than a few dozen milliseconds. Some column-oriented DBMSs (such as SAP HANA and Google PowerDrill) can only work in RAM. This approach encourages the allocation of a larger hardware budget than is necessary for real-time analysis. ClickHouse is designed to work on regular hard drives, which means the cost per GB of data storage is low, but SSD and additional RAM are also fully used if available. "},{"title":"Parallel Processing on Multiple Cores​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"docs/about-us/distinctive-features#parallel-processing-on-multiple-cores","content":"Large queries are parallelized naturally, taking all the necessary resources available on the current server. "},{"title":"Distributed Processing on Multiple Servers​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"docs/about-us/distinctive-features#distributed-processing-on-multiple-servers","content":"Almost none of the columnar DBMSs mentioned above have support for distributed query processing. In ClickHouse, data can reside on different shards. Each shard can be a group of replicas used for fault tolerance. All shards are used to run a query in parallel, transparently for the user. "},{"title":"SQL Support​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"docs/about-us/distinctive-features#sql-support","content":"ClickHouse supports a declarative query language based on SQL that is identical to the ANSI SQL standard in many cases. Supported queries include GROUP BY, ORDER BY, subqueries in FROM, JOIN clause, IN operator, window functions and scalar subqueries. Correlated (dependent) subqueries are not supported at the time of writing but might become available in the future. "},{"title":"Vector Computation Engine​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"docs/about-us/distinctive-features#vector-engine","content":"Data is not only stored by columns but is processed by vectors (parts of columns), which allows achieving high CPU efficiency. "},{"title":"Real-time Data Updates​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"docs/about-us/distinctive-features#real-time-data-updates","content":"ClickHouse supports tables with a primary key. To quickly perform queries on the range of the primary key, the data is sorted incrementally using the merge tree. Due to this, data can continually be added to the table. No locks are taken when new data is ingested. "},{"title":"Primary Index​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"docs/about-us/distinctive-features#primary-index","content":"Having a data physically sorted by primary key makes it possible to extract data for its specific values or value ranges with low latency, less than a few dozen milliseconds. "},{"title":"Secondary Indexes​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"docs/about-us/distinctive-features#secondary-indexes","content":"Unlike other database management systems, secondary indexes in ClickHouse does not point to specific rows or row ranges. Instead, they allow the database to know in advance that all rows in some data parts wouldn’t match the query filtering conditions and do not read them at all, thus they are called data skipping indexes. "},{"title":"Suitable for Online Queries​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"docs/about-us/distinctive-features#suitable-for-online-queries","content":"Most OLAP database management systems do not aim for online queries with sub-second latencies. In alternative systems, report building time of tens of seconds or even minutes is often considered acceptable. Sometimes it takes even more which forces to prepare reports offline (in advance or by responding with “come back later”). In ClickHouse low latency means that queries can be processed without delay and without trying to prepare an answer in advance, right at the same moment while the user interface page is loading. In other words, online. "},{"title":"Support for Approximated Calculations​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"docs/about-us/distinctive-features#support-for-approximated-calculations","content":"ClickHouse provides various ways to trade accuracy for performance: Aggregate functions for approximated calculation of the number of distinct values, medians, and quantiles.Running a query based on a part (sample) of data and getting an approximated result. In this case, proportionally less data is retrieved from the disk.Running an aggregation for a limited number of random keys, instead of for all keys. Under certain conditions for key distribution in the data, this provides a reasonably accurate result while using fewer resources. "},{"title":"Adaptive Join Algorithm​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"docs/about-us/distinctive-features#adaptive-join-algorithm","content":"ClickHouse adaptively chooses how to JOIN multiple tables, by preferring hash-join algorithm and falling back to the merge-join algorithm if there’s more than one large table. "},{"title":"Data Replication and Data Integrity Support​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"docs/about-us/distinctive-features#data-replication-and-data-integrity-support","content":"ClickHouse uses asynchronous multi-master replication. After being written to any available replica, all the remaining replicas retrieve their copy in the background. The system maintains identical data on different replicas. Recovery after most failures is performed automatically, or semi-automatically in complex cases. For more information, see the section Data replication. "},{"title":"Role-Based Access Control​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"docs/about-us/distinctive-features#role-based-access-control","content":"ClickHouse implements user account management using SQL queries and allows for role-based access control configuration similar to what can be found in ANSI SQL standard and popular relational database management systems. "},{"title":"Features that Can Be Considered Disadvantages​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"docs/about-us/distinctive-features#clickhouse-features-that-can-be-considered-disadvantages","content":"No full-fledged transactions.Lack of ability to modify or delete already inserted data with a high rate and low latency. There are batch deletes and updates available to clean up or modify data, for example, to comply with GDPR.The sparse index makes ClickHouse not so efficient for point queries retrieving single rows by their keys. Original article "},{"title":"ClickHouse History","type":0,"sectionRef":"#","url":"docs/about-us/history","content":"","keywords":""},{"title":"Usage in Yandex.Metrica and Other Yandex Services​","type":1,"pageTitle":"ClickHouse History","url":"docs/about-us/history#usage-in-yandex-metrica-and-other-yandex-services","content":"ClickHouse serves multiple purposes in Yandex.Metrica. Its main task is to build reports in online mode using non-aggregated data. It uses a cluster of 374 servers, which store over 20.3 trillion rows in the database. The volume of compressed data is about 2 PB, without accounting for duplicates and replicas. The volume of uncompressed data (in TSV format) would be approximately 17 PB. ClickHouse also plays a key role in the following processes: Storing data for Session Replay from Yandex.Metrica.Processing intermediate data.Building global reports with Analytics.Running queries for debugging the Yandex.Metrica engine.Analyzing logs from the API and the user interface. Nowadays, there are multiple dozen ClickHouse installations in other Yandex services and departments: search verticals, e-commerce, advertisement, business analytics, mobile development, personal services, and others. "},{"title":"Aggregated and Non-aggregated Data​","type":1,"pageTitle":"ClickHouse History","url":"docs/about-us/history#aggregated-and-non-aggregated-data","content":"There is a widespread opinion that to calculate statistics effectively, you must aggregate data since this reduces the volume of data. But data aggregation comes with a lot of limitations: You must have a pre-defined list of required reports.The user can’t make custom reports.When aggregating over a large number of distinct keys, the data volume is barely reduced, so aggregation is useless.For a large number of reports, there are too many aggregation variations (combinatorial explosion).When aggregating keys with high cardinality (such as URLs), the volume of data is not reduced by much (less than twofold).For this reason, the volume of data with aggregation might grow instead of shrink.Users do not view all the reports we generate for them. A large portion of those calculations is useless.The logical integrity of data may be violated for various aggregations. If we do not aggregate anything and work with non-aggregated data, this might reduce the volume of calculations. However, with aggregation, a significant part of the work is taken offline and completed relatively calmly. In contrast, online calculations require calculating as fast as possible, since the user is waiting for the result. Yandex.Metrica has a specialized system for aggregating data called Metrage, which was used for the majority of reports. Starting in 2009, Yandex.Metrica also used a specialized OLAP database for non-aggregated data called OLAPServer, which was previously used for the report builder. OLAPServer worked well for non-aggregated data, but it had many restrictions that did not allow it to be used for all reports as desired. These included the lack of support for data types (only numbers), and the inability to incrementally update data in real-time (it could only be done by rewriting data daily). OLAPServer is not a DBMS, but a specialized DB. The initial goal for ClickHouse was to remove the limitations of OLAPServer and solve the problem of working with non-aggregated data for all reports, but over the years, it has grown into a general-purpose database management system suitable for a wide range of analytical tasks. Original article "},{"title":"Performance","type":0,"sectionRef":"#","url":"docs/about-us/performance","content":"","keywords":""},{"title":"Throughput for a Single Large Query​","type":1,"pageTitle":"Performance","url":"docs/about-us/performance#throughput-for-a-single-large-query","content":"Throughput can be measured in rows per second or megabytes per second. If the data is placed in the page cache, a query that is not too complex is processed on modern hardware at a speed of approximately 2-10 GB/s of uncompressed data on a single server (for the most straightforward cases, the speed may reach 30 GB/s). If data is not placed in the page cache, the speed depends on the disk subsystem and the data compression rate. For example, if the disk subsystem allows reading data at 400 MB/s, and the data compression rate is 3, the speed is expected to be around 1.2 GB/s. To get the speed in rows per second, divide the speed in bytes per second by the total size of the columns used in the query. For example, if 10 bytes of columns are extracted, the speed is expected to be around 100-200 million rows per second. The processing speed increases almost linearly for distributed processing, but only if the number of rows resulting from aggregation or sorting is not too large. "},{"title":"Latency When Processing Short Queries​","type":1,"pageTitle":"Performance","url":"docs/about-us/performance#latency-when-processing-short-queries","content":"If a query uses a primary key and does not select too many columns and rows to process (hundreds of thousands), you can expect less than 50 milliseconds of latency (single digits of milliseconds in the best case) if data is placed in the page cache. Otherwise, latency is mostly dominated by the number of seeks. If you use rotating disk drives, for a system that is not overloaded, the latency can be estimated with this formula: seek time (10 ms) * count of columns queried * count of data parts. "},{"title":"Throughput When Processing a Large Quantity of Short Queries​","type":1,"pageTitle":"Performance","url":"docs/about-us/performance#throughput-when-processing-a-large-quantity-of-short-queries","content":"Under the same conditions, ClickHouse can handle several hundred queries per second on a single server (up to several thousand in the best case). Since this scenario is not typical for analytical DBMSs, we recommend expecting a maximum of 100 queries per second. "},{"title":"Performance When Inserting Data​","type":1,"pageTitle":"Performance","url":"docs/about-us/performance#performance-when-inserting-data","content":"We recommend inserting data in packets of at least 1000 rows, or no more than a single request per second. When inserting to a MergeTree table from a tab-separated dump, the insertion speed can be from 50 to 200 MB/s. If the inserted rows are around 1 KB in size, the speed will be from 50,000 to 200,000 rows per second. If the rows are small, the performance can be higher in rows per second (on Banner System data -&gt; 500,000 rows per second; on Graphite data -&gt; 1,000,000 rows per second). To improve performance, you can make multiple INSERT queries in parallel, which scales linearly. Original article "},{"title":"ClickHouse Commercial Support Service","type":0,"sectionRef":"#","url":"docs/about-us/support","content":"ClickHouse Commercial Support Service ClickHouse provides Support Services for our customers running ClickHouse in production environments. Our objective is a Support Services team that represents the ClickHouse product – unparalleled performance, ease of use, and exceptionally fast, high-quality results. For details, visit our Support Services page.","keywords":""},{"title":"ClickHouse Adopters","type":0,"sectionRef":"#","url":"docs/about-us/adopters","content":"ClickHouse Adopters warning The following list of companies using ClickHouse and their success stories is assembled from public sources, thus might differ from current reality. We’d appreciate it if you share the story of adopting ClickHouse in your company and add it to the list, but please make sure you won’t have any NDA issues by doing so. Providing updates with publications from other companies is also useful. Company\tIndustry\tUsecase\tCluster Size\t(Un)Compressed Data Size*\tReference2gis\tMaps\tMonitoring\t—\t—\tTalk in Russian, July 2019 Adapty\tSubscription Analytics\tMain product\t—\t—\tTweet, November 2021 Admiral\tMartech\tEngagement Management\t—\t—\tWebinar Slides, June 2020 AdScribe\tAds\tTV Analytics\t—\t—\tA quote from CTO Ahrefs\tSEO\tAnalytics\t—\t—\tJob listing Alibaba Cloud\tCloud\tManaged Service\t—\t—\tOfficial Website Alibaba Cloud\tCloud\tE-MapReduce\t—\t—\tOfficial Website Aloha Browser\tMobile App\tBrowser backend\t—\t—\tSlides in Russian, May 2019 Altinity\tCloud, SaaS\tMain product\t—\t—\tOfficial Website Amadeus\tTravel\tAnalytics\t—\t—\tPress Release, April 2018 ApiRoad\tAPI marketplace\tAnalytics\t—\t—\tBlog post, November 2018, March 2020 Appsflyer\tMobile analytics\tMain product\t—\t—\tTalk in Russian, July 2019 ArenaData\tData Platform\tMain product\t—\t—\tSlides in Russian, December 2019 Argedor\tClickHouse support\t—\t—\t—\tOfficial website Avito\tClassifieds\tMonitoring\t—\t—\tMeetup, April 2020 Badoo\tDating\tTimeseries\t—\t1.6 mln events/sec (2018)\tSlides in Russian, December 2019 Beeline\tTelecom\tData Platform\t—\t—\tBlog post, July 2021 Benocs\tNetwork Telemetry and Analytics\tMain Product\t—\t—\tSlides in English, October 2017 BIGO\tVideo\tComputing Platform\t—\t—\tBlog Article, August 2020 BiliBili\tVideo sharing\t—\t—\t—\tBlog post, June 2021 Bloomberg\tFinance, Media\tMonitoring\t—\t—\tJob opening, September 2021, slides, May 2018 Bloxy\tBlockchain\tAnalytics\t—\t—\tSlides in Russian, August 2018 Bytedance\tSocial platforms\t—\t—\t—\tThe ClickHouse Meetup East, October 2020 CardsMobile\tFinance\tAnalytics\t—\t—\tVC.ru CARTO\tBusiness Intelligence\tGeo analytics\t—\t—\tGeospatial processing with ClickHouse CERN\tResearch\tExperiment\t—\t—\tPress release, April 2012 Checkly\tSoftware Development\tAnalytics\t—\t—\tTweet, October 2021 ChelPipe Group\tAnalytics\t—\t—\t—\tBlog post, June 2021 Cisco\tNetworking\tTraffic analysis\t—\t—\tLightning talk, October 2019 Citadel Securities\tFinance\t—\t—\t—\tContribution, March 2019 Citymobil\tTaxi\tAnalytics\t—\t—\tBlog Post in Russian, March 2020 Cloudflare\tCDN\tTraffic analysis\t36 servers\t—\tBlog post, May 2017, Blog post, March 2018 Comcast\tMedia\tCDN Traffic Analysis\t—\t—\tApacheCon 2019 Talk ContentSquare\tWeb analytics\tMain product\t—\t—\tBlog post in French, November 2018 Corunet\tAnalytics\tMain product\t—\t—\tSlides in English, April 2019 CraiditX 氪信\tFinance AI\tAnalysis\t—\t—\tSlides in English, November 2019 Crazypanda\tGames —\t—\tLive session on ClickHouse meetup Criteo\tRetail\tMain product\t—\t—\tSlides in English, October 2018 Cryptology\tDigital Assets Trading Platform\t—\t—\t—\tJob advertisement, March 2021 Dataliance for China Telecom\tTelecom\tAnalytics\t—\t—\tSlides in Chinese, January 2018 Datafold\tData Quality\t—\t—\t—\tJob advertisement, April 2022 Deutsche Bank\tFinance\tBI Analytics\t—\t—\tSlides in English, October 2019 Deepl\tMachine Learning\t—\t—\t—\tVideo, October 2021 Deeplay\tGaming Analytics\t—\t—\t—\tJob advertisement, 2020 Diva-e\tDigital consulting\tMain Product\t—\t—\tSlides in English, September 2019 Ecommpay\tPayment Processing\tLogs\t—\t—\tVideo, Nov 2019 Ecwid\tE-commerce SaaS\tMetrics, Logging\t—\t—\tSlides in Russian, April 2019 eBay\tE-commerce\tLogs, Metrics and Events\t—\t—\tOfficial website, Sep 2020 Exness\tTrading\tMetrics, Logging\t—\t—\tTalk in Russian, May 2019 EventBunker.io\tServerless Data Processing\t—\t—\t—\tTweet, April 2021 FastNetMon\tDDoS Protection\tMain Product —\tOfficial website Firebolt\tAnalytics\tMain product\t-\t-\tYouTube Tech Talk Flipkart\te-Commerce\t—\t—\t—\tTalk in English, July 2020 FunCorp\tGames —\t14 bn records/day as of Jan 2021\tArticle Futurra Group\tAnalytics\t—\t—\t—\tArticle in Russian, December 2021 Geniee\tAd network\tMain product\t—\t—\tBlog post in Japanese, July 2017 Genotek\tBioinformatics\tMain product\t—\t—\tVideo, August 2020 Gigapipe\tManaged ClickHouse\tMain product\t—\t—\tOfficial website Gigasheet\tAnalytics\tMain product\t—\t—\tDirect Reference, February 2022 Glaber\tMonitoring\tMain product\t—\t—\tWebsite GraphCDN\tCDN\tTraffic Analytics\t—\t—\tBlog Post in English, August 2021 Grouparoo\tData Warehouse Integrations\tMain product\t—\t—\tOfficial Website, November 2021 HUYA\tVideo Streaming\tAnalytics\t—\t—\tSlides in Chinese, October 2018 Hydrolix\tCloud data platform\tMain product\t—\t—\tDocumentation Hystax\tCloud Operations\tObservability Analytics\t-\t-\tBlog ICA\tFinTech\tRisk Management\t—\t—\tBlog Post in English, Sep 2020 Idealista\tReal Estate\tAnalytics\t—\t—\tBlog Post in English, April 2019 Infobaleen\tAI markting tool\tAnalytics\t—\t—\tOfficial site Infovista\tNetworks\tAnalytics\t—\t—\tSlides in English, October 2019 InnoGames\tGames\tMetrics, Logging\t—\t—\tSlides in Russian, September 2019 Instabug\tAPM Platform\tMain product\t—\t—\tA quote from Co-Founder Instana\tAPM Platform\tMain product\t—\t—\tTwitter post Integros\tPlatform for video services\tAnalytics\t—\t—\tSlides in Russian, May 2019 Ippon Technologies\tTechnology Consulting\t—\t—\t—\tTalk in English, July 2020 Ivi\tOnline Cinema\tAnalytics, Monitoring\t—\t—\tArticle in Russian, Jan 2018 Jinshuju 金数据\tBI Analytics\tMain product\t—\t—\tSlides in Chinese, October 2019 Jitsu\tCloud Software\tData Pipeline\t—\t—\tDocumentation, Hacker News post JuiceFS\tStorage\tShopping Cart\t-\t-\tBlog kakaocorp\tInternet company\t—\t—\t—\tif(kakao)2020, if(kakao)2021 Kodiak Data\tClouds\tMain product\t—\t—\tSlides in Engish, April 2018 Kontur\tSoftware Development\tMetrics\t—\t—\tTalk in Russian, November 2018 Kuaishou\tVideo\t—\t—\t—\tClickHouse Meetup, October 2018 KGK Global\tVehicle monitoring\t—\t—\t—\tPress release, June 2021 LANCOM Systems\tNetwork Solutions\tTraffic analysis\t-\t-\tClickHouse Operator for Kubernetes, [Hacker News post] (https://news.ycombinator.com/item?id=29413660) Lawrence Berkeley National Laboratory\tResearch\tTraffic analysis\t5 servers\t55 TiB\tSlides in English, April 2019 Lever\tTalent Management\tRecruiting\t-\t-\tHacker News post LifeStreet\tAd network\tMain product\t75 servers (3 replicas)\t5.27 PiB\tBlog post in Russian, February 2017 Lookforsale\tE-Commerce\t—\t—\t—\tJob Posting, December 2021 Mail.ru Cloud Solutions\tCloud services\tMain product\t—\t—\tArticle in Russian MAXILECT\tAd Tech, Blockchain, ML, AI\t—\t—\t—\tJob advertisement, 2021 Marilyn\tAdvertising\tStatistics\t—\t—\tTalk in Russian, June 2017 Mello\tMarketing\tAnalytics\t1 server\t—\tArticle, October 2020 MessageBird\tTelecommunications\tStatistics\t—\t—\tSlides in English, November 2018 Microsoft\tWeb Analytics\tClarity (Main Product)\t—\t—\tA question on GitHub MindsDB\tMachine Learning\tMain Product\t—\t—\tOfficial Website MUX\tOnline Video\tVideo Analytics\t—\t—\tTalk in English, August 2019 MGID\tAd network\tWeb-analytics\t—\t—\tBlog post in Russian, April 2020 Muse Group\tMusic Software\tPerformance Monitoring\t—\t—\tBlog post in Russian, January 2021 Netskope\tNetwork Security\t—\t—\t—\tJob advertisement, March 2021 NIC Labs\tNetwork Monitoring\tRaTA-DNS\t—\t—\tBlog post, March 2021 NLMK\tSteel\tMonitoring\t—\t—\tArticle in Russian, Jan 2022 NOC Project\tNetwork Monitoring\tAnalytics\tMain Product\t—\tOfficial Website Noction\tNetwork Technology\tMain Product\t—\t—\tOfficial Website ntop\tNetwork Monitoning\tMonitoring\t—\t—\tOfficial website, Jan 2022 Nuna Inc.\tHealth Data Analytics\t—\t—\t—\tTalk in English, July 2020 Ok.ru\tSocial Network\t—\t72 servers\t810 TB compressed, 50bn rows/day, 1.5 TB/day\tSmartData conference, October 2021 Omnicomm\tTransportation Monitoring\t—\t—\t—\tFacebook post, October 2021 OneAPM\tMonitoring and Data Analysis\tMain product\t—\t—\tSlides in Chinese, October 2018 Opensee\tFinancial Analytics\tMain product\t-\t-\tBlog Open Targets\tGenome Research\tGenome Search\t—\t—\tTweet, October 2021, Blog OZON\tE-commerce\t—\t—\t—\tOfficial website Panelbear\tAnalytics\tMonitoring and Analytics\t—\t—\tTech Stack, November 2020 Percent 百分点\tAnalytics\tMain Product\t—\t—\tSlides in Chinese, June 2019 Percona\tPerformance analysis\tPercona Monitoring and Management\t—\t—\tOfficial website, Mar 2020 Plausible\tAnalytics\tMain Product\t—\t—\tBlog post, June 2020 PostHog\tProduct Analytics\tMain Product\t—\t—\tRelease Notes, October 2020, Blog, November 2021 Postmates\tDelivery\t—\t—\t—\tTalk in English, July 2020 Pragma Innovation\tTelemetry and Big Data Analysis\tMain product\t—\t—\tSlides in English, October 2018 PRANA\tIndustrial predictive analytics\tMain product\t—\t—\tNews (russian), Feb 2021 QINGCLOUD\tCloud services\tMain product\t—\t—\tSlides in Chinese, October 2018 Qrator\tDDoS protection\tMain product\t—\t—\tBlog Post, March 2019 R-Vision\tInformation Security\t—\t—\t—\tArticle in Russian, December 2021 Raiffeisenbank\tBanking\tAnalytics\t—\t—\tLecture in Russian, December 2020 Rambler\tInternet services\tAnalytics\t—\t—\tTalk in Russian, April 2018 Replica\tUrban Planning\tAnalytics\t—\t—\tJob advertisement Retell\tSpeech synthesis\tAnalytics\t—\t—\tBlog Article, August 2020 Rollbar\tSoftware Development\tMain Product\t—\t—\tOfficial Website Rspamd\tAntispam\tAnalytics\t—\t—\tOfficial Website RuSIEM\tSIEM\tMain Product\t—\t—\tOfficial Website S7 Airlines\tAirlines\tMetrics, Logging\t—\t—\tTalk in Russian, March 2019 Sber\tBanking, Fintech, Retail, Cloud, Media\t—\t128 servers\t&gt;1 PB\tJob advertisement, March 2021 scireum GmbH\te-Commerce\tMain product\t—\t—\tTalk in German, February 2020 Segment\tData processing\tMain product\t9 * i3en.3xlarge nodes 7.5TB NVME SSDs, 96GB Memory, 12 vCPUs\t—\tSlides, 2019 sembot.io\tShopping Ads\t—\t—\t—\tA comment on LinkedIn, 2020 SEMrush\tMarketing\tMain product\t—\t—\tSlides in Russian, August 2018 Sentry\tSoftware Development\tMain product\t—\t—\tBlog Post in English, May 2019 seo.do\tAnalytics\tMain product\t—\t—\tSlides in English, November 2019 SGK\tGovernment Social Security\tAnalytics\t—\t—\tSlides in English, November 2019 SigNoz\tObservability Platform\tMain Product\t—\t—\tSource code Sina\tNews\t—\t—\t—\tSlides in Chinese, October 2018 Sipfront\tSoftware Development\tAnalytics\t—\t—\tTweet, October 2021 SMI2\tNews\tAnalytics\t—\t—\tBlog Post in Russian, November 2017 Spark New Zealand\tTelecommunications\tSecurity Operations\t—\t—\tBlog Post, Feb 2020 Splitbee\tAnalytics\tMain Product\t—\t—\tBlog Post, Mai 2021 Splunk\tBusiness Analytics\tMain product\t—\t—\tSlides in English, January 2018 Spotify\tMusic\tExperimentation\t—\t—\tSlides, July 2018 Staffcop\tInformation Security\tMain Product\t—\t—\tOfficial website, Documentation Suning\tE-Commerce\tUser behaviour analytics\t—\t—\tBlog article Superwall\tMonetization Tooling\tMain product\t—\t—\tWord of mouth, Jan 2022 Teralytics\tMobility\tAnalytics\t—\t—\tTech blog Tencent\tBig Data\tData processing\t—\t—\tSlides in Chinese, October 2018 Tencent\tMessaging\tLogging\t—\t—\tTalk in Chinese, November 2019 Tencent Music Entertainment (TME)\tBigData\tData processing\t—\t—\tBlog in Chinese, June 2020 Tesla\tElectric vehicle and clean energy company\t—\t—\t—\tVacancy description, March 2021 Timeflow\tSoftware\tAnalytics\t—\t—\tBlog Tinybird\tReal-time Data Products\tData processing\t—\t—\tOfficial website Traffic Stars\tAD network\t—\t300 servers in Europe/US\t1.8 PiB, 700 000 insert rps (as of 2021)\tSlides in Russian, May 2018 Uber\tTaxi\tLogging\t—\t—\tSlides, February 2020 UseTech\tSoftware Development\t—\t—\t—\tJob Posting, December 2021 UTMSTAT\tAnalytics\tMain product\t—\t—\tBlog post, June 2020 Vercel\tTraffic and Performance Analytics\t—\t—\t—\tDirect reference, October 2021 VKontakte\tSocial Network\tStatistics, Logging\t—\t—\tSlides in Russian, August 2018 VMware\tCloud\tVeloCloud, SDN\t—\t—\tProduct documentation Walmart Labs\tInternet, Retail\t—\t—\t—\tTalk in English, July 2020 Wargaming\tGames —\t—\tInterview Wildberries\tE-commerce —\t—\tOfficial website Wisebits\tIT Solutions\tAnalytics\t—\t—\tSlides in Russian, May 2019 Workato\tAutomation Software\t—\t—\t—\tTalk in English, July 2020 Xenoss\tMarketing, Advertising\t—\t—\t—\tInstagram, March 2021 Xiaoxin Tech\tEducation\tCommon purpose\t—\t—\tSlides in English, November 2019 Ximalaya\tAudio sharing\tOLAP\t—\t—\tSlides in English, November 2019 Yandex Cloud\tPublic Cloud\tMain product\t—\t—\tTalk in Russian, December 2019 Yandex DataLens\tBusiness Intelligence\tMain product\t—\t—\tSlides in Russian, December 2019 Yandex Market\te-Commerce\tMetrics, Logging\t—\t—\tTalk in Russian, January 2019 Yandex Metrica\tWeb analytics\tMain product\t630 servers in one cluster, 360 servers in another cluster, 1862 servers in one department\t133 PiB / 8.31 PiB / 120 trillion records\tSlides, February 2020 Yellowfin\tAnalytics\tMain product\t-\t-\tIntegration Yotascale\tCloud\tData pipeline\t—\t2 bn records/day\tLinkedIn (Accomplishments) Your Analytics\tProduct Analytics\tMain Product\t—\t-\tTweet, November 2021 Zagrava Trading\t—\t—\t—\t—\tJob offer, May 2021 ЦВТ\tSoftware Development\tMetrics, Logging\t—\t—\tBlog Post, March 2019, in Russian МКБ\tBank\tWeb-system monitoring\t—\t—\tSlides in Russian, September 2019 ЦФТ\tBanking, Financial products, Payments\t—\t—\t—\tMeetup in Russian, April 2020 Цифровой Рабочий\tIndustrial IoT, Analytics\t—\t—\t—\tBlog post in Russian, March 2021 ООО «МПЗ Богородский»\tAgriculture\t—\t—\t—\tArticle in Russian, November 2020 ДомКлик\tReal Estate\t—\t—\t—\tArticle in Russian, October 2021 АС &quot;Стрела&quot;\tTransportation\t—\t—\t—\tJob posting, Jan 2022 Original article","keywords":""},{"title":"Connecting Grafana to ClickHouse","type":0,"sectionRef":"#","url":"docs/connect-a-ui/grafana-and-clickhouse","content":"","keywords":"clickhouse grafana connect integrate"},{"title":"1. Install the Grafana Plugin for ClickHouse​","type":1,"pageTitle":"Connecting Grafana to ClickHouse","url":"docs/connect-a-ui/grafana-and-clickhouse#1--install-the-grafana-plugin-for-clickhouse","content":"Before Grafana can talk to ClickHouse, you need to install the appropriate Grafana plugin. Assuming you are logged in to Grafana, follow these steps: From the Configuration page, select the Plugins tab. Search for ClickHouse and click on the Signed plugin by Grafana Labs: On the next screen, click the Install button: "},{"title":"2. Define a ClickHouse data source​","type":1,"pageTitle":"Connecting Grafana to ClickHouse","url":"docs/connect-a-ui/grafana-and-clickhouse#2-define-a-clickhouse-data-source","content":"Once the installation is complete, click the Create a ClickHouse data source button. (You can also add a data source from the Data sources tab on the Configuration page.) Either scroll down and find the ClickHouse data source type, or you can search for it in the search bar of the Add data source page. Either way, select the ClickHouse data source type and the following dialog appears: Enter your server settings and credentials. The key settings are: Name: a Grafana setting - give your data source any name you like Server address: the URL of your ClickHouse serviceServer port: 9000 for unsecure, 9440 for secure (unless you modified the ClickHouse ports)Username and Password: enter your ClickHouse user credentials. If you have not configured users and passwords, then try default for the username and leave the password empty.Default database: a Grafana setting - you can specify a database that Grafana defaults to when using this data source (this property can be left blank) Click the Save &amp; test button to verify that Grafana can connect to your ClickHouse service. If successful, you will see a Data source is working message: "},{"title":"3. Build a dashboard​","type":1,"pageTitle":"Connecting Grafana to ClickHouse","url":"docs/connect-a-ui/grafana-and-clickhouse#3-build-a-dashboard","content":"From the left menu, click on the Dashboards icon and select Browse. Then select the New Dashboard button: Click the Add a new panel button. From here, you can build a visualization based on a query. From the Data source dropdown, select your ClickHouse data source that you defined earlier. Then you can either use the Query Builder to build a query visually, or switch to the SQL Editor and enter a SQL query (as shown here): That's it! You are now ready to build visualizations and dashboards in Grafana. "},{"title":"Connecting Metabase to ClickHouse","type":0,"sectionRef":"#","url":"docs/connect-a-ui/metabase-and-clickhouse","content":"","keywords":"clickhouse metabase connect integrate ui"},{"title":"1. Download the ClickHouse plugin for Metabase​","type":1,"pageTitle":"Connecting Metabase to ClickHouse","url":"docs/connect-a-ui/metabase-and-clickhouse#1--download-the-clickhouse-plugin-for-metabase","content":"If you do not have a plugins folder, create one as a subfolder of where you have metabase.jar saved. The plugin is a JAR file named clickhouse.metabase-driver.jar. Download the latest version of the JAR file at https://github.com/enqueue/metabase-clickhouse-driver/releases/latest Save clickhouse.metabase-driver.jar in your plugins folder. Start (or restart) Metabase so that the driver gets loaded properly. Access Metabse at http://hostname:3000. On the initial startup, you will see a welcome screen and have to work your way through a list of questions. If prompted to select a database, select &quot;I'll add my data later&quot;: "},{"title":"2. Connect Metabase to ClickHouse​","type":1,"pageTitle":"Connecting Metabase to ClickHouse","url":"docs/connect-a-ui/metabase-and-clickhouse#2--connect-metabase-to-clickhouse","content":"Click on the gear icon in the top-right corner and select Admin Settings to visit your Metabase admin page. Click on Add a database. Alternately, you can click on the Databases tab and select the Add database button. If your driver installation worked, you will see ClickHouse in the dropdown menu for Database type: Give your database a Display name, which is a Metabase setting - so use any name you like. Enter the connection details of your ClickHouse database. For example: Click the Save button and Metabase will scan your database for tables. "},{"title":"3. Run a SQL query​","type":1,"pageTitle":"Connecting Metabase to ClickHouse","url":"docs/connect-a-ui/metabase-and-clickhouse#3-run-a-sql-query","content":"Exit the Admin settings by clicking the Exit admin button in the top-right corner. In the top-right corner, click the + New menu and notice you can ask questions, run SQL queries, and build a dashboard: For example, here is a SQL query executed on a table named hits that returns the top ten most-visited URLs: "},{"title":"4. Ask a question​","type":1,"pageTitle":"Connecting Metabase to ClickHouse","url":"docs/connect-a-ui/metabase-and-clickhouse#4-ask-a-question","content":"Click on + New and select Question. Notice you can build a question by starting wtih a database and table. For example, the following question is being asked of a table named hits in the Web Traffic Database: Here is a simple question that calculates the top 20 most-visited URLs in the table: Click the Visualize button to see the results in a tabular view. Below the results, click the Visualization button to change the visualization to a bar chart (or any of the other options avaialable): Find more information about Metabase and how to build dashboards by visiting the Metabase documentation. "},{"title":"Connect Superset to ClickHouse","type":0,"sectionRef":"#","url":"docs/connect-a-ui/superset-and-clickhouse","content":"","keywords":"clickhouse superset connect integrate ui"},{"title":"1. Install the Drivers​","type":1,"pageTitle":"Connect Superset to ClickHouse","url":"docs/connect-a-ui/superset-and-clickhouse#1-install-the-drivers","content":"Superset uses the clickhouse-sqlalchemy driver, which requires the clickhouse-driver to connect to ClickHouse. The details of clickhouse-driver are at https://pypi.org/project/clickhouse-driver/ and can be installed with the following command: pip install clickhouse-driver Now install the ClickHouse SQLAlchemy driver: pip install clickhouse-sqlalchemy Start (or restart) Superset. "},{"title":"2. Connect Superset to ClickHouse​","type":1,"pageTitle":"Connect Superset to ClickHouse","url":"docs/connect-a-ui/superset-and-clickhouse#2-connect-superset-to-clickhouse","content":"Within Superset, select Data from the top menu and then Databases from the drop-down menu. Add a new database by clicking the + Database button: In the first step, select ClickHouse as the type of database: In the second step, enter a display name for your database and the connection URI. The DISPLAY NAME can be any name you prefer. The SQLALCHEMY URI is the important setting - it has the following format: clickhouse+native://username:password@hostname/database_name In the example below, ClickHouse is running on localhost with the default user and no password. The name of the database is covid19db. Use the TEST CONNECTION button to verify that Superset is connecting to your ClickHouse database properly: Click the CONNECT button to complete the setup wizard, and you should see your database in the list of databases. "},{"title":"3. Add a Dataset​","type":1,"pageTitle":"Connect Superset to ClickHouse","url":"docs/connect-a-ui/superset-and-clickhouse#3-add-a-dataset","content":"To define new charts (visualizations) in Superset, you need to define a dataset. From the top menu in Superset, select Data, then Datasets from the drop-down menu. Click the button for adding a dataset. Select your new database as the datasource and you should see the tables defined in your database. For example, the covid19db database has a table named daily_totals: Click the ADD button at the bottom of the dialog window and your table appears in the list of datasets. You are ready to build a dashboard and analyze your ClickHouse data! "},{"title":"4. Creating charts and a dashboard in Superset​","type":1,"pageTitle":"Connect Superset to ClickHouse","url":"docs/connect-a-ui/superset-and-clickhouse#4--creating-charts-and-a-dashboard-in-superset","content":"If you are familiar with Superset, then you will feel right at home with this next section. If you are new to Superset, well...it's like a lot of the other cool visualization tools out there in the world - it doesn't take long to get started, but the details and nuances get learned over time as you use the tool. You start with a dashboard. From the top menu in Superset, select Dashboards. Click the button in the upper-right to add a new dashboard. The following dashboard is named Covid-19 Dashboard: To create a new chart, select Charts from the top menu and click the button to add a new chart. You will be shown a lot of options. The following example shows a Big Number chart using the daily_totals dataset from the CHOOSE A DATASET drop-down: You need to add a metric to a Big Number. The column named DATA and the section named Query with a METRIC field show a red warning because they are not defined yet. To add a metric, click Add metric and a small dialog window appears: The following example uses the SUM metric, found on the the SIMPLE tab. It sums the values of the new_cases column: To view the actual number, click the RUN QUERY button: Click the SAVE button to save the chart, then select Covid-19 Dashboard under the ADD TO DASHBOARD drop-down, then SAVE &amp; GO TO DASHBOARD saves the chart and adds it to the dashboard: That's it. Building dashboards in Superset based on data in ClickHouse opens up a whole world of blazing fast data analytics! "},{"title":"Connecting Tableau to ClickHouse","type":0,"sectionRef":"#","url":"docs/connect-a-ui/tableau-and-clickhouse","content":"","keywords":"clickhouse tableau connect integrate ui"},{"title":"1. Download the JDBC Driver​","type":1,"pageTitle":"Connecting Tableau to ClickHouse","url":"docs/connect-a-ui/tableau-and-clickhouse#1--download-the-jdbc-driver","content":"The Tableau connector is an extension of the ClickHouse JDBC driver, so you need to download the JDBC driver and save it in the correct folder. {{&lt; detail-tag &quot;Show instructions&quot; &quot;2&quot; &gt;}} Download the latest version of the ClickHouse JDBC driver at https://github.com/ClickHouse/clickhouse-jdbc/releases/. (We used this version of the driver for this tutorial.)  {{% notice note %}} Make sure you download the clickhouse-jdbc-x.x.x-shaded.jar JAR file. {{% /notice %}} Store the JDBC driver in the following folder (based on your OS): Operating System\tDestination folderMacOS\t~/Library/Tableau/Drivers Windows\tC:\\Program Files\\Tableau\\Drivers That's it. The driver will be found the next time you start Tableau. {{&lt; /detail-tag &gt;}}  "},{"title":"3. Download the Connector​","type":1,"pageTitle":"Connecting Tableau to ClickHouse","url":"docs/connect-a-ui/tableau-and-clickhouse#3-download-the-connector","content":"ANALYTIKA PLUS has built a handy connector for simplifying connections to ClickHouse from Tableau. You can view the details of the project in Github. Follow these steps to download the connector... {{&lt; detail-tag &quot;Show instructions&quot; &quot;3&quot; &gt;}} The connector is built in a taco file (short for Tableau Connector). Download the latest version at https://github.com/analytikaplus/clickhouse-tableau-connector-jdbc/releases/. (For this lesson, we downloaded v0.1.1 of clickhouse_jdbc.taco.) Store clickhouse_jdbc.taco in the following folder (based on your OS): Operating System\tDestination folderMacOS\t~/Documents/My Tableau Repository/Connectors Windows\tC:\\Users[Windows User]\\Documents\\My Tableau Repository\\Connectors  The connector is now ready to go. {{&lt; /detail-tag &gt;}}  "},{"title":"4. Configure a ClickHouse data source in Tableau​","type":1,"pageTitle":"Connecting Tableau to ClickHouse","url":"docs/connect-a-ui/tableau-and-clickhouse#4--configure-a-clickhouse-data-source-in-tableau","content":"Now that you have the driver and connector in the approriate folders on your machine, let's see how to define a data source in Tableau that connects to the TPCD database in ClickHouse. {{&lt; detail-tag &quot;Show instructions&quot; &quot;4&quot; &gt;}} Start Tableau. (If you already had it running, then restart it.) From the left-side menu, click on More under the To a Server section. If everything worked properly, you should see ClickHouse (JDBC) by ANALYTIKA PLUS in the list of installed connectors:  Click on ClickHouse (JDBC) by ANALYTIKA PLUS and a dialog window pops up. Enter the following details: Setting\tValueServer\tlocalhost Port\t8123 Database\tdefault Username\tdefault Password\tleave blank  Your settings should look like:  {{% notice note %}} Our ClickHouse database is named TPCD, but you must set the Database to default in the dialog above, then select TPCD for the Schema in the next step. (This is likely due to a bug in the connector, so this behavior could change, but for now you must use default as the database.) {{% /notice %}} Click the Sign In button and you should see a new Tableau workbook:  Select TPCD from the Schema dropdown and you should see the list of tables in TPCD:  You are now ready to build some visualizations in Tableau! {{&lt; /detail-tag &gt;}}  "},{"title":"5. Building Visualizations in Tableau​","type":1,"pageTitle":"Connecting Tableau to ClickHouse","url":"docs/connect-a-ui/tableau-and-clickhouse#5-building-visualizations-in-tableau","content":"Now that have a ClickHouse data source configured in Tableau, let's visualize the data... {{&lt; detail-tag &quot;Show instructions&quot; &quot;5&quot; &gt;}} Drag the CUSTOMER table onto the workbook. Notice the columns appear, but the data table is empty:  Click the Update Now button and 100 rows from CUSTOMER will populate the table. Drag the ORDERS table into the workbook, then set Custkey as the relationship field between the two tables:  You now have the ORDERS and LINEITEM tables associated with each other as your data source, so you can use this relationship to answer questions about the data. Select the Sheet 1 tab at the bottom of the workbook.  Suppose you want to know how many specific items were ordered each year. Drag Orderdate from ORDERS into the Columns section (the horizontal field), then drag Quantity from LINEITEM into the Rows. Tableau will generate the following line chart:  Not a very exciting line chart, but the dataset was generated by a script and built for testing query performance, so you will notice there is not a lot of variations in the simulated orders of the TCPD data. Suppose you want to know the average order amount (in dollars) by quarter and also by shipping mode (air, mail, ship, truck, etc.): Click the New Worksheet tab create a new sheetDrag OrderDate from ORDERS into Columns and change it from Year to QuarterDrag Shipmode from LINEITEM into Rows  You should see the following:  The Abc values are just filling in the space until you drag a metric onto the table. Drag Totalprice from ORDERS onto the table. Notice the default calculation is to SUM the Totalpricess:  Click on SUM and change the Measure to Average. From the same dropdown menu, select Format change the Numbers to Currency (Standard):  Well done! You have successfully connected Tableau to ClickHouse, and you have opened up a whole world of possibilities for analyzing and visualizing your ClickHouse data. {{% notice note %}} Tableau is great, and we love that it connects so nicely to ClickHouse! If you are new to Tableau, check out their documentation for help on building dashboards and visualizations. {{% /notice %}} {{&lt; /detail-tag &gt;}}  Summary: You can connect Tableau to ClickHouse using the generic ODBC/JDBC ClickHouse driver, but we really like how this tool from ANALYTIKA PLUS simplifies the process of setting up the connection. If you have any issues with the connector, feel free to reach out to ANALYTIKA PLUS on GitHub.  What's next: Check out the following lessons to continue your journey: The Ingest Nginx Logs into ClickHouse using Vector lesson demonstrates how to stream a log file into ClickHouseCheck out What's New in ClickHouse 21.10View all of our lessons on the Learn ClickHouse home page "},{"title":"How to Build ClickHouse on Linux for AARCH64 (ARM64) Architecture","type":0,"sectionRef":"#","url":"docs/en/development/build-cross-arm","content":"","keywords":""},{"title":"Install Clang-13​","type":1,"pageTitle":"How to Build ClickHouse on Linux for AARCH64 (ARM64) Architecture","url":"docs/en/development/build-cross-arm#install-clang-13","content":"Follow the instructions from https://apt.llvm.org/ for your Ubuntu or Debian setup or do sudo bash -c &quot;$(wget -O - https://apt.llvm.org/llvm.sh)&quot;  "},{"title":"Install Cross-Compilation Toolset​","type":1,"pageTitle":"How to Build ClickHouse on Linux for AARCH64 (ARM64) Architecture","url":"docs/en/development/build-cross-arm#install-cross-compilation-toolset","content":"cd ClickHouse mkdir -p build-aarch64/cmake/toolchain/linux-aarch64 wget 'https://developer.arm.com/-/media/Files/downloads/gnu-a/8.3-2019.03/binrel/gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu.tar.xz?revision=2e88a73f-d233-4f96-b1f4-d8b36e9bb0b9&amp;la=en' -O gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu.tar.xz tar xJf gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu.tar.xz -C build-aarch64/cmake/toolchain/linux-aarch64 --strip-components=1  "},{"title":"Build ClickHouse​","type":1,"pageTitle":"How to Build ClickHouse on Linux for AARCH64 (ARM64) Architecture","url":"docs/en/development/build-cross-arm#build-clickhouse","content":"cd ClickHouse mkdir build-arm64 CC=clang-13 CXX=clang++-13 cmake . -Bbuild-arm64 -DCMAKE_TOOLCHAIN_FILE=cmake/linux/toolchain-aarch64.cmake ninja -C build-arm64  The resulting binary will run only on Linux with the AARCH64 CPU architecture. "},{"title":"How to Build ClickHouse on Linux for RISC-V 64 Architecture","type":0,"sectionRef":"#","url":"docs/en/development/build-cross-riscv","content":"","keywords":""},{"title":"Install Clang-13​","type":1,"pageTitle":"How to Build ClickHouse on Linux for RISC-V 64 Architecture","url":"docs/en/development/build-cross-riscv#install-clang-13","content":"Follow the instructions from https://apt.llvm.org/ for your Ubuntu or Debian setup or do sudo bash -c &quot;$(wget -O - https://apt.llvm.org/llvm.sh)&quot;  "},{"title":"Build ClickHouse​","type":1,"pageTitle":"How to Build ClickHouse on Linux for RISC-V 64 Architecture","url":"docs/en/development/build-cross-riscv#build-clickhouse","content":"cd ClickHouse mkdir build-riscv64 CC=clang-13 CXX=clang++-13 cmake . -Bbuild-riscv64 -G Ninja -DCMAKE_TOOLCHAIN_FILE=cmake/linux/toolchain-riscv64.cmake -DGLIBC_COMPATIBILITY=OFF -DENABLE_LDAP=OFF -DOPENSSL_NO_ASM=ON -DENABLE_JEMALLOC=ON -DENABLE_PARQUET=OFF -DENABLE_ORC=OFF -DUSE_UNWIND=OFF -DENABLE_GRPC=OFF -DENABLE_HDFS=OFF -DENABLE_MYSQL=OFF ninja -C build-riscv64  The resulting binary will run only on Linux with the RISC-V 64 CPU architecture. "},{"title":"How to add test queries to ClickHouse CI","type":0,"sectionRef":"#","url":"docs/en/development/adding_test_queries","content":"","keywords":""},{"title":"Why adding tests​","type":1,"pageTitle":"How to add test queries to ClickHouse CI","url":"docs/en/development/adding_test_queries#why-adding-tests","content":"Why/when you should add a test case into ClickHouse code: 1) you use some complicated scenarios / feature combinations / you have some corner case which is probably not widely used 2) you see that certain behavior gets changed between version w/o notifications in the changelog 3) you just want to help to improve ClickHouse quality and ensure the features you use will not be broken in the future releases 4) once the test is added/accepted, you can be sure the corner case you check will never be accidentally broken. 5) you will be a part of great open-source community 6) your name will be visible in the system.contributors table! 7) you will make a world bit better :) "},{"title":"Steps to do​","type":1,"pageTitle":"How to add test queries to ClickHouse CI","url":"docs/en/development/adding_test_queries#steps-to-do","content":"Prerequisite​ I assume you run some Linux machine (you can use docker / virtual machines on other OS) and any modern browser / internet connection, and you have some basic Linux &amp; SQL skills. Any highly specialized knowledge is not needed (so you don't need to know C++ or know something about how ClickHouse CI works). Preparation​ 1) create GitHub account (if you haven't one yet) 2) setup git # for Ubuntu sudo apt-get update sudo apt-get install git git config --global user.name &quot;John Doe&quot; # fill with your name git config --global user.email &quot;email@example.com&quot; # fill with your email  3) fork ClickHouse project - just open https://github.com/ClickHouse/ClickHouse and press fork button in the top right corner: 4) clone your fork to some folder on your PC, for example, ~/workspace/ClickHouse mkdir ~/workspace &amp;&amp; cd ~/workspace git clone https://github.com/&lt; your GitHub username&gt;/ClickHouse cd ClickHouse git remote add upstream https://github.com/ClickHouse/ClickHouse  New branch for the test​ 1) create a new branch from the latest clickhouse master cd ~/workspace/ClickHouse git fetch upstream git checkout -b name_for_a_branch_with_my_test upstream/master  Install &amp; run clickhouse​ 1) install clickhouse-server (follow official docs) 2) install test configurations (it will use Zookeeper mock implementation and adjust some settings) cd ~/workspace/ClickHouse/tests/config sudo ./install.sh  3) run clickhouse-server sudo systemctl restart clickhouse-server  Creating the test file​ 1) find the number for your test - find the file with the biggest number in tests/queries/0_stateless/ $ cd ~/workspace/ClickHouse $ ls tests/queries/0_stateless/[0-9]*.reference | tail -n 1 tests/queries/0_stateless/01520_client_print_query_id.reference  Currently, the last number for the test is 01520, so my test will have the number 01521 2) create an SQL file with the next number and name of the feature you test touch tests/queries/0_stateless/01521_dummy_test.sql  3) edit SQL file with your favorite editor (see hint of creating tests below) vim tests/queries/0_stateless/01521_dummy_test.sql  4) run the test, and put the result of that into the reference file: clickhouse-client -nmT &lt; tests/queries/0_stateless/01521_dummy_test.sql | tee tests/queries/0_stateless/01521_dummy_test.reference  5) ensure everything is correct, if the test output is incorrect (due to some bug for example), adjust the reference file using text editor. How to create a good test​ A test should be minimal - create only tables related to tested functionality, remove unrelated columns and parts of queryfast - should not take longer than a few seconds (better subseconds)correct - fails then feature is not working - deterministic isolated / stateless don't rely on some environment thingsdon't rely on timing when possible try to cover corner cases (zeros / Nulls / empty sets / throwing exceptions)to test that query return errors, you can put special comment after the query: -- { serverError 60 } or -- { clientError 20 }don't switch databases (unless necessary)you can create several table replicas on the same node if neededyou can use one of the test cluster definitions when needed (see system.clusters)use number / numbers_mt / zeros / zeros_mt and similar for queries / to initialize data when applicableclean up the created objects after test and before the test (DROP IF EXISTS) - in case of some dirty stateprefer sync mode of operations (mutations, merges, etc.)use other SQL files in the 0_stateless folder as an exampleensure the feature / feature combination you want to test is not yet covered with existing tests Test naming rules​ It's important to name tests correctly, so one could turn some tests subset off in clickhouse-test invocation. Tester flag\tWhat should be in test name\tWhen flag should be added\t--[no-]zookeeper\t&quot;zookeeper&quot; or &quot;replica&quot;\tTest uses tables from ReplicatedMergeTree family --[no-]shard\t&quot;shard&quot; or &quot;distributed&quot; or &quot;global&quot;\tTest using connections to 127.0.0.2 or similar --[no-]long\t&quot;long&quot; or &quot;deadlock&quot; or &quot;race&quot;\tTest runs longer than 60 seconds\t Commit / push / create PR.​ 1) commit &amp; push your changes cd ~/workspace/ClickHouse git add tests/queries/0_stateless/01521_dummy_test.sql git add tests/queries/0_stateless/01521_dummy_test.reference git commit # use some nice commit message when possible git push origin HEAD  2) use a link which was shown during the push, to create a PR into the main repo 3) adjust the PR title and contents, in Changelog category (leave one) keepBuild/Testing/Packaging Improvement, fill the rest of the fields if you want. "},{"title":"Browse ClickHouse Source Code","type":0,"sectionRef":"#","url":"docs/en/development/browse-code","content":"Browse ClickHouse Source Code You can use the Woboq online code browser available here. It provides code navigation and semantic highlighting, search and indexing. The code snapshot is updated daily. Also, you can browse sources on GitHub as usual. If you’re interested what IDE to use, we recommend CLion, QT Creator, VS Code and KDevelop (with caveats). You can use any favorite IDE. Vim and Emacs also count.","keywords":""},{"title":"How to Build ClickHouse on Linux","type":0,"sectionRef":"#","url":"docs/en/development/build","content":"","keywords":""},{"title":"Normal Build for Development on Ubuntu​","type":1,"pageTitle":"How to Build ClickHouse on Linux","url":"docs/en/development/build#normal-build-for-development-on-ubuntu","content":"The following tutorial is based on the Ubuntu Linux system. With appropriate changes, it should also work on any other Linux distribution. "},{"title":"Install Git, CMake, Python and Ninja​","type":1,"pageTitle":"How to Build ClickHouse on Linux","url":"docs/en/development/build#install-git-cmake-python-and-ninja","content":"$ sudo apt-get install git cmake python ninja-build  Or cmake3 instead of cmake on older systems. "},{"title":"Install the latest clang (recommended)​","type":1,"pageTitle":"How to Build ClickHouse on Linux","url":"docs/en/development/build#install-the-latest-clang-recommended","content":"On Ubuntu/Debian you can use the automatic installation script (check official webpage) sudo bash -c &quot;$(wget -O - https://apt.llvm.org/llvm.sh)&quot;  For other Linux distribution - check the availability of the prebuild packages or build clang from sources. Use the latest clang for Builds​ $ export CC=clang-14 $ export CXX=clang++-14  In this example we use version 14 that is the latest as of Feb 2022. Gcc can also be used though it is discouraged. "},{"title":"Checkout ClickHouse Sources​","type":1,"pageTitle":"How to Build ClickHouse on Linux","url":"docs/en/development/build#checkout-clickhouse-sources","content":"$ git clone --recursive git@github.com:ClickHouse/ClickHouse.git  or $ git clone --recursive https://github.com/ClickHouse/ClickHouse.git  "},{"title":"Build ClickHouse​","type":1,"pageTitle":"How to Build ClickHouse on Linux","url":"docs/en/development/build#build-clickhouse","content":"$ cd ClickHouse $ mkdir build $ cd build $ cmake .. $ ninja  To create an executable, run ninja clickhouse. This will create the programs/clickhouse executable, which can be used with client or server arguments. "},{"title":"How to Build ClickHouse on Any Linux​","type":1,"pageTitle":"How to Build ClickHouse on Linux","url":"docs/en/development/build#how-to-build-clickhouse-on-any-linux","content":"The build requires the following components: Git (is used only to checkout the sources, it’s not needed for the build)CMake 3.10 or newerNinjaC++ compiler: clang-13 or newerLinker: lld If all the components are installed, you may build in the same way as the steps above. Example for Ubuntu Eoan: sudo apt update sudo apt install git cmake ninja-build clang++ python git clone --recursive https://github.com/ClickHouse/ClickHouse.git mkdir build &amp;&amp; cd build cmake ../ClickHouse ninja  Example for OpenSUSE Tumbleweed: sudo zypper install git cmake ninja clang-c++ python lld git clone --recursive https://github.com/ClickHouse/ClickHouse.git mkdir build &amp;&amp; cd build cmake ../ClickHouse ninja  Example for Fedora Rawhide: sudo yum update yum --nogpg install git cmake make clang-c++ python3 git clone --recursive https://github.com/ClickHouse/ClickHouse.git mkdir build &amp;&amp; cd build cmake ../ClickHouse make -j $(nproc)  Here is an example of how to build clang and all the llvm infrastructure from sources:  git clone git@github.com:llvm/llvm-project.git mkdir llvm-build &amp;&amp; cd llvm-build cmake -DCMAKE_BUILD_TYPE:STRING=Release -DLLVM_ENABLE_PROJECTS=all ../llvm-project/llvm/ make -j16 sudo make install hash clang clang --version  You can install the older clang like clang-11 from packages and then use it to build the new clang from sources. Here is an example of how to install the new cmake from the official website: wget https://github.com/Kitware/CMake/releases/download/v3.22.2/cmake-3.22.2-linux-x86_64.sh chmod +x cmake-3.22.2-linux-x86_64.sh ./cmake-3.22.2-linux-x86_64.sh export PATH=/home/milovidov/work/cmake-3.22.2-linux-x86_64/bin/:${PATH} hash cmake  "},{"title":"How to Build ClickHouse Debian Package​","type":1,"pageTitle":"How to Build ClickHouse on Linux","url":"docs/en/development/build#how-to-build-clickhouse-debian-package","content":""},{"title":"Install Git​","type":1,"pageTitle":"How to Build ClickHouse on Linux","url":"docs/en/development/build#install-git","content":"$ sudo apt-get update $ sudo apt-get install git python debhelper lsb-release fakeroot sudo debian-archive-keyring debian-keyring  "},{"title":"Checkout ClickHouse Sources​","type":1,"pageTitle":"How to Build ClickHouse on Linux","url":"docs/en/development/build#checkout-clickhouse-sources-1","content":"$ git clone --recursive --branch master https://github.com/ClickHouse/ClickHouse.git $ cd ClickHouse  "},{"title":"Run Release Script​","type":1,"pageTitle":"How to Build ClickHouse on Linux","url":"docs/en/development/build#run-release-script","content":"$ ./release  "},{"title":"You Don’t Have to Build ClickHouse​","type":1,"pageTitle":"How to Build ClickHouse on Linux","url":"docs/en/development/build#you-dont-have-to-build-clickhouse","content":"ClickHouse is available in pre-built binaries and packages. Binaries are portable and can be run on any Linux flavour. They are built for stable, prestable and testing releases as long as for every commit to master and for every pull request. To find the freshest build from master, go to commits page, click on the first green checkmark or red cross near commit, and click to the “Details” link right after “ClickHouse Build Check”. "},{"title":"Faster builds for development: Split build configuration​","type":1,"pageTitle":"How to Build ClickHouse on Linux","url":"docs/en/development/build#split-build","content":"Normally, ClickHouse is statically linked into a single static clickhouse binary with minimal dependencies. This is convenient for distribution, but it means that on every change the entire binary needs to be linked, which is slow and may be inconvenient for development. There is an alternative configuration which instead creates dynamically loaded shared libraries and separate binaries clickhouse-server, clickhouse-client etc., allowing for faster incremental builds. To use it, add the following flags to your cmake invocation: -DUSE_STATIC_LIBRARIES=0 -DSPLIT_SHARED_LIBRARIES=1 -DCLICKHOUSE_SPLIT_BINARY=1  Note that the split build has several drawbacks: There is no single clickhouse binary, and you have to run clickhouse-server, clickhouse-client, etc.Risk of segfault if you run any of the programs while rebuilding the project.You cannot run the integration tests since they only work a single complete binary.You can't easily copy the binaries elsewhere. Instead of moving a single binary you'll need to copy all binaries and libraries. Original article "},{"title":"How to Build ClickHouse on Linux for Mac OS X","type":0,"sectionRef":"#","url":"docs/en/development/build-cross-osx","content":"","keywords":""},{"title":"Install Clang-13​","type":1,"pageTitle":"How to Build ClickHouse on Linux for Mac OS X","url":"docs/en/development/build-cross-osx#install-clang-13","content":"Follow the instructions from https://apt.llvm.org/ for your Ubuntu or Debian setup. For example the commands for Bionic are like: sudo echo &quot;deb [trusted=yes] http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main&quot; &gt;&gt; /etc/apt/sources.list sudo apt-get install clang-13  "},{"title":"Install Cross-Compilation Toolset​","type":1,"pageTitle":"How to Build ClickHouse on Linux for Mac OS X","url":"docs/en/development/build-cross-osx#install-cross-compilation-toolset","content":"Let’s remember the path where we install cctools as ${CCTOOLS} mkdir ${CCTOOLS} cd ${CCTOOLS} git clone https://github.com/tpoechtrager/apple-libtapi.git cd apple-libtapi INSTALLPREFIX=${CCTOOLS} ./build.sh ./install.sh cd .. git clone https://github.com/tpoechtrager/cctools-port.git cd cctools-port/cctools ./configure --prefix=$(readlink -f ${CCTOOLS}) --with-libtapi=$(readlink -f ${CCTOOLS}) --target=x86_64-apple-darwin make install  Also, we need to download macOS X SDK into the working tree. cd ClickHouse wget 'https://github.com/phracker/MacOSX-SDKs/releases/download/10.15/MacOSX10.15.sdk.tar.xz' mkdir -p build-darwin/cmake/toolchain/darwin-x86_64 tar xJf MacOSX10.15.sdk.tar.xz -C build-darwin/cmake/toolchain/darwin-x86_64 --strip-components=1  "},{"title":"Build ClickHouse​","type":1,"pageTitle":"How to Build ClickHouse on Linux for Mac OS X","url":"docs/en/development/build-cross-osx#build-clickhouse","content":"cd ClickHouse mkdir build-darwin cd build-darwin CC=clang-13 CXX=clang++-13 cmake -DCMAKE_AR:FILEPATH=${CCTOOLS}/bin/aarch64-apple-darwin-ar -DCMAKE_INSTALL_NAME_TOOL=${CCTOOLS}/bin/aarch64-apple-darwin-install_name_tool -DCMAKE_RANLIB:FILEPATH=${CCTOOLS}/bin/aarch64-apple-darwin-ranlib -DLINKER_NAME=${CCTOOLS}/bin/aarch64-apple-darwin-ld -DCMAKE_TOOLCHAIN_FILE=cmake/darwin/toolchain-x86_64.cmake .. ninja  The resulting binary will have a Mach-O executable format and can’t be run on Linux. "},{"title":"Third-Party Libraries Used","type":0,"sectionRef":"#","url":"docs/en/development/contrib","content":"","keywords":""},{"title":"Guidelines for adding new third-party libraries and maintaining custom changes in them​","type":1,"pageTitle":"Third-Party Libraries Used","url":"docs/en/development/contrib#adding-third-party-libraries","content":"All external third-party code should reside in the dedicated directories under contrib directory of ClickHouse repo. Prefer Git submodules, when available.Fork/mirror the official repo in Clickhouse-extras. Prefer official GitHub repos, when available.Branch from the branch you want to integrate, e.g., master -&gt; clickhouse/master, or release/vX.Y.Z -&gt; clickhouse/release/vX.Y.Z.All forks in Clickhouse-extras can be automatically synchronized with upstreams. clickhouse/... branches will remain unaffected, since virtually nobody is going to use that naming pattern in their upstream repos.Add submodules under contrib of ClickHouse repo that refer the above forks/mirrors. Set the submodules to track the corresponding clickhouse/... branches.Every time the custom changes have to be made in the library code, a dedicated branch should be created, like clickhouse/my-fix. Then this branch should be merged into the branch, that is tracked by the submodule, e.g., clickhouse/master or clickhouse/release/vX.Y.Z.No code should be pushed in any branch of the forks in Clickhouse-extras, whose names do not follow clickhouse/... pattern.Always write the custom changes with the official repo in mind. Once the PR is merged from (a feature/fix branch in) your personal fork into the fork in Clickhouse-extras, and the submodule is bumped in ClickHouse repo, consider opening another PR from (a feature/fix branch in) the fork in Clickhouse-extras to the official repo of the library. This will make sure, that 1) the contribution has more than a single use case and importance, 2) others will also benefit from it, 3) the change will not remain a maintenance burden solely on ClickHouse developers.When a submodule needs to start using a newer code from the original branch (e.g., master), and since the custom changes might be merged in the branch it is tracking (e.g., clickhouse/master) and so it may diverge from its original counterpart (i.e., master), a careful merge should be carried out first, i.e., master -&gt; clickhouse/master, and only then the submodule can be bumped in ClickHouse. "},{"title":"Database Engines","type":0,"sectionRef":"#","url":"docs/en/engines/database-engines/","content":"Database Engines Database engines allow you to work with tables. By default, ClickHouse uses the Atomic database engine, which provides configurable table engines and an SQL dialect. Here is a complete list of available database engines. Follow the links for more details: Atomic MySQL MaterializedMySQL Lazy PostgreSQL Replicated SQLite","keywords":""},{"title":"How to Build ClickHouse on Mac OS X","type":0,"sectionRef":"#","url":"docs/en/development/build-osx","content":"","keywords":""},{"title":"Install Homebrew​","type":1,"pageTitle":"How to Build ClickHouse on Mac OS X","url":"docs/en/development/build-osx#install-homebrew","content":"/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot; # ...and follow the printed instructions on any additional steps required to complete the installation.  "},{"title":"Install Xcode and Command Line Tools​","type":1,"pageTitle":"How to Build ClickHouse on Mac OS X","url":"docs/en/development/build-osx#install-xcode-and-command-line-tools","content":"Install the latest Xcode from App Store. Open it at least once to accept the end-user license agreement and automatically install the required components. Then, make sure that the latest Command Line Tools are installed and selected in the system: sudo rm -rf /Library/Developer/CommandLineTools sudo xcode-select --install  "},{"title":"Install Required Compilers, Tools, and Libraries​","type":1,"pageTitle":"How to Build ClickHouse on Mac OS X","url":"docs/en/development/build-osx#install-required-compilers-tools-and-libraries","content":"brew update brew install cmake ninja libtool gettext llvm gcc binutils  "},{"title":"Checkout ClickHouse Sources​","type":1,"pageTitle":"How to Build ClickHouse on Mac OS X","url":"docs/en/development/build-osx#checkout-clickhouse-sources","content":"git clone --recursive git@github.com:ClickHouse/ClickHouse.git # ...alternatively, you can use https://github.com/ClickHouse/ClickHouse.git as the repo URL.  "},{"title":"Build ClickHouse​","type":1,"pageTitle":"How to Build ClickHouse on Mac OS X","url":"docs/en/development/build-osx#build-clickhouse","content":"To build using Homebrew's vanilla Clang compiler (the only recommended way): cd ClickHouse rm -rf build mkdir build cd build cmake -DCMAKE_C_COMPILER=$(brew --prefix llvm)/bin/clang -DCMAKE_CXX_COMPILER=$(brew --prefix llvm)/bin/clang++ -DCMAKE_AR=$(brew --prefix llvm)/bin/llvm-ar -DCMAKE_RANLIB=$(brew --prefix llvm)/bin/llvm-ranlib -DOBJCOPY_PATH=$(brew --prefix llvm)/bin/llvm-objcopy -DCMAKE_BUILD_TYPE=RelWithDebInfo .. cmake --build . --config RelWithDebInfo # The resulting binary will be created at: ./programs/clickhouse  To build using Xcode's native AppleClang compiler in Xcode IDE (this option is only for development builds and workflows, and is not recommended unless you know what you are doing): cd ClickHouse rm -rf build mkdir build cd build XCODE_IDE=1 ALLOW_APPLECLANG=1 cmake -G Xcode -DCMAKE_BUILD_TYPE=Debug -DENABLE_JEMALLOC=OFF .. cmake --open . # ...then, in Xcode IDE select ALL_BUILD scheme and start the building process. # The resulting binary will be created at: ./programs/Debug/clickhouse  To build using Homebrew's vanilla GCC compiler (this option is only for development experiments, and is absolutely not recommended unless you really know what you are doing): cd ClickHouse rm -rf build mkdir build cd build cmake -DCMAKE_C_COMPILER=$(brew --prefix gcc)/bin/gcc-11 -DCMAKE_CXX_COMPILER=$(brew --prefix gcc)/bin/g++-11 -DCMAKE_AR=$(brew --prefix gcc)/bin/gcc-ar-11 -DCMAKE_RANLIB=$(brew --prefix gcc)/bin/gcc-ranlib-11 -DOBJCOPY_PATH=$(brew --prefix binutils)/bin/objcopy -DCMAKE_BUILD_TYPE=RelWithDebInfo .. cmake --build . --config RelWithDebInfo # The resulting binary will be created at: ./programs/clickhouse  "},{"title":"Caveats​","type":1,"pageTitle":"How to Build ClickHouse on Mac OS X","url":"docs/en/development/build-osx#caveats","content":"If you intend to run clickhouse-server, make sure to increase the system’s maxfiles variable. note You’ll need to use sudo. To do so, create the /Library/LaunchDaemons/limit.maxfiles.plist file with the following content: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!DOCTYPE plist PUBLIC &quot;-//Apple//DTD PLIST 1.0//EN&quot; &quot;http://www.apple.com/DTDs/PropertyList-1.0.dtd&quot;&gt; &lt;plist version=&quot;1.0&quot;&gt; &lt;dict&gt; &lt;key&gt;Label&lt;/key&gt; &lt;string&gt;limit.maxfiles&lt;/string&gt; &lt;key&gt;ProgramArguments&lt;/key&gt; &lt;array&gt; &lt;string&gt;launchctl&lt;/string&gt; &lt;string&gt;limit&lt;/string&gt; &lt;string&gt;maxfiles&lt;/string&gt; &lt;string&gt;524288&lt;/string&gt; &lt;string&gt;524288&lt;/string&gt; &lt;/array&gt; &lt;key&gt;RunAtLoad&lt;/key&gt; &lt;true/&gt; &lt;key&gt;ServiceIPC&lt;/key&gt; &lt;false/&gt; &lt;/dict&gt; &lt;/plist&gt;  Give the file correct permissions: sudo chown root:wheel /Library/LaunchDaemons/limit.maxfiles.plist  Validate that the file is correct: plutil /Library/LaunchDaemons/limit.maxfiles.plist  Load the file (or reboot): sudo launchctl load -w /Library/LaunchDaemons/limit.maxfiles.plist  To check if it’s working, use the ulimit -n or launchctl limit maxfiles commands. "},{"title":"Running ClickHouse server​","type":1,"pageTitle":"How to Build ClickHouse on Mac OS X","url":"docs/en/development/build-osx#running-clickhouse-server","content":"cd ClickHouse ./build/programs/clickhouse-server --config-file ./programs/server/config.xml  Original article "},{"title":"Continuous Integration Checks","type":0,"sectionRef":"#","url":"docs/en/development/continuous-integration","content":"","keywords":""},{"title":"Merge With Master​","type":1,"pageTitle":"Continuous Integration Checks","url":"docs/en/development/continuous-integration#merge-with-master","content":"Verifies that the PR can be merged to master. If not, it will fail with the message 'Cannot fetch mergecommit'. To fix this check, resolve the conflict as described in the GitHub documentation, or merge the master branch to your pull request branch using git. "},{"title":"Docs check​","type":1,"pageTitle":"Continuous Integration Checks","url":"docs/en/development/continuous-integration#docs-check","content":"Tries to build the ClickHouse documentation website. It can fail if you changed something in the documentation. Most probable reason is that some cross-link in the documentation is wrong. Go to the check report and look for ERROR and WARNING messages. "},{"title":"Report Details​","type":1,"pageTitle":"Continuous Integration Checks","url":"docs/en/development/continuous-integration#report-details","content":"Status page exampledocs_output.txt contains the building log. Successful result example "},{"title":"Description Check​","type":1,"pageTitle":"Continuous Integration Checks","url":"docs/en/development/continuous-integration#description-check","content":"Check that the description of your pull request conforms to the templatePULL_REQUEST_TEMPLATE.md. You have to specify a changelog category for your change (e.g., Bug Fix), and write a user-readable message describing the change for CHANGELOG.md "},{"title":"Push To Dockerhub​","type":1,"pageTitle":"Continuous Integration Checks","url":"docs/en/development/continuous-integration#push-to-dockerhub","content":"Builds docker images used for build and tests, then pushes them to DockerHub. "},{"title":"Marker Check​","type":1,"pageTitle":"Continuous Integration Checks","url":"docs/en/development/continuous-integration#marker-check","content":"This check means that the CI system started to process the pull request. When it has 'pending' status, it means that not all checks have been started yet. After all checks have been started, it changes status to 'success'. "},{"title":"Style Check​","type":1,"pageTitle":"Continuous Integration Checks","url":"docs/en/development/continuous-integration#style-check","content":"Performs some simple regex-based checks of code style, using the utils/check-style/check-style binary (note that it can be run locally). If it fails, fix the style errors following the code style guide. "},{"title":"Report Details​","type":1,"pageTitle":"Continuous Integration Checks","url":"docs/en/development/continuous-integration#report-details-1","content":"Status page exampleoutput.txt contains the check resulting errors (invalid tabulation etc), blank page means no errors. Successful result example. "},{"title":"Fast Test​","type":1,"pageTitle":"Continuous Integration Checks","url":"docs/en/development/continuous-integration#fast-test","content":"Normally this is the first check that is ran for a PR. It builds ClickHouse and runs most of stateless functional tests, omitting some. If it fails, further checks are not started until it is fixed. Look at the report to see which tests fail, then reproduce the failure locally as described here. "},{"title":"Report Details​","type":1,"pageTitle":"Continuous Integration Checks","url":"docs/en/development/continuous-integration#report-details-2","content":"Status page example Status Page Files​ runlog.out.log is the general log that includes all other logs.test_log.txtsubmodule_log.txt contains the messages about cloning and checkouting needed submodules.stderr.logstdout.logclickhouse-server.logclone_log.txtinstall_log.txtclickhouse-server.err.logbuild_log.txtcmake_log.txt contains messages about the C/C++ and Linux flags check. Status Page Columns​ Test name contains the name of the test (without the path e.g. all types of tests will be stripped to the name).Test status -- one of Skipped, Success, or Fail.Test time, sec. -- empty on this test. "},{"title":"Build Check​","type":1,"pageTitle":"Continuous Integration Checks","url":"docs/en/development/continuous-integration#build-check","content":"Builds ClickHouse in various configurations for use in further steps. You have to fix the builds that fail. Build logs often has enough information to fix the error, but you might have to reproduce the failure locally. The cmake options can be found in the build log, grepping for cmake. Use these options and follow the general build process. "},{"title":"Report Details​","type":1,"pageTitle":"Continuous Integration Checks","url":"docs/en/development/continuous-integration#report-details-3","content":"Status page example. Compiler: gcc-9 or clang-10 (or clang-10-xx for other architectures e.g. clang-10-freebsd).Build type: Debug or RelWithDebInfo (cmake).Sanitizer: none (without sanitizers), address (ASan), memory (MSan), undefined (UBSan), or thread (TSan).Splitted splitted is a split buildStatus: success or failBuild log: link to the building and files copying log, useful when build failed.Build time.Artifacts: build result files (with XXX being the server version e.g. 20.8.1.4344). clickhouse-client_XXX_all.debclickhouse-common-static-dbg_XXX[+asan, +msan, +ubsan, +tsan]_amd64.debclickhouse-common-staticXXX_amd64.debclickhouse-server_XXX_all.debclickhouse_XXX_amd64.buildinfoclickhouse_XXX_amd64.changesclickhouse: Main built binary.clickhouse-odbc-bridgeunit_tests_dbms: GoogleTest binary with ClickHouse unit tests.shared_build.tgz: build with shared libraries.performance.tgz: Special package for performance tests. "},{"title":"Special Build Check​","type":1,"pageTitle":"Continuous Integration Checks","url":"docs/en/development/continuous-integration#special-build-check","content":"Performs static analysis and code style checks using clang-tidy. The report is similar to the build check. Fix the errors found in the build log. "},{"title":"Functional Stateless Tests​","type":1,"pageTitle":"Continuous Integration Checks","url":"docs/en/development/continuous-integration#functional-stateless-tests","content":"Runs stateless functional tests for ClickHouse binaries built in various configurations -- release, debug, with sanitizers, etc. Look at the report to see which tests fail, then reproduce the failure locally as described here. Note that you have to use the correct build configuration to reproduce -- a test might fail under AddressSanitizer but pass in Debug. Download the binary from CI build checks page, or build it locally. "},{"title":"Functional Stateful Tests​","type":1,"pageTitle":"Continuous Integration Checks","url":"docs/en/development/continuous-integration#functional-stateful-tests","content":"Runs stateful functional tests. Treat them in the same way as the functional stateless tests. The difference is that they require hits and visits tables from the clickstream dataset to run. "},{"title":"Integration Tests​","type":1,"pageTitle":"Continuous Integration Checks","url":"docs/en/development/continuous-integration#integration-tests","content":"Runs integration tests. "},{"title":"Testflows Check​","type":1,"pageTitle":"Continuous Integration Checks","url":"docs/en/development/continuous-integration#testflows-check","content":"Runs some tests using Testflows test system. See here how to run them locally. "},{"title":"Stress Test​","type":1,"pageTitle":"Continuous Integration Checks","url":"docs/en/development/continuous-integration#stress-test","content":"Runs stateless functional tests concurrently from several clients to detect concurrency-related errors. If it fails: * Fix all other test failures first; * Look at the report to find the server logs and check them for possible causes of error.  "},{"title":"Split Build Smoke Test​","type":1,"pageTitle":"Continuous Integration Checks","url":"docs/en/development/continuous-integration#split-build-smoke-test","content":"Checks that the server build in split buildconfiguration can start and run simple queries. If it fails: * Fix other test errors first; * Build the server in [split build](/docs/testing/docs/en/development/build#split-build) configuration locally and check whether it can start and run `select 1`.  "},{"title":"Compatibility Check​","type":1,"pageTitle":"Continuous Integration Checks","url":"docs/en/development/continuous-integration#compatibility-check","content":"Checks that clickhouse binary runs on distributions with old libc versions. If it fails, ask a maintainer for help. "},{"title":"AST Fuzzer​","type":1,"pageTitle":"Continuous Integration Checks","url":"docs/en/development/continuous-integration#ast-fuzzer","content":"Runs randomly generated queries to catch program errors. If it fails, ask a maintainer for help. "},{"title":"Performance Tests​","type":1,"pageTitle":"Continuous Integration Checks","url":"docs/en/development/continuous-integration#performance-tests","content":"Measure changes in query performance. This is the longest check that takes just below 6 hours to run. The performance test report is described in detail here. "},{"title":"Overview of ClickHouse Architecture","type":0,"sectionRef":"#","url":"docs/en/development/architecture","content":"","keywords":""},{"title":"Columns​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"docs/en/development/architecture#columns","content":"IColumn interface is used to represent columns in memory (actually, chunks of columns). This interface provides helper methods for the implementation of various relational operators. Almost all operations are immutable: they do not modify the original column, but create a new modified one. For example, the IColumn :: filter method accepts a filter byte mask. It is used for the WHERE and HAVING relational operators. Additional examples: the IColumn :: permute method to support ORDER BY, the IColumn :: cut method to support LIMIT. Various IColumn implementations (ColumnUInt8, ColumnString, and so on) are responsible for the memory layout of columns. The memory layout is usually a contiguous array. For the integer type of columns, it is just one contiguous array, like std :: vector. For String and Array columns, it is two vectors: one for all array elements, placed contiguously, and a second one for offsets to the beginning of each array. There is also ColumnConst that stores just one value in memory, but looks like a column. "},{"title":"Field​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"docs/en/development/architecture#field","content":"Nevertheless, it is possible to work with individual values as well. To represent an individual value, the Field is used. Field is just a discriminated union of UInt64, Int64, Float64, String and Array. IColumn has the operator [] method to get the n-th value as a Field, and the insert method to append a Field to the end of a column. These methods are not very efficient, because they require dealing with temporary Field objects representing an individual value. There are more efficient methods, such as insertFrom, insertRangeFrom, and so on. Field does not have enough information about a specific data type for a table. For example, UInt8, UInt16, UInt32, and UInt64 are all represented as UInt64 in a Field. "},{"title":"Leaky Abstractions​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"docs/en/development/architecture#leaky-abstractions","content":"IColumn has methods for common relational transformations of data, but they do not meet all needs. For example, ColumnUInt64 does not have a method to calculate the sum of two columns, and ColumnString does not have a method to run a substring search. These countless routines are implemented outside of IColumn. Various functions on columns can be implemented in a generic, non-efficient way using IColumn methods to extract Field values, or in a specialized way using knowledge of inner memory layout of data in a specific IColumn implementation. It is implemented by casting functions to a specific IColumn type and deal with internal representation directly. For example, ColumnUInt64 has the getData method that returns a reference to an internal array, then a separate routine reads or fills that array directly. We have “leaky abstractions” to allow efficient specializations of various routines. "},{"title":"Data Types​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"docs/en/development/architecture#data_types","content":"IDataType is responsible for serialization and deserialization: for reading and writing chunks of columns or individual values in binary or text form. IDataType directly corresponds to data types in tables. For example, there are DataTypeUInt32, DataTypeDateTime, DataTypeString and so on. IDataType and IColumn are only loosely related to each other. Different data types can be represented in memory by the same IColumn implementations. For example, DataTypeUInt32 and DataTypeDateTime are both represented by ColumnUInt32 or ColumnConstUInt32. In addition, the same data type can be represented by different IColumn implementations. For example, DataTypeUInt8 can be represented by ColumnUInt8 or ColumnConstUInt8. IDataType only stores metadata. For instance, DataTypeUInt8 does not store anything at all (except virtual pointer vptr) and DataTypeFixedString stores just N (the size of fixed-size strings). IDataType has helper methods for various data formats. Examples are methods to serialize a value with possible quoting, to serialize a value for JSON, and to serialize a value as part of the XML format. There is no direct correspondence to data formats. For example, the different data formats Pretty and TabSeparated can use the same serializeTextEscaped helper method from the IDataType interface. "},{"title":"Block​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"docs/en/development/architecture#block","content":"A Block is a container that represents a subset (chunk) of a table in memory. It is just a set of triples: (IColumn, IDataType, column name). During query execution, data is processed by Blocks. If we have a Block, we have data (in the IColumn object), we have information about its type (in IDataType) that tells us how to deal with that column, and we have the column name. It could be either the original column name from the table or some artificial name assigned for getting temporary results of calculations. When we calculate some function over columns in a block, we add another column with its result to the block, and we do not touch columns for arguments of the function because operations are immutable. Later, unneeded columns can be removed from the block, but not modified. It is convenient for the elimination of common subexpressions. Blocks are created for every processed chunk of data. Note that for the same type of calculation, the column names and types remain the same for different blocks, and only column data changes. It is better to split block data from the block header because small block sizes have a high overhead of temporary strings for copying shared_ptrs and column names. "},{"title":"Block Streams​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"docs/en/development/architecture#block-streams","content":"Block streams are for processing data. We use streams of blocks to read data from somewhere, perform data transformations, or write data to somewhere. IBlockInputStream has the read method to fetch the next block while available. IBlockOutputStream has the write method to push the block somewhere. Streams are responsible for: Reading or writing to a table. The table just returns a stream for reading or writing blocks.Implementing data formats. For example, if you want to output data to a terminal in Pretty format, you create a block output stream where you push blocks, and it formats them.Performing data transformations. Let’s say you have IBlockInputStream and want to create a filtered stream. You create FilterBlockInputStream and initialize it with your stream. Then when you pull a block from FilterBlockInputStream, it pulls a block from your stream, filters it, and returns the filtered block to you. Query execution pipelines are represented this way. There are more sophisticated transformations. For example, when you pull from AggregatingBlockInputStream, it reads all data from its source, aggregates it, and then returns a stream of aggregated data for you. Another example: UnionBlockInputStream accepts many input sources in the constructor and also a number of threads. It launches multiple threads and reads from multiple sources in parallel. Block streams use the “pull” approach to control flow: when you pull a block from the first stream, it consequently pulls the required blocks from nested streams, and the entire execution pipeline will work. Neither “pull” nor “push” is the best solution, because control flow is implicit, and that limits the implementation of various features like simultaneous execution of multiple queries (merging many pipelines together). This limitation could be overcome with coroutines or just running extra threads that wait for each other. We may have more possibilities if we make control flow explicit: if we locate the logic for passing data from one calculation unit to another outside of those calculation units. Read this article for more thoughts. We should note that the query execution pipeline creates temporary data at each step. We try to keep block size small enough so that temporary data fits in the CPU cache. With that assumption, writing and reading temporary data is almost free in comparison with other calculations. We could consider an alternative, which is to fuse many operations in the pipeline together. It could make the pipeline as short as possible and remove much of the temporary data, which could be an advantage, but it also has drawbacks. For example, a split pipeline makes it easy to implement caching intermediate data, stealing intermediate data from similar queries running at the same time, and merging pipelines for similar queries. "},{"title":"Formats​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"docs/en/development/architecture#formats","content":"Data formats are implemented with block streams. There are “presentational” formats only suitable for the output of data to the client, such as Pretty format, which provides only IBlockOutputStream. And there are input/output formats, such as TabSeparated or JSONEachRow. There are also row streams: IRowInputStream and IRowOutputStream. They allow you to pull/push data by individual rows, not by blocks. And they are only needed to simplify the implementation of row-oriented formats. The wrappers BlockInputStreamFromRowInputStream and BlockOutputStreamFromRowOutputStream allow you to convert row-oriented streams to regular block-oriented streams. "},{"title":"I/O​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"docs/en/development/architecture#io","content":"For byte-oriented input/output, there are ReadBuffer and WriteBuffer abstract classes. They are used instead of C++ iostreams. Don’t worry: every mature C++ project is using something other than iostreams for good reasons. ReadBuffer and WriteBuffer are just a contiguous buffer and a cursor pointing to the position in that buffer. Implementations may own or not own the memory for the buffer. There is a virtual method to fill the buffer with the following data (for ReadBuffer) or to flush the buffer somewhere (for WriteBuffer). The virtual methods are rarely called. Implementations of ReadBuffer/WriteBuffer are used for working with files and file descriptors and network sockets, for implementing compression (CompressedWriteBuffer is initialized with another WriteBuffer and performs compression before writing data to it), and for other purposes – the names ConcatReadBuffer, LimitReadBuffer, and HashingWriteBuffer speak for themselves. Read/WriteBuffers only deal with bytes. There are functions from ReadHelpers and WriteHelpers header files to help with formatting input/output. For example, there are helpers to write a number in decimal format. Let’s look at what happens when you want to write a result set in JSON format to stdout. You have a result set ready to be fetched from IBlockInputStream. You create WriteBufferFromFileDescriptor(STDOUT_FILENO) to write bytes to stdout. You create JSONRowOutputStream, initialized with that WriteBuffer, to write rows in JSON to stdout. You create BlockOutputStreamFromRowOutputStream on top of it, to represent it as IBlockOutputStream. Then you call copyData to transfer data from IBlockInputStream to IBlockOutputStream, and everything works. Internally, JSONRowOutputStream will write various JSON delimiters and call the IDataType::serializeTextJSON method with a reference to IColumn and the row number as arguments. Consequently, IDataType::serializeTextJSON will call a method from WriteHelpers.h: for example, writeText for numeric types and writeJSONString for DataTypeString. "},{"title":"Tables​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"docs/en/development/architecture#tables","content":"The IStorage interface represents tables. Different implementations of that interface are different table engines. Examples are StorageMergeTree, StorageMemory, and so on. Instances of these classes are just tables. The key IStorage methods are read and write. There are also alter, rename, drop, and so on. The read method accepts the following arguments: the set of columns to read from a table, the AST query to consider, and the desired number of streams to return. It returns one or multiple IBlockInputStream objects and information about the stage of data processing that was completed inside a table engine during query execution. In most cases, the read method is only responsible for reading the specified columns from a table, not for any further data processing. All further data processing is done by the query interpreter and is outside the responsibility of IStorage. But there are notable exceptions: The AST query is passed to the read method, and the table engine can use it to derive index usage and to read fewer data from a table.Sometimes the table engine can process data itself to a specific stage. For example, StorageDistributed can send a query to remote servers, ask them to process data to a stage where data from different remote servers can be merged, and return that preprocessed data. The query interpreter then finishes processing the data. The table’s read method can return multiple IBlockInputStream objects to allow parallel data processing. These multiple block input streams can read from a table in parallel. Then you can wrap these streams with various transformations (such as expression evaluation or filtering) that can be calculated independently and create a UnionBlockInputStream on top of them, to read from multiple streams in parallel. There are also TableFunctions. These are functions that return a temporary IStorage object to use in the FROM clause of a query. To get a quick idea of how to implement your table engine, look at something simple, like StorageMemory or StorageTinyLog. As the result of the read method, IStorage returns QueryProcessingStage – information about what parts of the query were already calculated inside storage. "},{"title":"Parsers​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"docs/en/development/architecture#parsers","content":"A hand-written recursive descent parser parses a query. For example, ParserSelectQuery just recursively calls the underlying parsers for various parts of the query. Parsers create an AST. The AST is represented by nodes, which are instances of IAST. Parser generators are not used for historical reasons. "},{"title":"Interpreters​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"docs/en/development/architecture#interpreters","content":"Interpreters are responsible for creating the query execution pipeline from an AST. There are simple interpreters, such as InterpreterExistsQuery and InterpreterDropQuery, or the more sophisticated InterpreterSelectQuery. The query execution pipeline is a combination of block input or output streams. For example, the result of interpreting the SELECT query is the IBlockInputStream to read the result set from; the result of the INSERT query is the IBlockOutputStream to write data for insertion to, and the result of interpreting the INSERT SELECT query is the IBlockInputStream that returns an empty result set on the first read, but that copies data from SELECT to INSERT at the same time. InterpreterSelectQuery uses ExpressionAnalyzer and ExpressionActions machinery for query analysis and transformations. This is where most rule-based query optimizations are done. ExpressionAnalyzer is quite messy and should be rewritten: various query transformations and optimizations should be extracted to separate classes to allow modular transformations of query. "},{"title":"Functions​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"docs/en/development/architecture#functions","content":"There are ordinary functions and aggregate functions. For aggregate functions, see the next section. Ordinary functions do not change the number of rows – they work as if they are processing each row independently. In fact, functions are not called for individual rows, but for Block’s of data to implement vectorized query execution. There are some miscellaneous functions, like blockSize, rowNumberInBlock, and runningAccumulate, that exploit block processing and violate the independence of rows. ClickHouse has strong typing, so there’s no implicit type conversion. If a function does not support a specific combination of types, it throws an exception. But functions can work (be overloaded) for many different combinations of types. For example, the plus function (to implement the + operator) works for any combination of numeric types: UInt8 + Float32, UInt16 + Int8, and so on. Also, some variadic functions can accept any number of arguments, such as the concat function. Implementing a function may be slightly inconvenient because a function explicitly dispatches supported data types and supported IColumns. For example, the plus function has code generated by instantiation of a C++ template for each combination of numeric types, and constant or non-constant left and right arguments. It is an excellent place to implement runtime code generation to avoid template code bloat. Also, it makes it possible to add fused functions like fused multiply-add or to make multiple comparisons in one loop iteration. Due to vectorized query execution, functions are not short-circuited. For example, if you write WHERE f(x) AND g(y), both sides are calculated, even for rows, when f(x) is zero (except when f(x) is a zero constant expression). But if the selectivity of the f(x) condition is high, and calculation of f(x) is much cheaper than g(y), it’s better to implement multi-pass calculation. It would first calculate f(x), then filter columns by the result, and then calculate g(y) only for smaller, filtered chunks of data. "},{"title":"Aggregate Functions​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"docs/en/development/architecture#aggregate-functions","content":"Aggregate functions are stateful functions. They accumulate passed values into some state and allow you to get results from that state. They are managed with the IAggregateFunction interface. States can be rather simple (the state for AggregateFunctionCount is just a single UInt64 value) or quite complex (the state of AggregateFunctionUniqCombined is a combination of a linear array, a hash table, and a HyperLogLog probabilistic data structure). States are allocated in Arena (a memory pool) to deal with multiple states while executing a high-cardinality GROUP BY query. States can have a non-trivial constructor and destructor: for example, complicated aggregation states can allocate additional memory themselves. It requires some attention to creating and destroying states and properly passing their ownership and destruction order. Aggregation states can be serialized and deserialized to pass over the network during distributed query execution or to write them on the disk where there is not enough RAM. They can even be stored in a table with the DataTypeAggregateFunction to allow incremental aggregation of data. The serialized data format for aggregate function states is not versioned right now. It is ok if aggregate states are only stored temporarily. But we have the AggregatingMergeTree table engine for incremental aggregation, and people are already using it in production. It is the reason why backward compatibility is required when changing the serialized format for any aggregate function in the future. "},{"title":"Server​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"docs/en/development/architecture#server","content":"The server implements several different interfaces: An HTTP interface for any foreign clients.A TCP interface for the native ClickHouse client and for cross-server communication during distributed query execution.An interface for transferring data for replication. Internally, it is just a primitive multithreaded server without coroutines or fibers. Since the server is not designed to process a high rate of simple queries but to process a relatively low rate of complex queries, each of them can process a vast amount of data for analytics. The server initializes the Context class with the necessary environment for query execution: the list of available databases, users and access rights, settings, clusters, the process list, the query log, and so on. Interpreters use this environment. We maintain full backward and forward compatibility for the server TCP protocol: old clients can talk to new servers, and new clients can talk to old servers. But we do not want to maintain it eternally, and we are removing support for old versions after about one year. note For most external applications, we recommend using the HTTP interface because it is simple and easy to use. The TCP protocol is more tightly linked to internal data structures: it uses an internal format for passing blocks of data, and it uses custom framing for compressed data. We haven’t released a C library for that protocol because it requires linking most of the ClickHouse codebase, which is not practical. "},{"title":"Distributed Query Execution​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"docs/en/development/architecture#distributed-query-execution","content":"Servers in a cluster setup are mostly independent. You can create a Distributed table on one or all servers in a cluster. The Distributed table does not store data itself – it only provides a “view” to all local tables on multiple nodes of a cluster. When you SELECT from a Distributed table, it rewrites that query, chooses remote nodes according to load balancing settings, and sends the query to them. The Distributed table requests remote servers to process a query just up to a stage where intermediate results from different servers can be merged. Then it receives the intermediate results and merges them. The distributed table tries to distribute as much work as possible to remote servers and does not send much intermediate data over the network. Things become more complicated when you have subqueries in IN or JOIN clauses, and each of them uses a Distributed table. We have different strategies for the execution of these queries. There is no global query plan for distributed query execution. Each node has its local query plan for its part of the job. We only have simple one-pass distributed query execution: we send queries for remote nodes and then merge the results. But this is not feasible for complicated queries with high cardinality GROUP BYs or with a large amount of temporary data for JOIN. In such cases, we need to “reshuffle” data between servers, which requires additional coordination. ClickHouse does not support that kind of query execution, and we need to work on it. "},{"title":"Merge Tree​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"docs/en/development/architecture#merge-tree","content":"MergeTree is a family of storage engines that supports indexing by primary key. The primary key can be an arbitrary tuple of columns or expressions. Data in a MergeTree table is stored in “parts”. Each part stores data in the primary key order, so data is ordered lexicographically by the primary key tuple. All the table columns are stored in separate column.bin files in these parts. The files consist of compressed blocks. Each block is usually from 64 KB to 1 MB of uncompressed data, depending on the average value size. The blocks consist of column values placed contiguously one after the other. Column values are in the same order for each column (the primary key defines the order), so when you iterate by many columns, you get values for the corresponding rows. The primary key itself is “sparse”. It does not address every single row, but only some ranges of data. A separate primary.idx file has the value of the primary key for each N-th row, where N is called index_granularity (usually, N = 8192). Also, for each column, we have column.mrk files with “marks”, which are offsets to each N-th row in the data file. Each mark is a pair: the offset in the file to the beginning of the compressed block, and the offset in the decompressed block to the beginning of data. Usually, compressed blocks are aligned by marks, and the offset in the decompressed block is zero. Data for primary.idx always resides in memory, and data for column.mrk files is cached. When we are going to read something from a part in MergeTree, we look at primary.idx data and locate ranges that could contain requested data, then look at column.mrk data and calculate offsets for where to start reading those ranges. Because of sparseness, excess data may be read. ClickHouse is not suitable for a high load of simple point queries, because the entire range with index_granularity rows must be read for each key, and the entire compressed block must be decompressed for each column. We made the index sparse because we must be able to maintain trillions of rows per single server without noticeable memory consumption for the index. Also, because the primary key is sparse, it is not unique: it cannot check the existence of the key in the table at INSERT time. You could have many rows with the same key in a table. When you INSERT a bunch of data into MergeTree, that bunch is sorted by primary key order and forms a new part. There are background threads that periodically select some parts and merge them into a single sorted part to keep the number of parts relatively low. That’s why it is called MergeTree. Of course, merging leads to “write amplification”. All parts are immutable: they are only created and deleted, but not modified. When SELECT is executed, it holds a snapshot of the table (a set of parts). After merging, we also keep old parts for some time to make a recovery after failure easier, so if we see that some merged part is probably broken, we can replace it with its source parts. MergeTree is not an LSM tree because it does not contain MEMTABLE and LOG: inserted data is written directly to the filesystem. This behavior makes MergeTree much more suitable to insert data in batches. Therefore frequently inserting small amounts of rows is not ideal for MergeTree. For example, a couple of rows per second is OK, but doing it a thousand times a second is not optimal for MergeTree. However, there is an async insert mode for small inserts to overcome this limitation. We did it this way for simplicity’s sake, and because we are already inserting data in batches in our applications There are MergeTree engines that are doing additional work during background merges. Examples are CollapsingMergeTree and AggregatingMergeTree. This could be treated as special support for updates. Keep in mind that these are not real updates because users usually have no control over the time when background merges are executed, and data in a MergeTree table is almost always stored in more than one part, not in completely merged form. "},{"title":"Replication​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"docs/en/development/architecture#replication","content":"Replication in ClickHouse can be configured on a per-table basis. You could have some replicated and some non-replicated tables on the same server. You could also have tables replicated in different ways, such as one table with two-factor replication and another with three-factor. Replication is implemented in the ReplicatedMergeTree storage engine. The path in ZooKeeper is specified as a parameter for the storage engine. All tables with the same path in ZooKeeper become replicas of each other: they synchronize their data and maintain consistency. Replicas can be added and removed dynamically simply by creating or dropping a table. Replication uses an asynchronous multi-master scheme. You can insert data into any replica that has a session with ZooKeeper, and data is replicated to all other replicas asynchronously. Because ClickHouse does not support UPDATEs, replication is conflict-free. As there is no quorum acknowledgment of inserts, just-inserted data might be lost if one node fails. Metadata for replication is stored in ZooKeeper. There is a replication log that lists what actions to do. Actions are: get part; merge parts; drop a partition, and so on. Each replica copies the replication log to its queue and then executes the actions from the queue. For example, on insertion, the “get the part” action is created in the log, and every replica downloads that part. Merges are coordinated between replicas to get byte-identical results. All parts are merged in the same way on all replicas. One of the leaders initiates a new merge first and writes “merge parts” actions to the log. Multiple replicas (or all) can be leaders at the same time. A replica can be prevented from becoming a leader using the merge_tree setting replicated_can_become_leader. The leaders are responsible for scheduling background merges. Replication is physical: only compressed parts are transferred between nodes, not queries. Merges are processed on each replica independently in most cases to lower the network costs by avoiding network amplification. Large merged parts are sent over the network only in cases of significant replication lag. Besides, each replica stores its state in ZooKeeper as the set of parts and its checksums. When the state on the local filesystem diverges from the reference state in ZooKeeper, the replica restores its consistency by downloading missing and broken parts from other replicas. When there is some unexpected or broken data in the local filesystem, ClickHouse does not remove it, but moves it to a separate directory and forgets it. note The ClickHouse cluster consists of independent shards, and each shard consists of replicas. The cluster is not elastic, so after adding a new shard, data is not rebalanced between shards automatically. Instead, the cluster load is supposed to be adjusted to be uneven. This implementation gives you more control, and it is ok for relatively small clusters, such as tens of nodes. But for clusters with hundreds of nodes that we are using in production, this approach becomes a significant drawback. We should implement a table engine that spans across the cluster with dynamically replicated regions that could be split and balanced between clusters automatically. Original article "},{"title":"Lazy","type":0,"sectionRef":"#","url":"docs/en/engines/database-engines/lazy","content":"","keywords":""},{"title":"Creating a Database​","type":1,"pageTitle":"Lazy","url":"docs/en/engines/database-engines/lazy#creating-a-database","content":"CREATE DATABASE testlazy ENGINE = Lazy(expiration_time_in_seconds);  Original article "},{"title":"Getting Started Guide for Building ClickHouse","type":0,"sectionRef":"#","url":"docs/en/development/developer-instruction","content":"","keywords":""},{"title":"Creating a Repository on GitHub​","type":1,"pageTitle":"Getting Started Guide for Building ClickHouse","url":"docs/en/development/developer-instruction#creating-a-repository-on-github","content":"To start working with ClickHouse repository you will need a GitHub account. You probably already have one, but if you do not, please register at https://github.com. In case you do not have SSH keys, you should generate them and then upload them on GitHub. It is required for sending over your patches. It is also possible to use the same SSH keys that you use with any other SSH servers - probably you already have those. Create a fork of ClickHouse repository. To do that please click on the “fork” button in the upper right corner at https://github.com/ClickHouse/ClickHouse. It will fork your own copy of ClickHouse/ClickHouse to your account. The development process consists of first committing the intended changes into your fork of ClickHouse and then creating a “pull request” for these changes to be accepted into the main repository (ClickHouse/ClickHouse). To work with git repositories, please install git. To do that in Ubuntu you would run in the command line terminal: sudo apt update sudo apt install git  A brief manual on using Git can be found here: https://education.github.com/git-cheat-sheet-education.pdf. For a detailed manual on Git see https://git-scm.com/book/en/v2. "},{"title":"Cloning a Repository to Your Development Machine​","type":1,"pageTitle":"Getting Started Guide for Building ClickHouse","url":"docs/en/development/developer-instruction#cloning-a-repository-to-your-development-machine","content":"Next, you need to download the source files onto your working machine. This is called “to clone a repository” because it creates a local copy of the repository on your working machine. In the command line terminal run: git clone --recursive git@github.com:your_github_username/ClickHouse.git cd ClickHouse  Note: please, substitute your_github_username with what is appropriate! This command will create a directory ClickHouse containing the working copy of the project. It is important that the path to the working directory contains no whitespaces as it may lead to problems with running the build system. Please note that ClickHouse repository uses submodules. That is what the references to additional repositories are called (i.e. external libraries on which the project depends). It means that when cloning the repository you need to specify the --recursive flag as in the example above. If the repository has been cloned without submodules, to download them you need to run the following: git submodule init git submodule update  You can check the status with the command: git submodule status. If you get the following error message: Permission denied (publickey). fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists.  It generally means that the SSH keys for connecting to GitHub are missing. These keys are normally located in ~/.ssh. For SSH keys to be accepted you need to upload them in the settings section of GitHub UI. You can also clone the repository via https protocol: git clone --recursive https://github.com/ClickHouse/ClickHouse.git  This, however, will not let you send your changes to the server. You can still use it temporarily and add the SSH keys later replacing the remote address of the repository with git remote command. You can also add original ClickHouse repo’s address to your local repository to pull updates from there: git remote add upstream git@github.com:ClickHouse/ClickHouse.git  After successfully running this command you will be able to pull updates from the main ClickHouse repo by running git pull upstream master. "},{"title":"Working with Submodules​","type":1,"pageTitle":"Getting Started Guide for Building ClickHouse","url":"docs/en/development/developer-instruction#working-with-submodules","content":"Working with submodules in git could be painful. Next commands will help to manage it: # ! each command accepts # Update remote URLs for submodules. Barely rare case git submodule sync # Add new submodules git submodule init # Update existing submodules to the current state git submodule update # Two last commands could be merged together git submodule update --init  The next commands would help you to reset all submodules to the initial state (!WARNING! - any changes inside will be deleted): # Synchronizes submodules' remote URL with .gitmodules git submodule sync # Update the registered submodules with initialize not yet initialized git submodule update --init # Reset all changes done after HEAD git submodule foreach git reset --hard # Clean files from .gitignore git submodule foreach git clean -xfd # Repeat last 4 commands for all submodule git submodule foreach git submodule sync git submodule foreach git submodule update --init git submodule foreach git submodule foreach git reset --hard git submodule foreach git submodule foreach git clean -xfd  "},{"title":"Build System​","type":1,"pageTitle":"Getting Started Guide for Building ClickHouse","url":"docs/en/development/developer-instruction#build-system","content":"ClickHouse uses CMake and Ninja for building. CMake - a meta-build system that can generate Ninja files (build tasks). Ninja - a smaller build system with a focus on the speed used to execute those cmake generated tasks. To install on Ubuntu, Debian or Mint run sudo apt install cmake ninja-build. On CentOS, RedHat run sudo yum install cmake ninja-build. If you use Arch or Gentoo, you probably know it yourself how to install CMake. For installing CMake and Ninja on Mac OS X first install Homebrew and then install everything else via brew: /usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; brew install cmake ninja  Next, check the version of CMake: cmake --version. If it is below 3.12, you should install a newer version from the website: https://cmake.org/download/. "},{"title":"C++ Compiler​","type":1,"pageTitle":"Getting Started Guide for Building ClickHouse","url":"docs/en/development/developer-instruction#c-compiler","content":"Compilers Clang starting from version 11 is supported for building ClickHouse. Clang should be used instead of gcc. Though, our continuous integration (CI) platform runs checks for about a dozen of build combinations. On Ubuntu/Debian you can use the automatic installation script (check official webpage) sudo bash -c &quot;$(wget -O - https://apt.llvm.org/llvm.sh)&quot;  Mac OS X build is also supported. Just run brew install llvm "},{"title":"The Building Process​","type":1,"pageTitle":"Getting Started Guide for Building ClickHouse","url":"docs/en/development/developer-instruction#the-building-process","content":"Now that you are ready to build ClickHouse we recommend you to create a separate directory build inside ClickHouse that will contain all of the build artefacts: mkdir build cd build  You can have several different directories (build_release, build_debug, etc.) for different types of build. While inside the build directory, configure your build by running CMake. Before the first run, you need to define environment variables that specify compiler. export CC=clang CXX=clang++ cmake ..  If you installed clang using the automatic installation script above, also specify the version of clang installed in the first command, e.g. export CC=clang-13 CXX=clang++-13. The clang version will be in the script output. The CC variable specifies the compiler for C (short for C Compiler), and CXX variable instructs which C++ compiler is to be used for building. For a faster build, you can resort to the debug build type - a build with no optimizations. For that supply the following parameter -D CMAKE_BUILD_TYPE=Debug: cmake -D CMAKE_BUILD_TYPE=Debug ..  You can change the type of build by running this command in the build directory. Run ninja to build: ninja clickhouse-server clickhouse-client  Only the required binaries are going to be built in this example. If you require to build all the binaries (utilities and tests), you should run ninja with no parameters: ninja  Full build requires about 30GB of free disk space or 15GB to build the main binaries. When a large amount of RAM is available on build machine you should limit the number of build tasks run in parallel with -j param: ninja -j 1 clickhouse-server clickhouse-client  On machines with 4GB of RAM, it is recommended to specify 1, for 8GB of RAM -j 2 is recommended. If you get the message: ninja: error: loading 'build.ninja': No such file or directory, it means that generating a build configuration has failed and you need to inspect the message above. Upon the successful start of the building process, you’ll see the build progress - the number of processed tasks and the total number of tasks. While building messages about protobuf files in libhdfs2 library like libprotobuf WARNING may show up. They affect nothing and are safe to be ignored. Upon successful build you get an executable file ClickHouse/&lt;build_dir&gt;/programs/clickhouse: ls -l programs/clickhouse  "},{"title":"Running the Built Executable of ClickHouse​","type":1,"pageTitle":"Getting Started Guide for Building ClickHouse","url":"docs/en/development/developer-instruction#running-the-built-executable-of-clickhouse","content":"To run the server under the current user you need to navigate to ClickHouse/programs/server/ (located outside of build) and run: ../../build/programs/clickhouse server  In this case, ClickHouse will use config files located in the current directory. You can run clickhouse server from any directory specifying the path to a config file as a command-line parameter --config-file. To connect to ClickHouse with clickhouse-client in another terminal navigate to ClickHouse/build/programs/ and run ./clickhouse client. If you get Connection refused message on Mac OS X or FreeBSD, try specifying host address 127.0.0.1: clickhouse client --host 127.0.0.1  You can replace the production version of ClickHouse binary installed in your system with your custom-built ClickHouse binary. To do that install ClickHouse on your machine following the instructions from the official website. Next, run the following: sudo service clickhouse-server stop sudo cp ClickHouse/build/programs/clickhouse /usr/bin/ sudo service clickhouse-server start  Note that clickhouse-client, clickhouse-server and others are symlinks to the commonly shared clickhouse binary. You can also run your custom-built ClickHouse binary with the config file from the ClickHouse package installed on your system: sudo service clickhouse-server stop sudo -u clickhouse ClickHouse/build/programs/clickhouse server --config-file /etc/clickhouse-server/config.xml  "},{"title":"IDE (Integrated Development Environment)​","type":1,"pageTitle":"Getting Started Guide for Building ClickHouse","url":"docs/en/development/developer-instruction#ide-integrated-development-environment","content":"If you do not know which IDE to use, we recommend that you use CLion. CLion is commercial software, but it offers 30 days free trial period. It is also free of charge for students. CLion can be used both on Linux and on Mac OS X. KDevelop and QTCreator are other great alternatives of an IDE for developing ClickHouse. KDevelop comes in as a very handy IDE although unstable. If KDevelop crashes after a while upon opening project, you should click “Stop All” button as soon as it has opened the list of project’s files. After doing so KDevelop should be fine to work with. As simple code editors, you can use Sublime Text or Visual Studio Code, or Kate (all of which are available on Linux). Just in case, it is worth mentioning that CLion creates build path on its own, it also on its own selects debug for build type, for configuration it uses a version of CMake that is defined in CLion and not the one installed by you, and finally, CLion will use make to run build tasks instead of ninja. This is normal behaviour, just keep that in mind to avoid confusion. "},{"title":"Writing Code​","type":1,"pageTitle":"Getting Started Guide for Building ClickHouse","url":"docs/en/development/developer-instruction#writing-code","content":"The description of ClickHouse architecture can be found here: https://clickhouse.com/docs/en/development/architecture/ The Code Style Guide: https://clickhouse.com/docs/en/development/style/ Adding third-party libraries: https://clickhouse.com/docs/en/development/contrib/#adding-third-party-libraries Writing tests: https://clickhouse.com/docs/en/development/tests/ List of tasks: https://github.com/ClickHouse/ClickHouse/issues?q=is%3Aopen+is%3Aissue+label%3Ahacktoberfest "},{"title":"Test Data​","type":1,"pageTitle":"Getting Started Guide for Building ClickHouse","url":"docs/en/development/developer-instruction#test-data","content":"Developing ClickHouse often requires loading realistic datasets. It is particularly important for performance testing. We have a specially prepared set of anonymized data of web analytics. It requires additionally some 3GB of free disk space. Note that this data is not required to accomplish most of the development tasks. sudo apt install wget xz-utils wget https://datasets.clickhouse.com/hits/tsv/hits_v1.tsv.xz wget https://datasets.clickhouse.com/visits/tsv/visits_v1.tsv.xz xz -v -d hits_v1.tsv.xz xz -v -d visits_v1.tsv.xz clickhouse-client CREATE DATABASE IF NOT EXISTS test CREATE TABLE test.hits ( WatchID UInt64, JavaEnable UInt8, Title String, GoodEvent Int16, EventTime DateTime, EventDate Date, CounterID UInt32, ClientIP UInt32, ClientIP6 FixedString(16), RegionID UInt32, UserID UInt64, CounterClass Int8, OS UInt8, UserAgent UInt8, URL String, Referer String, URLDomain String, RefererDomain String, Refresh UInt8, IsRobot UInt8, RefererCategories Array(UInt16), URLCategories Array(UInt16), URLRegions Array(UInt32), RefererRegions Array(UInt32), ResolutionWidth UInt16, ResolutionHeight UInt16, ResolutionDepth UInt8, FlashMajor UInt8, FlashMinor UInt8, FlashMinor2 String, NetMajor UInt8, NetMinor UInt8, UserAgentMajor UInt16, UserAgentMinor FixedString(2), CookieEnable UInt8, JavascriptEnable UInt8, IsMobile UInt8, MobilePhone UInt8, MobilePhoneModel String, Params String, IPNetworkID UInt32, TraficSourceID Int8, SearchEngineID UInt16, SearchPhrase String, AdvEngineID UInt8, IsArtifical UInt8, WindowClientWidth UInt16, WindowClientHeight UInt16, ClientTimeZone Int16, ClientEventTime DateTime, SilverlightVersion1 UInt8, SilverlightVersion2 UInt8, SilverlightVersion3 UInt32, SilverlightVersion4 UInt16, PageCharset String, CodeVersion UInt32, IsLink UInt8, IsDownload UInt8, IsNotBounce UInt8, FUniqID UInt64, HID UInt32, IsOldCounter UInt8, IsEvent UInt8, IsParameter UInt8, DontCountHits UInt8, WithHash UInt8, HitColor FixedString(1), UTCEventTime DateTime, Age UInt8, Sex UInt8, Income UInt8, Interests UInt16, Robotness UInt8, GeneralInterests Array(UInt16), RemoteIP UInt32, RemoteIP6 FixedString(16), WindowName Int32, OpenerName Int32, HistoryLength Int16, BrowserLanguage FixedString(2), BrowserCountry FixedString(2), SocialNetwork String, SocialAction String, HTTPError UInt16, SendTiming Int32, DNSTiming Int32, ConnectTiming Int32, ResponseStartTiming Int32, ResponseEndTiming Int32, FetchTiming Int32, RedirectTiming Int32, DOMInteractiveTiming Int32, DOMContentLoadedTiming Int32, DOMCompleteTiming Int32, LoadEventStartTiming Int32, LoadEventEndTiming Int32, NSToDOMContentLoadedTiming Int32, FirstPaintTiming Int32, RedirectCount Int8, SocialSourceNetworkID UInt8, SocialSourcePage String, ParamPrice Int64, ParamOrderID String, ParamCurrency FixedString(3), ParamCurrencyID UInt16, GoalsReached Array(UInt32), OpenstatServiceName String, OpenstatCampaignID String, OpenstatAdID String, OpenstatSourceID String, UTMSource String, UTMMedium String, UTMCampaign String, UTMContent String, UTMTerm String, FromTag String, HasGCLID UInt8, RefererHash UInt64, URLHash UInt64, CLID UInt32, YCLID UInt64, ShareService String, ShareURL String, ShareTitle String, `ParsedParams.Key1` Array(String), `ParsedParams.Key2` Array(String), `ParsedParams.Key3` Array(String), `ParsedParams.Key4` Array(String), `ParsedParams.Key5` Array(String), `ParsedParams.ValueDouble` Array(Float64), IslandID FixedString(16), RequestNum UInt32, RequestTry UInt8) ENGINE = MergeTree PARTITION BY toYYYYMM(EventDate) SAMPLE BY intHash32(UserID) ORDER BY (CounterID, EventDate, intHash32(UserID), EventTime); CREATE TABLE test.visits ( CounterID UInt32, StartDate Date, Sign Int8, IsNew UInt8, VisitID UInt64, UserID UInt64, StartTime DateTime, Duration UInt32, UTCStartTime DateTime, PageViews Int32, Hits Int32, IsBounce UInt8, Referer String, StartURL String, RefererDomain String, StartURLDomain String, EndURL String, LinkURL String, IsDownload UInt8, TraficSourceID Int8, SearchEngineID UInt16, SearchPhrase String, AdvEngineID UInt8, PlaceID Int32, RefererCategories Array(UInt16), URLCategories Array(UInt16), URLRegions Array(UInt32), RefererRegions Array(UInt32), IsYandex UInt8, GoalReachesDepth Int32, GoalReachesURL Int32, GoalReachesAny Int32, SocialSourceNetworkID UInt8, SocialSourcePage String, MobilePhoneModel String, ClientEventTime DateTime, RegionID UInt32, ClientIP UInt32, ClientIP6 FixedString(16), RemoteIP UInt32, RemoteIP6 FixedString(16), IPNetworkID UInt32, SilverlightVersion3 UInt32, CodeVersion UInt32, ResolutionWidth UInt16, ResolutionHeight UInt16, UserAgentMajor UInt16, UserAgentMinor UInt16, WindowClientWidth UInt16, WindowClientHeight UInt16, SilverlightVersion2 UInt8, SilverlightVersion4 UInt16, FlashVersion3 UInt16, FlashVersion4 UInt16, ClientTimeZone Int16, OS UInt8, UserAgent UInt8, ResolutionDepth UInt8, FlashMajor UInt8, FlashMinor UInt8, NetMajor UInt8, NetMinor UInt8, MobilePhone UInt8, SilverlightVersion1 UInt8, Age UInt8, Sex UInt8, Income UInt8, JavaEnable UInt8, CookieEnable UInt8, JavascriptEnable UInt8, IsMobile UInt8, BrowserLanguage UInt16, BrowserCountry UInt16, Interests UInt16, Robotness UInt8, GeneralInterests Array(UInt16), Params Array(String), `Goals.ID` Array(UInt32), `Goals.Serial` Array(UInt32), `Goals.EventTime` Array(DateTime), `Goals.Price` Array(Int64), `Goals.OrderID` Array(String), `Goals.CurrencyID` Array(UInt32), WatchIDs Array(UInt64), ParamSumPrice Int64, ParamCurrency FixedString(3), ParamCurrencyID UInt16, ClickLogID UInt64, ClickEventID Int32, ClickGoodEvent Int32, ClickEventTime DateTime, ClickPriorityID Int32, ClickPhraseID Int32, ClickPageID Int32, ClickPlaceID Int32, ClickTypeID Int32, ClickResourceID Int32, ClickCost UInt32, ClickClientIP UInt32, ClickDomainID UInt32, ClickURL String, ClickAttempt UInt8, ClickOrderID UInt32, ClickBannerID UInt32, ClickMarketCategoryID UInt32, ClickMarketPP UInt32, ClickMarketCategoryName String, ClickMarketPPName String, ClickAWAPSCampaignName String, ClickPageName String, ClickTargetType UInt16, ClickTargetPhraseID UInt64, ClickContextType UInt8, ClickSelectType Int8, ClickOptions String, ClickGroupBannerID Int32, OpenstatServiceName String, OpenstatCampaignID String, OpenstatAdID String, OpenstatSourceID String, UTMSource String, UTMMedium String, UTMCampaign String, UTMContent String, UTMTerm String, FromTag String, HasGCLID UInt8, FirstVisit DateTime, PredLastVisit Date, LastVisit Date, TotalVisits UInt32, `TraficSource.ID` Array(Int8), `TraficSource.SearchEngineID` Array(UInt16), `TraficSource.AdvEngineID` Array(UInt8), `TraficSource.PlaceID` Array(UInt16), `TraficSource.SocialSourceNetworkID` Array(UInt8), `TraficSource.Domain` Array(String), `TraficSource.SearchPhrase` Array(String), `TraficSource.SocialSourcePage` Array(String), Attendance FixedString(16), CLID UInt32, YCLID UInt64, NormalizedRefererHash UInt64, SearchPhraseHash UInt64, RefererDomainHash UInt64, NormalizedStartURLHash UInt64, StartURLDomainHash UInt64, NormalizedEndURLHash UInt64, TopLevelDomain UInt64, URLScheme UInt64, OpenstatServiceNameHash UInt64, OpenstatCampaignIDHash UInt64, OpenstatAdIDHash UInt64, OpenstatSourceIDHash UInt64, UTMSourceHash UInt64, UTMMediumHash UInt64, UTMCampaignHash UInt64, UTMContentHash UInt64, UTMTermHash UInt64, FromHash UInt64, WebVisorEnabled UInt8, WebVisorActivity UInt32, `ParsedParams.Key1` Array(String), `ParsedParams.Key2` Array(String), `ParsedParams.Key3` Array(String), `ParsedParams.Key4` Array(String), `ParsedParams.Key5` Array(String), `ParsedParams.ValueDouble` Array(Float64), `Market.Type` Array(UInt8), `Market.GoalID` Array(UInt32), `Market.OrderID` Array(String), `Market.OrderPrice` Array(Int64), `Market.PP` Array(UInt32), `Market.DirectPlaceID` Array(UInt32), `Market.DirectOrderID` Array(UInt32), `Market.DirectBannerID` Array(UInt32), `Market.GoodID` Array(String), `Market.GoodName` Array(String), `Market.GoodQuantity` Array(Int32), `Market.GoodPrice` Array(Int64), IslandID FixedString(16)) ENGINE = CollapsingMergeTree(Sign) PARTITION BY toYYYYMM(StartDate) SAMPLE BY intHash32(UserID) ORDER BY (CounterID, StartDate, intHash32(UserID), VisitID); clickhouse-client --max_insert_block_size 100000 --query &quot;INSERT INTO test.hits FORMAT TSV&quot; &lt; hits_v1.tsv clickhouse-client --max_insert_block_size 100000 --query &quot;INSERT INTO test.visits FORMAT TSV&quot; &lt; visits_v1.tsv  "},{"title":"Creating Pull Request​","type":1,"pageTitle":"Getting Started Guide for Building ClickHouse","url":"docs/en/development/developer-instruction#creating-pull-request","content":"Navigate to your fork repository in GitHub’s UI. If you have been developing in a branch, you need to select that branch. There will be a “Pull request” button located on the screen. In essence, this means “create a request for accepting my changes into the main repository”. A pull request can be created even if the work is not completed yet. In this case please put the word “WIP” (work in progress) at the beginning of the title, it can be changed later. This is useful for cooperative reviewing and discussion of changes as well as for running all of the available tests. It is important that you provide a brief description of your changes, it will later be used for generating release changelogs. Testing will commence as soon as ClickHouse employees label your PR with a tag “can be tested”. The results of some first checks (e.g. code style) will come in within several minutes. Build check results will arrive within half an hour. And the main set of tests will report itself within an hour. The system will prepare ClickHouse binary builds for your pull request individually. To retrieve these builds click the “Details” link next to “ClickHouse build check” entry in the list of checks. There you will find direct links to the built .deb packages of ClickHouse which you can deploy even on your production servers (if you have no fear). Most probably some of the builds will fail at first times. This is due to the fact that we check builds both with gcc as well as with clang, with almost all of existing warnings (always with the -Werror flag) enabled for clang. On that same page, you can find all of the build logs so that you do not have to build ClickHouse in all of the possible ways. "},{"title":"Atomic","type":0,"sectionRef":"#","url":"docs/en/engines/database-engines/atomic","content":"","keywords":""},{"title":"Creating a Database​","type":1,"pageTitle":"Atomic","url":"docs/en/engines/database-engines/atomic#creating-a-database","content":"CREATE DATABASE test [ENGINE = Atomic];  "},{"title":"Specifics and recommendations​","type":1,"pageTitle":"Atomic","url":"docs/en/engines/database-engines/atomic#specifics-and-recommendations","content":""},{"title":"Table UUID​","type":1,"pageTitle":"Atomic","url":"docs/en/engines/database-engines/atomic#table-uuid","content":"All tables in database Atomic have persistent UUID and store data in directory /clickhouse_path/store/xxx/xxxyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy/, where xxxyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy is UUID of the table. Usually, the UUID is generated automatically, but the user can also explicitly specify the UUID in the same way when creating the table (this is not recommended). For example: CREATE TABLE name UUID '28f1c61c-2970-457a-bffe-454156ddcfef' (n UInt64) ENGINE = ...;  note You can use the show_table_uuid_in_table_create_query_if_not_nil setting to display the UUID with the SHOW CREATE query. "},{"title":"RENAME TABLE​","type":1,"pageTitle":"Atomic","url":"docs/en/engines/database-engines/atomic#rename-table","content":"RENAME queries are performed without changing the UUID or moving table data. These queries do not wait for the completion of queries using the table and are executed instantly. "},{"title":"DROP/DETACH TABLE​","type":1,"pageTitle":"Atomic","url":"docs/en/engines/database-engines/atomic#drop-detach-table","content":"On DROP TABLE no data is removed, database Atomic just marks table as dropped by moving metadata to /clickhouse_path/metadata_dropped/ and notifies background thread. Delay before final table data deletion is specified by the database_atomic_delay_before_drop_table_sec setting. You can specify synchronous mode using SYNC modifier. Use the database_atomic_wait_for_drop_and_detach_synchronously setting to do this. In this case DROP waits for running SELECT, INSERT and other queries which are using the table to finish. Table will be actually removed when it's not in use. "},{"title":"EXCHANGE TABLES/DICTIONARIES​","type":1,"pageTitle":"Atomic","url":"docs/en/engines/database-engines/atomic#exchange-tables","content":"EXCHANGE query swaps tables or dictionaries atomically. For instance, instead of this non-atomic operation: RENAME TABLE new_table TO tmp, old_table TO new_table, tmp TO old_table;  you can use one atomic query: EXCHANGE TABLES new_table AND old_table;  "},{"title":"ReplicatedMergeTree in Atomic Database​","type":1,"pageTitle":"Atomic","url":"docs/en/engines/database-engines/atomic#replicatedmergetree-in-atomic-database","content":"For ReplicatedMergeTree tables, it is recommended not to specify engine parameters - path in ZooKeeper and replica name. In this case, configuration parameters default_replica_path and default_replica_name will be used. If you want to specify engine parameters explicitly, it is recommended to use {uuid} macros. This is useful so that unique paths are automatically generated for each table in ZooKeeper. "},{"title":"See Also​","type":1,"pageTitle":"Atomic","url":"docs/en/engines/database-engines/atomic#see-also","content":"system.databases system table "},{"title":"ClickHouse Testing","type":0,"sectionRef":"#","url":"docs/en/development/tests","content":"","keywords":""},{"title":"Functional Tests​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#functional-tests","content":"Functional tests are the most simple and convenient to use. Most of ClickHouse features can be tested with functional tests and they are mandatory to use for every change in ClickHouse code that can be tested that way. Each functional test sends one or multiple queries to the running ClickHouse server and compares the result with reference. Tests are located in queries directory. There are two subdirectories: stateless and stateful. Stateless tests run queries without any preloaded test data - they often create small synthetic datasets on the fly, within the test itself. Stateful tests require preloaded test data from CLickHouse and it is available to general public. Each test can be one of two types: .sql and .sh. .sql test is the simple SQL script that is piped to clickhouse-client --multiquery --testmode. .sh test is a script that is run by itself. SQL tests are generally preferable to .sh tests. You should use .sh tests only when you have to test some feature that cannot be exercised from pure SQL, such as piping some input data into clickhouse-client or testing clickhouse-local. "},{"title":"Running a Test Locally​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#functional-test-locally","content":"Start the ClickHouse server locally, listening on the default port (9000). To run, for example, the test 01428_hash_set_nan_key, change to the repository folder and run the following command: PATH=$PATH:&lt;path to clickhouse-client&gt; tests/clickhouse-test 01428_hash_set_nan_key  For more options, see tests/clickhouse-test --help. You can simply run all tests or run subset of tests filtered by substring in test name: ./clickhouse-test substring. There are also options to run tests in parallel or in randomized order. "},{"title":"Adding a New Test​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#adding-a-new-test","content":"To add new test, create a .sql or .sh file in queries/0_stateless directory, check it manually and then generate .reference file in the following way: clickhouse-client -n --testmode &lt; 00000_test.sql &gt; 00000_test.reference or ./00000_test.sh &gt; ./00000_test.reference. Tests should use (create, drop, etc) only tables in test database that is assumed to be created beforehand; also tests can use temporary tables. "},{"title":"Choosing the Test Name​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#choosing-the-test-name","content":"The name of the test starts with a five-digit prefix followed by a descriptive name, such as 00422_hash_function_constexpr.sql. To choose the prefix, find the largest prefix already present in the directory, and increment it by one. In the meantime, some other tests might be added with the same numeric prefix, but this is OK and does not lead to any problems, you don't have to change it later. Some tests are marked with zookeeper, shard or long in their names. zookeeper is for tests that are using ZooKeeper. shard is for tests that requires server to listen 127.0.0.*; distributed or global have the same meaning. long is for tests that run slightly longer that one second. You can disable these groups of tests using --no-zookeeper, --no-shard and --no-long options, respectively. Make sure to add a proper prefix to your test name if it needs ZooKeeper or distributed queries. "},{"title":"Checking for an Error that Must Occur​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#checking-for-an-error-that-must-occur","content":"Sometimes you want to test that a server error occurs for an incorrect query. We support special annotations for this in SQL tests, in the following form: select x; -- { serverError 49 }  This test ensures that the server returns an error with code 49 about unknown column x. If there is no error, or the error is different, the test will fail. If you want to ensure that an error occurs on the client side, use clientError annotation instead. Do not check for a particular wording of error message, it may change in the future, and the test will needlessly break. Check only the error code. If the existing error code is not precise enough for your needs, consider adding a new one. "},{"title":"Testing a Distributed Query​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#testing-a-distributed-query","content":"If you want to use distributed queries in functional tests, you can leverage remote table function with 127.0.0.{1..2} addresses for the server to query itself; or you can use predefined test clusters in server configuration file like test_shard_localhost. Remember to add the words shard or distributed to the test name, so that it is run in CI in correct configurations, where the server is configured to support distributed queries. "},{"title":"Known Bugs​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#known-bugs","content":"If we know some bugs that can be easily reproduced by functional tests, we place prepared functional tests in tests/queries/bugs directory. These tests will be moved to tests/queries/0_stateless when bugs are fixed. "},{"title":"Integration Tests​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#integration-tests","content":"Integration tests allow testing ClickHouse in clustered configuration and ClickHouse interaction with other servers like MySQL, Postgres, MongoDB. They are useful to emulate network splits, packet drops, etc. These tests are run under Docker and create multiple containers with various software. See tests/integration/README.md on how to run these tests. Note that integration of ClickHouse with third-party drivers is not tested. Also, we currently do not have integration tests with our JDBC and ODBC drivers. "},{"title":"Unit Tests​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#unit-tests","content":"Unit tests are useful when you want to test not the ClickHouse as a whole, but a single isolated library or class. You can enable or disable build of tests with ENABLE_TESTS CMake option. Unit tests (and other test programs) are located in tests subdirectories across the code. To run unit tests, type ninja test. Some tests use gtest, but some are just programs that return non-zero exit code on test failure. It’s not necessary to have unit tests if the code is already covered by functional tests (and functional tests are usually much more simple to use). You can run individual gtest checks by calling the executable directly, for example: $ ./src/unit_tests_dbms --gtest_filter=LocalAddress*  "},{"title":"Performance Tests​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#performance-tests","content":"Performance tests allow to measure and compare performance of some isolated part of ClickHouse on synthetic queries. Tests are located at tests/performance. Each test is represented by .xml file with description of test case. Tests are run with docker/tests/performance-comparison tool . See the readme file for invocation. Each test run one or multiple queries (possibly with combinations of parameters) in a loop. Some tests can contain preconditions on preloaded test dataset. If you want to improve performance of ClickHouse in some scenario, and if improvements can be observed on simple queries, it is highly recommended to write a performance test. It always makes sense to use perf top or other perf tools during your tests. "},{"title":"Test Tools and Scripts​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#test-tools-and-scripts","content":"Some programs in tests directory are not prepared tests, but are test tools. For example, for Lexer there is a tool src/Parsers/tests/lexer that just do tokenization of stdin and writes colorized result to stdout. You can use these kind of tools as a code examples and for exploration and manual testing. "},{"title":"Miscellaneous Tests​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#miscellaneous-tests","content":"There are tests for machine learned models in tests/external_models. These tests are not updated and must be transferred to integration tests. There is separate test for quorum inserts. This test run ClickHouse cluster on separate servers and emulate various failure cases: network split, packet drop (between ClickHouse nodes, between ClickHouse and ZooKeeper, between ClickHouse server and client, etc.), kill -9, kill -STOP and kill -CONT , like Jepsen. Then the test checks that all acknowledged inserts was written and all rejected inserts was not. Quorum test was written by separate team before ClickHouse was open-sourced. This team no longer work with ClickHouse. Test was accidentally written in Java. For these reasons, quorum test must be rewritten and moved to integration tests. "},{"title":"Manual Testing​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#manual-testing","content":"When you develop a new feature, it is reasonable to also test it manually. You can do it with the following steps: Build ClickHouse. Run ClickHouse from the terminal: change directory to programs/clickhouse-server and run it with ./clickhouse-server. It will use configuration (config.xml, users.xml and files within config.d and users.d directories) from the current directory by default. To connect to ClickHouse server, run programs/clickhouse-client/clickhouse-client. Note that all clickhouse tools (server, client, etc) are just symlinks to a single binary named clickhouse. You can find this binary at programs/clickhouse. All tools can also be invoked as clickhouse tool instead of clickhouse-tool. Alternatively you can install ClickHouse package: either stable release from ClickHouse repository or you can build package for yourself with ./release in ClickHouse sources root. Then start the server with sudo clickhouse start (or stop to stop the server). Look for logs at /etc/clickhouse-server/clickhouse-server.log. When ClickHouse is already installed on your system, you can build a new clickhouse binary and replace the existing binary: $ sudo clickhouse stop $ sudo cp ./clickhouse /usr/bin/ $ sudo clickhouse start  Also you can stop system clickhouse-server and run your own with the same configuration but with logging to terminal: $ sudo clickhouse stop $ sudo -u clickhouse /usr/bin/clickhouse server --config-file /etc/clickhouse-server/config.xml  Example with gdb: $ sudo -u clickhouse gdb --args /usr/bin/clickhouse server --config-file /etc/clickhouse-server/config.xml  If the system clickhouse-server is already running and you do not want to stop it, you can change port numbers in your config.xml (or override them in a file in config.d directory), provide appropriate data path, and run it. clickhouse binary has almost no dependencies and works across wide range of Linux distributions. To quick and dirty test your changes on a server, you can simply scp your fresh built clickhouse binary to your server and then run it as in examples above. "},{"title":"Build Tests​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#build-tests","content":"Build tests allow to check that build is not broken on various alternative configurations and on some foreign systems. These tests are automated as well. Examples: cross-compile for Darwin x86_64 (Mac OS X)cross-compile for FreeBSD x86_64cross-compile for Linux AArch64build on Ubuntu with libraries from system packages (discouraged)build with shared linking of libraries (discouraged) For example, build with system packages is bad practice, because we cannot guarantee what exact version of packages a system will have. But this is really needed by Debian maintainers. For this reason we at least have to support this variant of build. Another example: shared linking is a common source of trouble, but it is needed for some enthusiasts. Though we cannot run all tests on all variant of builds, we want to check at least that various build variants are not broken. For this purpose we use build tests. We also test that there are no translation units that are too long to compile or require too much RAM. We also test that there are no too large stack frames. "},{"title":"Testing for Protocol Compatibility​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#testing-for-protocol-compatibility","content":"When we extend ClickHouse network protocol, we test manually that old clickhouse-client works with new clickhouse-server and new clickhouse-client works with old clickhouse-server (simply by running binaries from corresponding packages). We also test some cases automatically with integrational tests: if data written by old version of ClickHouse can be successfully read by the new version;do distributed queries work in a cluster with different ClickHouse versions. "},{"title":"Help from the Compiler​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#help-from-the-compiler","content":"Main ClickHouse code (that is located in dbms directory) is built with -Wall -Wextra -Werror and with some additional enabled warnings. Although these options are not enabled for third-party libraries. Clang has even more useful warnings - you can look for them with -Weverything and pick something to default build. For production builds, clang is used, but we also test make gcc builds. For development, clang is usually more convenient to use. You can build on your own machine with debug mode (to save battery of your laptop), but please note that compiler is able to generate more warnings with -O3 due to better control flow and inter-procedure analysis. When building with clang in debug mode, debug version of libc++ is used that allows to catch more errors at runtime. "},{"title":"Sanitizers​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#sanitizers","content":""},{"title":"Address sanitizer​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#address-sanitizer","content":"We run functional, integration, stress and unit tests under ASan on per-commit basis. "},{"title":"Thread sanitizer​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#thread-sanitizer","content":"We run functional, integration, stress and unit tests under TSan on per-commit basis. "},{"title":"Memory sanitizer​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#memory-sanitizer","content":"We run functional, integration, stress and unit tests under MSan on per-commit basis. "},{"title":"Undefined behaviour sanitizer​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#undefined-behaviour-sanitizer","content":"We run functional, integration, stress and unit tests under UBSan on per-commit basis. The code of some third-party libraries is not sanitized for UB. "},{"title":"Valgrind (Memcheck)​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#valgrind-memcheck","content":"We used to run functional tests under Valgrind overnight, but don't do it anymore. It takes multiple hours. Currently there is one known false positive in re2 library, see this article. "},{"title":"Fuzzing​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#fuzzing","content":"ClickHouse fuzzing is implemented both using libFuzzer and random SQL queries. All the fuzz testing should be performed with sanitizers (Address and Undefined). LibFuzzer is used for isolated fuzz testing of library code. Fuzzers are implemented as part of test code and have “_fuzzer” name postfixes. Fuzzer example can be found at src/Parsers/tests/lexer_fuzzer.cpp. LibFuzzer-specific configs, dictionaries and corpus are stored at tests/fuzz. We encourage you to write fuzz tests for every functionality that handles user input. Fuzzers are not built by default. To build fuzzers both -DENABLE_FUZZING=1 and -DENABLE_TESTS=1 options should be set. We recommend to disable Jemalloc while building fuzzers. Configuration used to integrate ClickHouse fuzzing to Google OSS-Fuzz can be found at docker/fuzz. We also use simple fuzz test to generate random SQL queries and to check that the server does not die executing them. You can find it in 00746_sql_fuzzy.pl. This test should be run continuously (overnight and longer). We also use sophisticated AST-based query fuzzer that is able to find huge amount of corner cases. It does random permutations and substitutions in queries AST. It remembers AST nodes from previous tests to use them for fuzzing of subsequent tests while processing them in random order. You can learn more about this fuzzer in this blog article. "},{"title":"Stress test​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#stress-test","content":"Stress tests are another case of fuzzing. It runs all functional tests in parallel in random order with a single server. Results of the tests are not checked. It is checked that: server does not crash, no debug or sanitizer traps are triggered;there are no deadlocks;the database structure is consistent;server can successfully stop after the test and start again without exceptions. There are five variants (Debug, ASan, TSan, MSan, UBSan). "},{"title":"Thread Fuzzer​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#thread-fuzzer","content":"Thread Fuzzer (please don't mix up with Thread Sanitizer) is another kind of fuzzing that allows to randomize thread order of execution. It helps to find even more special cases. "},{"title":"Security Audit​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#security-audit","content":"Our Security Team did some basic overview of ClickHouse capabilities from the security standpoint. "},{"title":"Static Analyzers​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#static-analyzers","content":"We run clang-tidy on per-commit basis. clang-static-analyzer checks are also enabled. clang-tidy is also used for some style checks. We have evaluated clang-tidy, Coverity, cppcheck, PVS-Studio, tscancode, CodeQL. You will find instructions for usage in tests/instructions/ directory. If you use CLion as an IDE, you can leverage some clang-tidy checks out of the box. We also use shellcheck for static analysis of shell scripts. "},{"title":"Hardening​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#hardening","content":"In debug build we are using custom allocator that does ASLR of user-level allocations. We also manually protect memory regions that are expected to be readonly after allocation. In debug build we also involve a customization of libc that ensures that no &quot;harmful&quot; (obsolete, insecure, not thread-safe) functions are called. Debug assertions are used extensively. In debug build, if exception with &quot;logical error&quot; code (implies a bug) is being thrown, the program is terminated prematurally. It allows to use exceptions in release build but make it an assertion in debug build. Debug version of jemalloc is used for debug builds. Debug version of libc++ is used for debug builds. "},{"title":"Runtime Integrity Checks​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#runtime-integrity-checks","content":"Data stored on disk is checksummed. Data in MergeTree tables is checksummed in three ways simultaneously* (compressed data blocks, uncompressed data blocks, the total checksum across blocks). Data transferred over network between client and server or between servers is also checksummed. Replication ensures bit-identical data on replicas. It is required to protect from faulty hardware (bit rot on storage media, bit flips in RAM on server, bit flips in RAM of network controller, bit flips in RAM of network switch, bit flips in RAM of client, bit flips on the wire). Note that bit flips are common and likely to occur even for ECC RAM and in presense of TCP checksums (if you manage to run thousands of servers processing petabytes of data each day). See the video (russian). ClickHouse provides diagnostics that will help ops engineers to find faulty hardware. * and it is not slow. "},{"title":"Code Style​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#code-style","content":"Code style rules are described here. To check for some common style violations, you can use utils/check-style script. To force proper style of your code, you can use clang-format. File .clang-format is located at the sources root. It mostly corresponding with our actual code style. But it’s not recommended to apply clang-format to existing files because it makes formatting worse. You can use clang-format-diff tool that you can find in clang source repository. Alternatively you can try uncrustify tool to reformat your code. Configuration is in uncrustify.cfg in the sources root. It is less tested than clang-format. CLion has its own code formatter that has to be tuned for our code style. We also use codespell to find typos in code. It is automated as well. "},{"title":"Test Coverage​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#test-coverage","content":"We also track test coverage but only for functional tests and only for clickhouse-server. It is performed on daily basis. "},{"title":"Tests for Tests​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#tests-for-tests","content":"There is automated check for flaky tests. It runs all new tests 100 times (for functional tests) or 10 times (for integration tests). If at least single time the test failed, it is considered flaky. "},{"title":"Testflows​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#testflows","content":"Testflows is an enterprise-grade open-source testing framework, which is used to test a subset of ClickHouse. "},{"title":"Test Automation​","type":1,"pageTitle":"ClickHouse Testing","url":"docs/en/development/tests#test-automation","content":"We run tests with GitHub Actions. Build jobs and tests are run in Sandbox on per commit basis. Resulting packages and test results are published in GitHub and can be downloaded by direct links. Artifacts are stored for several months. When you send a pull request on GitHub, we tag it as “can be tested” and our CI system will build ClickHouse packages (release, debug, with address sanitizer, etc) for you. We do not use Travis CI due to the limit on time and computational power. We do not use Jenkins. It was used before and now we are happy we are not using Jenkins. Original article "},{"title":"MySQL","type":0,"sectionRef":"#","url":"docs/en/engines/database-engines/mysql","content":"","keywords":""},{"title":"Creating a Database​","type":1,"pageTitle":"MySQL","url":"docs/en/engines/database-engines/mysql#creating-a-database","content":"CREATE DATABASE [IF NOT EXISTS] db_name [ON CLUSTER cluster] ENGINE = MySQL('host:port', ['database' | database], 'user', 'password')  Engine Parameters host:port — MySQL server address.database — Remote database name.user — MySQL user.password — User password. "},{"title":"Data Types Support​","type":1,"pageTitle":"MySQL","url":"docs/en/engines/database-engines/mysql#data_types-support","content":"MySQL\tClickHouseUNSIGNED TINYINT\tUInt8 TINYINT\tInt8 UNSIGNED SMALLINT\tUInt16 SMALLINT\tInt16 UNSIGNED INT, UNSIGNED MEDIUMINT\tUInt32 INT, MEDIUMINT\tInt32 UNSIGNED BIGINT\tUInt64 BIGINT\tInt64 FLOAT\tFloat32 DOUBLE\tFloat64 DATE\tDate DATETIME, TIMESTAMP\tDateTime BINARY\tFixedString All other MySQL data types are converted into String. Nullable is supported. "},{"title":"Global Variables Support​","type":1,"pageTitle":"MySQL","url":"docs/en/engines/database-engines/mysql#global-variables-support","content":"For better compatibility you may address global variables in MySQL style, as @@identifier. These variables are supported: versionmax_allowed_packet warning By now these variables are stubs and don't correspond to anything. Example: SELECT @@version;  "},{"title":"Examples of Use​","type":1,"pageTitle":"MySQL","url":"docs/en/engines/database-engines/mysql#examples-of-use","content":"Table in MySQL: mysql&gt; USE test; Database changed mysql&gt; CREATE TABLE `mysql_table` ( -&gt; `int_id` INT NOT NULL AUTO_INCREMENT, -&gt; `float` FLOAT NOT NULL, -&gt; PRIMARY KEY (`int_id`)); Query OK, 0 rows affected (0,09 sec) mysql&gt; insert into mysql_table (`int_id`, `float`) VALUES (1,2); Query OK, 1 row affected (0,00 sec) mysql&gt; select * from mysql_table; +------+-----+ | int_id | value | +------+-----+ | 1 | 2 | +------+-----+ 1 row in set (0,00 sec)  Database in ClickHouse, exchanging data with the MySQL server: CREATE DATABASE mysql_db ENGINE = MySQL('localhost:3306', 'test', 'my_user', 'user_password')  SHOW DATABASES  ┌─name─────┐ │ default │ │ mysql_db │ │ system │ └──────────┘  SHOW TABLES FROM mysql_db  ┌─name─────────┐ │ mysql_table │ └──────────────┘  SELECT * FROM mysql_db.mysql_table  ┌─int_id─┬─value─┐ │ 1 │ 2 │ └────────┴───────┘  INSERT INTO mysql_db.mysql_table VALUES (3,4)  SELECT * FROM mysql_db.mysql_table  ┌─int_id─┬─value─┐ │ 1 │ 2 │ │ 3 │ 4 │ └────────┴───────┘  Original article "},{"title":"PostgreSQL","type":0,"sectionRef":"#","url":"docs/en/engines/database-engines/postgresql","content":"","keywords":""},{"title":"Creating a Database​","type":1,"pageTitle":"PostgreSQL","url":"docs/en/engines/database-engines/postgresql#creating-a-database","content":"CREATE DATABASE test_database ENGINE = PostgreSQL('host:port', 'database', 'user', 'password'[, `schema`, `use_table_cache`]);  Engine Parameters host:port — PostgreSQL server address.database — Remote database name.user — PostgreSQL user.password — User password.schema — PostgreSQL schema.use_table_cache — Defines if the database table structure is cached or not. Optional. Default value: 0. "},{"title":"Data Types Support​","type":1,"pageTitle":"PostgreSQL","url":"docs/en/engines/database-engines/postgresql#data_types-support","content":"PostgerSQL\tClickHouseDATE\tDate TIMESTAMP\tDateTime REAL\tFloat32 DOUBLE\tFloat64 DECIMAL, NUMERIC\tDecimal SMALLINT\tInt16 INTEGER\tInt32 BIGINT\tInt64 SERIAL\tUInt32 BIGSERIAL\tUInt64 TEXT, CHAR\tString INTEGER\tNullable(Int32) ARRAY\tArray "},{"title":"Examples of Use​","type":1,"pageTitle":"PostgreSQL","url":"docs/en/engines/database-engines/postgresql#examples-of-use","content":"Database in ClickHouse, exchanging data with the PostgreSQL server: CREATE DATABASE test_database ENGINE = PostgreSQL('postgres1:5432', 'test_database', 'postgres', 'mysecretpassword', 1);  SHOW DATABASES;  ┌─name──────────┐ │ default │ │ test_database │ │ system │ └───────────────┘  SHOW TABLES FROM test_database;  ┌─name───────┐ │ test_table │ └────────────┘  Reading data from the PostgreSQL table: SELECT * FROM test_database.test_table;  ┌─id─┬─value─┐ │ 1 │ 2 │ └────┴───────┘  Writing data to the PostgreSQL table: INSERT INTO test_database.test_table VALUES (3,4); SELECT * FROM test_database.test_table;  ┌─int_id─┬─value─┐ │ 1 │ 2 │ │ 3 │ 4 │ └────────┴───────┘  Consider the table structure was modified in PostgreSQL: postgre&gt; ALTER TABLE test_table ADD COLUMN data Text  As the use_table_cache parameter was set to 1 when the database was created, the table structure in ClickHouse was cached and therefore not modified: DESCRIBE TABLE test_database.test_table;  ┌─name───┬─type──────────────┐ │ id │ Nullable(Integer) │ │ value │ Nullable(Integer) │ └────────┴───────────────────┘  After detaching the table and attaching it again, the structure was updated: DETACH TABLE test_database.test_table; ATTACH TABLE test_database.test_table; DESCRIBE TABLE test_database.test_table;  ┌─name───┬─type──────────────┐ │ id │ Nullable(Integer) │ │ value │ Nullable(Integer) │ │ data │ Nullable(String) │ └────────┴───────────────────┘  Original article "},{"title":"[experimental] MaterializedMySQL","type":0,"sectionRef":"#","url":"docs/en/engines/database-engines/materialized-mysql","content":"","keywords":""},{"title":"Creating a Database​","type":1,"pageTitle":"[experimental] MaterializedMySQL","url":"docs/en/engines/database-engines/materialized-mysql#creating-a-database","content":"CREATE DATABASE [IF NOT EXISTS] db_name [ON CLUSTER cluster] ENGINE = MaterializedMySQL('host:port', ['database' | database], 'user', 'password') [SETTINGS ...] [TABLE OVERRIDE table1 (...), TABLE OVERRIDE table2 (...)]  Engine Parameters host:port — MySQL server endpoint.database — MySQL database name.user — MySQL user.password — User password. Engine Settings max_rows_in_buffer — Maximum number of rows that data is allowed to cache in memory (for single table and the cache data unable to query). When this number is exceeded, the data will be materialized. Default: 65 505.max_bytes_in_buffer — Maximum number of bytes that data is allowed to cache in memory (for single table and the cache data unable to query). When this number is exceeded, the data will be materialized. Default: 1 048 576.max_flush_data_time — Maximum number of milliseconds that data is allowed to cache in memory (for database and the cache data unable to query). When this time is exceeded, the data will be materialized. Default: 1000.max_wait_time_when_mysql_unavailable — Retry interval when MySQL is not available (milliseconds). Negative value disables retry. Default: 1000.allows_query_when_mysql_lost — Allows to query a materialized table when MySQL is lost. Default: 0 (false).materialized_mysql_tables_list — a comma-separated list of mysql database tables, which will be replicated by MaterializedMySQL database engine. Default value: empty list — means whole tables will be replicated. CREATE DATABASE mysql ENGINE = MaterializedMySQL('localhost:3306', 'db', 'user', '***') SETTINGS allows_query_when_mysql_lost=true, max_wait_time_when_mysql_unavailable=10000;  Settings on MySQL-server Side For the correct work of MaterializedMySQL, there are few mandatory MySQL-side configuration settings that must be set: default_authentication_plugin = mysql_native_password since MaterializedMySQL can only authorize with this method.gtid_mode = on since GTID based logging is a mandatory for providing correct MaterializedMySQL replication. note While turning on gtid_mode you should also specify enforce_gtid_consistency = on. "},{"title":"Virtual Columns​","type":1,"pageTitle":"[experimental] MaterializedMySQL","url":"docs/en/engines/database-engines/materialized-mysql#virtual-columns","content":"When working with the MaterializedMySQL database engine, ReplacingMergeTree tables are used with virtual _sign and _version columns. _version — Transaction counter. Type UInt64._sign — Deletion mark. Type Int8. Possible values: 1 — Row is not deleted,-1 — Row is deleted. "},{"title":"Data Types Support​","type":1,"pageTitle":"[experimental] MaterializedMySQL","url":"docs/en/engines/database-engines/materialized-mysql#data_types-support","content":"MySQL\tClickHouseTINY\tInt8 SHORT\tInt16 INT24\tInt32 LONG\tUInt32 LONGLONG\tUInt64 FLOAT\tFloat32 DOUBLE\tFloat64 DECIMAL, NEWDECIMAL\tDecimal DATE, NEWDATE\tDate DATETIME, TIMESTAMP\tDateTime DATETIME2, TIMESTAMP2\tDateTime64 YEAR\tUInt16 TIME\tInt64 ENUM\tEnum STRING\tString VARCHAR, VAR_STRING\tString BLOB\tString GEOMETRY\tString BINARY\tFixedString BIT\tUInt64 SET\tUInt64 Nullable is supported. The data of TIME type in MySQL is converted to microseconds in ClickHouse. Other types are not supported. If MySQL table contains a column of such type, ClickHouse throws exception &quot;Unhandled data type&quot; and stops replication. "},{"title":"Specifics and Recommendations​","type":1,"pageTitle":"[experimental] MaterializedMySQL","url":"docs/en/engines/database-engines/materialized-mysql#specifics-and-recommendations","content":""},{"title":"Compatibility Restrictions​","type":1,"pageTitle":"[experimental] MaterializedMySQL","url":"docs/en/engines/database-engines/materialized-mysql#compatibility-restrictions","content":"Apart of the data types limitations there are few restrictions comparing to MySQL databases, that should be resolved before replication will be possible: Each table in MySQL should contain PRIMARY KEY. Replication for tables, those are containing rows with ENUM field values out of range (specified in ENUM signature) will not work. "},{"title":"DDL Queries​","type":1,"pageTitle":"[experimental] MaterializedMySQL","url":"docs/en/engines/database-engines/materialized-mysql#ddl-queries","content":"MySQL DDL queries are converted into the corresponding ClickHouse DDL queries (ALTER, CREATE, DROP, RENAME). If ClickHouse cannot parse some DDL query, the query is ignored. "},{"title":"Data Replication​","type":1,"pageTitle":"[experimental] MaterializedMySQL","url":"docs/en/engines/database-engines/materialized-mysql#data-replication","content":"MaterializedMySQL does not support direct INSERT, DELETE and UPDATE queries. However, they are supported in terms of data replication: MySQL INSERT query is converted into INSERT with _sign=1. MySQL DELETE query is converted into INSERT with _sign=-1. MySQL UPDATE query is converted into INSERT with _sign=-1 and INSERT with _sign=1 if the primary key has been changed, orINSERT with _sign=1 if not. "},{"title":"Selecting from MaterializedMySQL Tables​","type":1,"pageTitle":"[experimental] MaterializedMySQL","url":"docs/en/engines/database-engines/materialized-mysql#select","content":"SELECT query from MaterializedMySQL tables has some specifics: If _version is not specified in the SELECT query, theFINAL modifier is used, so only rows withMAX(_version) are returned for each primary key value. If _sign is not specified in the SELECT query, WHERE _sign=1 is used by default. So the deleted rows are not included into the result set. The result includes columns comments in case they exist in MySQL database tables. "},{"title":"Index Conversion​","type":1,"pageTitle":"[experimental] MaterializedMySQL","url":"docs/en/engines/database-engines/materialized-mysql#index-conversion","content":"MySQL PRIMARY KEY and INDEX clauses are converted into ORDER BY tuples in ClickHouse tables. ClickHouse has only one physical order, which is determined by ORDER BY clause. To create a new physical order, usematerialized views. Notes Rows with _sign=-1 are not deleted physically from the tables.Cascade UPDATE/DELETE queries are not supported by the MaterializedMySQL engine, as they are not visible in the MySQL binlog.Replication can be easily broken.Manual operations on database and tables are forbidden.MaterializedMySQL is affected by the optimize_on_insertsetting. Data is merged in the corresponding table in the MaterializedMySQL database when a table in the MySQL server changes. "},{"title":"Table Overrides​","type":1,"pageTitle":"[experimental] MaterializedMySQL","url":"docs/en/engines/database-engines/materialized-mysql#table-overrides","content":"Table overrides can be used to customize the ClickHouse DDL queries, allowing you to make schema optimizations for your application. This is especially useful for controlling partitioning, which is important for the overall performance of MaterializedMySQL. These are the schema conversion manipulations you can do with table overrides for MaterializedMySQL: Modify column type. Must be compatible with the original type, or replication will fail. For example, you can modify a UInt32 column to UInt64, but you can not modify a String column to Array(String).Modify column TTL.Modify column compression codec.Add ALIAS columns.Add skipping indexesAdd projections. Note that projection optimizations are disabled when using SELECT ... FINAL (which MaterializedMySQL does by default), so their utility is limited here.INDEX ... TYPE hypothesis as [described in the v21.12 blog post]](https://clickhouse.com/blog/en/2021/clickhouse-v21.12-released/) may be more useful in this case.Modify PARTITION BYModify ORDER BYModify PRIMARY KEYAdd SAMPLE BYAdd table TTL CREATE DATABASE db_name ENGINE = MaterializedMySQL(...) [SETTINGS ...] [TABLE OVERRIDE table_name ( [COLUMNS ( [col_name [datatype] [ALIAS expr] [CODEC(...)] [TTL expr], ...] [INDEX index_name expr TYPE indextype[(...)] GRANULARITY val, ...] [PROJECTION projection_name (SELECT &lt;COLUMN LIST EXPR&gt; [GROUP BY] [ORDER BY]), ...] )] [ORDER BY expr] [PRIMARY KEY expr] [PARTITION BY expr] [SAMPLE BY expr] [TTL expr] ), ...]  Example: CREATE DATABASE db_name ENGINE = MaterializedMySQL(...) TABLE OVERRIDE table1 ( COLUMNS ( userid UUID, category LowCardinality(String), timestamp DateTime CODEC(Delta, Default) ) PARTITION BY toYear(timestamp) ), TABLE OVERRIDE table2 ( COLUMNS ( client_ip String TTL created + INTERVAL 72 HOUR ) SAMPLE BY ip_hash )  The COLUMNS list is sparse; existing columns are modified as specified, extra ALIAS columns are added. It is not possible to add ordinary or MATERIALIZED columns. Modified columns with a different type must be assignable from the original type. There is currently no validation of this or similar issues when the CREATE DATABASE query executes, so extra care needs to be taken. You may specify overrides for tables that do not exist yet. warning It is easy to break replication with table overrides if not used with care. For example: If an ALIAS column is added with a table override, and a column with the same name is later added to the source MySQL table, the converted ALTER TABLE query in ClickHouse will fail and replication stops.It is currently possible to add overrides that reference nullable columns where not-nullable are required, such as inORDER BY or PARTITION BY. This will cause CREATE TABLE queries that will fail, also causing replication to stop. "},{"title":"Examples of Use​","type":1,"pageTitle":"[experimental] MaterializedMySQL","url":"docs/en/engines/database-engines/materialized-mysql#examples-of-use","content":"Queries in MySQL: mysql&gt; CREATE DATABASE db; mysql&gt; CREATE TABLE db.test (a INT PRIMARY KEY, b INT); mysql&gt; INSERT INTO db.test VALUES (1, 11), (2, 22); mysql&gt; DELETE FROM db.test WHERE a=1; mysql&gt; ALTER TABLE db.test ADD COLUMN c VARCHAR(16); mysql&gt; UPDATE db.test SET c='Wow!', b=222; mysql&gt; SELECT * FROM test;  ┌─a─┬───b─┬─c────┐ │ 2 │ 222 │ Wow! │ └───┴─────┴──────┘  Database in ClickHouse, exchanging data with the MySQL server: The database and the table created: CREATE DATABASE mysql ENGINE = MaterializedMySQL('localhost:3306', 'db', 'user', '***'); SHOW TABLES FROM mysql;  ┌─name─┐ │ test │ └──────┘  After inserting data: SELECT * FROM mysql.test;  ┌─a─┬──b─┐ │ 1 │ 11 │ │ 2 │ 22 │ └───┴────┘  After deleting data, adding the column and updating: SELECT * FROM mysql.test;  ┌─a─┬───b─┬─c────┐ │ 2 │ 222 │ Wow! │ └───┴─────┴──────┘  Original article "},{"title":"SQLite","type":0,"sectionRef":"#","url":"docs/en/engines/database-engines/sqlite","content":"","keywords":""},{"title":"Creating a Database​","type":1,"pageTitle":"SQLite","url":"docs/en/engines/database-engines/sqlite#creating-a-database","content":" CREATE DATABASE sqlite_database ENGINE = SQLite('db_path')  Engine Parameters db_path — Path to a file with SQLite database. "},{"title":"[experimental] MaterializedPostgreSQL","type":0,"sectionRef":"#","url":"docs/en/engines/database-engines/materialized-postgresql","content":"","keywords":""},{"title":"Creating a Database​","type":1,"pageTitle":"[experimental] MaterializedPostgreSQL","url":"docs/en/engines/database-engines/materialized-postgresql#creating-a-database","content":"CREATE DATABASE [IF NOT EXISTS] db_name [ON CLUSTER cluster] ENGINE = MaterializedPostgreSQL('host:port', 'database', 'user', 'password') [SETTINGS ...]  Engine Parameters host:port — PostgreSQL server endpoint.database — PostgreSQL database name.user — PostgreSQL user.password — User password. "},{"title":"Example of Use​","type":1,"pageTitle":"[experimental] MaterializedPostgreSQL","url":"docs/en/engines/database-engines/materialized-postgresql#example-of-use","content":"CREATE DATABASE postgres_db ENGINE = MaterializedPostgreSQL('postgres1:5432', 'postgres_database', 'postgres_user', 'postgres_password'); SHOW TABLES FROM postgres_db; ┌─name───┐ │ table1 │ └────────┘ SELECT * FROM postgresql_db.postgres_table;  "},{"title":"Dynamically adding new tables to replication​","type":1,"pageTitle":"[experimental] MaterializedPostgreSQL","url":"docs/en/engines/database-engines/materialized-postgresql#dynamically-adding-table-to-replication","content":"After MaterializedPostgreSQL database is created, it does not automatically detect new tables in according PostgreSQL database. Such tables can be added manually: ATTACH TABLE postgres_database.new_table;  warning Before version 22.1, adding a table to replication left an unremoved temporary replication slot (named {db_name}_ch_replication_slot_tmp). If attaching tables in ClickHouse version before 22.1, make sure to delete it manually (SELECT pg_drop_replication_slot('{db_name}_ch_replication_slot_tmp')). Otherwise disk usage will grow. This issue is fixed in 22.1. "},{"title":"Dynamically removing tables from replication​","type":1,"pageTitle":"[experimental] MaterializedPostgreSQL","url":"docs/en/engines/database-engines/materialized-postgresql#dynamically-removing-table-from-replication","content":"It is possible to remove specific tables from replication: DETACH TABLE postgres_database.table_to_remove;  "},{"title":"PostgreSQL schema​","type":1,"pageTitle":"[experimental] MaterializedPostgreSQL","url":"docs/en/engines/database-engines/materialized-postgresql#schema","content":"PostgreSQL schema can be configured in 3 ways (starting from version 21.12). One schema for one MaterializedPostgreSQL database engine. Requires to use setting materialized_postgresql_schema. Tables are accessed via table name only: CREATE DATABASE postgres_database ENGINE = MaterializedPostgreSQL('postgres1:5432', 'postgres_database', 'postgres_user', 'postgres_password') SETTINGS materialized_postgresql_schema = 'postgres_schema'; SELECT * FROM postgres_database.table1;  Any number of schemas with specified set of tables for one MaterializedPostgreSQL database engine. Requires to use setting materialized_postgresql_tables_list. Each table is written along with its schema. Tables are accessed via schema name and table name at the same time: CREATE DATABASE database1 ENGINE = MaterializedPostgreSQL('postgres1:5432', 'postgres_database', 'postgres_user', 'postgres_password') SETTINGS materialized_postgresql_tables_list = 'schema1.table1,schema2.table2,schema1.table3', materialized_postgresql_tables_list_with_schema = 1; SELECT * FROM database1.`schema1.table1`; SELECT * FROM database1.`schema2.table2`;  But in this case all tables in materialized_postgresql_tables_list must be written with its schema name. Requires materialized_postgresql_tables_list_with_schema = 1. Warning: for this case dots in table name are not allowed. Any number of schemas with full set of tables for one MaterializedPostgreSQL database engine. Requires to use setting materialized_postgresql_schema_list. CREATE DATABASE database1 ENGINE = MaterializedPostgreSQL('postgres1:5432', 'postgres_database', 'postgres_user', 'postgres_password') SETTINGS materialized_postgresql_schema_list = 'schema1,schema2,schema3'; SELECT * FROM database1.`schema1.table1`; SELECT * FROM database1.`schema1.table2`; SELECT * FROM database1.`schema2.table2`;  Warning: for this case dots in table name are not allowed. "},{"title":"Requirements​","type":1,"pageTitle":"[experimental] MaterializedPostgreSQL","url":"docs/en/engines/database-engines/materialized-postgresql#requirements","content":"The wal_level setting must have a value logical and max_replication_slots parameter must have a value at least 2 in the PostgreSQL config file. Each replicated table must have one of the following replica identity: primary key (by default) index postgres# CREATE TABLE postgres_table (a Integer NOT NULL, b Integer, c Integer NOT NULL, d Integer, e Integer NOT NULL); postgres# CREATE unique INDEX postgres_table_index on postgres_table(a, c, e); postgres# ALTER TABLE postgres_table REPLICA IDENTITY USING INDEX postgres_table_index;  The primary key is always checked first. If it is absent, then the index, defined as replica identity index, is checked. If the index is used as a replica identity, there has to be only one such index in a table. You can check what type is used for a specific table with the following command: postgres# SELECT CASE relreplident WHEN 'd' THEN 'default' WHEN 'n' THEN 'nothing' WHEN 'f' THEN 'full' WHEN 'i' THEN 'index' END AS replica_identity FROM pg_class WHERE oid = 'postgres_table'::regclass;  warning Replication of TOAST values is not supported. The default value for the data type will be used. "},{"title":"Settings​","type":1,"pageTitle":"[experimental] MaterializedPostgreSQL","url":"docs/en/engines/database-engines/materialized-postgresql#settings","content":"materialized_postgresql_tables_list {#materialized-postgresql-tables-list} Sets a comma-separated list of PostgreSQL database tables, which will be replicated via MaterializedPostgreSQL database engine. Default value: empty list — means whole PostgreSQL database will be replicated. materialized_postgresql_schema {#materialized-postgresql-schema} Default value: empty string. (Default schema is used) materialized_postgresql_schema_list {#materialized-postgresql-schema-list} Default value: empty list. (Default schema is used) materialized_postgresql_allow_automatic_update {#materialized-postgresql-allow-automatic-update} Do not use this setting before 22.1 version. Allows reloading table in the background, when schema changes are detected. DDL queries on the PostgreSQL side are not replicated via ClickHouse MaterializedPostgreSQL engine, because it is not allowed with PostgreSQL logical replication protocol, but the fact of DDL changes is detected transactionally. In this case, the default behaviour is to stop replicating those tables once DDL is detected. However, if this setting is enabled, then, instead of stopping the replication of those tables, they will be reloaded in the background via database snapshot without data losses and replication will continue for them. Possible values: 0 — The table is not automatically updated in the background, when schema changes are detected.1 — The table is automatically updated in the background, when schema changes are detected. Default value: 0. materialized_postgresql_max_block_size {#materialized-postgresql-max-block-size} Sets the number of rows collected in memory before flushing data into PostgreSQL database table. Possible values: Positive integer. Default value: 65536. materialized_postgresql_replication_slot {#materialized-postgresql-replication-slot} A user-created replication slot. Must be used together with materialized_postgresql_snapshot. materialized_postgresql_snapshot {#materialized-postgresql-snapshot} A text string identifying a snapshot, from which initial dump of PostgreSQL tables will be performed. Must be used together with materialized_postgresql_replication_slot. CREATE DATABASE database1 ENGINE = MaterializedPostgreSQL('postgres1:5432', 'postgres_database', 'postgres_user', 'postgres_password') SETTINGS materialized_postgresql_tables_list = 'table1,table2,table3'; SELECT * FROM database1.table1; The settings can be changed, if necessary, using a DDL query. But it is impossible to change the setting materialized_postgresql_tables_list. To update the list of tables in this setting use the ATTACH TABLE query. ALTER DATABASE postgres_database MODIFY SETTING materialized_postgresql_max_block_size = &lt;new_size&gt;;  "},{"title":"Notes​","type":1,"pageTitle":"[experimental] MaterializedPostgreSQL","url":"docs/en/engines/database-engines/materialized-postgresql#notes","content":""},{"title":"Failover of the logical replication slot​","type":1,"pageTitle":"[experimental] MaterializedPostgreSQL","url":"docs/en/engines/database-engines/materialized-postgresql#logical-replication-slot-failover","content":"Logical Replication Slots which exist on the primary are not available on standby replicas. So if there is a failover, new primary (the old physical standby) won’t be aware of any slots which were existing with old primary. This will lead to a broken replication from PostgreSQL. A solution to this is to manage replication slots yourself and define a permanent replication slot (some information can be found here). You'll need to pass slot name via materialized_postgresql_replication_slot setting, and it has to be exported with EXPORT SNAPSHOT option. The snapshot identifier needs to be passed via materialized_postgresql_snapshot setting. Please note that this should be used only if it is actually needed. If there is no real need for that or full understanding why, then it is better to allow the table engine to create and manage its own replication slot. Example (from @bchrobot) Configure replication slot in PostgreSQL. apiVersion: &quot;acid.zalan.do/v1&quot; kind: postgresql metadata: name: acid-demo-cluster spec: numberOfInstances: 2 postgresql: parameters: wal_level: logical patroni: slots: clickhouse_sync: type: logical database: demodb plugin: pgoutput Wait for replication slot to be ready, then begin a transaction and export the transaction snapshot identifier: BEGIN; SELECT pg_export_snapshot(); In ClickHouse create database: CREATE DATABASE demodb ENGINE = MaterializedPostgreSQL('postgres1:5432', 'postgres_database', 'postgres_user', 'postgres_password') SETTINGS materialized_postgresql_replication_slot = 'clickhouse_sync', materialized_postgresql_snapshot = '0000000A-0000023F-3', materialized_postgresql_tables_list = 'table1,table2,table3'; End the PostgreSQL transaction once replication to ClickHouse DB is confirmed. Verify that replication continues after failover: kubectl exec acid-demo-cluster-0 -c postgres -- su postgres -c 'patronictl failover --candidate acid-demo-cluster-1 --force'  "},{"title":"Required permissions​","type":1,"pageTitle":"[experimental] MaterializedPostgreSQL","url":"docs/en/engines/database-engines/materialized-postgresql#required-permissions","content":"CREATE PUBLICATION -- create query privilege. CREATE_REPLICATION_SLOT -- replication privelege. pg_drop_replication_slot -- replication privilege or superuser. DROP PUBLICATION -- owner of publication (username in MaterializedPostgreSQL engine itself). It is possible to avoid executing 2 and 3 commands and having those permissions. Use settings materialized_postgresql_replication_slot and materialized_postgresql_snapshot. But with much care. Access to tables: pg_publication pg_replication_slots pg_publication_tables "},{"title":"Table Engines for Integrations","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/integrations/","content":"Table Engines for Integrations ClickHouse provides various means for integrating with external systems, including table engines. Like with all other table engines, the configuration is done using CREATE TABLE or ALTER TABLE queries. Then from a user perspective, the configured integration looks like a normal table, but queries to it are proxied to the external system. This transparent querying is one of the key advantages of this approach over alternative integration methods, like external dictionaries or table functions, which require to use custom query methods on each use. List of supported integrations: ODBCJDBCMySQLMongoDBHDFSS3KafkaEmbeddedRocksDBRabbitMQPostgreSQLSQLiteHive","keywords":""},{"title":"Data Types Support​","type":1,"pageTitle":"SQLite","url":"docs/en/engines/database-engines/sqlite#data_types-support","content":"SQLite\tClickHouseINTEGER\tInt32 REAL\tFloat32 TEXT\tString BLOB\tString "},{"title":"Specifics and Recommendations​","type":1,"pageTitle":"SQLite","url":"docs/en/engines/database-engines/sqlite#specifics-and-recommendations","content":"SQLite stores the entire database (definitions, tables, indices, and the data itself) as a single cross-platform file on a host machine. During writing SQLite locks the entire database file, therefore write operations are performed sequentially. Read operations can be multitasked. SQLite does not require service management (such as startup scripts) or access control based on GRANT and passwords. Access control is handled by means of file-system permissions given to the database file itself. "},{"title":"Usage Example​","type":1,"pageTitle":"SQLite","url":"docs/en/engines/database-engines/sqlite#usage-example","content":"Database in ClickHouse, connected to the SQLite: CREATE DATABASE sqlite_db ENGINE = SQLite('sqlite.db'); SHOW TABLES FROM sqlite_db;  ┌──name───┐ │ table1 │ │ table2 │ └─────────┘  Shows the tables: SELECT * FROM sqlite_db.table1;  ┌─col1──┬─col2─┐ │ line1 │ 1 │ │ line2 │ 2 │ │ line3 │ 3 │ └───────┴──────┘  Inserting data into SQLite table from ClickHouse table: CREATE TABLE clickhouse_table(`col1` String,`col2` Int16) ENGINE = MergeTree() ORDER BY col2; INSERT INTO clickhouse_table VALUES ('text',10); INSERT INTO sqlite_db.table1 SELECT * FROM clickhouse_table; SELECT * FROM sqlite_db.table1;  ┌─col1──┬─col2─┐ │ line1 │ 1 │ │ line2 │ 2 │ │ line3 │ 3 │ │ text │ 10 │ └───────┴──────┘  "},{"title":"Table Engines","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/","content":"","keywords":""},{"title":"Engine Families​","type":1,"pageTitle":"Table Engines","url":"docs/en/engines/table-engines/#engine-families","content":""},{"title":"MergeTree​","type":1,"pageTitle":"Table Engines","url":"docs/en/engines/table-engines/#mergetree","content":"The most universal and functional table engines for high-load tasks. The property shared by these engines is quick data insertion with subsequent background data processing. MergeTree family engines support data replication (with Replicated* versions of engines), partitioning, secondary data-skipping indexes, and other features not supported in other engines. Engines in the family: MergeTreeReplacingMergeTreeSummingMergeTreeAggregatingMergeTreeCollapsingMergeTreeVersionedCollapsingMergeTreeGraphiteMergeTree "},{"title":"Log​","type":1,"pageTitle":"Table Engines","url":"docs/en/engines/table-engines/#log","content":"Lightweight engines with minimum functionality. They’re the most effective when you need to quickly write many small tables (up to approximately 1 million rows) and read them later as a whole. Engines in the family: TinyLogStripeLogLog "},{"title":"Integration Engines​","type":1,"pageTitle":"Table Engines","url":"docs/en/engines/table-engines/#integration-engines","content":"Engines for communicating with other data storage and processing systems. Engines in the family: ODBCJDBCMySQLMongoDBHDFSS3KafkaEmbeddedRocksDBRabbitMQPostgreSQL "},{"title":"Special Engines​","type":1,"pageTitle":"Table Engines","url":"docs/en/engines/table-engines/#special-engines","content":"Engines in the family: DistributedMaterializedViewDictionaryMergeFileNullSetJoinURLViewMemoryBuffer "},{"title":"Virtual Columns​","type":1,"pageTitle":"Table Engines","url":"docs/en/engines/table-engines/#table_engines-virtual_columns","content":"Virtual column is an integral table engine attribute that is defined in the engine source code. You shouldn’t specify virtual columns in the CREATE TABLE query and you can’t see them in SHOW CREATE TABLE and DESCRIBE TABLE query results. Virtual columns are also read-only, so you can’t insert data into virtual columns. To select data from a virtual column, you must specify its name in the SELECT query. SELECT * does not return values from virtual columns. If you create a table with a column that has the same name as one of the table virtual columns, the virtual column becomes inaccessible. We do not recommend doing this. To help avoid conflicts, virtual column names are usually prefixed with an underscore. Original article "},{"title":"[experimental] Replicated","type":0,"sectionRef":"#","url":"docs/en/engines/database-engines/replicated","content":"","keywords":""},{"title":"Creating a Database​","type":1,"pageTitle":"[experimental] Replicated","url":"docs/en/engines/database-engines/replicated#creating-a-database","content":" CREATE DATABASE testdb ENGINE = Replicated('zoo_path', 'shard_name', 'replica_name') [SETTINGS ...]  Engine Parameters zoo_path — ZooKeeper path. The same ZooKeeper path corresponds to the same database.shard_name — Shard name. Database replicas are grouped into shards by shard_name.replica_name — Replica name. Replica names must be different for all replicas of the same shard. warning For ReplicatedMergeTree tables if no arguments provided, then default arguments are used: /clickhouse/tables/{uuid}/{shard} and {replica}. These can be changed in the server settings default_replica_path and default_replica_name. Macro {uuid} is unfolded to table's uuid, {shard} and {replica} are unfolded to values from server config, not from database engine arguments. But in the future, it will be possible to use shard_name and replica_name of Replicated database. "},{"title":"Specifics and Recommendations​","type":1,"pageTitle":"[experimental] Replicated","url":"docs/en/engines/database-engines/replicated#specifics-and-recommendations","content":"DDL queries with Replicated database work in a similar way to ON CLUSTER queries, but with minor differences. First, the DDL request tries to execute on the initiator (the host that originally received the request from the user). If the request is not fulfilled, then the user immediately receives an error, other hosts do not try to fulfill it. If the request has been successfully completed on the initiator, then all other hosts will automatically retry until they complete it. The initiator will try to wait for the query to be completed on other hosts (no longer than distributed_ddl_task_timeout) and will return a table with the query execution statuses on each host. The behavior in case of errors is regulated by the distributed_ddl_output_mode setting, for a Replicated database it is better to set it to null_status_on_timeout — i.e. if some hosts did not have time to execute the request for distributed_ddl_task_timeout, then do not throw an exception, but show the NULL status for them in the table. The system.clusters system table contains a cluster named like the replicated database, which consists of all replicas of the database. This cluster is updated automatically when creating/deleting replicas, and it can be used for Distributed tables. When creating a new replica of the database, this replica creates tables by itself. If the replica has been unavailable for a long time and has lagged behind the replication log — it checks its local metadata with the current metadata in ZooKeeper, moves the extra tables with data to a separate non-replicated database (so as not to accidentally delete anything superfluous), creates the missing tables, updates the table names if they have been renamed. The data is replicated at the ReplicatedMergeTree level, i.e. if the table is not replicated, the data will not be replicated (the database is responsible only for metadata). ALTER TABLE ATTACH|FETCH|DROP|DROP DETACHED|DETACH PARTITION|PART queries are allowed but not replicated. The database engine will only add/fetch/remove the partition/part to the current replica. However, if the table itself uses a Replicated table engine, then the data will be replicated after using ATTACH. "},{"title":"Usage Example​","type":1,"pageTitle":"[experimental] Replicated","url":"docs/en/engines/database-engines/replicated#usage-example","content":"Creating a cluster with three hosts: node1 :) CREATE DATABASE r ENGINE=Replicated('some/path/r','shard1','replica1'); node2 :) CREATE DATABASE r ENGINE=Replicated('some/path/r','shard1','other_replica'); node3 :) CREATE DATABASE r ENGINE=Replicated('some/path/r','other_shard','{replica}');  Running the DDL-query: CREATE TABLE r.rmt (n UInt64) ENGINE=ReplicatedMergeTree ORDER BY n;  ┌─────hosts────────────┬──status─┬─error─┬─num_hosts_remaining─┬─num_hosts_active─┐ │ shard1|replica1 │ 0 │ │ 2 │ 0 │ │ shard1|other_replica │ 0 │ │ 1 │ 0 │ │ other_shard|r1 │ 0 │ │ 0 │ 0 │ └──────────────────────┴─────────┴───────┴─────────────────────┴──────────────────┘  Showing the system table: SELECT cluster, shard_num, replica_num, host_name, host_address, port, is_local FROM system.clusters WHERE cluster='r';  ┌─cluster─┬─shard_num─┬─replica_num─┬─host_name─┬─host_address─┬─port─┬─is_local─┐ │ r │ 1 │ 1 │ node3 │ 127.0.0.1 │ 9002 │ 0 │ │ r │ 2 │ 1 │ node2 │ 127.0.0.1 │ 9001 │ 0 │ │ r │ 2 │ 2 │ node1 │ 127.0.0.1 │ 9000 │ 1 │ └─────────┴───────────┴─────────────┴───────────┴──────────────┴──────┴──────────┘  Creating a distributed table and inserting the data: node2 :) CREATE TABLE r.d (n UInt64) ENGINE=Distributed('r','r','rmt', n % 2); node3 :) INSERT INTO r SELECT * FROM numbers(10); node1 :) SELECT materialize(hostName()) AS host, groupArray(n) FROM r.d GROUP BY host;  ┌─hosts─┬─groupArray(n)─┐ │ node1 │ [1,3,5,7,9] │ │ node2 │ [0,2,4,6,8] │ └───────┴───────────────┘  Adding replica on the one more host: node4 :) CREATE DATABASE r ENGINE=Replicated('some/path/r','other_shard','r2');  The cluster configuration will look like this: ┌─cluster─┬─shard_num─┬─replica_num─┬─host_name─┬─host_address─┬─port─┬─is_local─┐ │ r │ 1 │ 1 │ node3 │ 127.0.0.1 │ 9002 │ 0 │ │ r │ 1 │ 2 │ node4 │ 127.0.0.1 │ 9003 │ 0 │ │ r │ 2 │ 1 │ node2 │ 127.0.0.1 │ 9001 │ 0 │ │ r │ 2 │ 2 │ node1 │ 127.0.0.1 │ 9000 │ 1 │ └─────────┴───────────┴─────────────┴───────────┴──────────────┴──────┴──────────┘  The distributed table also will get data from the new host: node2 :) SELECT materialize(hostName()) AS host, groupArray(n) FROM r.d GROUP BY host;  ┌─hosts─┬─groupArray(n)─┐ │ node2 │ [1,3,5,7,9] │ │ node4 │ [0,2,4,6,8] │ └───────┴───────────────┘  "},{"title":"ExternalDistributed","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/integrations/ExternalDistributed","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"ExternalDistributed","url":"docs/en/engines/table-engines/integrations/ExternalDistributed#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [TTL expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [TTL expr2], ... ) ENGINE = ExternalDistributed('engine', 'host:port', 'database', 'table', 'user', 'password');  See a detailed description of the CREATE TABLE query. The table structure can differ from the original table structure: Column names should be the same as in the original table, but you can use just some of these columns and in any order.Column types may differ from those in the original table. ClickHouse tries to cast values to the ClickHouse data types. Engine Parameters engine — The table engine MySQL or PostgreSQL.host:port — MySQL or PostgreSQL server address.database — Remote database name.table — Remote table name.user — User name.password — User password. "},{"title":"Implementation Details​","type":1,"pageTitle":"ExternalDistributed","url":"docs/en/engines/table-engines/integrations/ExternalDistributed#implementation-details","content":"Supports multiple replicas that must be listed by | and shards must be listed by ,. For example: CREATE TABLE test_shards (id UInt32, name String, age UInt32, money UInt32) ENGINE = ExternalDistributed('MySQL', `mysql{1|2}:3306,mysql{3|4}:3306`, 'clickhouse', 'test_replicas', 'root', 'clickhouse');  When specifying replicas, one of the available replicas is selected for each of the shards when reading. If the connection fails, the next replica is selected, and so on for all the replicas. If the connection attempt fails for all the replicas, the attempt is repeated the same way several times. You can specify any number of shards and any number of replicas for each shard. See Also MySQL table enginePostgreSQL table engineDistributed table engine Original article "},{"title":"EmbeddedRocksDB Engine","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/integrations/embedded-rocksdb","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"EmbeddedRocksDB Engine","url":"docs/en/engines/table-engines/integrations/embedded-rocksdb#table_engine-EmbeddedRocksDB-creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE = EmbeddedRocksDB PRIMARY KEY(primary_key_name)  Required parameters: primary_key_name – any column name in the column list.primary key must be specified, it supports only one column in the primary key. The primary key will be serialized in binary as a rocksdb key.columns other than the primary key will be serialized in binary as rocksdb value in corresponding order.queries with key equals or in filtering will be optimized to multi keys lookup from rocksdb. Example: CREATE TABLE test ( `key` String, `v1` UInt32, `v2` String, `v3` Float32 ) ENGINE = EmbeddedRocksDB PRIMARY KEY key  "},{"title":"Metrics​","type":1,"pageTitle":"EmbeddedRocksDB Engine","url":"docs/en/engines/table-engines/integrations/embedded-rocksdb#metrics","content":"There is also system.rocksdb table, that expose rocksdb statistics: SELECT name, value FROM system.rocksdb ┌─name──────────────────────┬─value─┐ │ no.file.opens │ 1 │ │ number.block.decompressed │ 1 │ └───────────────────────────┴───────┘  "},{"title":"Configuration​","type":1,"pageTitle":"EmbeddedRocksDB Engine","url":"docs/en/engines/table-engines/integrations/embedded-rocksdb#configuration","content":"You can also change any rocksdb options using config: &lt;rocksdb&gt; &lt;options&gt; &lt;max_background_jobs&gt;8&lt;/max_background_jobs&gt; &lt;/options&gt; &lt;column_family_options&gt; &lt;num_levels&gt;2&lt;/num_levels&gt; &lt;/column_family_options&gt; &lt;tables&gt; &lt;table&gt; &lt;name&gt;TABLE&lt;/name&gt; &lt;options&gt; &lt;max_background_jobs&gt;8&lt;/max_background_jobs&gt; &lt;/options&gt; &lt;column_family_options&gt; &lt;num_levels&gt;2&lt;/num_levels&gt; &lt;/column_family_options&gt; &lt;/table&gt; &lt;/tables&gt; &lt;/rocksdb&gt;  Original article "},{"title":"JDBC","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/integrations/jdbc","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"JDBC","url":"docs/en/engines/table-engines/integrations/jdbc#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name ( columns list... ) ENGINE = JDBC(datasource_uri, external_database, external_table)  Engine Parameters datasource_uri — URI or name of an external DBMS. URI Format: jdbc:&lt;driver_name&gt;://&lt;host_name&gt;:&lt;port&gt;/?user=&lt;username&gt;&amp;password=&lt;password&gt;. Example for MySQL: jdbc:mysql://localhost:3306/?user=root&amp;password=root. external_database — Database in an external DBMS. external_table — Name of the table in external_database or a select query like select * from table1 where column1=1. "},{"title":"Usage Example​","type":1,"pageTitle":"JDBC","url":"docs/en/engines/table-engines/integrations/jdbc#usage-example","content":"Creating a table in MySQL server by connecting directly with it’s console client: mysql&gt; CREATE TABLE `test`.`test` ( -&gt; `int_id` INT NOT NULL AUTO_INCREMENT, -&gt; `int_nullable` INT NULL DEFAULT NULL, -&gt; `float` FLOAT NOT NULL, -&gt; `float_nullable` FLOAT NULL DEFAULT NULL, -&gt; PRIMARY KEY (`int_id`)); Query OK, 0 rows affected (0,09 sec) mysql&gt; insert into test (`int_id`, `float`) VALUES (1,2); Query OK, 1 row affected (0,00 sec) mysql&gt; select * from test; +------+----------+-----+----------+ | int_id | int_nullable | float | float_nullable | +------+----------+-----+----------+ | 1 | NULL | 2 | NULL | +------+----------+-----+----------+ 1 row in set (0,00 sec)  Creating a table in ClickHouse server and selecting data from it: CREATE TABLE jdbc_table ( `int_id` Int32, `int_nullable` Nullable(Int32), `float` Float32, `float_nullable` Nullable(Float32) ) ENGINE JDBC('jdbc:mysql://localhost:3306/?user=root&amp;password=root', 'test', 'test')  SELECT * FROM jdbc_table  ┌─int_id─┬─int_nullable─┬─float─┬─float_nullable─┐ │ 1 │ ᴺᵁᴸᴸ │ 2 │ ᴺᵁᴸᴸ │ └────────┴──────────────┴───────┴────────────────┘  INSERT INTO jdbc_table(`int_id`, `float`) SELECT toInt32(number), toFloat32(number * 1.0) FROM system.numbers  "},{"title":"See Also​","type":1,"pageTitle":"JDBC","url":"docs/en/engines/table-engines/integrations/jdbc#see-also","content":"JDBC table function. Original article "},{"title":"HDFS","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/integrations/hdfs","content":"","keywords":""},{"title":"Usage​","type":1,"pageTitle":"HDFS","url":"docs/en/engines/table-engines/integrations/hdfs#usage","content":"ENGINE = HDFS(URI, format)  Engine Parameters URI - whole file URI in HDFS. The path part of URI may contain globs. In this case the table would be readonly.format - specifies one of the available file formats. To performSELECT queries, the format must be supported for input, and to performINSERT queries – for output. The available formats are listed in theFormats section. Example: 1. Set up the hdfs_engine_table table: CREATE TABLE hdfs_engine_table (name String, value UInt32) ENGINE=HDFS('hdfs://hdfs1:9000/other_storage', 'TSV')  2. Fill file: INSERT INTO hdfs_engine_table VALUES ('one', 1), ('two', 2), ('three', 3)  3. Query the data: SELECT * FROM hdfs_engine_table LIMIT 2  ┌─name─┬─value─┐ │ one │ 1 │ │ two │ 2 │ └──────┴───────┘  "},{"title":"Implementation Details​","type":1,"pageTitle":"HDFS","url":"docs/en/engines/table-engines/integrations/hdfs#implementation-details","content":"Reads and writes can be parallel.Zero-copy replication is supported. Not supported: ALTER and SELECT...SAMPLE operations.Indexes. Globs in path Multiple path components can have globs. For being processed file should exists and matches to the whole path pattern. Listing of files determines during SELECT (not at CREATE moment). * — Substitutes any number of any characters except / including empty string.? — Substitutes any single character.{some_string,another_string,yet_another_one} — Substitutes any of strings 'some_string', 'another_string', 'yet_another_one'.{N..M} — Substitutes any number in range from N to M including both borders. Constructions with {} are similar to the remote table function. Example Suppose we have several files in TSV format with the following URIs on HDFS: 'hdfs://hdfs1:9000/some_dir/some_file_1''hdfs://hdfs1:9000/some_dir/some_file_2''hdfs://hdfs1:9000/some_dir/some_file_3''hdfs://hdfs1:9000/another_dir/some_file_1''hdfs://hdfs1:9000/another_dir/some_file_2''hdfs://hdfs1:9000/another_dir/some_file_3' There are several ways to make a table consisting of all six files: CREATE TABLE table_with_range (name String, value UInt32) ENGINE = HDFS('hdfs://hdfs1:9000/{some,another}_dir/some_file_{1..3}', 'TSV')  Another way: CREATE TABLE table_with_question_mark (name String, value UInt32) ENGINE = HDFS('hdfs://hdfs1:9000/{some,another}_dir/some_file_?', 'TSV')  Table consists of all the files in both directories (all files should satisfy format and schema described in query): CREATE TABLE table_with_asterisk (name String, value UInt32) ENGINE = HDFS('hdfs://hdfs1:9000/{some,another}_dir/*', 'TSV')  warning If the listing of files contains number ranges with leading zeros, use the construction with braces for each digit separately or use ?. Example Create table with files named file000, file001, … , file999: CREATE TABLE big_table (name String, value UInt32) ENGINE = HDFS('hdfs://hdfs1:9000/big_dir/file{0..9}{0..9}{0..9}', 'CSV')  "},{"title":"Configuration​","type":1,"pageTitle":"HDFS","url":"docs/en/engines/table-engines/integrations/hdfs#configuration","content":"Similar to GraphiteMergeTree, the HDFS engine supports extended configuration using the ClickHouse config file. There are two configuration keys that you can use: global (hdfs) and user-level (hdfs_*). The global configuration is applied first, and then the user-level configuration is applied (if it exists).  &lt;!-- Global configuration options for HDFS engine type --&gt; &lt;hdfs&gt; &lt;hadoop_kerberos_keytab&gt;/tmp/keytab/clickhouse.keytab&lt;/hadoop_kerberos_keytab&gt; &lt;hadoop_kerberos_principal&gt;clickuser@TEST.CLICKHOUSE.TECH&lt;/hadoop_kerberos_principal&gt; &lt;hadoop_security_authentication&gt;kerberos&lt;/hadoop_security_authentication&gt; &lt;/hdfs&gt; &lt;!-- Configuration specific for user &quot;root&quot; --&gt; &lt;hdfs_root&gt; &lt;hadoop_kerberos_principal&gt;root@TEST.CLICKHOUSE.TECH&lt;/hadoop_kerberos_principal&gt; &lt;/hdfs_root&gt;  "},{"title":"Configuration Options​","type":1,"pageTitle":"HDFS","url":"docs/en/engines/table-engines/integrations/hdfs#configuration-options","content":"Supported by libhdfs3​ parameter\tdefault valuerpc_client_connect_tcpnodelay\ttrue dfs_client_read_shortcircuit\ttrue output_replace-datanode-on-failure\ttrue input_notretry-another-node\tfalse input_localread_mappedfile\ttrue dfs_client_use_legacy_blockreader_local\tfalse rpc_client_ping_interval\t10 * 1000 rpc_client_connect_timeout\t600 * 1000 rpc_client_read_timeout\t3600 * 1000 rpc_client_write_timeout\t3600 * 1000 rpc_client_socekt_linger_timeout\t-1 rpc_client_connect_retry\t10 rpc_client_timeout\t3600 * 1000 dfs_default_replica\t3 input_connect_timeout\t600 * 1000 input_read_timeout\t3600 * 1000 input_write_timeout\t3600 * 1000 input_localread_default_buffersize\t1 1024 1024 dfs_prefetchsize\t10 input_read_getblockinfo_retry\t3 input_localread_blockinfo_cachesize\t1000 input_read_max_retry\t60 output_default_chunksize\t512 output_default_packetsize\t64 * 1024 output_default_write_retry\t10 output_connect_timeout\t600 * 1000 output_read_timeout\t3600 * 1000 output_write_timeout\t3600 * 1000 output_close_timeout\t3600 * 1000 output_packetpool_size\t1024 output_heeartbeat_interval\t10 * 1000 dfs_client_failover_max_attempts\t15 dfs_client_read_shortcircuit_streams_cache_size\t256 dfs_client_socketcache_expiryMsec\t3000 dfs_client_socketcache_capacity\t16 dfs_default_blocksize\t64 1024 1024 dfs_default_uri\t&quot;hdfs://localhost:9000&quot; hadoop_security_authentication\t&quot;simple&quot; hadoop_security_kerberos_ticket_cache_path\t&quot;&quot; dfs_client_log_severity\t&quot;INFO&quot; dfs_domain_socket_path\t&quot;&quot; HDFS Configuration Reference might explain some parameters. ClickHouse extras​ parameter\tdefault valuehadoop_kerberos_keytab\t&quot;&quot; hadoop_kerberos_principal\t&quot;&quot; hadoop_kerberos_kinit_command\tkinit libhdfs3_conf\t&quot;&quot; "},{"title":"Limitations​","type":1,"pageTitle":"HDFS","url":"docs/en/engines/table-engines/integrations/hdfs#limitations","content":"hadoop_security_kerberos_ticket_cache_path and libhdfs3_conf can be global only, not user specific "},{"title":"Kerberos support​","type":1,"pageTitle":"HDFS","url":"docs/en/engines/table-engines/integrations/hdfs#kerberos-support","content":"If the hadoop_security_authentication parameter has the value kerberos, ClickHouse authenticates via Kerberos. Parameters are here and hadoop_security_kerberos_ticket_cache_path may be of help. Note that due to libhdfs3 limitations only old-fashioned approach is supported, datanode communications are not secured by SASL (HADOOP_SECURE_DN_USER is a reliable indicator of such security approach). Use tests/integration/test_storage_kerberized_hdfs/hdfs_configs/bootstrap.sh for reference. If hadoop_kerberos_keytab, hadoop_kerberos_principal or hadoop_kerberos_kinit_command is specified, kinit will be invoked. hadoop_kerberos_keytab and hadoop_kerberos_principal are mandatory in this case. kinit tool and krb5 configuration files are required. "},{"title":"HDFS Namenode HA support​","type":1,"pageTitle":"HDFS","url":"docs/en/engines/table-engines/integrations/hdfs#namenode-ha","content":"libhdfs3 support HDFS namenode HA. Copy hdfs-site.xml from an HDFS node to /etc/clickhouse-server/.Add following piece to ClickHouse config file:  &lt;hdfs&gt; &lt;libhdfs3_conf&gt;/etc/clickhouse-server/hdfs-site.xml&lt;/libhdfs3_conf&gt; &lt;/hdfs&gt;  Then use dfs.nameservices tag value of hdfs-site.xml as the namenode address in the HDFS URI. For example, replace hdfs://appadmin@192.168.101.11:8020/abc/ with hdfs://appadmin@my_nameservice/abc/. "},{"title":"Virtual Columns​","type":1,"pageTitle":"HDFS","url":"docs/en/engines/table-engines/integrations/hdfs#virtual-columns","content":"_path — Path to the file._file — Name of the file. See Also Virtual columns Original article "},{"title":"MaterializedPostgreSQL","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/integrations/materialized-postgresql","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"MaterializedPostgreSQL","url":"docs/en/engines/table-engines/integrations/materialized-postgresql#creating-a-table","content":"CREATE TABLE postgresql_db.postgresql_replica (key UInt64, value UInt64) ENGINE = MaterializedPostgreSQL('postgres1:5432', 'postgres_database', 'postgresql_replica', 'postgres_user', 'postgres_password') PRIMARY KEY key;  Engine Parameters host:port — PostgreSQL server address.database — Remote database name.table — Remote table name.user — PostgreSQL user.password — User password. "},{"title":"Requirements​","type":1,"pageTitle":"MaterializedPostgreSQL","url":"docs/en/engines/table-engines/integrations/materialized-postgresql#requirements","content":"The wal_level setting must have a value logical and max_replication_slots parameter must have a value at least 2 in the PostgreSQL config file. A table with MaterializedPostgreSQL engine must have a primary key — the same as a replica identity index (by default: primary key) of a PostgreSQL table (see details on replica identity index). Only database Atomic is allowed. "},{"title":"Virtual columns​","type":1,"pageTitle":"MaterializedPostgreSQL","url":"docs/en/engines/table-engines/integrations/materialized-postgresql#virtual-columns","content":"_version — Transaction counter. Type: UInt64. _sign — Deletion mark. Type: Int8. Possible values: 1 — Row is not deleted,-1 — Row is deleted. These columns do not need to be added when a table is created. They are always accessible in SELECT query._version column equals LSN position in WAL, so it might be used to check how up-to-date replication is. CREATE TABLE postgresql_db.postgresql_replica (key UInt64, value UInt64) ENGINE = MaterializedPostgreSQL('postgres1:5432', 'postgres_database', 'postgresql_replica', 'postgres_user', 'postgres_password') PRIMARY KEY key; SELECT key, value, _version FROM postgresql_db.postgresql_replica;  warning Replication of TOAST values is not supported. The default value for the data type will be used. Original article "},{"title":"MongoDB","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/integrations/mongodb","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"MongoDB","url":"docs/en/engines/table-engines/integrations/mongodb#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name ( name1 [type1], name2 [type2], ... ) ENGINE = MongoDB(host:port, database, collection, user, password [, options]);  Engine Parameters host:port — MongoDB server address. database — Remote database name. collection — Remote collection name. user — MongoDB user. password — User password. options — MongoDB connection string options (optional parameter). "},{"title":"Usage Example​","type":1,"pageTitle":"MongoDB","url":"docs/en/engines/table-engines/integrations/mongodb#usage-example","content":"Create a table in ClickHouse which allows to read data from MongoDB collection: CREATE TABLE mongo_table ( key UInt64, data String ) ENGINE = MongoDB('mongo1:27017', 'test', 'simple_table', 'testuser', 'clickhouse');  To read from an SSL secured MongoDB server: CREATE TABLE mongo_table_ssl ( key UInt64, data String ) ENGINE = MongoDB('mongo2:27017', 'test', 'simple_table', 'testuser', 'clickhouse', 'ssl=true');  Query: SELECT COUNT() FROM mongo_table;  ┌─count()─┐ │ 4 │ └─────────┘  You can also adjust connection timeout: CREATE TABLE mongo_table ( key UInt64, data String ) ENGINE = MongoDB('mongo2:27017', 'test', 'simple_table', 'testuser', 'clickhouse', 'connectTimeoutMS=100000');  Original article "},{"title":"ODBC","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/integrations/odbc","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"ODBC","url":"docs/en/engines/table-engines/integrations/odbc#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1], name2 [type2], ... ) ENGINE = ODBC(connection_settings, external_database, external_table)  See a detailed description of the CREATE TABLE query. The table structure can differ from the source table structure: Column names should be the same as in the source table, but you can use just some of these columns and in any order.Column types may differ from those in the source table. ClickHouse tries to cast values to the ClickHouse data types.The external_table_functions_use_nulls setting defines how to handle Nullable columns. Default value: 1. If 0, the table function does not make Nullable columns and inserts default values instead of nulls. This is also applicable for NULL values inside arrays. Engine Parameters connection_settings — Name of the section with connection settings in the odbc.ini file.external_database — Name of a database in an external DBMS.external_table — Name of a table in the external_database. "},{"title":"Usage Example​","type":1,"pageTitle":"ODBC","url":"docs/en/engines/table-engines/integrations/odbc#usage-example","content":"Retrieving data from the local MySQL installation via ODBC This example is checked for Ubuntu Linux 18.04 and MySQL server 5.7. Ensure that unixODBC and MySQL Connector are installed. By default (if installed from packages), ClickHouse starts as user clickhouse. Thus, you need to create and configure this user in the MySQL server. $ sudo mysql  mysql&gt; CREATE USER 'clickhouse'@'localhost' IDENTIFIED BY 'clickhouse'; mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'clickhouse'@'clickhouse' WITH GRANT OPTION;  Then configure the connection in /etc/odbc.ini. $ cat /etc/odbc.ini [mysqlconn] DRIVER = /usr/local/lib/libmyodbc5w.so SERVER = 127.0.0.1 PORT = 3306 DATABASE = test USERNAME = clickhouse PASSWORD = clickhouse  You can check the connection using the isql utility from the unixODBC installation. $ isql -v mysqlconn +-------------------------+ | Connected! | | | ...  Table in MySQL: mysql&gt; CREATE TABLE `test`.`test` ( -&gt; `int_id` INT NOT NULL AUTO_INCREMENT, -&gt; `int_nullable` INT NULL DEFAULT NULL, -&gt; `float` FLOAT NOT NULL, -&gt; `float_nullable` FLOAT NULL DEFAULT NULL, -&gt; PRIMARY KEY (`int_id`)); Query OK, 0 rows affected (0,09 sec) mysql&gt; insert into test (`int_id`, `float`) VALUES (1,2); Query OK, 1 row affected (0,00 sec) mysql&gt; select * from test; +------+----------+-----+----------+ | int_id | int_nullable | float | float_nullable | +------+----------+-----+----------+ | 1 | NULL | 2 | NULL | +------+----------+-----+----------+ 1 row in set (0,00 sec)  Table in ClickHouse, retrieving data from the MySQL table: CREATE TABLE odbc_t ( `int_id` Int32, `float_nullable` Nullable(Float32) ) ENGINE = ODBC('DSN=mysqlconn', 'test', 'test')  SELECT * FROM odbc_t  ┌─int_id─┬─float_nullable─┐ │ 1 │ ᴺᵁᴸᴸ │ └────────┴────────────────┘  "},{"title":"See Also​","type":1,"pageTitle":"ODBC","url":"docs/en/engines/table-engines/integrations/odbc#see-also","content":"ODBC external dictionariesODBC table function Original article "},{"title":"MySQL","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/integrations/mysql","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"MySQL","url":"docs/en/engines/table-engines/integrations/mysql#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [TTL expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [TTL expr2], ... ) ENGINE = MySQL('host:port', 'database', 'table', 'user', 'password'[, replace_query, 'on_duplicate_clause']) SETTINGS [connection_pool_size=16, ] [connection_max_tries=3, ] [connection_wait_timeout=5, ] /* 0 -- do not wait */ [connection_auto_close=true ] ;  See a detailed description of the CREATE TABLE query. The table structure can differ from the original MySQL table structure: Column names should be the same as in the original MySQL table, but you can use just some of these columns and in any order.Column types may differ from those in the original MySQL table. ClickHouse tries to cast values to the ClickHouse data types.The external_table_functions_use_nulls setting defines how to handle Nullable columns. Default value: 1. If 0, the table function does not make Nullable columns and inserts default values instead of nulls. This is also applicable for NULL values inside arrays. Engine Parameters host:port — MySQL server address. database — Remote database name. table — Remote table name. user — MySQL user. password — User password. replace_query — Flag that converts INSERT INTO queries to REPLACE INTO. If replace_query=1, the query is substituted. on_duplicate_clause — The ON DUPLICATE KEY on_duplicate_clause expression that is added to the INSERT query. Example: INSERT INTO t (c1,c2) VALUES ('a', 2) ON DUPLICATE KEY UPDATE c2 = c2 + 1, where on_duplicate_clause is UPDATE c2 = c2 + 1. See the MySQL documentation to find which on_duplicate_clause you can use with the ON DUPLICATE KEY clause. To specify on_duplicate_clause you need to pass 0 to the replace_query parameter. If you simultaneously pass replace_query = 1 and on_duplicate_clause, ClickHouse generates an exception. Simple WHERE clauses such as =, !=, &gt;, &gt;=, &lt;, &lt;= are executed on the MySQL server. The rest of the conditions and the LIMIT sampling constraint are executed in ClickHouse only after the query to MySQL finishes. Supports multiple replicas that must be listed by |. For example: CREATE TABLE test_replicas (id UInt32, name String, age UInt32, money UInt32) ENGINE = MySQL(`mysql{2|3|4}:3306`, 'clickhouse', 'test_replicas', 'root', 'clickhouse');  "},{"title":"Usage Example​","type":1,"pageTitle":"MySQL","url":"docs/en/engines/table-engines/integrations/mysql#usage-example","content":"Table in MySQL: mysql&gt; CREATE TABLE `test`.`test` ( -&gt; `int_id` INT NOT NULL AUTO_INCREMENT, -&gt; `int_nullable` INT NULL DEFAULT NULL, -&gt; `float` FLOAT NOT NULL, -&gt; `float_nullable` FLOAT NULL DEFAULT NULL, -&gt; PRIMARY KEY (`int_id`)); Query OK, 0 rows affected (0,09 sec) mysql&gt; insert into test (`int_id`, `float`) VALUES (1,2); Query OK, 1 row affected (0,00 sec) mysql&gt; select * from test; +------+----------+-----+----------+ | int_id | int_nullable | float | float_nullable | +------+----------+-----+----------+ | 1 | NULL | 2 | NULL | +------+----------+-----+----------+ 1 row in set (0,00 sec)  Table in ClickHouse, retrieving data from the MySQL table created above: CREATE TABLE mysql_table ( `float_nullable` Nullable(Float32), `int_id` Int32 ) ENGINE = MySQL('localhost:3306', 'test', 'test', 'bayonet', '123')  SELECT * FROM mysql_table  ┌─float_nullable─┬─int_id─┐ │ ᴺᵁᴸᴸ │ 1 │ └────────────────┴────────┘  "},{"title":"Settings​","type":1,"pageTitle":"MySQL","url":"docs/en/engines/table-engines/integrations/mysql#mysql-settings","content":"Default settings are not very efficient, since they do not even reuse connections. These settings allow you to increase the number of queries run by the server per second. "},{"title":"connection_auto_close​","type":1,"pageTitle":"MySQL","url":"docs/en/engines/table-engines/integrations/mysql#connection-auto-close","content":"Allows to automatically close the connection after query execution, i.e. disable connection reuse. Possible values: 1 — Auto-close connection is allowed, so the connection reuse is disabled0 — Auto-close connection is not allowed, so the connection reuse is enabled Default value: 1. "},{"title":"connection_max_tries​","type":1,"pageTitle":"MySQL","url":"docs/en/engines/table-engines/integrations/mysql#connection-max-tries","content":"Sets the number of retries for pool with failover. Possible values: Positive integer.0 — There are no retries for pool with failover. Default value: 3. "},{"title":"connection_pool_size​","type":1,"pageTitle":"MySQL","url":"docs/en/engines/table-engines/integrations/mysql#connection-pool-size","content":"Size of connection pool (if all connections are in use, the query will wait until some connection will be freed). Possible values: Positive integer. Default value: 16. "},{"title":"See Also​","type":1,"pageTitle":"MySQL","url":"docs/en/engines/table-engines/integrations/mysql#see-also","content":"The mysql table functionUsing MySQL as a source of external dictionary Original article "},{"title":"Kafka","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/integrations/kafka","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"Kafka","url":"docs/en/engines/table-engines/integrations/kafka#table_engine-kafka-creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE = Kafka() SETTINGS kafka_broker_list = 'host:port', kafka_topic_list = 'topic1,topic2,...', kafka_group_name = 'group_name', kafka_format = 'data_format'[,] [kafka_row_delimiter = 'delimiter_symbol',] [kafka_schema = '',] [kafka_num_consumers = N,] [kafka_max_block_size = 0,] [kafka_skip_broken_messages = N,] [kafka_commit_every_batch = 0,] [kafka_thread_per_consumer = 0]  Required parameters: kafka_broker_list — A comma-separated list of brokers (for example, localhost:9092).kafka_topic_list — A list of Kafka topics.kafka_group_name — A group of Kafka consumers. Reading margins are tracked for each group separately. If you do not want messages to be duplicated in the cluster, use the same group name everywhere.kafka_format — Message format. Uses the same notation as the SQL FORMAT function, such as JSONEachRow. For more information, see the Formats section. Optional parameters: kafka_row_delimiter — Delimiter character, which ends the message.kafka_schema — Parameter that must be used if the format requires a schema definition. For example, Cap’n Proto requires the path to the schema file and the name of the root schema.capnp:Message object.kafka_num_consumers — The number of consumers per table. Default: 1. Specify more consumers if the throughput of one consumer is insufficient. The total number of consumers should not exceed the number of partitions in the topic, since only one consumer can be assigned per partition.kafka_max_block_size — The maximum batch size (in messages) for poll (default: max_block_size).kafka_skip_broken_messages — Kafka message parser tolerance to schema-incompatible messages per block. Default: 0. If kafka_skip_broken_messages = N then the engine skips N Kafka messages that cannot be parsed (a message equals a row of data).kafka_commit_every_batch — Commit every consumed and handled batch instead of a single commit after writing a whole block (default: 0).kafka_thread_per_consumer — Provide independent thread for each consumer (default: 0). When enabled, every consumer flush the data independently, in parallel (otherwise — rows from several consumers squashed to form one block). Examples:  CREATE TABLE queue ( timestamp UInt64, level String, message String ) ENGINE = Kafka('localhost:9092', 'topic', 'group1', 'JSONEachRow'); SELECT * FROM queue LIMIT 5; CREATE TABLE queue2 ( timestamp UInt64, level String, message String ) ENGINE = Kafka SETTINGS kafka_broker_list = 'localhost:9092', kafka_topic_list = 'topic', kafka_group_name = 'group1', kafka_format = 'JSONEachRow', kafka_num_consumers = 4; CREATE TABLE queue3 ( timestamp UInt64, level String, message String ) ENGINE = Kafka('localhost:9092', 'topic', 'group1') SETTINGS kafka_format = 'JSONEachRow', kafka_num_consumers = 4;  Deprecated Method for Creating a Table warning Do not use this method in new projects. If possible, switch old projects to the method described above. Kafka(kafka_broker_list, kafka_topic_list, kafka_group_name, kafka_format [, kafka_row_delimiter, kafka_schema, kafka_num_consumers, kafka_skip_broken_messages])  "},{"title":"Description​","type":1,"pageTitle":"Kafka","url":"docs/en/engines/table-engines/integrations/kafka#description","content":"The delivered messages are tracked automatically, so each message in a group is only counted once. If you want to get the data twice, then create a copy of the table with another group name. Groups are flexible and synced on the cluster. For instance, if you have 10 topics and 5 copies of a table in a cluster, then each copy gets 2 topics. If the number of copies changes, the topics are redistributed across the copies automatically. Read more about this at http://kafka.apache.org/intro. SELECT is not particularly useful for reading messages (except for debugging), because each message can be read only once. It is more practical to create real-time threads using materialized views. To do this: Use the engine to create a Kafka consumer and consider it a data stream.Create a table with the desired structure.Create a materialized view that converts data from the engine and puts it into a previously created table. When the MATERIALIZED VIEW joins the engine, it starts collecting data in the background. This allows you to continually receive messages from Kafka and convert them to the required format using SELECT. One kafka table can have as many materialized views as you like, they do not read data from the kafka table directly, but receive new records (in blocks), this way you can write to several tables with different detail level (with grouping - aggregation and without). Example:  CREATE TABLE queue ( timestamp UInt64, level String, message String ) ENGINE = Kafka('localhost:9092', 'topic', 'group1', 'JSONEachRow'); CREATE TABLE daily ( day Date, level String, total UInt64 ) ENGINE = SummingMergeTree(day, (day, level), 8192); CREATE MATERIALIZED VIEW consumer TO daily AS SELECT toDate(toDateTime(timestamp)) AS day, level, count() as total FROM queue GROUP BY day, level; SELECT level, sum(total) FROM daily GROUP BY level;  To improve performance, received messages are grouped into blocks the size of max_insert_block_size. If the block wasn’t formed within stream_flush_interval_ms milliseconds, the data will be flushed to the table regardless of the completeness of the block. To stop receiving topic data or to change the conversion logic, detach the materialized view:  DETACH TABLE consumer; ATTACH TABLE consumer;  If you want to change the target table by using ALTER, we recommend disabling the material view to avoid discrepancies between the target table and the data from the view. "},{"title":"Configuration​","type":1,"pageTitle":"Kafka","url":"docs/en/engines/table-engines/integrations/kafka#configuration","content":"Similar to GraphiteMergeTree, the Kafka engine supports extended configuration using the ClickHouse config file. There are two configuration keys that you can use: global (kafka) and topic-level (kafka_*). The global configuration is applied first, and then the topic-level configuration is applied (if it exists).  &lt;!-- Global configuration options for all tables of Kafka engine type --&gt; &lt;kafka&gt; &lt;debug&gt;cgrp&lt;/debug&gt; &lt;auto_offset_reset&gt;smallest&lt;/auto_offset_reset&gt; &lt;/kafka&gt; &lt;!-- Configuration specific for topic &quot;logs&quot; --&gt; &lt;kafka_logs&gt; &lt;retry_backoff_ms&gt;250&lt;/retry_backoff_ms&gt; &lt;fetch_min_bytes&gt;100000&lt;/fetch_min_bytes&gt; &lt;/kafka_logs&gt;  For a list of possible configuration options, see the librdkafka configuration reference. Use the underscore (_) instead of a dot in the ClickHouse configuration. For example, check.crcs=true will be &lt;check_crcs&gt;true&lt;/check_crcs&gt;. "},{"title":"Kerberos support​","type":1,"pageTitle":"Kafka","url":"docs/en/engines/table-engines/integrations/kafka#kafka-kerberos-support","content":"To deal with Kerberos-aware Kafka, add security_protocol child element with sasl_plaintext value. It is enough if Kerberos ticket-granting ticket is obtained and cached by OS facilities. ClickHouse is able to maintain Kerberos credentials using a keytab file. Consider sasl_kerberos_service_name, sasl_kerberos_keytab, sasl_kerberos_principal and sasl.kerberos.kinit.cmd child elements. Example:  &lt;!-- Kerberos-aware Kafka --&gt; &lt;kafka&gt; &lt;security_protocol&gt;SASL_PLAINTEXT&lt;/security_protocol&gt; &lt;sasl_kerberos_keytab&gt;/home/kafkauser/kafkauser.keytab&lt;/sasl_kerberos_keytab&gt; &lt;sasl_kerberos_principal&gt;kafkauser/kafkahost@EXAMPLE.COM&lt;/sasl_kerberos_principal&gt; &lt;/kafka&gt;  "},{"title":"Virtual Columns​","type":1,"pageTitle":"Kafka","url":"docs/en/engines/table-engines/integrations/kafka#virtual-columns","content":"_topic — Kafka topic._key — Key of the message._offset — Offset of the message._timestamp — Timestamp of the message._timestamp_ms — Timestamp in milliseconds of the message._partition — Partition of Kafka topic. See Also Virtual columnsbackground_message_broker_schedule_pool_size Original article "},{"title":"How to Write C++ Code","type":0,"sectionRef":"#","url":"docs/en/development/style","content":"","keywords":""},{"title":"General Recommendations​","type":1,"pageTitle":"How to Write C++ Code","url":"docs/en/development/style#general-recommendations","content":"1. The following are recommendations, not requirements. 2. If you are editing code, it makes sense to follow the formatting of the existing code. 3. Code style is needed for consistency. Consistency makes it easier to read the code, and it also makes it easier to search the code. 4. Many of the rules do not have logical reasons; they are dictated by established practices. "},{"title":"Formatting​","type":1,"pageTitle":"How to Write C++ Code","url":"docs/en/development/style#formatting","content":"1. Most of the formatting will be done automatically by clang-format. 2. Indents are 4 spaces. Configure your development environment so that a tab adds four spaces. 3. Opening and closing curly brackets must be on a separate line. inline void readBoolText(bool &amp; x, ReadBuffer &amp; buf) { char tmp = '0'; readChar(tmp, buf); x = tmp != '0'; }  4. If the entire function body is a single statement, it can be placed on a single line. Place spaces around curly braces (besides the space at the end of the line). inline size_t mask() const { return buf_size() - 1; } inline size_t place(HashValue x) const { return x &amp; mask(); }  5. For functions. Don’t put spaces around brackets. void reinsert(const Value &amp; x)  memcpy(&amp;buf[place_value], &amp;x, sizeof(x));  6. In if, for, while and other expressions, a space is inserted in front of the opening bracket (as opposed to function calls). for (size_t i = 0; i &lt; rows; i += storage.index_granularity)  7. Add spaces around binary operators (+, -, *, /, %, …) and the ternary operator ?:. UInt16 year = (s[0] - '0') * 1000 + (s[1] - '0') * 100 + (s[2] - '0') * 10 + (s[3] - '0'); UInt8 month = (s[5] - '0') * 10 + (s[6] - '0'); UInt8 day = (s[8] - '0') * 10 + (s[9] - '0');  8. If a line feed is entered, put the operator on a new line and increase the indent before it. if (elapsed_ns) message &lt;&lt; &quot; (&quot; &lt;&lt; rows_read_on_server * 1000000000 / elapsed_ns &lt;&lt; &quot; rows/s., &quot; &lt;&lt; bytes_read_on_server * 1000.0 / elapsed_ns &lt;&lt; &quot; MB/s.) &quot;;  9. You can use spaces for alignment within a line, if desired. dst.ClickLogID = click.LogID; dst.ClickEventID = click.EventID; dst.ClickGoodEvent = click.GoodEvent;  10. Don’t use spaces around the operators ., -&gt;. If necessary, the operator can be wrapped to the next line. In this case, the offset in front of it is increased. 11. Do not use a space to separate unary operators (--, ++, *, &amp;, …) from the argument. 12. Put a space after a comma, but not before it. The same rule goes for a semicolon inside a for expression. 13. Do not use spaces to separate the [] operator. 14. In a template &lt;...&gt; expression, use a space between template and &lt;; no spaces after &lt; or before &gt;. template &lt;typename TKey, typename TValue&gt; struct AggregatedStatElement {}  15. In classes and structures, write public, private, and protected on the same level as class/struct, and indent the rest of the code. template &lt;typename T&gt; class MultiVersion { public: /// Version of object for usage. shared_ptr manage lifetime of version. using Version = std::shared_ptr&lt;const T&gt;; ... }  16. If the same namespace is used for the entire file, and there isn’t anything else significant, an offset is not necessary inside namespace. 17. If the block for an if, for, while, or other expression consists of a single statement, the curly brackets are optional. Place the statement on a separate line, instead. This rule is also valid for nested if, for, while, … But if the inner statement contains curly brackets or else, the external block should be written in curly brackets. /// Finish write. for (auto &amp; stream : streams) stream.second-&gt;finalize();  18. There shouldn’t be any spaces at the ends of lines. 19. Source files are UTF-8 encoded. 20. Non-ASCII characters can be used in string literals. &lt;&lt; &quot;, &quot; &lt;&lt; (timer.elapsed() / chunks_stats.hits) &lt;&lt; &quot; μsec/hit.&quot;;  21. Do not write multiple expressions in a single line. 22. Group sections of code inside functions and separate them with no more than one empty line. 23. Separate functions, classes, and so on with one or two empty lines. 24. A const (related to a value) must be written before the type name. //correct const char * pos const std::string &amp; s //incorrect char const * pos  25. When declaring a pointer or reference, the * and &amp; symbols should be separated by spaces on both sides. //correct const char * pos //incorrect const char* pos const char *pos  26. When using template types, alias them with the using keyword (except in the simplest cases). In other words, the template parameters are specified only in using and aren’t repeated in the code. using can be declared locally, such as inside a function. //correct using FileStreams = std::map&lt;std::string, std::shared_ptr&lt;Stream&gt;&gt;; FileStreams streams; //incorrect std::map&lt;std::string, std::shared_ptr&lt;Stream&gt;&gt; streams;  27. Do not declare several variables of different types in one statement. //incorrect int x, *y;  28. Do not use C-style casts. //incorrect std::cerr &lt;&lt; (int)c &lt;&lt;; std::endl; //correct std::cerr &lt;&lt; static_cast&lt;int&gt;(c) &lt;&lt; std::endl;  29. In classes and structs, group members and functions separately inside each visibility scope. 30. For small classes and structs, it is not necessary to separate the method declaration from the implementation. The same is true for small methods in any classes or structs. For templated classes and structs, do not separate the method declarations from the implementation (because otherwise they must be defined in the same translation unit). 31. You can wrap lines at 140 characters, instead of 80. 32. Always use the prefix increment/decrement operators if postfix is not required. for (Names::const_iterator it = column_names.begin(); it != column_names.end(); ++it)  "},{"title":"Comments​","type":1,"pageTitle":"How to Write C++ Code","url":"docs/en/development/style#comments","content":"1. Be sure to add comments for all non-trivial parts of code. This is very important. Writing the comment might help you realize that the code isn’t necessary, or that it is designed wrong. /** Part of piece of memory, that can be used. * For example, if internal_buffer is 1MB, and there was only 10 bytes loaded to buffer from file for reading, * then working_buffer will have size of only 10 bytes * (working_buffer.end() will point to position right after those 10 bytes available for read). */  2. Comments can be as detailed as necessary. 3. Place comments before the code they describe. In rare cases, comments can come after the code, on the same line. /** Parses and executes the query. */ void executeQuery( ReadBuffer &amp; istr, /// Where to read the query from (and data for INSERT, if applicable) WriteBuffer &amp; ostr, /// Where to write the result Context &amp; context, /// DB, tables, data types, engines, functions, aggregate functions... BlockInputStreamPtr &amp; query_plan, /// Here could be written the description on how query was executed QueryProcessingStage::Enum stage = QueryProcessingStage::Complete /// Up to which stage process the SELECT query )  4. Comments should be written in English only. 5. If you are writing a library, include detailed comments explaining it in the main header file. 6. Do not add comments that do not provide additional information. In particular, do not leave empty comments like this: /* * Procedure Name: * Original procedure name: * Author: * Date of creation: * Dates of modification: * Modification authors: * Original file name: * Purpose: * Intent: * Designation: * Classes used: * Constants: * Local variables: * Parameters: * Date of creation: * Purpose: */  The example is borrowed from the resource http://home.tamk.fi/~jaalto/course/coding-style/doc/unmaintainable-code/. 7. Do not write garbage comments (author, creation date ..) at the beginning of each file. 8. Single-line comments begin with three slashes: /// and multi-line comments begin with /**. These comments are considered “documentation”. Note: You can use Doxygen to generate documentation from these comments. But Doxygen is not generally used because it is more convenient to navigate the code in the IDE. 9. Multi-line comments must not have empty lines at the beginning and end (except the line that closes a multi-line comment). 10. For commenting out code, use basic comments, not “documenting” comments. 11. Delete the commented out parts of the code before committing. 12. Do not use profanity in comments or code. 13. Do not use uppercase letters. Do not use excessive punctuation. /// WHAT THE FAIL???  14. Do not use comments to make delimeters. ///******************************************************  15. Do not start discussions in comments. /// Why did you do this stuff?  16. There’s no need to write a comment at the end of a block describing what it was about. /// for  "},{"title":"Names​","type":1,"pageTitle":"How to Write C++ Code","url":"docs/en/development/style#names","content":"1. Use lowercase letters with underscores in the names of variables and class members. size_t max_block_size;  2. For the names of functions (methods), use camelCase beginning with a lowercase letter. std::string getName() const override { return &quot;Memory&quot;; }  3. For the names of classes (structs), use CamelCase beginning with an uppercase letter. Prefixes other than I are not used for interfaces. class StorageMemory : public IStorage  4. using are named the same way as classes. 5. Names of template type arguments: in simple cases, use T; T, U; T1, T2. For more complex cases, either follow the rules for class names, or add the prefix T. template &lt;typename TKey, typename TValue&gt; struct AggregatedStatElement  6. Names of template constant arguments: either follow the rules for variable names, or use N in simple cases. template &lt;bool without_www&gt; struct ExtractDomain  7. For abstract classes (interfaces) you can add the I prefix. class IBlockInputStream  8. If you use a variable locally, you can use the short name. In all other cases, use a name that describes the meaning. bool info_successfully_loaded = false;  9. Names of defines and global constants use ALL_CAPS with underscores. #define MAX_SRC_TABLE_NAMES_TO_STORE 1000  10. File names should use the same style as their contents. If a file contains a single class, name the file the same way as the class (CamelCase). If the file contains a single function, name the file the same way as the function (camelCase). 11. If the name contains an abbreviation, then: For variable names, the abbreviation should use lowercase letters mysql_connection (not mySQL_connection).For names of classes and functions, keep the uppercase letters in the abbreviationMySQLConnection (not MySqlConnection). 12. Constructor arguments that are used just to initialize the class members should be named the same way as the class members, but with an underscore at the end. FileQueueProcessor( const std::string &amp; path_, const std::string &amp; prefix_, std::shared_ptr&lt;FileHandler&gt; handler_) : path(path_), prefix(prefix_), handler(handler_), log(&amp;Logger::get(&quot;FileQueueProcessor&quot;)) { }  The underscore suffix can be omitted if the argument is not used in the constructor body. 13. There is no difference in the names of local variables and class members (no prefixes required). timer (not m_timer)  14. For the constants in an enum, use CamelCase with a capital letter. ALL_CAPS is also acceptable. If the enum is non-local, use an enum class. enum class CompressionMethod { QuickLZ = 0, LZ4 = 1, };  15. All names must be in English. Transliteration of Hebrew words is not allowed. not T_PAAMAYIM_NEKUDOTAYIM  16. Abbreviations are acceptable if they are well known (when you can easily find the meaning of the abbreviation in Wikipedia or in a search engine). `AST`, `SQL`. Not `NVDH` (some random letters)  Incomplete words are acceptable if the shortened version is common use. You can also use an abbreviation if the full name is included next to it in the comments. 17. File names with C++ source code must have the .cpp extension. Header files must have the .h extension. "},{"title":"How to Write Code​","type":1,"pageTitle":"How to Write C++ Code","url":"docs/en/development/style#how-to-write-code","content":"1. Memory management. Manual memory deallocation (delete) can only be used in library code. In library code, the delete operator can only be used in destructors. In application code, memory must be freed by the object that owns it. Examples: The easiest way is to place an object on the stack, or make it a member of another class.For a large number of small objects, use containers.For automatic deallocation of a small number of objects that reside in the heap, use shared_ptr/unique_ptr. 2. Resource management. Use RAII and see above. 3. Error handling. Use exceptions. In most cases, you only need to throw an exception, and do not need to catch it (because of RAII). In offline data processing applications, it’s often acceptable to not catch exceptions. In servers that handle user requests, it’s usually enough to catch exceptions at the top level of the connection handler. In thread functions, you should catch and keep all exceptions to rethrow them in the main thread after join. /// If there weren't any calculations yet, calculate the first block synchronously if (!started) { calculate(); started = true; } else /// If calculations are already in progress, wait for the result pool.wait(); if (exception) exception-&gt;rethrow();  Never hide exceptions without handling. Never just blindly put all exceptions to log. //Not correct catch (...) {}  If you need to ignore some exceptions, do so only for specific ones and rethrow the rest. catch (const DB::Exception &amp; e) { if (e.code() == ErrorCodes::UNKNOWN_AGGREGATE_FUNCTION) return nullptr; else throw; }  When using functions with response codes or errno, always check the result and throw an exception in case of error. if (0 != close(fd)) throwFromErrno(&quot;Cannot close file &quot; + file_name, ErrorCodes::CANNOT_CLOSE_FILE);  You can use assert to check invariants in code. 4. Exception types. There is no need to use complex exception hierarchy in application code. The exception text should be understandable to a system administrator. 5. Throwing exceptions from destructors. This is not recommended, but it is allowed. Use the following options: Create a function (done() or finalize()) that will do all the work in advance that might lead to an exception. If that function was called, there should be no exceptions in the destructor later.Tasks that are too complex (such as sending messages over the network) can be put in separate method that the class user will have to call before destruction.If there is an exception in the destructor, it’s better to log it than to hide it (if the logger is available).In simple applications, it is acceptable to rely on std::terminate (for cases of noexcept by default in C++11) to handle exceptions. 6. Anonymous code blocks. You can create a separate code block inside a single function in order to make certain variables local, so that the destructors are called when exiting the block. Block block = data.in-&gt;read(); { std::lock_guard&lt;std::mutex&gt; lock(mutex); data.ready = true; data.block = block; } ready_any.set();  7. Multithreading. In offline data processing programs: Try to get the best possible performance on a single CPU core. You can then parallelize your code if necessary. In server applications: Use the thread pool to process requests. At this point, we haven’t had any tasks that required userspace context switching. Fork is not used for parallelization. 8. Syncing threads. Often it is possible to make different threads use different memory cells (even better: different cache lines,) and to not use any thread synchronization (except joinAll). If synchronization is required, in most cases, it is sufficient to use mutex under lock_guard. In other cases use system synchronization primitives. Do not use busy wait. Atomic operations should be used only in the simplest cases. Do not try to implement lock-free data structures unless it is your primary area of expertise. 9. Pointers vs references. In most cases, prefer references. 10. const. Use constant references, pointers to constants, const_iterator, and const methods. Consider const to be default and use non-const only when necessary. When passing variables by value, using const usually does not make sense. 11. unsigned. Use unsigned if necessary. 12. Numeric types. Use the types UInt8, UInt16, UInt32, UInt64, Int8, Int16, Int32, and Int64, as well as size_t, ssize_t, and ptrdiff_t. Don’t use these types for numbers: signed/unsigned long, long long, short, signed/unsigned char, char. 13. Passing arguments. Pass complex values by value if they are going to be moved and use std::move; pass by reference if you want to update value in a loop. If a function captures ownership of an object created in the heap, make the argument type shared_ptr or unique_ptr. 14. Return values. In most cases, just use return. Do not write return std::move(res). If the function allocates an object on heap and returns it, use shared_ptr or unique_ptr. In rare cases (updating a value in a loop) you might need to return the value via an argument. In this case, the argument should be a reference. using AggregateFunctionPtr = std::shared_ptr&lt;IAggregateFunction&gt;; /** Allows creating an aggregate function by its name. */ class AggregateFunctionFactory { public: AggregateFunctionFactory(); AggregateFunctionPtr get(const String &amp; name, const DataTypes &amp; argument_types) const;  15. namespace. There is no need to use a separate namespace for application code. Small libraries do not need this, either. For medium to large libraries, put everything in a namespace. In the library’s .h file, you can use namespace detail to hide implementation details not needed for the application code. In a .cpp file, you can use a static or anonymous namespace to hide symbols. Also, a namespace can be used for an enum to prevent the corresponding names from falling into an external namespace (but it’s better to use an enum class). 16. Deferred initialization. If arguments are required for initialization, then you normally shouldn’t write a default constructor. If later you’ll need to delay initialization, you can add a default constructor that will create an invalid object. Or, for a small number of objects, you can use shared_ptr/unique_ptr. Loader(DB::Connection * connection_, const std::string &amp; query, size_t max_block_size_); /// For deferred initialization Loader() {}  17. Virtual functions. If the class is not intended for polymorphic use, you do not need to make functions virtual. This also applies to the destructor. 18. Encodings. Use UTF-8 everywhere. Use std::string and char *. Do not use std::wstring and wchar_t. 19. Logging. See the examples everywhere in the code. Before committing, delete all meaningless and debug logging, and any other types of debug output. Logging in cycles should be avoided, even on the Trace level. Logs must be readable at any logging level. Logging should only be used in application code, for the most part. Log messages must be written in English. The log should preferably be understandable for the system administrator. Do not use profanity in the log. Use UTF-8 encoding in the log. In rare cases you can use non-ASCII characters in the log. 20. Input-output. Don’t use iostreams in internal cycles that are critical for application performance (and never use stringstream). Use the DB/IO library instead. 21. Date and time. See the DateLUT library. 22. include. Always use #pragma once instead of include guards. 23. using. using namespace is not used. You can use using with something specific. But make it local inside a class or function. 24. Do not use trailing return type for functions unless necessary. auto f() -&gt; void  25. Declaration and initialization of variables. //right way std::string s = &quot;Hello&quot;; std::string s{&quot;Hello&quot;}; //wrong way auto s = std::string{&quot;Hello&quot;};  26. For virtual functions, write virtual in the base class, but write override instead of virtual in descendent classes. "},{"title":"Unused Features of C++​","type":1,"pageTitle":"How to Write C++ Code","url":"docs/en/development/style#unused-features-of-c","content":"1. Virtual inheritance is not used. 2. Exception specifiers from C++03 are not used. "},{"title":"Platform​","type":1,"pageTitle":"How to Write C++ Code","url":"docs/en/development/style#platform","content":"1. We write code for a specific platform. But other things being equal, cross-platform or portable code is preferred. 2. Language: C++20 (see the list of available C++20 features). 3. Compiler: clang. At this time (April 2021), the code is compiled using clang version 11. (It can also be compiled using gcc version 10, but it's untested and not suitable for production usage). The standard library is used (libc++). 4.OS: Linux Ubuntu, not older than Precise. 5.Code is written for x86_64 CPU architecture. The CPU instruction set is the minimum supported set among our servers. Currently, it is SSE 4.2. 6. Use -Wall -Wextra -Werror compilation flags. Also -Weverything is used with few exceptions. 7. Use static linking with all libraries except those that are difficult to connect to statically (see the output of the ldd command). 8. Code is developed and debugged with release settings. "},{"title":"Tools​","type":1,"pageTitle":"How to Write C++ Code","url":"docs/en/development/style#tools","content":"1. KDevelop is a good IDE. 2. For debugging, use gdb, valgrind (memcheck), strace, -fsanitize=..., or tcmalloc_minimal_debug. 3. For profiling, use Linux Perf, valgrind (callgrind), or strace -cf. 4. Sources are in Git. 5. Assembly uses CMake. 6. Programs are released using deb packages. 7. Commits to master must not break the build. Though only selected revisions are considered workable. 8. Make commits as often as possible, even if the code is only partially ready. Use branches for this purpose. If your code in the master branch is not buildable yet, exclude it from the build before the push. You’ll need to finish it or remove it within a few days. 9. For non-trivial changes, use branches and publish them on the server. 10. Unused code is removed from the repository. "},{"title":"Libraries​","type":1,"pageTitle":"How to Write C++ Code","url":"docs/en/development/style#libraries","content":"1. The C++20 standard library is used (experimental extensions are allowed), as well as boost and Poco frameworks. 2. It is not allowed to use libraries from OS packages. It is also not allowed to use pre-installed libraries. All libraries should be placed in form of source code in contrib directory and built with ClickHouse. See Guidelines for adding new third-party libraries for details. 3. Preference is always given to libraries that are already in use. "},{"title":"General Recommendations​","type":1,"pageTitle":"How to Write C++ Code","url":"docs/en/development/style#general-recommendations-1","content":"1. Write as little code as possible. 2. Try the simplest solution. 3. Don’t write code until you know how it’s going to work and how the inner loop will function. 4. In the simplest cases, use using instead of classes or structs. 5. If possible, do not write copy constructors, assignment operators, destructors (other than a virtual one, if the class contains at least one virtual function), move constructors or move assignment operators. In other words, the compiler-generated functions must work correctly. You can use default. 6. Code simplification is encouraged. Reduce the size of your code where possible. "},{"title":"Additional Recommendations​","type":1,"pageTitle":"How to Write C++ Code","url":"docs/en/development/style#additional-recommendations","content":"1. Explicitly specifying std:: for types from stddef.h is not recommended. In other words, we recommend writing size_t instead std::size_t, because it’s shorter. It is acceptable to add std::. 2. Explicitly specifying std:: for functions from the standard C library is not recommended. In other words, write memcpy instead of std::memcpy. The reason is that there are similar non-standard functions, such as memmem. We do use these functions on occasion. These functions do not exist in namespace std. If you write std::memcpy instead of memcpy everywhere, then memmem without std:: will look strange. Nevertheless, you can still use std:: if you prefer it. 3. Using functions from C when the same ones are available in the standard C++ library. This is acceptable if it is more efficient. For example, use memcpy instead of std::copy for copying large chunks of memory. 4. Multiline function arguments. Any of the following wrapping styles are allowed: function( T1 x1, T2 x2)  function( size_t left, size_t right, const &amp; RangesInDataParts ranges, size_t limit)  function(size_t left, size_t right, const &amp; RangesInDataParts ranges, size_t limit)  function(size_t left, size_t right, const &amp; RangesInDataParts ranges, size_t limit)  function( size_t left, size_t right, const &amp; RangesInDataParts ranges, size_t limit)  Original article "},{"title":"Hive","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/integrations/hive","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"Hive","url":"docs/en/engines/table-engines/integrations/hive#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [ALIAS expr1], name2 [type2] [ALIAS expr2], ... ) ENGINE = Hive('thrift://host:port', 'database', 'table'); PARTITION BY expr  See a detailed description of the CREATE TABLE query. The table structure can differ from the original Hive table structure: Column names should be the same as in the original Hive table, but you can use just some of these columns and in any order, also you can use some alias columns calculated from other columns.Column types should be the same from those in the original Hive table.Partition by expression should be consistent with the original Hive table, and columns in partition by expression should be in the table structure. Engine Parameters thrift://host:port — Hive Metastore address database — Remote database name. table — Remote table name. "},{"title":"Usage Example​","type":1,"pageTitle":"Hive","url":"docs/en/engines/table-engines/integrations/hive#usage-example","content":""},{"title":"How to Use Local Cache for HDFS Filesystem​","type":1,"pageTitle":"Hive","url":"docs/en/engines/table-engines/integrations/hive#how-to-use-local-cache-for-hdfs-filesystem","content":"We strongly advice you to enable local cache for remote filesystems. Benchmark shows that its almost 2x faster with cache. Before using cache, add it to config.xml &lt;local_cache_for_remote_fs&gt; &lt;enable&gt;true&lt;/enable&gt; &lt;root_dir&gt;local_cache&lt;/root_dir&gt; &lt;limit_size&gt;559096952&lt;/limit_size&gt; &lt;bytes_read_before_flush&gt;1048576&lt;/bytes_read_before_flush&gt; &lt;/local_cache_for_remote_fs&gt;  enable: ClickHouse will maintain local cache for remote filesystem(HDFS) after startup if true.root_dir: Required. The root directory to store local cache files for remote filesystem.limit_size: Required. The maximum size(in bytes) of local cache files.bytes_read_before_flush: Control bytes before flush to local filesystem when downloading file from remote filesystem. The default value is 1MB. When ClickHouse is started up with local cache for remote filesystem enabled, users can still choose not to use cache with settings use_local_cache_for_remote_fs = 0 in their query. use_local_cache_for_remote_fs is false in default. "},{"title":"Query Hive Table with ORC Input Format​","type":1,"pageTitle":"Hive","url":"docs/en/engines/table-engines/integrations/hive#query-hive-table-with-orc-input-format","content":"Create Table in Hive​ hive &gt; CREATE TABLE `test`.`test_orc`( `f_tinyint` tinyint, `f_smallint` smallint, `f_int` int, `f_integer` int, `f_bigint` bigint, `f_float` float, `f_double` double, `f_decimal` decimal(10,0), `f_timestamp` timestamp, `f_date` date, `f_string` string, `f_varchar` varchar(100), `f_bool` boolean, `f_binary` binary, `f_array_int` array&lt;int&gt;, `f_array_string` array&lt;string&gt;, `f_array_float` array&lt;float&gt;, `f_array_array_int` array&lt;array&lt;int&gt;&gt;, `f_array_array_string` array&lt;array&lt;string&gt;&gt;, `f_array_array_float` array&lt;array&lt;float&gt;&gt;) PARTITIONED BY ( `day` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat' LOCATION 'hdfs://testcluster/data/hive/test.db/test_orc' OK Time taken: 0.51 seconds hive &gt; insert into test.test_orc partition(day='2021-09-18') select 1, 2, 3, 4, 5, 6.11, 7.22, 8.333, current_timestamp(), current_date(), 'hello world', 'hello world', 'hello world', true, 'hello world', array(1, 2, 3), array('hello world', 'hello world'), array(float(1.1), float(1.2)), array(array(1, 2), array(3, 4)), array(array('a', 'b'), array('c', 'd')), array(array(float(1.11), float(2.22)), array(float(3.33), float(4.44))); OK Time taken: 36.025 seconds hive &gt; select * from test.test_orc; OK 1 2 3 4 5 6.11 7.22 8 2021-11-05 12:38:16.314 2021-11-05 hello world hello world hello world true hello world [1,2,3] [&quot;hello world&quot;,&quot;hello world&quot;] [1.1,1.2] [[1,2],[3,4]] [[&quot;a&quot;,&quot;b&quot;],[&quot;c&quot;,&quot;d&quot;]] [[1.11,2.22],[3.33,4.44]] 2021-09-18 Time taken: 0.295 seconds, Fetched: 1 row(s)  Create Table in ClickHouse​ Table in ClickHouse, retrieving data from the Hive table created above: CREATE TABLE test.test_orc ( `f_tinyint` Int8, `f_smallint` Int16, `f_int` Int32, `f_integer` Int32, `f_bigint` Int64, `f_float` Float32, `f_double` Float64, `f_decimal` Float64, `f_timestamp` DateTime, `f_date` Date, `f_string` String, `f_varchar` String, `f_bool` Bool, `f_binary` String, `f_array_int` Array(Int32), `f_array_string` Array(String), `f_array_float` Array(Float32), `f_array_array_int` Array(Array(Int32)), `f_array_array_string` Array(Array(String)), `f_array_array_float` Array(Array(Float32)), `day` String ) ENGINE = Hive('thrift://202.168.117.26:9083', 'test', 'test_orc') PARTITION BY day  SELECT * FROM test.test_orc settings input_format_orc_allow_missing_columns = 1\\G  SELECT * FROM test.test_orc SETTINGS input_format_orc_allow_missing_columns = 1 Query id: c3eaffdc-78ab-43cd-96a4-4acc5b480658 Row 1: ────── f_tinyint: 1 f_smallint: 2 f_int: 3 f_integer: 4 f_bigint: 5 f_float: 6.11 f_double: 7.22 f_decimal: 8 f_timestamp: 2021-12-04 04:00:44 f_date: 2021-12-03 f_string: hello world f_varchar: hello world f_bool: true f_binary: hello world f_array_int: [1,2,3] f_array_string: ['hello world','hello world'] f_array_float: [1.1,1.2] f_array_array_int: [[1,2],[3,4]] f_array_array_string: [['a','b'],['c','d']] f_array_array_float: [[1.11,2.22],[3.33,4.44]] day: 2021-09-18 1 rows in set. Elapsed: 0.078 sec.  "},{"title":"Query Hive Table with Parquet Input Format​","type":1,"pageTitle":"Hive","url":"docs/en/engines/table-engines/integrations/hive#query-hive-table-with-parquet-input-format","content":"Create Table in Hive​ hive &gt; CREATE TABLE `test`.`test_parquet`( `f_tinyint` tinyint, `f_smallint` smallint, `f_int` int, `f_integer` int, `f_bigint` bigint, `f_float` float, `f_double` double, `f_decimal` decimal(10,0), `f_timestamp` timestamp, `f_date` date, `f_string` string, `f_varchar` varchar(100), `f_char` char(100), `f_bool` boolean, `f_binary` binary, `f_array_int` array&lt;int&gt;, `f_array_string` array&lt;string&gt;, `f_array_float` array&lt;float&gt;, `f_array_array_int` array&lt;array&lt;int&gt;&gt;, `f_array_array_string` array&lt;array&lt;string&gt;&gt;, `f_array_array_float` array&lt;array&lt;float&gt;&gt;) PARTITIONED BY ( `day` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION 'hdfs://testcluster/data/hive/test.db/test_parquet' OK Time taken: 0.51 seconds hive &gt; insert into test.test_parquet partition(day='2021-09-18') select 1, 2, 3, 4, 5, 6.11, 7.22, 8.333, current_timestamp(), current_date(), 'hello world', 'hello world', 'hello world', true, 'hello world', array(1, 2, 3), array('hello world', 'hello world'), array(float(1.1), float(1.2)), array(array(1, 2), array(3, 4)), array(array('a', 'b'), array('c', 'd')), array(array(float(1.11), float(2.22)), array(float(3.33), float(4.44))); OK Time taken: 36.025 seconds hive &gt; select * from test.test_parquet; OK 1 2 3 4 5 6.11 7.22 8 2021-12-14 17:54:56.743 2021-12-14 hello world hello world hello world true hello world [1,2,3] [&quot;hello world&quot;,&quot;hello world&quot;] [1.1,1.2] [[1,2],[3,4]] [[&quot;a&quot;,&quot;b&quot;],[&quot;c&quot;,&quot;d&quot;]] [[1.11,2.22],[3.33,4.44]] 2021-09-18 Time taken: 0.766 seconds, Fetched: 1 row(s)  Create Table in ClickHouse​ Table in ClickHouse, retrieving data from the Hive table created above: CREATE TABLE test.test_parquet ( `f_tinyint` Int8, `f_smallint` Int16, `f_int` Int32, `f_integer` Int32, `f_bigint` Int64, `f_float` Float32, `f_double` Float64, `f_decimal` Float64, `f_timestamp` DateTime, `f_date` Date, `f_string` String, `f_varchar` String, `f_char` String, `f_bool` Bool, `f_binary` String, `f_array_int` Array(Int32), `f_array_string` Array(String), `f_array_float` Array(Float32), `f_array_array_int` Array(Array(Int32)), `f_array_array_string` Array(Array(String)), `f_array_array_float` Array(Array(Float32)), `day` String ) ENGINE = Hive('thrift://localhost:9083', 'test', 'test_parquet') PARTITION BY day  SELECT * FROM test.test_parquet settings input_format_parquet_allow_missing_columns = 1\\G  SELECT * FROM test_parquet SETTINGS input_format_parquet_allow_missing_columns = 1 Query id: 4e35cf02-c7b2-430d-9b81-16f438e5fca9 Row 1: ────── f_tinyint: 1 f_smallint: 2 f_int: 3 f_integer: 4 f_bigint: 5 f_float: 6.11 f_double: 7.22 f_decimal: 8 f_timestamp: 2021-12-14 17:54:56 f_date: 2021-12-14 f_string: hello world f_varchar: hello world f_char: hello world f_bool: true f_binary: hello world f_array_int: [1,2,3] f_array_string: ['hello world','hello world'] f_array_float: [1.1,1.2] f_array_array_int: [[1,2],[3,4]] f_array_array_string: [['a','b'],['c','d']] f_array_array_float: [[1.11,2.22],[3.33,4.44]] day: 2021-09-18 1 rows in set. Elapsed: 0.357 sec.  "},{"title":"Query Hive Table with Text Input Format​","type":1,"pageTitle":"Hive","url":"docs/en/engines/table-engines/integrations/hive#query-hive-table-with-text-input-format","content":"Create Table in Hive​ hive &gt; CREATE TABLE `test`.`test_text`( `f_tinyint` tinyint, `f_smallint` smallint, `f_int` int, `f_integer` int, `f_bigint` bigint, `f_float` float, `f_double` double, `f_decimal` decimal(10,0), `f_timestamp` timestamp, `f_date` date, `f_string` string, `f_varchar` varchar(100), `f_char` char(100), `f_bool` boolean, `f_binary` binary, `f_array_int` array&lt;int&gt;, `f_array_string` array&lt;string&gt;, `f_array_float` array&lt;float&gt;, `f_array_array_int` array&lt;array&lt;int&gt;&gt;, `f_array_array_string` array&lt;array&lt;string&gt;&gt;, `f_array_array_float` array&lt;array&lt;float&gt;&gt;) PARTITIONED BY ( `day` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 'hdfs://testcluster/data/hive/test.db/test_text' Time taken: 0.1 seconds, Fetched: 34 row(s) hive &gt; insert into test.test_text partition(day='2021-09-18') select 1, 2, 3, 4, 5, 6.11, 7.22, 8.333, current_timestamp(), current_date(), 'hello world', 'hello world', 'hello world', true, 'hello world', array(1, 2, 3), array('hello world', 'hello world'), array(float(1.1), float(1.2)), array(array(1, 2), array(3, 4)), array(array('a', 'b'), array('c', 'd')), array(array(float(1.11), float(2.22)), array(float(3.33), float(4.44))); OK Time taken: 36.025 seconds hive &gt; select * from test.test_text; OK 1 2 3 4 5 6.11 7.22 8 2021-12-14 18:11:17.239 2021-12-14 hello world hello world hello world true hello world [1,2,3] [&quot;hello world&quot;,&quot;hello world&quot;] [1.1,1.2] [[1,2],[3,4]] [[&quot;a&quot;,&quot;b&quot;],[&quot;c&quot;,&quot;d&quot;]] [[1.11,2.22],[3.33,4.44]] 2021-09-18 Time taken: 0.624 seconds, Fetched: 1 row(s)  Create Table in ClickHouse​ Table in ClickHouse, retrieving data from the Hive table created above: CREATE TABLE test.test_text ( `f_tinyint` Int8, `f_smallint` Int16, `f_int` Int32, `f_integer` Int32, `f_bigint` Int64, `f_float` Float32, `f_double` Float64, `f_decimal` Float64, `f_timestamp` DateTime, `f_date` Date, `f_string` String, `f_varchar` String, `f_char` String, `f_bool` Bool, `day` String ) ENGINE = Hive('thrift://localhost:9083', 'test', 'test_text') PARTITION BY day  SELECT * FROM test.test_text settings input_format_skip_unknown_fields = 1, input_format_with_names_use_header = 1, date_time_input_format = 'best_effort'\\G  SELECT * FROM test.test_text SETTINGS input_format_skip_unknown_fields = 1, input_format_with_names_use_header = 1, date_time_input_format = 'best_effort' Query id: 55b79d35-56de-45b9-8be6-57282fbf1f44 Row 1: ────── f_tinyint: 1 f_smallint: 2 f_int: 3 f_integer: 4 f_bigint: 5 f_float: 6.11 f_double: 7.22 f_decimal: 8 f_timestamp: 2021-12-14 18:11:17 f_date: 2021-12-14 f_string: hello world f_varchar: hello world f_char: hello world f_bool: true day: 2021-09-18  Original article "},{"title":"SQLite","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/integrations/sqlite","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"SQLite","url":"docs/en/engines/table-engines/integrations/sqlite#creating-a-table","content":" CREATE TABLE [IF NOT EXISTS] [db.]table_name ( name1 [type1], name2 [type2], ... ) ENGINE = SQLite('db_path', 'table')  Engine Parameters db_path — Path to SQLite file with a database.table — Name of a table in the SQLite database. "},{"title":"Usage Example​","type":1,"pageTitle":"SQLite","url":"docs/en/engines/table-engines/integrations/sqlite#usage-example","content":"Shows a query creating the SQLite table: SHOW CREATE TABLE sqlite_db.table2;  CREATE TABLE SQLite.table2 ( `col1` Nullable(Int32), `col2` Nullable(String) ) ENGINE = SQLite('sqlite.db','table2');  Returns the data from the table: SELECT * FROM sqlite_db.table2 ORDER BY col1;  ┌─col1─┬─col2──┐ │ 1 │ text1 │ │ 2 │ text2 │ │ 3 │ text3 │ └──────┴───────┘  See Also SQLite enginesqlite table function Original article "},{"title":"Log","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/log-family/log","content":"Log The engine belongs to the family of Log engines. See the common properties of Log engines and their differences in the Log Engine Family article. Log differs from TinyLog in that a small file of &quot;marks&quot; resides with the column files. These marks are written on every data block and contain offsets that indicate where to start reading the file in order to skip the specified number of rows. This makes it possible to read table data in multiple threads. For concurrent data access, the read operations can be performed simultaneously, while write operations block reads and each other. The Log engine does not support indexes. Similarly, if writing to a table failed, the table is broken, and reading from it returns an error. The Log engine is appropriate for temporary data, write-once tables, and for testing or demonstration purposes. Original article","keywords":""},{"title":"Stripelog","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/log-family/stripelog","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"Stripelog","url":"docs/en/engines/table-engines/log-family/stripelog#table_engines-stripelog-creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( column1_name [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], column2_name [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE = StripeLog  See the detailed description of the CREATE TABLE query. "},{"title":"Writing the Data​","type":1,"pageTitle":"Stripelog","url":"docs/en/engines/table-engines/log-family/stripelog#table_engines-stripelog-writing-the-data","content":"The StripeLog engine stores all the columns in one file. For each INSERT query, ClickHouse appends the data block to the end of a table file, writing columns one by one. For each table ClickHouse writes the files: data.bin — Data file.index.mrk — File with marks. Marks contain offsets for each column of each data block inserted. The StripeLog engine does not support the ALTER UPDATE and ALTER DELETE operations. "},{"title":"Reading the Data​","type":1,"pageTitle":"Stripelog","url":"docs/en/engines/table-engines/log-family/stripelog#table_engines-stripelog-reading-the-data","content":"The file with marks allows ClickHouse to parallelize the reading of data. This means that a SELECT query returns rows in an unpredictable order. Use the ORDER BY clause to sort rows. "},{"title":"Example of Use​","type":1,"pageTitle":"Stripelog","url":"docs/en/engines/table-engines/log-family/stripelog#table_engines-stripelog-example-of-use","content":"Creating a table: CREATE TABLE stripe_log_table ( timestamp DateTime, message_type String, message String ) ENGINE = StripeLog  Inserting data: INSERT INTO stripe_log_table VALUES (now(),'REGULAR','The first regular message') INSERT INTO stripe_log_table VALUES (now(),'REGULAR','The second regular message'),(now(),'WARNING','The first warning message')  We used two INSERT queries to create two data blocks inside the data.bin file. ClickHouse uses multiple threads when selecting data. Each thread reads a separate data block and returns resulting rows independently as it finishes. As a result, the order of blocks of rows in the output does not match the order of the same blocks in the input in most cases. For example: SELECT * FROM stripe_log_table  ┌───────────timestamp─┬─message_type─┬─message────────────────────┐ │ 2019-01-18 14:27:32 │ REGULAR │ The second regular message │ │ 2019-01-18 14:34:53 │ WARNING │ The first warning message │ └─────────────────────┴──────────────┴────────────────────────────┘ ┌───────────timestamp─┬─message_type─┬─message───────────────────┐ │ 2019-01-18 14:23:43 │ REGULAR │ The first regular message │ └─────────────────────┴──────────────┴───────────────────────────┘  Sorting the results (ascending order by default): SELECT * FROM stripe_log_table ORDER BY timestamp  ┌───────────timestamp─┬─message_type─┬─message────────────────────┐ │ 2019-01-18 14:23:43 │ REGULAR │ The first regular message │ │ 2019-01-18 14:27:32 │ REGULAR │ The second regular message │ │ 2019-01-18 14:34:53 │ WARNING │ The first warning message │ └─────────────────────┴──────────────┴────────────────────────────┘  Original article "},{"title":"Log Engine Family","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/log-family/","content":"","keywords":""},{"title":"Common Properties​","type":1,"pageTitle":"Log Engine Family","url":"docs/en/engines/table-engines/log-family/#common-properties","content":"Engines: Store data on a disk. Append data to the end of file when writing. Support locks for concurrent data access. During INSERT queries, the table is locked, and other queries for reading and writing data both wait for the table to unlock. If there are no data writing queries, any number of data reading queries can be performed concurrently. Do not support mutations. Do not support indexes. This means that SELECT queries for ranges of data are not efficient. Do not write data atomically. You can get a table with corrupted data if something breaks the write operation, for example, abnormal server shutdown. "},{"title":"Differences​","type":1,"pageTitle":"Log Engine Family","url":"docs/en/engines/table-engines/log-family/#differences","content":"The TinyLog engine is the simplest in the family and provides the poorest functionality and lowest efficiency. The TinyLog engine does not support parallel data reading by several threads in a single query. It reads data slower than other engines in the family that support parallel reading from a single query and it uses almost as many file descriptors as the Log engine because it stores each column in a separate file. Use it only in simple scenarios. The Log and StripeLog engines support parallel data reading. When reading data, ClickHouse uses multiple threads. Each thread processes a separate data block. The Log engine uses a separate file for each column of the table. StripeLog stores all the data in one file. As a result, the StripeLog engine uses fewer file descriptors, but the Log engine provides higher efficiency when reading data. Original article "},{"title":"PostgreSQL","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/integrations/postgresql","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"PostgreSQL","url":"docs/en/engines/table-engines/integrations/postgresql#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [TTL expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [TTL expr2], ... ) ENGINE = PostgreSQL('host:port', 'database', 'table', 'user', 'password'[, `schema`]);  See a detailed description of the CREATE TABLE query. The table structure can differ from the original PostgreSQL table structure: Column names should be the same as in the original PostgreSQL table, but you can use just some of these columns and in any order.Column types may differ from those in the original PostgreSQL table. ClickHouse tries to cast values to the ClickHouse data types.The external_table_functions_use_nulls setting defines how to handle Nullable columns. Default value: 1. If 0, the table function does not make Nullable columns and inserts default values instead of nulls. This is also applicable for NULL values inside arrays. Engine Parameters host:port — PostgreSQL server address.database — Remote database name.table — Remote table name.user — PostgreSQL user.password — User password.schema — Non-default table schema. Optional.on conflict ... — example: ON CONFLICT DO NOTHING. Optional. Note: adding this option will make insertion less efficient. or via config (since version 21.11): &lt;named_collections&gt; &lt;postgres1&gt; &lt;host&gt;&lt;/host&gt; &lt;port&gt;&lt;/port&gt; &lt;username&gt;&lt;/username&gt; &lt;password&gt;&lt;/password&gt; &lt;table&gt;&lt;/table&gt; &lt;/postgres1&gt; &lt;postgres2&gt; &lt;host&gt;&lt;/host&gt; &lt;port&gt;&lt;/port&gt; &lt;username&gt;&lt;/username&gt; &lt;password&gt;&lt;/password&gt; &lt;/postgres2&gt; &lt;/named_collections&gt;  Some parameters can be overriden by key value arguments: SELECT * FROM postgresql(postgres1, schema='schema1', table='table1');  "},{"title":"Implementation Details​","type":1,"pageTitle":"PostgreSQL","url":"docs/en/engines/table-engines/integrations/postgresql#implementation-details","content":"SELECT queries on PostgreSQL side run as COPY (SELECT ...) TO STDOUT inside read-only PostgreSQL transaction with commit after each SELECT query. Simple WHERE clauses such as =, !=, &gt;, &gt;=, &lt;, &lt;=, and IN are executed on the PostgreSQL server. All joins, aggregations, sorting, IN [ array ] conditions and the LIMIT sampling constraint are executed in ClickHouse only after the query to PostgreSQL finishes. INSERT queries on PostgreSQL side run as COPY &quot;table_name&quot; (field1, field2, ... fieldN) FROM STDIN inside PostgreSQL transaction with auto-commit after each INSERT statement. PostgreSQL Array types are converted into ClickHouse arrays. warning Be careful - in PostgreSQL an array data, created like a type_name[], may contain multi-dimensional arrays of different dimensions in different table rows in same column. But in ClickHouse it is only allowed to have multidimensional arrays of the same count of dimensions in all table rows in same column. Supports multiple replicas that must be listed by |. For example: CREATE TABLE test_replicas (id UInt32, name String) ENGINE = PostgreSQL(`postgres{2|3|4}:5432`, 'clickhouse', 'test_replicas', 'postgres', 'mysecretpassword');  Replicas priority for PostgreSQL dictionary source is supported. The bigger the number in map, the less the priority. The highest priority is 0. In the example below replica example01-1 has the highest priority: &lt;postgresql&gt; &lt;port&gt;5432&lt;/port&gt; &lt;user&gt;clickhouse&lt;/user&gt; &lt;password&gt;qwerty&lt;/password&gt; &lt;replica&gt; &lt;host&gt;example01-1&lt;/host&gt; &lt;priority&gt;1&lt;/priority&gt; &lt;/replica&gt; &lt;replica&gt; &lt;host&gt;example01-2&lt;/host&gt; &lt;priority&gt;2&lt;/priority&gt; &lt;/replica&gt; &lt;db&gt;db_name&lt;/db&gt; &lt;table&gt;table_name&lt;/table&gt; &lt;where&gt;id=10&lt;/where&gt; &lt;invalidate_query&gt;SQL_QUERY&lt;/invalidate_query&gt; &lt;/postgresql&gt; &lt;/source&gt;  "},{"title":"Usage Example​","type":1,"pageTitle":"PostgreSQL","url":"docs/en/engines/table-engines/integrations/postgresql#usage-example","content":"Table in PostgreSQL: postgres=# CREATE TABLE &quot;public&quot;.&quot;test&quot; ( &quot;int_id&quot; SERIAL, &quot;int_nullable&quot; INT NULL DEFAULT NULL, &quot;float&quot; FLOAT NOT NULL, &quot;str&quot; VARCHAR(100) NOT NULL DEFAULT '', &quot;float_nullable&quot; FLOAT NULL DEFAULT NULL, PRIMARY KEY (int_id)); CREATE TABLE postgres=# INSERT INTO test (int_id, str, &quot;float&quot;) VALUES (1,'test',2); INSERT 0 1 postgresql&gt; SELECT * FROM test; int_id | int_nullable | float | str | float_nullable --------+--------------+-------+------+---------------- 1 | | 2 | test | (1 row)  Table in ClickHouse, retrieving data from the PostgreSQL table created above: CREATE TABLE default.postgresql_table ( `float_nullable` Nullable(Float32), `str` String, `int_id` Int32 ) ENGINE = PostgreSQL('localhost:5432', 'public', 'test', 'postges_user', 'postgres_password');  SELECT * FROM postgresql_table WHERE str IN ('test');  ┌─float_nullable─┬─str──┬─int_id─┐ │ ᴺᵁᴸᴸ │ test │ 1 │ └────────────────┴──────┴────────┘  Using Non-default Schema: postgres=# CREATE SCHEMA &quot;nice.schema&quot;; postgres=# CREATE TABLE &quot;nice.schema&quot;.&quot;nice.table&quot; (a integer); postgres=# INSERT INTO &quot;nice.schema&quot;.&quot;nice.table&quot; SELECT i FROM generate_series(0, 99) as t(i)  CREATE TABLE pg_table_schema_with_dots (a UInt32) ENGINE PostgreSQL('localhost:5432', 'clickhouse', 'nice.table', 'postgrsql_user', 'password', 'nice.schema');  See Also The postgresql table functionUsing PostgreSQL as a source of external dictionary Original article "},{"title":"TinyLog","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/log-family/tinylog","content":"TinyLog The engine belongs to the log engine family. See Log Engine Family for common properties of log engines and their differences. This table engine is typically used with the write-once method: write data one time, then read it as many times as necessary. For example, you can use TinyLog-type tables for intermediary data that is processed in small batches. Note that storing data in a large number of small tables is inefficient. Queries are executed in a single stream. In other words, this engine is intended for relatively small tables (up to about 1,000,000 rows). It makes sense to use this table engine if you have many small tables, since it’s simpler than the Log engine (fewer files need to be opened). Original article","keywords":""},{"title":"S3 Table Engine","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/integrations/s3","content":"","keywords":""},{"title":"Create Table​","type":1,"pageTitle":"S3 Table Engine","url":"docs/en/engines/table-engines/integrations/s3#creating-a-table","content":"CREATE TABLE s3_engine_table (name String, value UInt32) ENGINE = S3(path, [aws_access_key_id, aws_secret_access_key,] format, [compression]) [SETTINGS ...]  Engine parameters path — Bucket url with path to file. Supports following wildcards in readonly mode: *, ?, {abc,def} and {N..M} where N, M — numbers, 'abc', 'def' — strings. For more information see below.format — The format of the file.aws_access_key_id, aws_secret_access_key - Long-term credentials for the AWS account user. You can use these to authenticate your requests. Parameter is optional. If credentials are not specified, they are used from the configuration file. For more information see Using S3 for Data Storage.compression — Compression type. Supported values: none, gzip/gz, brotli/br, xz/LZMA, zstd/zst. Parameter is optional. By default, it will autodetect compression by file extension. Example CREATE TABLE s3_engine_table (name String, value UInt32) ENGINE=S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'gzip') SETTINGS input_format_with_names_use_header = 0; INSERT INTO s3_engine_table VALUES ('one', 1), ('two', 2), ('three', 3); SELECT * FROM s3_engine_table LIMIT 2;  ┌─name─┬─value─┐ │ one │ 1 │ │ two │ 2 │ └──────┴───────┘  "},{"title":"Virtual columns​","type":1,"pageTitle":"S3 Table Engine","url":"docs/en/engines/table-engines/integrations/s3#virtual-columns","content":"_path — Path to the file._file — Name of the file. For more information about virtual columns see here. "},{"title":"Implementation Details​","type":1,"pageTitle":"S3 Table Engine","url":"docs/en/engines/table-engines/integrations/s3#implementation-details","content":"Reads and writes can be parallelZero-copy replication is supported. Not supported: ALTER and SELECT...SAMPLE operations.Indexes. "},{"title":"Wildcards In Path​","type":1,"pageTitle":"S3 Table Engine","url":"docs/en/engines/table-engines/integrations/s3#wildcards-in-path","content":"path argument can specify multiple files using bash-like wildcards. For being processed file should exist and match to the whole path pattern. Listing of files is determined during SELECT (not at CREATE moment). * — Substitutes any number of any characters except / including empty string.? — Substitutes any single character.{some_string,another_string,yet_another_one} — Substitutes any of strings 'some_string', 'another_string', 'yet_another_one'.{N..M} — Substitutes any number in range from N to M including both borders. N and M can have leading zeroes e.g. 000..078. Constructions with {} are similar to the remote table function. warning If the listing of files contains number ranges with leading zeros, use the construction with braces for each digit separately or use ?. Example with wildcards 1 Create table with files named file-000.csv, file-001.csv, … , file-999.csv: CREATE TABLE big_table (name String, value UInt32) ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/my_folder/file-{000..999}.csv', 'CSV');  Example with wildcards 2 Suppose we have several files in CSV format with the following URIs on S3: 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_1.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_2.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_3.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_1.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_2.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_3.csv' There are several ways to make a table consisting of all six files: Specify the range of file postfixes: CREATE TABLE table_with_range (name String, value UInt32) ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_{1..3}', 'CSV');  Take all files with some_file_ prefix (there should be no extra files with such prefix in both folders): CREATE TABLE table_with_question_mark (name String, value UInt32) ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_?', 'CSV');  Take all the files in both folders (all files should satisfy format and schema described in query): CREATE TABLE table_with_asterisk (name String, value UInt32) ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/*', 'CSV');  "},{"title":"S3-related Settings​","type":1,"pageTitle":"S3 Table Engine","url":"docs/en/engines/table-engines/integrations/s3#settings","content":"The following settings can be set before query execution or placed into configuration file. s3_max_single_part_upload_size — The maximum size of object to upload using singlepart upload to S3. Default value is 64Mb.s3_min_upload_part_size — The minimum size of part to upload during multipart upload to S3 Multipart upload. Default value is 512Mb.s3_max_redirects — Max number of S3 redirects hops allowed. Default value is 10.s3_single_read_retries — The maximum number of attempts during single read. Default value is 4. Security consideration: if malicious user can specify arbitrary S3 URLs, s3_max_redirects must be set to zero to avoid SSRF attacks; or alternatively, remote_host_filter must be specified in server configuration. "},{"title":"Endpoint-based Settings​","type":1,"pageTitle":"S3 Table Engine","url":"docs/en/engines/table-engines/integrations/s3#endpoint-settings","content":"The following settings can be specified in configuration file for given endpoint (which will be matched by exact prefix of a URL): endpoint — Specifies prefix of an endpoint. Mandatory.access_key_id and secret_access_key — Specifies credentials to use with given endpoint. Optional.use_environment_credentials — If set to true, S3 client will try to obtain credentials from environment variables and Amazon EC2 metadata for given endpoint. Optional, default value is false.region — Specifies S3 region name. Optional.use_insecure_imds_request — If set to true, S3 client will use insecure IMDS request while obtaining credentials from Amazon EC2 metadata. Optional, default value is false.header — Adds specified HTTP header to a request to given endpoint. Optional, can be speficied multiple times.server_side_encryption_customer_key_base64 — If specified, required headers for accessing S3 objects with SSE-C encryption will be set. Optional.max_single_read_retries — The maximum number of attempts during single read. Default value is 4. Optional. Example: &lt;s3&gt; &lt;endpoint-name&gt; &lt;endpoint&gt;https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/&lt;/endpoint&gt; &lt;!-- &lt;access_key_id&gt;ACCESS_KEY_ID&lt;/access_key_id&gt; --&gt; &lt;!-- &lt;secret_access_key&gt;SECRET_ACCESS_KEY&lt;/secret_access_key&gt; --&gt; &lt;!-- &lt;region&gt;us-west-1&lt;/region&gt; --&gt; &lt;!-- &lt;use_environment_credentials&gt;false&lt;/use_environment_credentials&gt; --&gt; &lt;!-- &lt;use_insecure_imds_request&gt;false&lt;/use_insecure_imds_request&gt; --&gt; &lt;!-- &lt;header&gt;Authorization: Bearer SOME-TOKEN&lt;/header&gt; --&gt; &lt;!-- &lt;server_side_encryption_customer_key_base64&gt;BASE64-ENCODED-KEY&lt;/server_side_encryption_customer_key_base64&gt; --&gt; &lt;!-- &lt;max_single_read_retries&gt;4&lt;/max_single_read_retries&gt; --&gt; &lt;/endpoint-name&gt; &lt;/s3&gt;  "},{"title":"See also​","type":1,"pageTitle":"S3 Table Engine","url":"docs/en/engines/table-engines/integrations/s3#see-also","content":"s3 table function Original article "},{"title":"MergeTree Engine Family","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/mergetree-family/","content":"MergeTree Engine Family Table engines from the MergeTree family are the core of ClickHouse data storage capabilities. They provide most features for resilience and high-performance data retrieval: columnar storage, custom partitioning, sparse primary index, secondary data-skipping indexes, etc. Base MergeTree table engine can be considered the default table engine for single-node ClickHouse instances because it is versatile and practical for a wide range of use cases. For production usage ReplicatedMergeTree is the way to go, because it adds high-availability to all features of regular MergeTree engine. A bonus is automatic data deduplication on data ingestion, so the software can safely retry if there was some network issue during insert. All other engines of MergeTree family add extra functionality for some specific use cases. Usually, it’s implemented as additional data manipulation in background. The main downside of MergeTree engines is that they are rather heavy-weight. So the typical pattern is to have not so many of them. If you need many small tables, for example for temporary data, consider Log engine family.","keywords":""},{"title":"RabbitMQ Engine","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/integrations/rabbitmq","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"RabbitMQ Engine","url":"docs/en/engines/table-engines/integrations/rabbitmq#table_engine-rabbitmq-creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE = RabbitMQ SETTINGS rabbitmq_host_port = 'host:port' [or rabbitmq_address = 'amqp(s)://guest:guest@localhost/vhost'], rabbitmq_exchange_name = 'exchange_name', rabbitmq_format = 'data_format'[,] [rabbitmq_exchange_type = 'exchange_type',] [rabbitmq_routing_key_list = 'key1,key2,...',] [rabbitmq_secure = 0,] [rabbitmq_row_delimiter = 'delimiter_symbol',] [rabbitmq_schema = '',] [rabbitmq_num_consumers = N,] [rabbitmq_num_queues = N,] [rabbitmq_queue_base = 'queue',] [rabbitmq_deadletter_exchange = 'dl-exchange',] [rabbitmq_persistent = 0,] [rabbitmq_skip_broken_messages = N,] [rabbitmq_max_block_size = N,] [rabbitmq_flush_interval_ms = N] [rabbitmq_queue_settings_list = 'x-dead-letter-exchange=my-dlx,x-max-length=10,x-overflow=reject-publish']  Required parameters: rabbitmq_host_port – host:port (for example, localhost:5672).rabbitmq_exchange_name – RabbitMQ exchange name.rabbitmq_format – Message format. Uses the same notation as the SQL FORMAT function, such as JSONEachRow. For more information, see the Formats section. Optional parameters: rabbitmq_exchange_type – The type of RabbitMQ exchange: direct, fanout, topic, headers, consistent_hash. Default: fanout.rabbitmq_routing_key_list – A comma-separated list of routing keys.rabbitmq_row_delimiter – Delimiter character, which ends the message.rabbitmq_schema – Parameter that must be used if the format requires a schema definition. For example, Cap’n Proto requires the path to the schema file and the name of the root schema.capnp:Message object.rabbitmq_num_consumers – The number of consumers per table. Default: 1. Specify more consumers if the throughput of one consumer is insufficient.rabbitmq_num_queues – Total number of queues. Default: 1. Increasing this number can significantly improve performance.rabbitmq_queue_base - Specify a hint for queue names. Use cases of this setting are described below.rabbitmq_deadletter_exchange - Specify name for a dead letter exchange. You can create another table with this exchange name and collect messages in cases when they are republished to dead letter exchange. By default dead letter exchange is not specified.rabbitmq_persistent - If set to 1 (true), in insert query delivery mode will be set to 2 (marks messages as 'persistent'). Default: 0.rabbitmq_skip_broken_messages – RabbitMQ message parser tolerance to schema-incompatible messages per block. Default: 0. If rabbitmq_skip_broken_messages = N then the engine skips N RabbitMQ messages that cannot be parsed (a message equals a row of data).rabbitmq_max_block_sizerabbitmq_flush_interval_msrabbitmq_queue_settings_list - allows to set RabbitMQ settings when creating a queue. Available settings: x-max-length, x-max-length-bytes, x-message-ttl, x-expires, x-priority, x-max-priority, x-overflow, x-dead-letter-exchange, x-queue-type. The durable setting is enabled automatically for the queue. SSL connection: Use either rabbitmq_secure = 1 or amqps in connection address: rabbitmq_address = 'amqps://guest:guest@localhost/vhost'. The default behaviour of the used library is not to check if the created TLS connection is sufficiently secure. Whether the certificate is expired, self-signed, missing or invalid: the connection is simply permitted. More strict checking of certificates can possibly be implemented in the future. Also format settings can be added along with rabbitmq-related settings. Example:  CREATE TABLE queue ( key UInt64, value UInt64, date DateTime ) ENGINE = RabbitMQ SETTINGS rabbitmq_host_port = 'localhost:5672', rabbitmq_exchange_name = 'exchange1', rabbitmq_format = 'JSONEachRow', rabbitmq_num_consumers = 5, date_time_input_format = 'best_effort';  The RabbitMQ server configuration should be added using the ClickHouse config file. Required configuration:  &lt;rabbitmq&gt; &lt;username&gt;root&lt;/username&gt; &lt;password&gt;clickhouse&lt;/password&gt; &lt;/rabbitmq&gt;  Additional configuration:  &lt;rabbitmq&gt; &lt;vhost&gt;clickhouse&lt;/vhost&gt; &lt;/rabbitmq&gt;  "},{"title":"Description​","type":1,"pageTitle":"RabbitMQ Engine","url":"docs/en/engines/table-engines/integrations/rabbitmq#description","content":"SELECT is not particularly useful for reading messages (except for debugging), because each message can be read only once. It is more practical to create real-time threads using materialized views. To do this: Use the engine to create a RabbitMQ consumer and consider it a data stream.Create a table with the desired structure.Create a materialized view that converts data from the engine and puts it into a previously created table. When the MATERIALIZED VIEW joins the engine, it starts collecting data in the background. This allows you to continually receive messages from RabbitMQ and convert them to the required format using SELECT. One RabbitMQ table can have as many materialized views as you like. Data can be channeled based on rabbitmq_exchange_type and the specified rabbitmq_routing_key_list. There can be no more than one exchange per table. One exchange can be shared between multiple tables - it enables routing into multiple tables at the same time. Exchange type options: direct - Routing is based on the exact matching of keys. Example table key list: key1,key2,key3,key4,key5, message key can equal any of them.fanout - Routing to all tables (where exchange name is the same) regardless of the keys.topic - Routing is based on patterns with dot-separated keys. Examples: *.logs, records.*.*.2020, *.2018,*.2019,*.2020.headers - Routing is based on key=value matches with a setting x-match=all or x-match=any. Example table key list: x-match=all,format=logs,type=report,year=2020.consistent_hash - Data is evenly distributed between all bound tables (where the exchange name is the same). Note that this exchange type must be enabled with RabbitMQ plugin: rabbitmq-plugins enable rabbitmq_consistent_hash_exchange. Setting rabbitmq_queue_base may be used for the following cases: to let different tables share queues, so that multiple consumers could be registered for the same queues, which makes a better performance. If using rabbitmq_num_consumers and/or rabbitmq_num_queues settings, the exact match of queues is achieved in case these parameters are the same.to be able to restore reading from certain durable queues when not all messages were successfully consumed. To resume consumption from one specific queue - set its name in rabbitmq_queue_base setting and do not specify rabbitmq_num_consumers and rabbitmq_num_queues (defaults to 1). To resume consumption from all queues, which were declared for a specific table - just specify the same settings: rabbitmq_queue_base, rabbitmq_num_consumers, rabbitmq_num_queues. By default, queue names will be unique to tables.to reuse queues as they are declared durable and not auto-deleted. (Can be deleted via any of RabbitMQ CLI tools.) To improve performance, received messages are grouped into blocks the size of max_insert_block_size. If the block wasn’t formed within stream_flush_interval_ms milliseconds, the data will be flushed to the table regardless of the completeness of the block. If rabbitmq_num_consumers and/or rabbitmq_num_queues settings are specified along with rabbitmq_exchange_type, then: rabbitmq-consistent-hash-exchange plugin must be enabled.message_id property of the published messages must be specified (unique for each message/batch). For insert query there is message metadata, which is added for each published message: messageID and republished flag (true, if published more than once) - can be accessed via message headers. Do not use the same table for inserts and materialized views. Example:  CREATE TABLE queue ( key UInt64, value UInt64 ) ENGINE = RabbitMQ SETTINGS rabbitmq_host_port = 'localhost:5672', rabbitmq_exchange_name = 'exchange1', rabbitmq_exchange_type = 'headers', rabbitmq_routing_key_list = 'format=logs,type=report,year=2020', rabbitmq_format = 'JSONEachRow', rabbitmq_num_consumers = 5; CREATE TABLE daily (key UInt64, value UInt64) ENGINE = MergeTree() ORDER BY key; CREATE MATERIALIZED VIEW consumer TO daily AS SELECT key, value FROM queue; SELECT key, value FROM daily ORDER BY key;  "},{"title":"Virtual Columns​","type":1,"pageTitle":"RabbitMQ Engine","url":"docs/en/engines/table-engines/integrations/rabbitmq#virtual-columns","content":"_exchange_name - RabbitMQ exchange name._channel_id - ChannelID, on which consumer, who received the message, was declared._delivery_tag - DeliveryTag of the received message. Scoped per channel._redelivered - redelivered flag of the message._message_id - messageID of the received message; non-empty if was set, when message was published._timestamp - timestamp of the received message; non-empty if was set, when message was published. Original article "},{"title":"ReplacingMergeTree","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/mergetree-family/replacingmergetree","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"ReplacingMergeTree","url":"docs/en/engines/table-engines/mergetree-family/replacingmergetree#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE = ReplacingMergeTree([ver]) [PARTITION BY expr] [ORDER BY expr] [PRIMARY KEY expr] [SAMPLE BY expr] [SETTINGS name=value, ...]  For a description of request parameters, see statement description. warning Uniqueness of rows is determined by the ORDER BY table section, not PRIMARY KEY. ReplacingMergeTree Parameters ver — column with the version number. Type UInt*, Date, DateTime or DateTime64. Optional parameter. When merging, ReplacingMergeTree from all the rows with the same sorting key leaves only one: The last in the selection, if ver not set. A selection is a set of rows in a set of parts participating in the merge. The most recently created part (the last insert) will be the last one in the selection. Thus, after deduplication, the very last row from the most recent insert will remain for each unique sorting key.With the maximum version, if ver specified. Query clauses When creating a ReplacingMergeTree table the same clauses are required, as when creating a MergeTree table. Deprecated Method for Creating a Table warning Do not use this method in new projects and, if possible, switch old projects to the method described above. CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE [=] ReplacingMergeTree(date-column [, sampling_expression], (primary, key), index_granularity, [ver]) All of the parameters excepting ver have the same meaning as in MergeTree. ver - column with the version. Optional parameter. For a description, see the text above. "},{"title":"AggregatingMergeTree","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/mergetree-family/aggregatingmergetree","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"AggregatingMergeTree","url":"docs/en/engines/table-engines/mergetree-family/aggregatingmergetree#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE = AggregatingMergeTree() [PARTITION BY expr] [ORDER BY expr] [SAMPLE BY expr] [TTL expr] [SETTINGS name=value, ...]  For a description of request parameters, see request description. Query clauses When creating a AggregatingMergeTree table the same clauses are required, as when creating a MergeTree table. Deprecated Method for Creating a Table warning Do not use this method in new projects and, if possible, switch the old projects to the method described above. CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE [=] AggregatingMergeTree(date-column [, sampling_expression], (primary, key), index_granularity) All of the parameters have the same meaning as in MergeTree. "},{"title":"SELECT and INSERT​","type":1,"pageTitle":"AggregatingMergeTree","url":"docs/en/engines/table-engines/mergetree-family/aggregatingmergetree#select-and-insert","content":"To insert data, use INSERT SELECT query with aggregate -State- functions. When selecting data from AggregatingMergeTree table, use GROUP BY clause and the same aggregate functions as when inserting data, but using -Merge suffix. In the results of SELECT query, the values of AggregateFunction type have implementation-specific binary representation for all of the ClickHouse output formats. If dump data into, for example, TabSeparated format with SELECT query then this dump can be loaded back using INSERT query. "},{"title":"Example of an Aggregated Materialized View​","type":1,"pageTitle":"AggregatingMergeTree","url":"docs/en/engines/table-engines/mergetree-family/aggregatingmergetree#example-of-an-aggregated-materialized-view","content":"AggregatingMergeTree materialized view that watches the test.visits table: CREATE MATERIALIZED VIEW test.basic ENGINE = AggregatingMergeTree() PARTITION BY toYYYYMM(StartDate) ORDER BY (CounterID, StartDate) AS SELECT CounterID, StartDate, sumState(Sign) AS Visits, uniqState(UserID) AS Users FROM test.visits GROUP BY CounterID, StartDate;  Inserting data into the test.visits table. INSERT INTO test.visits ...  The data are inserted in both the table and view test.basic that will perform the aggregation. To get the aggregated data, we need to execute a query such as SELECT ... GROUP BY ... from the view test.basic: SELECT StartDate, sumMerge(Visits) AS Visits, uniqMerge(Users) AS Users FROM test.basic GROUP BY StartDate ORDER BY StartDate;  Original article "},{"title":"CollapsingMergeTree","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/mergetree-family/collapsingmergetree","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"CollapsingMergeTree","url":"docs/en/engines/table-engines/mergetree-family/collapsingmergetree#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE = CollapsingMergeTree(sign) [PARTITION BY expr] [ORDER BY expr] [SAMPLE BY expr] [SETTINGS name=value, ...]  For a description of query parameters, see query description. CollapsingMergeTree Parameters sign — Name of the column with the type of row: 1 is a “state” row, -1 is a “cancel” row. Column data type — Int8. Query clauses When creating a CollapsingMergeTree table, the same query clauses are required, as when creating a MergeTree table. Deprecated Method for Creating a Table warning Do not use this method in new projects and, if possible, switch old projects to the method described above. CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE [=] CollapsingMergeTree(date-column [, sampling_expression], (primary, key), index_granularity, sign) All of the parameters excepting sign have the same meaning as in MergeTree. sign — Name of the column with the type of row: 1 — “state” row, -1 — “cancel” row. Column Data Type — Int8. "},{"title":"Collapsing​","type":1,"pageTitle":"CollapsingMergeTree","url":"docs/en/engines/table-engines/mergetree-family/collapsingmergetree#table_engine-collapsingmergetree-collapsing","content":""},{"title":"Data​","type":1,"pageTitle":"CollapsingMergeTree","url":"docs/en/engines/table-engines/mergetree-family/collapsingmergetree#data","content":"Consider the situation where you need to save continually changing data for some object. It sounds logical to have one row for an object and update it at any change, but update operation is expensive and slow for DBMS because it requires rewriting of the data in the storage. If you need to write data quickly, update not acceptable, but you can write the changes of an object sequentially as follows. Use the particular column Sign. If Sign = 1 it means that the row is a state of an object, let’s call it “state” row. If Sign = -1 it means the cancellation of the state of an object with the same attributes, let’s call it “cancel” row. For example, we want to calculate how much pages users checked at some site and how long they were there. At some moment we write the following row with the state of user activity: ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐ │ 4324182021466249494 │ 5 │ 146 │ 1 │ └─────────────────────┴───────────┴──────────┴──────┘  At some moment later we register the change of user activity and write it with the following two rows. ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐ │ 4324182021466249494 │ 5 │ 146 │ -1 │ │ 4324182021466249494 │ 6 │ 185 │ 1 │ └─────────────────────┴───────────┴──────────┴──────┘  The first row cancels the previous state of the object (user). It should copy the sorting key fields of the cancelled state excepting Sign. The second row contains the current state. As we need only the last state of user activity, the rows ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐ │ 4324182021466249494 │ 5 │ 146 │ 1 │ │ 4324182021466249494 │ 5 │ 146 │ -1 │ └─────────────────────┴───────────┴──────────┴──────┘  can be deleted collapsing the invalid (old) state of an object. CollapsingMergeTree does this while merging of the data parts. Why we need 2 rows for each change read in the Algorithm paragraph. Peculiar properties of such approach The program that writes the data should remember the state of an object to be able to cancel it. “Cancel” string should contain copies of the sorting key fields of the “state” string and the opposite Sign. It increases the initial size of storage but allows to write the data quickly.Long growing arrays in columns reduce the efficiency of the engine due to load for writing. The more straightforward data, the higher the efficiency.The SELECT results depend strongly on the consistency of object changes history. Be accurate when preparing data for inserting. You can get unpredictable results in inconsistent data, for example, negative values for non-negative metrics such as session depth. "},{"title":"Algorithm​","type":1,"pageTitle":"CollapsingMergeTree","url":"docs/en/engines/table-engines/mergetree-family/collapsingmergetree#table_engine-collapsingmergetree-collapsing-algorithm","content":"When ClickHouse merges data parts, each group of consecutive rows with the same sorting key (ORDER BY) is reduced to not more than two rows, one with Sign = 1 (“state” row) and another with Sign = -1 (“cancel” row). In other words, entries collapse. For each resulting data part ClickHouse saves: The first “cancel” and the last “state” rows, if the number of “state” and “cancel” rows matches and the last row is a “state” row.The last “state” row, if there are more “state” rows than “cancel” rows.The first “cancel” row, if there are more “cancel” rows than “state” rows.None of the rows, in all other cases. Also when there are at least 2 more “state” rows than “cancel” rows, or at least 2 more “cancel” rows then “state” rows, the merge continues, but ClickHouse treats this situation as a logical error and records it in the server log. This error can occur if the same data were inserted more than once. Thus, collapsing should not change the results of calculating statistics. Changes gradually collapsed so that in the end only the last state of almost every object left. The Sign is required because the merging algorithm does not guarantee that all of the rows with the same sorting key will be in the same resulting data part and even on the same physical server. ClickHouse process SELECT queries with multiple threads, and it can not predict the order of rows in the result. The aggregation is required if there is a need to get completely “collapsed” data from CollapsingMergeTree table. To finalize collapsing, write a query with GROUP BY clause and aggregate functions that account for the sign. For example, to calculate quantity, use sum(Sign) instead of count(). To calculate the sum of something, use sum(Sign * x) instead of sum(x), and so on, and also add HAVING sum(Sign) &gt; 0. The aggregates count, sum and avg could be calculated this way. The aggregate uniq could be calculated if an object has at least one state not collapsed. The aggregates min and max could not be calculated because CollapsingMergeTree does not save the values history of the collapsed states. If you need to extract data without aggregation (for example, to check whether rows are present whose newest values match certain conditions), you can use the FINAL modifier for the FROM clause. This approach is significantly less efficient. "},{"title":"Example of Use​","type":1,"pageTitle":"CollapsingMergeTree","url":"docs/en/engines/table-engines/mergetree-family/collapsingmergetree#example-of-use","content":"Example data: ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐ │ 4324182021466249494 │ 5 │ 146 │ 1 │ │ 4324182021466249494 │ 5 │ 146 │ -1 │ │ 4324182021466249494 │ 6 │ 185 │ 1 │ └─────────────────────┴───────────┴──────────┴──────┘  Creation of the table: CREATE TABLE UAct ( UserID UInt64, PageViews UInt8, Duration UInt8, Sign Int8 ) ENGINE = CollapsingMergeTree(Sign) ORDER BY UserID  Insertion of the data: INSERT INTO UAct VALUES (4324182021466249494, 5, 146, 1)  INSERT INTO UAct VALUES (4324182021466249494, 5, 146, -1),(4324182021466249494, 6, 185, 1)  We use two INSERT queries to create two different data parts. If we insert the data with one query ClickHouse creates one data part and will not perform any merge ever. Getting the data: SELECT * FROM UAct  ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐ │ 4324182021466249494 │ 5 │ 146 │ -1 │ │ 4324182021466249494 │ 6 │ 185 │ 1 │ └─────────────────────┴───────────┴──────────┴──────┘ ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐ │ 4324182021466249494 │ 5 │ 146 │ 1 │ └─────────────────────┴───────────┴──────────┴──────┘  What do we see and where is collapsing? With two INSERT queries, we created 2 data parts. The SELECT query was performed in 2 threads, and we got a random order of rows. Collapsing not occurred because there was no merge of the data parts yet. ClickHouse merges data part in an unknown moment which we can not predict. Thus we need aggregation: SELECT UserID, sum(PageViews * Sign) AS PageViews, sum(Duration * Sign) AS Duration FROM UAct GROUP BY UserID HAVING sum(Sign) &gt; 0  ┌──────────────UserID─┬─PageViews─┬─Duration─┐ │ 4324182021466249494 │ 6 │ 185 │ └─────────────────────┴───────────┴──────────┘  If we do not need aggregation and want to force collapsing, we can use FINAL modifier for FROM clause. SELECT * FROM UAct FINAL  ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐ │ 4324182021466249494 │ 6 │ 185 │ 1 │ └─────────────────────┴───────────┴──────────┴──────┘  This way of selecting the data is very inefficient. Don’t use it for big tables. "},{"title":"Example of Another Approach​","type":1,"pageTitle":"CollapsingMergeTree","url":"docs/en/engines/table-engines/mergetree-family/collapsingmergetree#example-of-another-approach","content":"Example data: ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐ │ 4324182021466249494 │ 5 │ 146 │ 1 │ │ 4324182021466249494 │ -5 │ -146 │ -1 │ │ 4324182021466249494 │ 6 │ 185 │ 1 │ └─────────────────────┴───────────┴──────────┴──────┘  The idea is that merges take into account only key fields. And in the “Cancel” line we can specify negative values that equalize the previous version of the row when summing without using the Sign column. For this approach, it is necessary to change the data type PageViews,Duration to store negative values of UInt8 -&gt; Int16. CREATE TABLE UAct ( UserID UInt64, PageViews Int16, Duration Int16, Sign Int8 ) ENGINE = CollapsingMergeTree(Sign) ORDER BY UserID  Let’s test the approach: insert into UAct values(4324182021466249494, 5, 146, 1); insert into UAct values(4324182021466249494, -5, -146, -1); insert into UAct values(4324182021466249494, 6, 185, 1); select * from UAct final; // avoid using final in production (just for a test or small tables)  ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐ │ 4324182021466249494 │ 6 │ 185 │ 1 │ └─────────────────────┴───────────┴──────────┴──────┘  SELECT UserID, sum(PageViews) AS PageViews, sum(Duration) AS Duration FROM UAct GROUP BY UserID  ┌──────────────UserID─┬─PageViews─┬─Duration─┐ │ 4324182021466249494 │ 6 │ 185 │ └─────────────────────┴───────────┴──────────┘  select count() FROM UAct  ┌─count()─┐ │ 3 │ └─────────┘  optimize table UAct final; select * FROM UAct  ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐ │ 4324182021466249494 │ 6 │ 185 │ 1 │ └─────────────────────┴───────────┴──────────┴──────┘  Original article "},{"title":"Custom Partitioning Key","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/mergetree-family/custom-partitioning-key","content":"Custom Partitioning Key warning In most cases you do not need a partition key, and in most other cases you do not need a partition key more granular than by months. Partitioning does not speed up queries (in contrast to the ORDER BY expression). You should never use too granular of partitioning. Don't partition your data by client identifiers or names. Instead, make a client identifier or name the first column in the ORDER BY expression. Partitioning is available for the MergeTree family tables (including replicated tables). Materialized views based on MergeTree tables support partitioning, as well. A partition is a logical combination of records in a table by a specified criterion. You can set a partition by an arbitrary criterion, such as by month, by day, or by event type. Each partition is stored separately to simplify manipulations of this data. When accessing the data, ClickHouse uses the smallest subset of partitions possible. The partition is specified in the PARTITION BY expr clause when creating a table. The partition key can be any expression from the table columns. For example, to specify partitioning by month, use the expression toYYYYMM(date_column): CREATE TABLE visits ( VisitDate Date, Hour UInt8, ClientID UUID ) ENGINE = MergeTree() PARTITION BY toYYYYMM(VisitDate) ORDER BY Hour; The partition key can also be a tuple of expressions (similar to the primary key). For example: ENGINE = ReplicatedCollapsingMergeTree('/clickhouse/tables/name', 'replica1', Sign) PARTITION BY (toMonday(StartDate), EventType) ORDER BY (CounterID, StartDate, intHash32(UserID)); In this example, we set partitioning by the event types that occurred during the current week. By default, the floating-point partition key is not supported. To use it enable the setting allow_floating_point_partition_key. When inserting new data to a table, this data is stored as a separate part (chunk) sorted by the primary key. In 10-15 minutes after inserting, the parts of the same partition are merged into the entire part. info A merge only works for data parts that have the same value for the partitioning expression. This means you shouldn’t make overly granular partitions (more than about a thousand partitions). Otherwise, the SELECT query performs poorly because of an unreasonably large number of files in the file system and open file descriptors. Use the system.parts table to view the table parts and partitions. For example, let’s assume that we have a visits table with partitioning by month. Let’s perform the SELECT query for the system.parts table: SELECT partition, name, active FROM system.parts WHERE table = 'visits' ┌─partition─┬─name──────────────┬─active─┐ │ 201901 │ 201901_1_3_1 │ 0 │ │ 201901 │ 201901_1_9_2_11 │ 1 │ │ 201901 │ 201901_8_8_0 │ 0 │ │ 201901 │ 201901_9_9_0 │ 0 │ │ 201902 │ 201902_4_6_1_11 │ 1 │ │ 201902 │ 201902_10_10_0_11 │ 1 │ │ 201902 │ 201902_11_11_0_11 │ 1 │ └───────────┴───────────────────┴────────┘ The partition column contains the names of the partitions. There are two partitions in this example: 201901 and 201902. You can use this column value to specify the partition name in ALTER … PARTITION queries. The name column contains the names of the partition data parts. You can use this column to specify the name of the part in the ALTER ATTACH PART query. Let’s break down the name of the part: 201901_1_9_2_11: 201901 is the partition name.1 is the minimum number of the data block.9 is the maximum number of the data block.2 is the chunk level (the depth of the merge tree it is formed from).11 is the mutation version (if a part mutated) info The parts of old-type tables have the name: 20190117_20190123_2_2_0 (minimum date - maximum date - minimum block number - maximum block number - level). The active column shows the status of the part. 1 is active; 0 is inactive. The inactive parts are, for example, source parts remaining after merging to a larger part. The corrupted data parts are also indicated as inactive. As you can see in the example, there are several separated parts of the same partition (for example, 201901_1_3_1 and 201901_1_9_2). This means that these parts are not merged yet. ClickHouse merges the inserted parts of data periodically, approximately 15 minutes after inserting. In addition, you can perform a non-scheduled merge using the OPTIMIZE query. Example: OPTIMIZE TABLE visits PARTITION 201902; ┌─partition─┬─name─────────────┬─active─┐ │ 201901 │ 201901_1_3_1 │ 0 │ │ 201901 │ 201901_1_9_2_11 │ 1 │ │ 201901 │ 201901_8_8_0 │ 0 │ │ 201901 │ 201901_9_9_0 │ 0 │ │ 201902 │ 201902_4_6_1 │ 0 │ │ 201902 │ 201902_4_11_2_11 │ 1 │ │ 201902 │ 201902_10_10_0 │ 0 │ │ 201902 │ 201902_11_11_0 │ 0 │ └───────────┴──────────────────┴────────┘ Inactive parts will be deleted approximately 10 minutes after merging. Another way to view a set of parts and partitions is to go into the directory of the table: /var/lib/clickhouse/data/&lt;database&gt;/&lt;table&gt;/. For example: /var/lib/clickhouse/data/default/visits$ ls -l total 40 drwxr-xr-x 2 clickhouse clickhouse 4096 Feb 1 16:48 201901_1_3_1 drwxr-xr-x 2 clickhouse clickhouse 4096 Feb 5 16:17 201901_1_9_2_11 drwxr-xr-x 2 clickhouse clickhouse 4096 Feb 5 15:52 201901_8_8_0 drwxr-xr-x 2 clickhouse clickhouse 4096 Feb 5 15:52 201901_9_9_0 drwxr-xr-x 2 clickhouse clickhouse 4096 Feb 5 16:17 201902_10_10_0 drwxr-xr-x 2 clickhouse clickhouse 4096 Feb 5 16:17 201902_11_11_0 drwxr-xr-x 2 clickhouse clickhouse 4096 Feb 5 16:19 201902_4_11_2_11 drwxr-xr-x 2 clickhouse clickhouse 4096 Feb 5 12:09 201902_4_6_1 drwxr-xr-x 2 clickhouse clickhouse 4096 Feb 1 16:48 detached The folders ‘201901_1_1_0’, ‘201901_1_7_1’ and so on are the directories of the parts. Each part relates to a corresponding partition and contains data just for a certain month (the table in this example has partitioning by month). The detached directory contains parts that were detached from the table using the DETACH query. The corrupted parts are also moved to this directory, instead of being deleted. The server does not use the parts from the detached directory. You can add, delete, or modify the data in this directory at any time – the server will not know about this until you run the ATTACH query. Note that on the operating server, you cannot manually change the set of parts or their data on the file system, since the server will not know about it. For non-replicated tables, you can do this when the server is stopped, but it isn’t recommended. For replicated tables, the set of parts cannot be changed in any case. ClickHouse allows you to perform operations with the partitions: delete them, copy from one table to another, or create a backup. See the list of all operations in the section Manipulations With Partitions and Parts. Original article","keywords":""},{"title":"Special Table Engines","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/special/","content":"Special Table Engines There are three main categories of table engines: MergeTree engine family for main production use.Log engine family for small temporary data.Table engines for integrations. The remaining engines are unique in their purpose and are not grouped into families yet, thus they are placed in this “special” category.","keywords":""},{"title":"SummingMergeTree","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/mergetree-family/summingmergetree","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"SummingMergeTree","url":"docs/en/engines/table-engines/mergetree-family/summingmergetree#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE = SummingMergeTree([columns]) [PARTITION BY expr] [ORDER BY expr] [SAMPLE BY expr] [SETTINGS name=value, ...]  For a description of request parameters, see request description. Parameters of SummingMergeTree columns - a tuple with the names of columns where values will be summarized. Optional parameter. The columns must be of a numeric type and must not be in the primary key. If columns not specified, ClickHouse summarizes the values in all columns with a numeric data type that are not in the primary key. Query clauses When creating a SummingMergeTree table the same clauses are required, as when creating a MergeTree table. Deprecated Method for Creating a Table warning Do not use this method in new projects and, if possible, switch the old projects to the method described above. CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE [=] SummingMergeTree(date-column [, sampling_expression], (primary, key), index_granularity, [columns]) All of the parameters excepting columns have the same meaning as in MergeTree. columns — tuple with names of columns values of which will be summarized. Optional parameter. For a description, see the text above. "},{"title":"Usage Example​","type":1,"pageTitle":"SummingMergeTree","url":"docs/en/engines/table-engines/mergetree-family/summingmergetree#usage-example","content":"Consider the following table: CREATE TABLE summtt ( key UInt32, value UInt32 ) ENGINE = SummingMergeTree() ORDER BY key  Insert data to it: INSERT INTO summtt Values(1,1),(1,2),(2,1)  ClickHouse may sum all the rows not completely (see below), so we use an aggregate function sum and GROUP BY clause in the query. SELECT key, sum(value) FROM summtt GROUP BY key  ┌─key─┬─sum(value)─┐ │ 2 │ 1 │ │ 1 │ 3 │ └─────┴────────────┘  "},{"title":"Data Processing​","type":1,"pageTitle":"SummingMergeTree","url":"docs/en/engines/table-engines/mergetree-family/summingmergetree#data-processing","content":"When data are inserted into a table, they are saved as-is. ClickHouse merges the inserted parts of data periodically and this is when rows with the same primary key are summed and replaced with one for each resulting part of data. ClickHouse can merge the data parts so that different resulting parts of data can consist rows with the same primary key, i.e. the summation will be incomplete. Therefore (SELECT) an aggregate function sum() and GROUP BY clause should be used in a query as described in the example above. "},{"title":"Common Rules for Summation​","type":1,"pageTitle":"SummingMergeTree","url":"docs/en/engines/table-engines/mergetree-family/summingmergetree#common-rules-for-summation","content":"The values in the columns with the numeric data type are summarized. The set of columns is defined by the parameter columns. If the values were 0 in all of the columns for summation, the row is deleted. If column is not in the primary key and is not summarized, an arbitrary value is selected from the existing ones. The values are not summarized for columns in the primary key. "},{"title":"The Summation in the Aggregatefunction Columns​","type":1,"pageTitle":"SummingMergeTree","url":"docs/en/engines/table-engines/mergetree-family/summingmergetree#the-summation-in-the-aggregatefunction-columns","content":"For columns of AggregateFunction type ClickHouse behaves as AggregatingMergeTree engine aggregating according to the function. "},{"title":"Nested Structures​","type":1,"pageTitle":"SummingMergeTree","url":"docs/en/engines/table-engines/mergetree-family/summingmergetree#nested-structures","content":"Table can have nested data structures that are processed in a special way. If the name of a nested table ends with Map and it contains at least two columns that meet the following criteria: the first column is numeric (*Int*, Date, DateTime) or a string (String, FixedString), let’s call it key,the other columns are arithmetic (*Int*, Float32/64), let’s call it (values...), then this nested table is interpreted as a mapping of key =&gt; (values...), and when merging its rows, the elements of two data sets are merged by key with a summation of the corresponding (values...). Examples: [(1, 100)] + [(2, 150)] -&gt; [(1, 100), (2, 150)] [(1, 100)] + [(1, 150)] -&gt; [(1, 250)] [(1, 100)] + [(1, 150), (2, 150)] -&gt; [(1, 250), (2, 150)] [(1, 100), (2, 150)] + [(1, -100)] -&gt; [(2, 150)]  When requesting data, use the sumMap(key, value) function for aggregation of Map. For nested data structure, you do not need to specify its columns in the tuple of columns for summation. Original article "},{"title":"Buffer Table Engine","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/special/buffer","content":"Buffer Table Engine Buffers the data to write in RAM, periodically flushing it to another table. During the read operation, data is read from the buffer and the other table simultaneously. Buffer(database, table, num_layers, min_time, max_time, min_rows, max_rows, min_bytes, max_bytes) Engine parameters: database – Database name. Instead of the database name, you can use a constant expression that returns a string.table – Table to flush data to.num_layers – Parallelism layer. Physically, the table will be represented as num_layers of independent buffers. Recommended value: 16.min_time, max_time, min_rows, max_rows, min_bytes, and max_bytes – Conditions for flushing data from the buffer. Optional engine parameters: flush_time, flush_rows, flush_bytes – Conditions for flushing data from the buffer, that will happen only in background (omitted or zero means no flush* parameters). Data is flushed from the buffer and written to the destination table if all the min* conditions or at least one max* condition are met. Also, if at least one flush* condition are met flush initiated in background, this is different from max*, since flush* allows you to configure background flushes separately to avoid adding latency for INSERT (into Buffer) queries. min_time, max_time, flush_time – Condition for the time in seconds from the moment of the first write to the buffer.min_rows, max_rows, flush_rows – Condition for the number of rows in the buffer.min_bytes, max_bytes, flush_bytes – Condition for the number of bytes in the buffer. During the write operation, data is inserted to a num_layers number of random buffers. Or, if the data part to insert is large enough (greater than max_rows or max_bytes), it is written directly to the destination table, omitting the buffer. The conditions for flushing the data are calculated separately for each of the num_layers buffers. For example, if num_layers = 16 and max_bytes = 100000000, the maximum RAM consumption is 1.6 GB. Example: CREATE TABLE merge.hits_buffer AS merge.hits ENGINE = Buffer(merge, hits, 16, 10, 100, 10000, 1000000, 10000000, 100000000) Creating a merge.hits_buffer table with the same structure as merge.hits and using the Buffer engine. When writing to this table, data is buffered in RAM and later written to the ‘merge.hits’ table. 16 buffers are created. The data in each of them is flushed if either 100 seconds have passed, or one million rows have been written, or 100 MB of data have been written; or if simultaneously 10 seconds have passed and 10,000 rows and 10 MB of data have been written. For example, if just one row has been written, after 100 seconds it will be flushed, no matter what. But if many rows have been written, the data will be flushed sooner. When the server is stopped, with DROP TABLE or DETACH TABLE, buffer data is also flushed to the destination table. You can set empty strings in single quotation marks for the database and table name. This indicates the absence of a destination table. In this case, when the data flush conditions are reached, the buffer is simply cleared. This may be useful for keeping a window of data in memory. When reading from a Buffer table, data is processed both from the buffer and from the destination table (if there is one). Note that the Buffer tables does not support an index. In other words, data in the buffer is fully scanned, which might be slow for large buffers. (For data in a subordinate table, the index that it supports will be used.) If the set of columns in the Buffer table does not match the set of columns in a subordinate table, a subset of columns that exist in both tables is inserted. If the types do not match for one of the columns in the Buffer table and a subordinate table, an error message is entered in the server log, and the buffer is cleared. The same thing happens if the subordinate table does not exist when the buffer is flushed. warning Running ALTER on the Buffer table in releases made before 26 Oct 2021 will cause a Block structure mismatch error (see #15117 and #30565), so deleting the Buffer table and then recreating is the only option. It is advisable to check that this error is fixed in your release before trying to run ALTER on the Buffer table. If the server is restarted abnormally, the data in the buffer is lost. FINAL and SAMPLE do not work correctly for Buffer tables. These conditions are passed to the destination table, but are not used for processing data in the buffer. If these features are required we recommend only using the Buffer table for writing, while reading from the destination table. When adding data to a Buffer, one of the buffers is locked. This causes delays if a read operation is simultaneously being performed from the table. Data that is inserted to a Buffer table may end up in the subordinate table in a different order and in different blocks. Because of this, a Buffer table is difficult to use for writing to a CollapsingMergeTree correctly. To avoid problems, you can set num_layers to 1. If the destination table is replicated, some expected characteristics of replicated tables are lost when writing to a Buffer table. The random changes to the order of rows and sizes of data parts cause data deduplication to quit working, which means it is not possible to have a reliable ‘exactly once’ write to replicated tables. Due to these disadvantages, we can only recommend using a Buffer table in rare cases. A Buffer table is used when too many INSERTs are received from a large number of servers over a unit of time and data can’t be buffered before insertion, which means the INSERTs can’t run fast enough. Note that it does not make sense to insert data one row at a time, even for Buffer tables. This will only produce a speed of a few thousand rows per second, while inserting larger blocks of data can produce over a million rows per second (see the section “Performance”). Original article","keywords":""},{"title":"Data Replication","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/mergetree-family/replication","content":"","keywords":""},{"title":"Creating Replicated Tables​","type":1,"pageTitle":"Data Replication","url":"docs/en/engines/table-engines/mergetree-family/replication#creating-replicated-tables","content":"The Replicated prefix is added to the table engine name. For example:ReplicatedMergeTree. Replicated*MergeTree parameters zoo_path — The path to the table in ZooKeeper.replica_name — The replica name in ZooKeeper.other_parameters — Parameters of an engine which is used for creating the replicated version, for example, version in ReplacingMergeTree. Example: CREATE TABLE table_name ( EventDate DateTime, CounterID UInt32, UserID UInt32, ver UInt16 ) ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/{layer}-{shard}/table_name', '{replica}', ver) PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate, intHash32(UserID)) SAMPLE BY intHash32(UserID);  Example in deprecated syntax CREATE TABLE table_name ( EventDate DateTime, CounterID UInt32, UserID UInt32 ) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{layer}-{shard}/table_name', '{replica}', EventDate, intHash32(UserID), (CounterID, EventDate, intHash32(UserID), EventTime), 8192);  As the example shows, these parameters can contain substitutions in curly brackets. The substituted values are taken from the macros section of the configuration file. Example: &lt;macros&gt; &lt;layer&gt;05&lt;/layer&gt; &lt;shard&gt;02&lt;/shard&gt; &lt;replica&gt;example05-02-1&lt;/replica&gt; &lt;/macros&gt;  The path to the table in ZooKeeper should be unique for each replicated table. Tables on different shards should have different paths. In this case, the path consists of the following parts: /clickhouse/tables/ is the common prefix. We recommend using exactly this one. {layer}-{shard} is the shard identifier. In this example it consists of two parts, since the example cluster uses bi-level sharding. For most tasks, you can leave just the {shard} substitution, which will be expanded to the shard identifier. table_name is the name of the node for the table in ZooKeeper. It is a good idea to make it the same as the table name. It is defined explicitly, because in contrast to the table name, it does not change after a RENAME query.HINT: you could add a database name in front of table_name as well. E.g. db_name.table_name The two built-in substitutions {database} and {table} can be used, they expand into the table name and the database name respectively (unless these macros are defined in the macros section). So the zookeeper path can be specified as '/clickhouse/tables/{layer}-{shard}/{database}/{table}'. Be careful with table renames when using these built-in substitutions. The path in Zookeeper cannot be changed, and when the table is renamed, the macros will expand into a different path, the table will refer to a path that does not exist in Zookeeper, and will go into read-only mode. The replica name identifies different replicas of the same table. You can use the server name for this, as in the example. The name only needs to be unique within each shard. You can define the parameters explicitly instead of using substitutions. This might be convenient for testing and for configuring small clusters. However, you can’t use distributed DDL queries (ON CLUSTER) in this case. When working with large clusters, we recommend using substitutions because they reduce the probability of error. You can specify default arguments for Replicated table engine in the server configuration file. For instance: &lt;default_replica_path&gt;/clickhouse/tables/{shard}/{database}/{table}&lt;/default_replica_path&gt; &lt;default_replica_name&gt;{replica}&lt;/default_replica_name&gt;  In this case, you can omit arguments when creating tables: CREATE TABLE table_name ( x UInt32 ) ENGINE = ReplicatedMergeTree ORDER BY x;  It is equivalent to: CREATE TABLE table_name ( x UInt32 ) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/{database}/table_name', '{replica}') ORDER BY x;  Run the CREATE TABLE query on each replica. This query creates a new replicated table, or adds a new replica to an existing one. If you add a new replica after the table already contains some data on other replicas, the data will be copied from the other replicas to the new one after running the query. In other words, the new replica syncs itself with the others. To delete a replica, run DROP TABLE. However, only one replica is deleted – the one that resides on the server where you run the query. "},{"title":"Recovery After Failures​","type":1,"pageTitle":"Data Replication","url":"docs/en/engines/table-engines/mergetree-family/replication#recovery-after-failures","content":"If ZooKeeper is unavailable when a server starts, replicated tables switch to read-only mode. The system periodically attempts to connect to ZooKeeper. If ZooKeeper is unavailable during an INSERT, or an error occurs when interacting with ZooKeeper, an exception is thrown. After connecting to ZooKeeper, the system checks whether the set of data in the local file system matches the expected set of data (ZooKeeper stores this information). If there are minor inconsistencies, the system resolves them by syncing data with the replicas. If the system detects broken data parts (with the wrong size of files) or unrecognized parts (parts written to the file system but not recorded in ZooKeeper), it moves them to the detached subdirectory (they are not deleted). Any missing parts are copied from the replicas. Note that ClickHouse does not perform any destructive actions such as automatically deleting a large amount of data. When the server starts (or establishes a new session with ZooKeeper), it only checks the quantity and sizes of all files. If the file sizes match but bytes have been changed somewhere in the middle, this is not detected immediately, but only when attempting to read the data for a SELECT query. The query throws an exception about a non-matching checksum or size of a compressed block. In this case, data parts are added to the verification queue and copied from the replicas if necessary. If the local set of data differs too much from the expected one, a safety mechanism is triggered. The server enters this in the log and refuses to launch. The reason for this is that this case may indicate a configuration error, such as if a replica on a shard was accidentally configured like a replica on a different shard. However, the thresholds for this mechanism are set fairly low, and this situation might occur during normal failure recovery. In this case, data is restored semi-automatically - by “pushing a button”. To start recovery, create the node /path_to_table/replica_name/flags/force_restore_data in ZooKeeper with any content, or run the command to restore all replicated tables: sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data  Then restart the server. On start, the server deletes these flags and starts recovery. "},{"title":"Recovery After Complete Data Loss​","type":1,"pageTitle":"Data Replication","url":"docs/en/engines/table-engines/mergetree-family/replication#recovery-after-complete-data-loss","content":"If all data and metadata disappeared from one of the servers, follow these steps for recovery: Install ClickHouse on the server. Define substitutions correctly in the config file that contains the shard identifier and replicas, if you use them.If you had unreplicated tables that must be manually duplicated on the servers, copy their data from a replica (in the directory /var/lib/clickhouse/data/db_name/table_name/).Copy table definitions located in /var/lib/clickhouse/metadata/ from a replica. If a shard or replica identifier is defined explicitly in the table definitions, correct it so that it corresponds to this replica. (Alternatively, start the server and make all the ATTACH TABLE queries that should have been in the .sql files in /var/lib/clickhouse/metadata/.)To start recovery, create the ZooKeeper node /path_to_table/replica_name/flags/force_restore_data with any content, or run the command to restore all replicated tables: sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data Then start the server (restart, if it is already running). Data will be downloaded from replicas. An alternative recovery option is to delete information about the lost replica from ZooKeeper (/path_to_table/replica_name), then create the replica again as described in “Creating replicated tables”. There is no restriction on network bandwidth during recovery. Keep this in mind if you are restoring many replicas at once. "},{"title":"Converting from MergeTree to ReplicatedMergeTree​","type":1,"pageTitle":"Data Replication","url":"docs/en/engines/table-engines/mergetree-family/replication#converting-from-mergetree-to-replicatedmergetree","content":"We use the term MergeTree to refer to all table engines in the MergeTree family, the same as for ReplicatedMergeTree. If you had a MergeTree table that was manually replicated, you can convert it to a replicated table. You might need to do this if you have already collected a large amount of data in a MergeTree table and now you want to enable replication. If the data differs on various replicas, first sync it, or delete this data on all the replicas except one. Rename the existing MergeTree table, then create a ReplicatedMergeTree table with the old name. Move the data from the old table to the detached subdirectory inside the directory with the new table data (/var/lib/clickhouse/data/db_name/table_name/). Then run ALTER TABLE ATTACH PARTITION on one of the replicas to add these data parts to the working set. "},{"title":"Converting from ReplicatedMergeTree to MergeTree​","type":1,"pageTitle":"Data Replication","url":"docs/en/engines/table-engines/mergetree-family/replication#converting-from-replicatedmergetree-to-mergetree","content":"Create a MergeTree table with a different name. Move all the data from the directory with the ReplicatedMergeTree table data to the new table’s data directory. Then delete the ReplicatedMergeTree table and restart the server. If you want to get rid of a ReplicatedMergeTree table without launching the server: Delete the corresponding .sql file in the metadata directory (/var/lib/clickhouse/metadata/).Delete the corresponding path in ZooKeeper (/path_to_table/replica_name). After this, you can launch the server, create a MergeTree table, move the data to its directory, and then restart the server. "},{"title":"Recovery When Metadata in the Zookeeper Cluster Is Lost or Damaged​","type":1,"pageTitle":"Data Replication","url":"docs/en/engines/table-engines/mergetree-family/replication#recovery-when-metadata-in-the-zookeeper-cluster-is-lost-or-damaged","content":"If the data in ZooKeeper was lost or damaged, you can save data by moving it to an unreplicated table as described above. See Also background_schedule_pool_sizebackground_fetches_pool_sizeexecute_merges_on_single_replica_time_thresholdmax_replicated_fetches_network_bandwidthmax_replicated_sends_network_bandwidth Original article "},{"title":"File Table Engine","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/special/file","content":"","keywords":""},{"title":"Usage in ClickHouse Server​","type":1,"pageTitle":"File Table Engine","url":"docs/en/engines/table-engines/special/file#usage-in-clickhouse-server","content":"File(Format)  The Format parameter specifies one of the available file formats. To performSELECT queries, the format must be supported for input, and to performINSERT queries – for output. The available formats are listed in theFormats section. ClickHouse does not allow specifying filesystem path for File. It will use folder defined by path setting in server configuration. When creating table using File(Format) it creates empty subdirectory in that folder. When data is written to that table, it’s put into data.Format file in that subdirectory. You may manually create this subfolder and file in server filesystem and then ATTACH it to table information with matching name, so you can query data from that file. warning Be careful with this functionality, because ClickHouse does not keep track of external changes to such files. The result of simultaneous writes via ClickHouse and outside of ClickHouse is undefined. "},{"title":"Example​","type":1,"pageTitle":"File Table Engine","url":"docs/en/engines/table-engines/special/file#example","content":"1. Set up the file_engine_table table: CREATE TABLE file_engine_table (name String, value UInt32) ENGINE=File(TabSeparated)  By default ClickHouse will create folder /var/lib/clickhouse/data/default/file_engine_table. 2. Manually create /var/lib/clickhouse/data/default/file_engine_table/data.TabSeparated containing: $ cat data.TabSeparated one 1 two 2  3. Query the data: SELECT * FROM file_engine_table  ┌─name─┬─value─┐ │ one │ 1 │ │ two │ 2 │ └──────┴───────┘  "},{"title":"Usage in ClickHouse-local​","type":1,"pageTitle":"File Table Engine","url":"docs/en/engines/table-engines/special/file#usage-in-clickhouse-local","content":"In clickhouse-local File engine accepts file path in addition to Format. Default input/output streams can be specified using numeric or human-readable names like 0 or stdin, 1 or stdout. It is possible to read and write compressed files based on an additional engine parameter or file extension (gz, br or xz). Example: $ echo -e &quot;1,2\\n3,4&quot; | clickhouse-local -q &quot;CREATE TABLE table (a Int64, b Int64) ENGINE = File(CSV, stdin); SELECT a, b FROM table; DROP TABLE table&quot;  "},{"title":"Details of Implementation​","type":1,"pageTitle":"File Table Engine","url":"docs/en/engines/table-engines/special/file#details-of-implementation","content":"Multiple SELECT queries can be performed concurrently, but INSERT queries will wait each other.Supported creating new file by INSERT query.If file exists, INSERT would append new values in it.Not supported: ALTERSELECT ... SAMPLEIndicesReplication Original article "},{"title":"GraphiteMergeTree","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/mergetree-family/graphitemergetree","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"GraphiteMergeTree","url":"docs/en/engines/table-engines/mergetree-family/graphitemergetree#creating-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( Path String, Time DateTime, Value &lt;Numeric_type&gt;, Version &lt;Numeric_type&gt; ... ) ENGINE = GraphiteMergeTree(config_section) [PARTITION BY expr] [ORDER BY expr] [SAMPLE BY expr] [SETTINGS name=value, ...]  See a detailed description of the CREATE TABLE query. A table for the Graphite data should have the following columns for the following data: Metric name (Graphite sensor). Data type: String. Time of measuring the metric. Data type: DateTime. Value of the metric. Data type: any numeric. Version of the metric. Data type: any numeric (ClickHouse saves the rows with the highest version or the last written if versions are the same. Other rows are deleted during the merge of data parts). The names of these columns should be set in the rollup configuration. GraphiteMergeTree parameters config_section — Name of the section in the configuration file, where are the rules of rollup set. Query clauses When creating a GraphiteMergeTree table, the same clauses are required, as when creating a MergeTree table. Deprecated Method for Creating a Table warning Do not use this method in new projects and, if possible, switch old projects to the method described above. CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( EventDate Date, Path String, Time DateTime, Value &lt;Numeric_type&gt;, Version &lt;Numeric_type&gt; ... ) ENGINE [=] GraphiteMergeTree(date-column [, sampling_expression], (primary, key), index_granularity, config_section) All of the parameters excepting config_section have the same meaning as in MergeTree. config_section — Name of the section in the configuration file, where are the rules of rollup set. "},{"title":"Rollup Configuration​","type":1,"pageTitle":"GraphiteMergeTree","url":"docs/en/engines/table-engines/mergetree-family/graphitemergetree#rollup-configuration","content":"The settings for rollup are defined by the graphite_rollup parameter in the server configuration. The name of the parameter could be any. You can create several configurations and use them for different tables. Rollup configuration structure:  required-columns patterns  "},{"title":"Required Columns​","type":1,"pageTitle":"GraphiteMergeTree","url":"docs/en/engines/table-engines/mergetree-family/graphitemergetree#required-columns","content":"path_column_name — The name of the column storing the metric name (Graphite sensor). Default value: Path.time_column_name — The name of the column storing the time of measuring the metric. Default value: Time.value_column_name — The name of the column storing the value of the metric at the time set in time_column_name. Default value: Value.version_column_name — The name of the column storing the version of the metric. Default value: Timestamp. "},{"title":"Patterns​","type":1,"pageTitle":"GraphiteMergeTree","url":"docs/en/engines/table-engines/mergetree-family/graphitemergetree#patterns","content":"Structure of the patterns section: pattern rule_type regexp function pattern rule_type regexp age + precision ... pattern rule_type regexp function age + precision ... pattern ... default function age + precision ...  warning Patterns must be strictly ordered: Patterns without function or retention.Patterns with both function and retention.Pattern default. When processing a row, ClickHouse checks the rules in the pattern sections. Each of pattern (including default) sections can contain function parameter for aggregation, retention parameters or both. If the metric name matches the regexp, the rules from the pattern section (or sections) are applied; otherwise, the rules from the default section are used. Fields for pattern and default sections: rule_type - a rule's type. It's applied only to a particular metrics. The engine use it to separate plain and tagged metrics. Optional parameter. Default value: all. It's unnecessary when performance is not critical, or only one metrics type is used, e.g. plain metrics. By default only one type of rules set is created. Otherwise, if any of special types is defined, two different sets are created. One for plain metrics (root.branch.leaf) and one for tagged metrics (root.branch.leaf;tag1=value1). The default rules are ended up in both sets. Valid values: - `all` (default) - a universal rule, used when `rule_type` is omitted. - `plain` - a rule for plain metrics. The field `regexp` is processed as regular expression. - `tagged` - a rule for tagged metrics (metrics are stored in DB in the format of `someName?tag1=value1&amp;tag2=value2&amp;tag3=value3`). Regular expression must be sorted by tags' names, first tag must be `__name__` if exists. The field `regexp` is processed as regular expression. - `tag_list` - a rule for tagged matrics, a simple DSL for easier metric description in graphite format `someName;tag1=value1;tag2=value2`, `someName`, or `tag1=value1;tag2=value2`. The field `regexp` is translated into a `tagged` rule. The sorting by tags' names is unnecessary, ti will be done automatically. A tag's value (but not a name) can be set as a regular expression, e.g. `env=(dev|staging)`. regexp – A pattern for the metric name (a regular or DSL).age – The minimum age of the data in seconds.precision– How precisely to define the age of the data in seconds. Should be a divisor for 86400 (seconds in a day).function – The name of the aggregating function to apply to data whose age falls within the range [age, age + precision]. Accepted functions: min / max / any / avg. The average is calculated imprecisely, like the average of the averages.  "},{"title":"Configuration Example without rules types​","type":1,"pageTitle":"GraphiteMergeTree","url":"docs/en/engines/table-engines/mergetree-family/graphitemergetree#configuration-example","content":"&lt;graphite_rollup&gt; &lt;version_column_name&gt;Version&lt;/version_column_name&gt; &lt;pattern&gt; &lt;regexp&gt;click_cost&lt;/regexp&gt; &lt;function&gt;any&lt;/function&gt; &lt;retention&gt; &lt;age&gt;0&lt;/age&gt; &lt;precision&gt;5&lt;/precision&gt; &lt;/retention&gt; &lt;retention&gt; &lt;age&gt;86400&lt;/age&gt; &lt;precision&gt;60&lt;/precision&gt; &lt;/retention&gt; &lt;/pattern&gt; &lt;default&gt; &lt;function&gt;max&lt;/function&gt; &lt;retention&gt; &lt;age&gt;0&lt;/age&gt; &lt;precision&gt;60&lt;/precision&gt; &lt;/retention&gt; &lt;retention&gt; &lt;age&gt;3600&lt;/age&gt; &lt;precision&gt;300&lt;/precision&gt; &lt;/retention&gt; &lt;retention&gt; &lt;age&gt;86400&lt;/age&gt; &lt;precision&gt;3600&lt;/precision&gt; &lt;/retention&gt; &lt;/default&gt; &lt;/graphite_rollup&gt;  "},{"title":"Configuration Example with rules types​","type":1,"pageTitle":"GraphiteMergeTree","url":"docs/en/engines/table-engines/mergetree-family/graphitemergetree#configuration-typed-example","content":"&lt;graphite_rollup&gt; &lt;version_column_name&gt;Version&lt;/version_column_name&gt; &lt;pattern&gt; &lt;rule_type&gt;plain&lt;/rule_type&gt; &lt;regexp&gt;click_cost&lt;/regexp&gt; &lt;function&gt;any&lt;/function&gt; &lt;retention&gt; &lt;age&gt;0&lt;/age&gt; &lt;precision&gt;5&lt;/precision&gt; &lt;/retention&gt; &lt;retention&gt; &lt;age&gt;86400&lt;/age&gt; &lt;precision&gt;60&lt;/precision&gt; &lt;/retention&gt; &lt;/pattern&gt; &lt;pattern&gt; &lt;rule_type&gt;tagged&lt;/rule_type&gt; &lt;regexp&gt;^((.*)|.)min\\?&lt;/regexp&gt; &lt;function&gt;min&lt;/function&gt; &lt;retention&gt; &lt;age&gt;0&lt;/age&gt; &lt;precision&gt;5&lt;/precision&gt; &lt;/retention&gt; &lt;retention&gt; &lt;age&gt;86400&lt;/age&gt; &lt;precision&gt;60&lt;/precision&gt; &lt;/retention&gt; &lt;/pattern&gt; &lt;pattern&gt; &lt;rule_type&gt;tagged&lt;/rule_type&gt; &lt;regexp&gt;&lt;![CDATA[^someName\\?(.*&amp;)*tag1=value1(&amp;|$)]]&gt;&lt;/regexp&gt; &lt;function&gt;min&lt;/function&gt; &lt;retention&gt; &lt;age&gt;0&lt;/age&gt; &lt;precision&gt;5&lt;/precision&gt; &lt;/retention&gt; &lt;retention&gt; &lt;age&gt;86400&lt;/age&gt; &lt;precision&gt;60&lt;/precision&gt; &lt;/retention&gt; &lt;/pattern&gt; &lt;pattern&gt; &lt;rule_type&gt;tag_list&lt;/rule_type&gt; &lt;regexp&gt;someName;tag2=value2&lt;/regexp&gt; &lt;retention&gt; &lt;age&gt;0&lt;/age&gt; &lt;precision&gt;5&lt;/precision&gt; &lt;/retention&gt; &lt;retention&gt; &lt;age&gt;86400&lt;/age&gt; &lt;precision&gt;60&lt;/precision&gt; &lt;/retention&gt; &lt;/pattern&gt; &lt;default&gt; &lt;function&gt;max&lt;/function&gt; &lt;retention&gt; &lt;age&gt;0&lt;/age&gt; &lt;precision&gt;60&lt;/precision&gt; &lt;/retention&gt; &lt;retention&gt; &lt;age&gt;3600&lt;/age&gt; &lt;precision&gt;300&lt;/precision&gt; &lt;/retention&gt; &lt;retention&gt; &lt;age&gt;86400&lt;/age&gt; &lt;precision&gt;3600&lt;/precision&gt; &lt;/retention&gt; &lt;/default&gt; &lt;/graphite_rollup&gt;  warning Data rollup is performed during merges. Usually, for old partitions, merges are not started, so for rollup it is necessary to trigger an unscheduled merge using optimize. Or use additional tools, for example graphite-ch-optimizer. "},{"title":"External Data for Query Processing","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/special/external-data","content":"External Data for Query Processing ClickHouse allows sending a server the data that is needed for processing a query, together with a SELECT query. This data is put in a temporary table (see the section “Temporary tables”) and can be used in the query (for example, in IN operators). For example, if you have a text file with important user identifiers, you can upload it to the server along with a query that uses filtration by this list. If you need to run more than one query with a large volume of external data, do not use this feature. It is better to upload the data to the DB ahead of time. External data can be uploaded using the command-line client (in non-interactive mode), or using the HTTP interface. In the command-line client, you can specify a parameters section in the format --external --file=... [--name=...] [--format=...] [--types=...|--structure=...] You may have multiple sections like this, for the number of tables being transmitted. –external – Marks the beginning of a clause.–file – Path to the file with the table dump, or -, which refers to stdin. Only a single table can be retrieved from stdin. The following parameters are optional: –name– Name of the table. If omitted, _data is used.–format – Data format in the file. If omitted, TabSeparated is used. One of the following parameters is required:–types – A list of comma-separated column types. For example: UInt64,String. The columns will be named _1, _2, …–structure– The table structure in the formatUserID UInt64, URL String. Defines the column names and types. The files specified in ‘file’ will be parsed by the format specified in ‘format’, using the data types specified in ‘types’ or ‘structure’. The table will be uploaded to the server and accessible there as a temporary table with the name in ‘name’. Examples: $ echo -ne &quot;1\\n2\\n3\\n&quot; | clickhouse-client --query=&quot;SELECT count() FROM test.visits WHERE TraficSourceID IN _data&quot; --external --file=- --types=Int8 849897 $ cat /etc/passwd | sed 's/:/\\t/g' | clickhouse-client --query=&quot;SELECT shell, count() AS c FROM passwd GROUP BY shell ORDER BY c DESC&quot; --external --file=- --name=passwd --structure='login String, unused String, uid UInt16, gid UInt16, comment String, home String, shell String' /bin/sh 20 /bin/false 5 /bin/bash 4 /usr/sbin/nologin 1 /bin/sync 1 When using the HTTP interface, external data is passed in the multipart/form-data format. Each table is transmitted as a separate file. The table name is taken from the file name. The query_string is passed the parameters name_format, name_types, and name_structure, where name is the name of the table that these parameters correspond to. The meaning of the parameters is the same as when using the command-line client. Example: $ cat /etc/passwd | sed 's/:/\\t/g' &gt; passwd.tsv $ curl -F 'passwd=@passwd.tsv;' 'http://localhost:8123/?query=SELECT+shell,+count()+AS+c+FROM+passwd+GROUP+BY+shell+ORDER+BY+c+DESC&amp;passwd_structure=login+String,+unused+String,+uid+UInt16,+gid+UInt16,+comment+String,+home+String,+shell+String' /bin/sh 20 /bin/false 5 /bin/bash 4 /usr/sbin/nologin 1 /bin/sync 1 For distributed query processing, the temporary tables are sent to all the remote servers.","keywords":""},{"title":"Distributed Table Engine","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/special/distributed","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"Distributed Table Engine","url":"docs/en/engines/table-engines/special/distributed#distributed-creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE = Distributed(cluster, database, table[, sharding_key[, policy_name]]) [SETTINGS name=value, ...]  "},{"title":"From a Table​","type":1,"pageTitle":"Distributed Table Engine","url":"docs/en/engines/table-engines/special/distributed#distributed-from-a-table","content":"When the Distributed table is pointing to a table on the current server you can adopt that table's schema: CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] AS [db2.]name2 ENGINE = Distributed(cluster, database, table[, sharding_key[, policy_name]]) [SETTINGS name=value, ...]  Distributed Parameters cluster - the cluster name in the server’s config file database - the name of a remote database table - the name of a remote table sharding_key - (optionally) sharding key policy_name - (optionally) policy name, it will be used to store temporary files for async send See Also insert_distributed_sync settingMergeTree for the examples Distributed Settings fsync_after_insert - do the fsync for the file data after asynchronous insert to Distributed. Guarantees that the OS flushed the whole inserted data to a file on the initiator node disk. fsync_directories - do the fsync for directories. Guarantees that the OS refreshed directory metadata after operations related to asynchronous inserts on Distributed table (after insert, after sending the data to shard, etc). bytes_to_throw_insert - if more than this number of compressed bytes will be pending for async INSERT, an exception will be thrown. 0 - do not throw. Default 0. bytes_to_delay_insert - if more than this number of compressed bytes will be pending for async INSERT, the query will be delayed. 0 - do not delay. Default 0. max_delay_to_insert - max delay of inserting data into Distributed table in seconds, if there are a lot of pending bytes for async send. Default 60. monitor_batch_inserts - same as distributed_directory_monitor_batch_inserts monitor_split_batch_on_failure - same as distributed_directory_monitor_split_batch_on_failure monitor_sleep_time_ms - same as distributed_directory_monitor_sleep_time_ms monitor_max_sleep_time_ms - same as distributed_directory_monitor_max_sleep_time_ms note Durability settings (fsync_...): Affect only asynchronous INSERTs (i.e. insert_distributed_sync=false) when data first stored on the initiator node disk and later asynchronously send to shards.May significantly decrease the inserts' performanceAffect writing the data stored inside Distributed table folder into the node which accepted your insert. If you need to have guarantees of writing data to underlying MergeTree tables - see durability settings (...fsync...) in system.merge_tree_settings For Insert limit settings (..._insert) see also: insert_distributed_sync settingprefer_localhost_replica settingbytes_to_throw_insert handled before bytes_to_delay_insert, so you should not set it to the value less then bytes_to_delay_insert Example CREATE TABLE hits_all AS hits ENGINE = Distributed(logs, default, hits[, sharding_key[, policy_name]]) SETTINGS fsync_after_insert=0, fsync_directories=0;  Data will be read from all servers in the logs cluster, from the default.hits table located on every server in the cluster. Data is not only read but is partially processed on the remote servers (to the extent that this is possible). For example, for a query with GROUP BY, data will be aggregated on remote servers, and the intermediate states of aggregate functions will be sent to the requestor server. Then data will be further aggregated. Instead of the database name, you can use a constant expression that returns a string. For example: currentDatabase(). "},{"title":"Clusters​","type":1,"pageTitle":"Distributed Table Engine","url":"docs/en/engines/table-engines/special/distributed#distributed-clusters","content":"Clusters are configured in the server configuration file: &lt;remote_servers&gt; &lt;logs&gt; &lt;!-- Inter-server per-cluster secret for Distributed queries default: no secret (no authentication will be performed) If set, then Distributed queries will be validated on shards, so at least: - such cluster should exist on the shard, - such cluster should have the same secret. And also (and which is more important), the initial_user will be used as current user for the query. --&gt; &lt;!-- &lt;secret&gt;&lt;/secret&gt; --&gt; &lt;shard&gt; &lt;!-- Optional. Shard weight when writing data. Default: 1. --&gt; &lt;weight&gt;1&lt;/weight&gt; &lt;!-- Optional. Whether to write data to just one of the replicas. Default: false (write data to all replicas). --&gt; &lt;internal_replication&gt;false&lt;/internal_replication&gt; &lt;replica&gt; &lt;!-- Optional. Priority of the replica for load balancing (see also load_balancing setting). Default: 1 (less value has more priority). --&gt; &lt;priority&gt;1&lt;/priority&gt; &lt;host&gt;example01-01-1&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;replica&gt; &lt;host&gt;example01-01-2&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;shard&gt; &lt;weight&gt;2&lt;/weight&gt; &lt;internal_replication&gt;false&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;example01-02-1&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;replica&gt; &lt;host&gt;example01-02-2&lt;/host&gt; &lt;secure&gt;1&lt;/secure&gt; &lt;port&gt;9440&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;/logs&gt; &lt;/remote_servers&gt;  Here a cluster is defined with the name logs that consists of two shards, each of which contains two replicas. Shards refer to the servers that contain different parts of the data (in order to read all the data, you must access all the shards). Replicas are duplicating servers (in order to read all the data, you can access the data on any one of the replicas). Cluster names must not contain dots. The parameters host, port, and optionally user, password, secure, compression are specified for each server: host – The address of the remote server. You can use either the domain or the IPv4 or IPv6 address. If you specify the domain, the server makes a DNS request when it starts, and the result is stored as long as the server is running. If the DNS request fails, the server does not start. If you change the DNS record, restart the server.port – The TCP port for messenger activity (tcp_port in the config, usually set to 9000). Not to be confused with http_port.user – Name of the user for connecting to a remote server. Default value is the default user. This user must have access to connect to the specified server. Access is configured in the users.xml file. For more information, see the section Access rights.password – The password for connecting to a remote server (not masked). Default value: empty string.secure - Whether to use a secure SSL/TLS connection. Usually also requires specifying the port (the default secure port is 9440). The server should listen on &lt;tcp_port_secure&gt;9440&lt;/tcp_port_secure&gt; and be configured with correct certificates.compression - Use data compression. Default value: true. When specifying replicas, one of the available replicas will be selected for each of the shards when reading. You can configure the algorithm for load balancing (the preference for which replica to access) – see the load_balancing setting. If the connection with the server is not established, there will be an attempt to connect with a short timeout. If the connection failed, the next replica will be selected, and so on for all the replicas. If the connection attempt failed for all the replicas, the attempt will be repeated the same way, several times. This works in favour of resiliency, but does not provide complete fault tolerance: a remote server might accept the connection, but might not work, or work poorly. You can specify just one of the shards (in this case, query processing should be called remote, rather than distributed) or up to any number of shards. In each shard, you can specify from one to any number of replicas. You can specify a different number of replicas for each shard. You can specify as many clusters as you wish in the configuration. To view your clusters, use the system.clusters table. The Distributed engine allows working with a cluster like a local server. However, the cluster's configuration cannot be specified dynamically, it has to be configured in the server config file. Usually, all servers in a cluster will have the same cluster config (though this is not required). Clusters from the config file are updated on the fly, without restarting the server. If you need to send a query to an unknown set of shards and replicas each time, you do not need to create a Distributed table – use the remote table function instead. See the section Table functions. "},{"title":"Writing data​","type":1,"pageTitle":"Distributed Table Engine","url":"docs/en/engines/table-engines/special/distributed#distributed-writing-data","content":"There are two methods for writing data to a cluster: First, you can define which servers to write which data to and perform the write directly on each shard. In other words, perform direct INSERT statements on the remote tables in the cluster that the Distributed table is pointing to. This is the most flexible solution as you can use any sharding scheme, even one that is non-trivial due to the requirements of the subject area. This is also the most optimal solution since data can be written to different shards completely independently. Second, you can perform INSERT statements on a Distributed table. In this case, the table will distribute the inserted data across the servers itself. In order to write to a Distributed table, it must have the sharding_key parameter configured (except if there is only one shard). Each shard can have a &lt;weight&gt; defined in the config file. By default, the weight is 1. Data is distributed across shards in the amount proportional to the shard weight. All shard weights are summed up, then each shard's weight is divided by the total to determine each shard's proportion. For example, if there are two shards and the first has a weight of 1 while the second has a weight of 2, the first will be sent one third (1 / 3) of inserted rows and the second will be sent two thirds (2 / 3). Each shard can have the internal_replication parameter defined in the config file. If this parameter is set to true, the write operation selects the first healthy replica and writes data to it. Use this if the tables underlying the Distributed table are replicated tables (e.g. any of the Replicated*MergeTree table engines). One of the table replicas will receive the write and it will be replicated to the other replicas automatically. If internal_replication is set to false (the default), data is written to all replicas. In this case, the Distributed table replicates data itself. This is worse than using replicated tables because the consistency of replicas is not checked and, over time, they will contain slightly different data. To select the shard that a row of data is sent to, the sharding expression is analyzed, and its remainder is taken from dividing it by the total weight of the shards. The row is sent to the shard that corresponds to the half-interval of the remainders from prev_weights to prev_weights + weight, where prev_weights is the total weight of the shards with the smallest number, and weight is the weight of this shard. For example, if there are two shards, and the first has a weight of 9 while the second has a weight of 10, the row will be sent to the first shard for the remainders from the range [0, 9), and to the second for the remainders from the range [9, 19). The sharding expression can be any expression from constants and table columns that returns an integer. For example, you can use the expression rand() for random distribution of data, or UserID for distribution by the remainder from dividing the user’s ID (then the data of a single user will reside on a single shard, which simplifies running IN and JOIN by users). If one of the columns is not distributed evenly enough, you can wrap it in a hash function e.g. intHash64(UserID). A simple remainder from the division is a limited solution for sharding and isn’t always appropriate. It works for medium and large volumes of data (dozens of servers), but not for very large volumes of data (hundreds of servers or more). In the latter case, use the sharding scheme required by the subject area rather than using entries in Distributed tables. You should be concerned about the sharding scheme in the following cases: Queries are used that require joining data (IN or JOIN) by a specific key. If data is sharded by this key, you can use local IN or JOIN instead of GLOBAL IN or GLOBAL JOIN, which is much more efficient.A large number of servers is used (hundreds or more) with a large number of small queries, for example, queries for data of individual clients (e.g. websites, advertisers, or partners). In order for the small queries to not affect the entire cluster, it makes sense to locate data for a single client on a single shard. Alternatively, you can set up bi-level sharding: divide the entire cluster into “layers”, where a layer may consist of multiple shards. Data for a single client is located on a single layer, but shards can be added to a layer as necessary, and data is randomly distributed within them. Distributed tables are created for each layer, and a single shared distributed table is created for global queries. Data is written asynchronously. When inserted in the table, the data block is just written to the local file system. The data is sent to the remote servers in the background as soon as possible. The periodicity for sending data is managed by the distributed_directory_monitor_sleep_time_ms and distributed_directory_monitor_max_sleep_time_ms settings. The Distributed engine sends each file with inserted data separately, but you can enable batch sending of files with the distributed_directory_monitor_batch_inserts setting. This setting improves cluster performance by better utilizing local server and network resources. You should check whether data is sent successfully by checking the list of files (data waiting to be sent) in the table directory: /var/lib/clickhouse/data/database/table/. The number of threads performing background tasks can be set by background_distributed_schedule_pool_size setting. If the server ceased to exist or had a rough restart (for example, due to a hardware failure) after an INSERT to a Distributed table, the inserted data might be lost. If a damaged data part is detected in the table directory, it is transferred to the broken subdirectory and no longer used. "},{"title":"Reading data​","type":1,"pageTitle":"Distributed Table Engine","url":"docs/en/engines/table-engines/special/distributed#distributed-reading-data","content":"When querying a Distributed table, SELECT queries are sent to all shards and work regardless of how data is distributed across the shards (they can be distributed completely randomly). When you add a new shard, you do not have to transfer old data into it. Instead, you can write new data to it by using a heavier weight – the data will be distributed slightly unevenly, but queries will work correctly and efficiently. When the max_parallel_replicas option is enabled, query processing is parallelized across all replicas within a single shard. For more information, see the section max_parallel_replicas. To learn more about how distibuted in and global in queries are processed, refer to this documentation. "},{"title":"Virtual Columns​","type":1,"pageTitle":"Distributed Table Engine","url":"docs/en/engines/table-engines/special/distributed#virtual-columns","content":"_shard_num — Contains the shard_num value from the table system.clusters. Type: UInt32. note Since remote and cluster table functions internally create temporary Distributed table, _shard_num is available there too. See Also Virtual columns descriptionbackground_distributed_schedule_pool_size settingshardNum() and shardCount() functions Original article "},{"title":"VersionedCollapsingMergeTree","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/mergetree-family/versionedcollapsingmergetree","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"VersionedCollapsingMergeTree","url":"docs/en/engines/table-engines/mergetree-family/versionedcollapsingmergetree#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE = VersionedCollapsingMergeTree(sign, version) [PARTITION BY expr] [ORDER BY expr] [SAMPLE BY expr] [SETTINGS name=value, ...]  For a description of query parameters, see the query description. Engine Parameters VersionedCollapsingMergeTree(sign, version)  sign — Name of the column with the type of row: 1 is a “state” row, -1 is a “cancel” row. The column data type should be Int8. version — Name of the column with the version of the object state. The column data type should be UInt*. Query Clauses When creating a VersionedCollapsingMergeTree table, the same clauses are required as when creating a MergeTree table. Deprecated Method for Creating a Table warning Do not use this method in new projects. If possible, switch old projects to the method described above. CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE [=] VersionedCollapsingMergeTree(date-column [, samp#table_engines_versionedcollapsingmergetreeling_expression], (primary, key), index_granularity, sign, version) All of the parameters except sign and version have the same meaning as in MergeTree. sign — Name of the column with the type of row: 1 is a “state” row, -1 is a “cancel” row. Column Data Type — Int8. version — Name of the column with the version of the object state. The column data type should be UInt*. "},{"title":"Collapsing​","type":1,"pageTitle":"VersionedCollapsingMergeTree","url":"docs/en/engines/table-engines/mergetree-family/versionedcollapsingmergetree#table_engines_versionedcollapsingmergetree","content":""},{"title":"Data​","type":1,"pageTitle":"VersionedCollapsingMergeTree","url":"docs/en/engines/table-engines/mergetree-family/versionedcollapsingmergetree#data","content":"Consider a situation where you need to save continually changing data for some object. It is reasonable to have one row for an object and update the row whenever there are changes. However, the update operation is expensive and slow for a DBMS because it requires rewriting the data in the storage. Update is not acceptable if you need to write data quickly, but you can write the changes to an object sequentially as follows. Use the Sign column when writing the row. If Sign = 1 it means that the row is a state of an object (let’s call it the “state” row). If Sign = -1 it indicates the cancellation of the state of an object with the same attributes (let’s call it the “cancel” row). Also use the Version column, which should identify each state of an object with a separate number. For example, we want to calculate how many pages users visited on some site and how long they were there. At some point in time we write the following row with the state of user activity: ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐ │ 4324182021466249494 │ 5 │ 146 │ 1 │ 1 | └─────────────────────┴───────────┴──────────┴──────┴─────────┘  At some point later we register the change of user activity and write it with the following two rows. ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐ │ 4324182021466249494 │ 5 │ 146 │ -1 │ 1 | │ 4324182021466249494 │ 6 │ 185 │ 1 │ 2 | └─────────────────────┴───────────┴──────────┴──────┴─────────┘  The first row cancels the previous state of the object (user). It should copy all of the fields of the canceled state except Sign. The second row contains the current state. Because we need only the last state of user activity, the rows ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐ │ 4324182021466249494 │ 5 │ 146 │ 1 │ 1 | │ 4324182021466249494 │ 5 │ 146 │ -1 │ 1 | └─────────────────────┴───────────┴──────────┴──────┴─────────┘  can be deleted, collapsing the invalid (old) state of the object. VersionedCollapsingMergeTree does this while merging the data parts. To find out why we need two rows for each change, see Algorithm. Notes on Usage The program that writes the data should remember the state of an object to be able to cancel it. “Cancel” string should contain copies of the primary key fields and the version of the “state” string and the opposite Sign. It increases the initial size of storage but allows to write the data quickly.Long growing arrays in columns reduce the efficiency of the engine due to the load for writing. The more straightforward the data, the better the efficiency.SELECT results depend strongly on the consistency of the history of object changes. Be accurate when preparing data for inserting. You can get unpredictable results with inconsistent data, such as negative values for non-negative metrics like session depth. "},{"title":"Algorithm​","type":1,"pageTitle":"VersionedCollapsingMergeTree","url":"docs/en/engines/table-engines/mergetree-family/versionedcollapsingmergetree#table_engines-versionedcollapsingmergetree-algorithm","content":"When ClickHouse merges data parts, it deletes each pair of rows that have the same primary key and version and different Sign. The order of rows does not matter. When ClickHouse inserts data, it orders rows by the primary key. If the Version column is not in the primary key, ClickHouse adds it to the primary key implicitly as the last field and uses it for ordering. "},{"title":"Selecting Data​","type":1,"pageTitle":"VersionedCollapsingMergeTree","url":"docs/en/engines/table-engines/mergetree-family/versionedcollapsingmergetree#selecting-data","content":"ClickHouse does not guarantee that all of the rows with the same primary key will be in the same resulting data part or even on the same physical server. This is true both for writing the data and for subsequent merging of the data parts. In addition, ClickHouse processes SELECT queries with multiple threads, and it cannot predict the order of rows in the result. This means that aggregation is required if there is a need to get completely “collapsed” data from a VersionedCollapsingMergeTree table. To finalize collapsing, write a query with a GROUP BY clause and aggregate functions that account for the sign. For example, to calculate quantity, use sum(Sign) instead of count(). To calculate the sum of something, use sum(Sign * x) instead of sum(x), and add HAVING sum(Sign) &gt; 0. The aggregates count, sum and avg can be calculated this way. The aggregate uniq can be calculated if an object has at least one non-collapsed state. The aggregates min and max can’t be calculated because VersionedCollapsingMergeTree does not save the history of values of collapsed states. If you need to extract the data with “collapsing” but without aggregation (for example, to check whether rows are present whose newest values match certain conditions), you can use the FINAL modifier for the FROM clause. This approach is inefficient and should not be used with large tables. "},{"title":"Example of Use​","type":1,"pageTitle":"VersionedCollapsingMergeTree","url":"docs/en/engines/table-engines/mergetree-family/versionedcollapsingmergetree#example-of-use","content":"Example data: ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐ │ 4324182021466249494 │ 5 │ 146 │ 1 │ 1 | │ 4324182021466249494 │ 5 │ 146 │ -1 │ 1 | │ 4324182021466249494 │ 6 │ 185 │ 1 │ 2 | └─────────────────────┴───────────┴──────────┴──────┴─────────┘  Creating the table: CREATE TABLE UAct ( UserID UInt64, PageViews UInt8, Duration UInt8, Sign Int8, Version UInt8 ) ENGINE = VersionedCollapsingMergeTree(Sign, Version) ORDER BY UserID  Inserting the data: INSERT INTO UAct VALUES (4324182021466249494, 5, 146, 1, 1)  INSERT INTO UAct VALUES (4324182021466249494, 5, 146, -1, 1),(4324182021466249494, 6, 185, 1, 2)  We use two INSERT queries to create two different data parts. If we insert the data with a single query, ClickHouse creates one data part and will never perform any merge. Getting the data: SELECT * FROM UAct  ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐ │ 4324182021466249494 │ 5 │ 146 │ 1 │ 1 │ └─────────────────────┴───────────┴──────────┴──────┴─────────┘ ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐ │ 4324182021466249494 │ 5 │ 146 │ -1 │ 1 │ │ 4324182021466249494 │ 6 │ 185 │ 1 │ 2 │ └─────────────────────┴───────────┴──────────┴──────┴─────────┘  What do we see here and where are the collapsed parts? We created two data parts using two INSERT queries. The SELECT query was performed in two threads, and the result is a random order of rows. Collapsing did not occur because the data parts have not been merged yet. ClickHouse merges data parts at an unknown point in time which we cannot predict. This is why we need aggregation: SELECT UserID, sum(PageViews * Sign) AS PageViews, sum(Duration * Sign) AS Duration, Version FROM UAct GROUP BY UserID, Version HAVING sum(Sign) &gt; 0  ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Version─┐ │ 4324182021466249494 │ 6 │ 185 │ 2 │ └─────────────────────┴───────────┴──────────┴─────────┘  If we do not need aggregation and want to force collapsing, we can use the FINAL modifier for the FROM clause. SELECT * FROM UAct FINAL  ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐ │ 4324182021466249494 │ 6 │ 185 │ 1 │ 2 │ └─────────────────────┴───────────┴──────────┴──────┴─────────┘  This is a very inefficient way to select data. Don’t use it for large tables. Original article "},{"title":"Join Table Engine","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/special/join","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"Join Table Engine","url":"docs/en/engines/table-engines/special/join#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [TTL expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [TTL expr2], ) ENGINE = Join(join_strictness, join_type, k1[, k2, ...])  See the detailed description of the CREATE TABLE query. Engine Parameters join_strictness – JOIN strictness.join_type – JOIN type.k1[, k2, ...] – Key columns from the USING clause that the JOIN operation is made with. Enter join_strictness and join_type parameters without quotes, for example, Join(ANY, LEFT, col1). They must match the JOIN operation that the table will be used for. If the parameters do not match, ClickHouse does not throw an exception and may return incorrect data. "},{"title":"Specifics and Recommendations​","type":1,"pageTitle":"Join Table Engine","url":"docs/en/engines/table-engines/special/join#specifics-and-recommendations","content":""},{"title":"Data Storage​","type":1,"pageTitle":"Join Table Engine","url":"docs/en/engines/table-engines/special/join#data-storage","content":"Join table data is always located in the RAM. When inserting rows into a table, ClickHouse writes data blocks to the directory on the disk so that they can be restored when the server restarts. If the server restarts incorrectly, the data block on the disk might get lost or damaged. In this case, you may need to manually delete the file with damaged data. "},{"title":"Selecting and Inserting Data​","type":1,"pageTitle":"Join Table Engine","url":"docs/en/engines/table-engines/special/join#selecting-and-inserting-data","content":"You can use INSERT queries to add data to the Join-engine tables. If the table was created with the ANY strictness, data for duplicate keys are ignored. With the ALL strictness, all rows are added. Main use-cases for Join-engine tables are following: Place the table to the right side in a JOIN clause.Call the joinGet function, which lets you extract data from the table the same way as from a dictionary. "},{"title":"Deleting Data​","type":1,"pageTitle":"Join Table Engine","url":"docs/en/engines/table-engines/special/join#deleting-data","content":"ALTER DELETE queries for Join-engine tables are implemented as mutations. DELETE mutation reads filtered data and overwrites data of memory and disk. "},{"title":"Limitations and Settings​","type":1,"pageTitle":"Join Table Engine","url":"docs/en/engines/table-engines/special/join#join-limitations-and-settings","content":"When creating a table, the following settings are applied: join_use_nullsmax_rows_in_joinmax_bytes_in_joinjoin_overflow_modejoin_any_take_last_rowpersistent The Join-engine tables can’t be used in GLOBAL JOIN operations. The Join-engine allows to specify join_use_nulls setting in the CREATE TABLE statement. SELECT query should have the same join_use_nulls value. "},{"title":"Usage Examples​","type":1,"pageTitle":"Join Table Engine","url":"docs/en/engines/table-engines/special/join#example","content":"Creating the left-side table: CREATE TABLE id_val(`id` UInt32, `val` UInt32) ENGINE = TinyLog;  INSERT INTO id_val VALUES (1,11)(2,12)(3,13);  Creating the right-side Join table: CREATE TABLE id_val_join(`id` UInt32, `val` UInt8) ENGINE = Join(ANY, LEFT, id);  INSERT INTO id_val_join VALUES (1,21)(1,22)(3,23);  Joining the tables: SELECT * FROM id_val ANY LEFT JOIN id_val_join USING (id);  ┌─id─┬─val─┬─id_val_join.val─┐ │ 1 │ 11 │ 21 │ │ 2 │ 12 │ 0 │ │ 3 │ 13 │ 23 │ └────┴─────┴─────────────────┘  As an alternative, you can retrieve data from the Join table, specifying the join key value: SELECT joinGet('id_val_join', 'val', toUInt32(1));  ┌─joinGet('id_val_join', 'val', toUInt32(1))─┐ │ 21 │ └────────────────────────────────────────────┘  Deleting a row from the Join table: ALTER TABLE id_val_join DELETE WHERE id = 3;  ┌─id─┬─val─┐ │ 1 │ 21 │ └────┴─────┘  Original article "},{"title":"Dictionary Table Engine","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/special/dictionary","content":"","keywords":""},{"title":"Example​","type":1,"pageTitle":"Dictionary Table Engine","url":"docs/en/engines/table-engines/special/dictionary#example","content":"As an example, consider a dictionary of products with the following configuration: &lt;dictionaries&gt; &lt;dictionary&gt; &lt;name&gt;products&lt;/name&gt; &lt;source&gt; &lt;odbc&gt; &lt;table&gt;products&lt;/table&gt; &lt;connection_string&gt;DSN=some-db-server&lt;/connection_string&gt; &lt;/odbc&gt; &lt;/source&gt; &lt;lifetime&gt; &lt;min&gt;300&lt;/min&gt; &lt;max&gt;360&lt;/max&gt; &lt;/lifetime&gt; &lt;layout&gt; &lt;flat/&gt; &lt;/layout&gt; &lt;structure&gt; &lt;id&gt; &lt;name&gt;product_id&lt;/name&gt; &lt;/id&gt; &lt;attribute&gt; &lt;name&gt;title&lt;/name&gt; &lt;type&gt;String&lt;/type&gt; &lt;null_value&gt;&lt;/null_value&gt; &lt;/attribute&gt; &lt;/structure&gt; &lt;/dictionary&gt; &lt;/dictionaries&gt;  Query the dictionary data: SELECT name, type, key, attribute.names, attribute.types, bytes_allocated, element_count, source FROM system.dictionaries WHERE name = 'products'  ┌─name─────┬─type─┬─key────┬─attribute.names─┬─attribute.types─┬─bytes_allocated─┬─element_count─┬─source──────────┐ │ products │ Flat │ UInt64 │ ['title'] │ ['String'] │ 23065376 │ 175032 │ ODBC: .products │ └──────────┴──────┴────────┴─────────────────┴─────────────────┴─────────────────┴───────────────┴─────────────────┘  You can use the dictGet* function to get the dictionary data in this format. This view isn’t helpful when you need to get raw data, or when performing a JOIN operation. For these cases, you can use the Dictionary engine, which displays the dictionary data in a table. Syntax: CREATE TABLE %table_name% (%fields%) engine = Dictionary(%dictionary_name%)`  Usage example: create table products (product_id UInt64, title String) Engine = Dictionary(products);   Ok  Take a look at what’s in the table. select * from products limit 1;  ┌────product_id─┬─title───────────┐ │ 152689 │ Some item │ └───────────────┴─────────────────┘  See Also Dictionary function Original article "},{"title":"Memory Table Engine","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/special/memory","content":"Memory Table Engine The Memory engine stores data in RAM, in uncompressed form. Data is stored in exactly the same form as it is received when read. In other words, reading from this table is completely free. Concurrent data access is synchronized. Locks are short: read and write operations do not block each other. Indexes are not supported. Reading is parallelized. Maximal productivity (over 10 GB/sec) is reached on simple queries, because there is no reading from the disk, decompressing, or deserializing data. (We should note that in many cases, the productivity of the MergeTree engine is almost as high.) When restarting a server, data disappears from the table and the table becomes empty. Normally, using this table engine is not justified. However, it can be used for tests, and for tasks where maximum speed is required on a relatively small number of rows (up to approximately 100,000,000). The Memory engine is used by the system for temporary tables with external query data (see the section “External data for processing a query”), and for implementing GLOBAL IN (see the section “IN operators”). Original article","keywords":""},{"title":"MergeTree","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/mergetree-family/mergetree","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#table_engine-mergetree-creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [TTL expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [TTL expr2], ... INDEX index_name1 expr1 TYPE type1(...) GRANULARITY value1, INDEX index_name2 expr2 TYPE type2(...) GRANULARITY value2, ... PROJECTION projection_name_1 (SELECT &lt;COLUMN LIST EXPR&gt; [GROUP BY] [ORDER BY]), PROJECTION projection_name_2 (SELECT &lt;COLUMN LIST EXPR&gt; [GROUP BY] [ORDER BY]) ) ENGINE = MergeTree() ORDER BY expr [PARTITION BY expr] [PRIMARY KEY expr] [SAMPLE BY expr] [TTL expr [DELETE|TO DISK 'xxx'|TO VOLUME 'xxx' [, ...] ] [WHERE conditions] [GROUP BY key_expr [SET v1 = aggr_func(v1) [, v2 = aggr_func(v2) ...]] ] ] [SETTINGS name=value, ...]  For a description of parameters, see the CREATE query description. "},{"title":"Query Clauses​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#mergetree-query-clauses","content":"ENGINE — Name and parameters of the engine. ENGINE = MergeTree(). The MergeTree engine does not have parameters. ORDER BY — The sorting key. A tuple of column names or arbitrary expressions. Example: ORDER BY (CounterID, EventDate). ClickHouse uses the sorting key as a primary key if the primary key is not defined obviously by the PRIMARY KEY clause. Use the ORDER BY tuple() syntax, if you do not need sorting. See Selecting the Primary Key. PARTITION BY — The partitioning key. Optional. In most cases you don't need partition key, and in most other cases you don't need partition key more granular than by months. Partitioning does not speed up queries (in contrast to the ORDER BY expression). You should never use too granular partitioning. Don't partition your data by client identifiers or names (instead make client identifier or name the first column in the ORDER BY expression). For partitioning by month, use the toYYYYMM(date_column) expression, where date_column is a column with a date of the type Date. The partition names here have the &quot;YYYYMM&quot; format. PRIMARY KEY — The primary key if it differs from the sorting key. Optional. By default the primary key is the same as the sorting key (which is specified by the ORDER BY clause). Thus in most cases it is unnecessary to specify a separate PRIMARY KEY clause. SAMPLE BY — An expression for sampling. Optional. If a sampling expression is used, the primary key must contain it. The result of a sampling expression must be an unsigned integer. Example: SAMPLE BY intHash32(UserID) ORDER BY (CounterID, EventDate, intHash32(UserID)). TTL — A list of rules specifying storage duration of rows and defining logic of automatic parts movement between disks and volumes. Optional. Expression must have one Date or DateTime column as a result. Example:TTL date + INTERVAL 1 DAY Type of the rule DELETE|TO DISK 'xxx'|TO VOLUME 'xxx'|GROUP BY specifies an action to be done with the part if the expression is satisfied (reaches current time): removal of expired rows, moving a part (if expression is satisfied for all rows in a part) to specified disk (TO DISK 'xxx') or to volume (TO VOLUME 'xxx'), or aggregating values in expired rows. Default type of the rule is removal (DELETE). List of multiple rules can be specified, but there should be no more than one DELETE rule. For more details, see TTL for columns and tables SETTINGS — Additional parameters that control the behavior of the MergeTree (optional): index_granularity — Maximum number of data rows between the marks of an index. Default value: 8192. See Data Storage.index_granularity_bytes — Maximum size of data granules in bytes. Default value: 10Mb. To restrict the granule size only by number of rows, set to 0 (not recommended). See Data Storage.min_index_granularity_bytes — Min allowed size of data granules in bytes. Default value: 1024b. To provide a safeguard against accidentally creating tables with very low index_granularity_bytes. See Data Storage.enable_mixed_granularity_parts — Enables or disables transitioning to control the granule size with the index_granularity_bytes setting. Before version 19.11, there was only the index_granularity setting for restricting granule size. The index_granularity_bytes setting improves ClickHouse performance when selecting data from tables with big rows (tens and hundreds of megabytes). If you have tables with big rows, you can enable this setting for the tables to improve the efficiency of SELECT queries.use_minimalistic_part_header_in_zookeeper — Storage method of the data parts headers in ZooKeeper. If use_minimalistic_part_header_in_zookeeper=1, then ZooKeeper stores less data. For more information, see the setting description in “Server configuration parameters”.min_merge_bytes_to_use_direct_io — The minimum data volume for merge operation that is required for using direct I/O access to the storage disk. When merging data parts, ClickHouse calculates the total storage volume of all the data to be merged. If the volume exceeds min_merge_bytes_to_use_direct_io bytes, ClickHouse reads and writes the data to the storage disk using the direct I/O interface (O_DIRECT option). If min_merge_bytes_to_use_direct_io = 0, then direct I/O is disabled. Default value: 10 * 1024 * 1024 * 1024 bytes.merge_with_ttl_timeout — Minimum delay in seconds before repeating a merge with delete TTL. Default value: 14400 seconds (4 hours).merge_with_recompression_ttl_timeout — Minimum delay in seconds before repeating a merge with recompression TTL. Default value: 14400 seconds (4 hours).try_fetch_recompressed_part_timeout — Timeout (in seconds) before starting merge with recompression. During this time ClickHouse tries to fetch recompressed part from replica which assigned this merge with recompression. Default value: 7200 seconds (2 hours).write_final_mark — Enables or disables writing the final index mark at the end of data part (after the last byte). Default value: 1. Don’t turn it off.merge_max_block_size — Maximum number of rows in block for merge operations. Default value: 8192.storage_policy — Storage policy. See Using Multiple Block Devices for Data Storage.min_bytes_for_wide_part, min_rows_for_wide_part — Minimum number of bytes/rows in a data part that can be stored in Wide format. You can set one, both or none of these settings. See Data Storage.max_parts_in_total — Maximum number of parts in all partitions.max_compress_block_size — Maximum size of blocks of uncompressed data before compressing for writing to a table. You can also specify this setting in the global settings (see max_compress_block_size setting). The value specified when table is created overrides the global value for this setting.min_compress_block_size — Minimum size of blocks of uncompressed data required for compression when writing the next mark. You can also specify this setting in the global settings (see min_compress_block_size setting). The value specified when table is created overrides the global value for this setting.max_partitions_to_read — Limits the maximum number of partitions that can be accessed in one query. You can also specify setting max_partitions_to_read in the global setting. Example of Sections Setting ENGINE MergeTree() PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate, intHash32(UserID)) SAMPLE BY intHash32(UserID) SETTINGS index_granularity=8192  In the example, we set partitioning by month. We also set an expression for sampling as a hash by the user ID. This allows you to pseudorandomize the data in the table for each CounterID and EventDate. If you define a SAMPLE clause when selecting the data, ClickHouse will return an evenly pseudorandom data sample for a subset of users. The index_granularity setting can be omitted because 8192 is the default value. Deprecated Method for Creating a Table warning Do not use this method in new projects. If possible, switch old projects to the method described above. CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE [=] MergeTree(date-column [, sampling_expression], (primary, key), index_granularity) MergeTree() Parameters date-column — The name of a column of the Date type. ClickHouse automatically creates partitions by month based on this column. The partition names are in the &quot;YYYYMM&quot; format.sampling_expression — An expression for sampling.(primary, key) — Primary key. Type: Tuple()index_granularity — The granularity of an index. The number of data rows between the “marks” of an index. The value 8192 is appropriate for most tasks. Example MergeTree(EventDate, intHash32(UserID), (CounterID, EventDate, intHash32(UserID)), 8192) The MergeTree engine is configured in the same way as in the example above for the main engine configuration method. "},{"title":"Data Storage​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#mergetree-data-storage","content":"A table consists of data parts sorted by primary key. When data is inserted in a table, separate data parts are created and each of them is lexicographically sorted by primary key. For example, if the primary key is (CounterID, Date), the data in the part is sorted by CounterID, and within each CounterID, it is ordered by Date. Data belonging to different partitions are separated into different parts. In the background, ClickHouse merges data parts for more efficient storage. Parts belonging to different partitions are not merged. The merge mechanism does not guarantee that all rows with the same primary key will be in the same data part. Data parts can be stored in Wide or Compact format. In Wide format each column is stored in a separate file in a filesystem, in Compact format all columns are stored in one file. Compact format can be used to increase performance of small and frequent inserts. Data storing format is controlled by the min_bytes_for_wide_part and min_rows_for_wide_part settings of the table engine. If the number of bytes or rows in a data part is less then the corresponding setting's value, the part is stored in Compact format. Otherwise it is stored in Wide format. If none of these settings is set, data parts are stored in Wide format. Each data part is logically divided into granules. A granule is the smallest indivisible data set that ClickHouse reads when selecting data. ClickHouse does not split rows or values, so each granule always contains an integer number of rows. The first row of a granule is marked with the value of the primary key for the row. For each data part, ClickHouse creates an index file that stores the marks. For each column, whether it’s in the primary key or not, ClickHouse also stores the same marks. These marks let you find data directly in column files. The granule size is restricted by the index_granularity and index_granularity_bytes settings of the table engine. The number of rows in a granule lays in the [1, index_granularity] range, depending on the size of the rows. The size of a granule can exceed index_granularity_bytes if the size of a single row is greater than the value of the setting. In this case, the size of the granule equals the size of the row. "},{"title":"Primary Keys and Indexes in Queries​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#primary-keys-and-indexes-in-queries","content":"Take the (CounterID, Date) primary key as an example. In this case, the sorting and index can be illustrated as follows:  Whole data: [---------------------------------------------] CounterID: [aaaaaaaaaaaaaaaaaabbbbcdeeeeeeeeeeeeefgggggggghhhhhhhhhiiiiiiiiikllllllll] Date: [1111111222222233331233211111222222333211111112122222223111112223311122333] Marks: | | | | | | | | | | | a,1 a,2 a,3 b,3 e,2 e,3 g,1 h,2 i,1 i,3 l,3 Marks numbers: 0 1 2 3 4 5 6 7 8 9 10  If the data query specifies: CounterID in ('a', 'h'), the server reads the data in the ranges of marks [0, 3) and [6, 8).CounterID IN ('a', 'h') AND Date = 3, the server reads the data in the ranges of marks [1, 3) and [7, 8).Date = 3, the server reads the data in the range of marks [1, 10]. The examples above show that it is always more effective to use an index than a full scan. A sparse index allows extra data to be read. When reading a single range of the primary key, up to index_granularity * 2 extra rows in each data block can be read. Sparse indexes allow you to work with a very large number of table rows, because in most cases, such indexes fit in the computer’s RAM. ClickHouse does not require a unique primary key. You can insert multiple rows with the same primary key. You can use Nullable-typed expressions in the PRIMARY KEY and ORDER BY clauses but it is strongly discouraged. To allow this feature, turn on the allow_nullable_key setting. The NULLS_LAST principle applies for NULL values in the ORDER BY clause. "},{"title":"Selecting the Primary Key​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#selecting-the-primary-key","content":"The number of columns in the primary key is not explicitly limited. Depending on the data structure, you can include more or fewer columns in the primary key. This may: Improve the performance of an index. If the primary key is (a, b), then adding another column c will improve the performance if the following conditions are met: There are queries with a condition on column c.Long data ranges (several times longer than the index_granularity) with identical values for (a, b) are common. In other words, when adding another column allows you to skip quite long data ranges. Improve data compression. ClickHouse sorts data by primary key, so the higher the consistency, the better the compression. Provide additional logic when merging data parts in the CollapsingMergeTree and SummingMergeTree engines. In this case it makes sense to specify the sorting key that is different from the primary key. A long primary key will negatively affect the insert performance and memory consumption, but extra columns in the primary key do not affect ClickHouse performance during SELECT queries. You can create a table without a primary key using the ORDER BY tuple() syntax. In this case, ClickHouse stores data in the order of inserting. If you want to save data order when inserting data by INSERT ... SELECT queries, set max_insert_threads = 1. To select data in the initial order, use single-threaded SELECT queries. "},{"title":"Choosing a Primary Key that Differs from the Sorting Key​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#choosing-a-primary-key-that-differs-from-the-sorting-key","content":"It is possible to specify a primary key (an expression with values that are written in the index file for each mark) that is different from the sorting key (an expression for sorting the rows in data parts). In this case the primary key expression tuple must be a prefix of the sorting key expression tuple. This feature is helpful when using the SummingMergeTree andAggregatingMergeTree table engines. In a common case when using these engines, the table has two types of columns: dimensions and measures. Typical queries aggregate values of measure columns with arbitrary GROUP BY and filtering by dimensions. Because SummingMergeTree and AggregatingMergeTree aggregate rows with the same value of the sorting key, it is natural to add all dimensions to it. As a result, the key expression consists of a long list of columns and this list must be frequently updated with newly added dimensions. In this case it makes sense to leave only a few columns in the primary key that will provide efficient range scans and add the remaining dimension columns to the sorting key tuple. ALTER of the sorting key is a lightweight operation because when a new column is simultaneously added to the table and to the sorting key, existing data parts do not need to be changed. Since the old sorting key is a prefix of the new sorting key and there is no data in the newly added column, the data is sorted by both the old and new sorting keys at the moment of table modification. "},{"title":"Use of Indexes and Partitions in Queries​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#use-of-indexes-and-partitions-in-queries","content":"For SELECT queries, ClickHouse analyzes whether an index can be used. An index can be used if the WHERE/PREWHERE clause has an expression (as one of the conjunction elements, or entirely) that represents an equality or inequality comparison operation, or if it has IN or LIKE with a fixed prefix on columns or expressions that are in the primary key or partitioning key, or on certain partially repetitive functions of these columns, or logical relationships of these expressions. Thus, it is possible to quickly run queries on one or many ranges of the primary key. In this example, queries will be fast when run for a specific tracking tag, for a specific tag and date range, for a specific tag and date, for multiple tags with a date range, and so on. Let’s look at the engine configured as follows:  ENGINE MergeTree() PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate) SETTINGS index_granularity=8192  In this case, in queries: SELECT count() FROM table WHERE EventDate = toDate(now()) AND CounterID = 34 SELECT count() FROM table WHERE EventDate = toDate(now()) AND (CounterID = 34 OR CounterID = 42) SELECT count() FROM table WHERE ((EventDate &gt;= toDate('2014-01-01') AND EventDate &lt;= toDate('2014-01-31')) OR EventDate = toDate('2014-05-01')) AND CounterID IN (101500, 731962, 160656) AND (CounterID = 101500 OR EventDate != toDate('2014-05-01'))  ClickHouse will use the primary key index to trim improper data and the monthly partitioning key to trim partitions that are in improper date ranges. The queries above show that the index is used even for complex expressions. Reading from the table is organized so that using the index can’t be slower than a full scan. In the example below, the index can’t be used. SELECT count() FROM table WHERE CounterID = 34 OR URL LIKE '%upyachka%'  To check whether ClickHouse can use the index when running a query, use the settings force_index_by_date and force_primary_key. The key for partitioning by month allows reading only those data blocks which contain dates from the proper range. In this case, the data block may contain data for many dates (up to an entire month). Within a block, data is sorted by primary key, which might not contain the date as the first column. Because of this, using a query with only a date condition that does not specify the primary key prefix will cause more data to be read than for a single date. "},{"title":"Use of Index for Partially-monotonic Primary Keys​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#use-of-index-for-partially-monotonic-primary-keys","content":"Consider, for example, the days of the month. They form a monotonic sequence for one month, but not monotonic for more extended periods. This is a partially-monotonic sequence. If a user creates the table with partially-monotonic primary key, ClickHouse creates a sparse index as usual. When a user selects data from this kind of table, ClickHouse analyzes the query conditions. If the user wants to get data between two marks of the index and both these marks fall within one month, ClickHouse can use the index in this particular case because it can calculate the distance between the parameters of a query and index marks. ClickHouse cannot use an index if the values of the primary key in the query parameter range do not represent a monotonic sequence. In this case, ClickHouse uses the full scan method. ClickHouse uses this logic not only for days of the month sequences, but for any primary key that represents a partially-monotonic sequence. "},{"title":"Data Skipping Indexes​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#table_engine-mergetree-data_skipping-indexes","content":"The index declaration is in the columns section of the CREATE query. INDEX index_name expr TYPE type(...) GRANULARITY granularity_value  For tables from the *MergeTree family, data skipping indices can be specified. These indices aggregate some information about the specified expression on blocks, which consist of granularity_value granules (the size of the granule is specified using the index_granularity setting in the table engine). Then these aggregates are used in SELECT queries for reducing the amount of data to read from the disk by skipping big blocks of data where the where query cannot be satisfied. Example CREATE TABLE table_name ( u64 UInt64, i32 Int32, s String, ... INDEX a (u64 * i32, s) TYPE minmax GRANULARITY 3, INDEX b (u64 * length(s)) TYPE set(1000) GRANULARITY 4 ) ENGINE = MergeTree() ...  Indices from the example can be used by ClickHouse to reduce the amount of data to read from disk in the following queries: SELECT count() FROM table WHERE s &amp;lt; 'z' SELECT count() FROM table WHERE u64 * i32 == 10 AND u64 * length(s) &amp;gt;= 1234  Available Types of Indices​ minmax Stores extremes of the specified expression (if the expression is tuple, then it stores extremes for each element of tuple), uses stored info for skipping blocks of data like the primary key. set(max_rows) Stores unique values of the specified expression (no more than max_rows rows, max_rows=0 means “no limits”). Uses the values to check if the WHERE expression is not satisfiable on a block of data. ngrambf_v1(n, size_of_bloom_filter_in_bytes, number_of_hash_functions, random_seed) Stores a Bloom filter that contains all ngrams from a block of data. Works only with datatypes: String, FixedString and Map. Can be used for optimization of EQUALS, LIKE and IN expressions. n — ngram size,size_of_bloom_filter_in_bytes — Bloom filter size in bytes (you can use large values here, for example, 256 or 512, because it can be compressed well).number_of_hash_functions — The number of hash functions used in the Bloom filter.random_seed — The seed for Bloom filter hash functions. tokenbf_v1(size_of_bloom_filter_in_bytes, number_of_hash_functions, random_seed) The same as ngrambf_v1, but stores tokens instead of ngrams. Tokens are sequences separated by non-alphanumeric characters. bloom_filter([false_positive]) — Stores a Bloom filter for the specified columns. The optional false_positive parameter is the probability of receiving a false positive response from the filter. Possible values: (0, 1). Default value: 0.025. Supported data types: Int*, UInt*, Float*, Enum, Date, DateTime, String, FixedString, Array, LowCardinality, Nullable, UUID, Map. For Map data type client can specify if index should be created for keys or values using mapKeys or mapValues function. The following functions can use the filter: equals, notEquals, in, notIn, has, hasAny, hasAll. Example of index creation for Map data type INDEX map_key_index mapKeys(map_column) TYPE bloom_filter GRANULARITY 1 INDEX map_key_index mapValues(map_column) TYPE bloom_filter GRANULARITY 1  INDEX sample_index (u64 * length(s)) TYPE minmax GRANULARITY 4 INDEX sample_index2 (u64 * length(str), i32 + f64 * 100, date, str) TYPE set(100) GRANULARITY 4 INDEX sample_index3 (lower(str), str) TYPE ngrambf_v1(3, 256, 2, 0) GRANULARITY 4  Functions Support​ Conditions in the WHERE clause contains calls of the functions that operate with columns. If the column is a part of an index, ClickHouse tries to use this index when performing the functions. ClickHouse supports different subsets of functions for using indexes. The set index can be used with all functions. Function subsets for other indexes are shown in the table below. Function (operator) / Index\tprimary key\tminmax\tngrambf_v1\ttokenbf_v1\tbloom_filterequals (=, ==)\t✔\t✔\t✔\t✔\t✔ notEquals(!=, &lt;&gt;)\t✔\t✔\t✔\t✔\t✔ like\t✔\t✔\t✔\t✔\t✗ notLike\t✔\t✔\t✔\t✔\t✗ startsWith\t✔\t✔\t✔\t✔\t✗ endsWith\t✗\t✗\t✔\t✔\t✗ multiSearchAny\t✗\t✗\t✔\t✗\t✗ in\t✔\t✔\t✔\t✔\t✔ notIn\t✔\t✔\t✔\t✔\t✔ less (&lt;)\t✔\t✔\t✗\t✗\t✗ greater (&gt;)\t✔\t✔\t✗\t✗\t✗ lessOrEquals (&lt;=)\t✔\t✔\t✗\t✗\t✗ greaterOrEquals (&gt;=)\t✔\t✔\t✗\t✗\t✗ empty\t✔\t✔\t✗\t✗\t✗ notEmpty\t✔\t✔\t✗\t✗\t✗ hasToken\t✗\t✗\t✗\t✔\t✗ Functions with a constant argument that is less than ngram size can’t be used by ngrambf_v1 for query optimization. note Bloom filters can have false positive matches, so the ngrambf_v1, tokenbf_v1, and bloom_filter indexes can not be used for optimizing queries where the result of a function is expected to be false. For example: Can be optimized: s LIKE '%test%'NOT s NOT LIKE '%test%'s = 1NOT s != 1startsWith(s, 'test') Can not be optimized: NOT s LIKE '%test%'s NOT LIKE '%test%'NOT s = 1s != 1NOT startsWith(s, 'test') "},{"title":"Projections​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#projections","content":"Projections are like materialized views but defined in part-level. It provides consistency guarantees along with automatic usage in queries. Projections are an experimental feature. To enable them you must set the allow_experimental_projection_optimization to 1. See also the force_optimize_projection setting. Projections are not supported in the SELECT statements with the FINAL modifier. "},{"title":"Projection Query​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#projection-query","content":"A projection query is what defines a projection. It implicitly selects data from the parent table.Syntax SELECT &lt;column list expr&gt; [GROUP BY] &lt;group keys expr&gt; [ORDER BY] &lt;expr&gt;  Projections can be modified or dropped with the ALTER statement. "},{"title":"Projection Storage​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#projection-storage","content":"Projections are stored inside the part directory. It's similar to an index but contains a subdirectory that stores an anonymous MergeTree table's part. The table is induced by the definition query of the projection. If there is a GROUP BY clause, the underlying storage engine becomes AggregatingMergeTree, and all aggregate functions are converted to AggregateFunction. If there is an ORDER BY clause, the MergeTree table uses it as its primary key expression. During the merge process the projection part is merged via its storage's merge routine. The checksum of the parent table's part is combined with the projection's part. Other maintenance jobs are similar to skip indices. "},{"title":"Query Analysis​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#projection-query-analysis","content":"Check if the projection can be used to answer the given query, that is, it generates the same answer as querying the base table.Select the best feasible match, which contains the least granules to read.The query pipeline which uses projections will be different from the one that uses the original parts. If the projection is absent in some parts, we can add the pipeline to &quot;project&quot; it on the fly. "},{"title":"Concurrent Data Access​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#concurrent-data-access","content":"For concurrent table access, we use multi-versioning. In other words, when a table is simultaneously read and updated, data is read from a set of parts that is current at the time of the query. There are no lengthy locks. Inserts do not get in the way of read operations. Reading from a table is automatically parallelized. "},{"title":"TTL for Columns and Tables​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#table_engine-mergetree-ttl","content":"Determines the lifetime of values. The TTL clause can be set for the whole table and for each individual column. Table-level TTL can also specify the logic of automatic moving data between disks and volumes, or recompressing parts where all the data has been expired. Expressions must evaluate to Date or DateTime data type. Syntax Setting time-to-live for a column: TTL time_column TTL time_column + interval  To define interval, use time interval operators, for example: TTL date_time + INTERVAL 1 MONTH TTL date_time + INTERVAL 15 HOUR  "},{"title":"Column TTL​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#mergetree-column-ttl","content":"When the values in the column expire, ClickHouse replaces them with the default values for the column data type. If all the column values in the data part expire, ClickHouse deletes this column from the data part in a filesystem. The TTL clause can’t be used for key columns. Examples Creating a table with TTL: CREATE TABLE example_table ( d DateTime, a Int TTL d + INTERVAL 1 MONTH, b Int TTL d + INTERVAL 1 MONTH, c String ) ENGINE = MergeTree PARTITION BY toYYYYMM(d) ORDER BY d;  Adding TTL to a column of an existing table ALTER TABLE example_table MODIFY COLUMN c String TTL d + INTERVAL 1 DAY;  Altering TTL of the column ALTER TABLE example_table MODIFY COLUMN c String TTL d + INTERVAL 1 MONTH;  "},{"title":"Table TTL​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#mergetree-table-ttl","content":"Table can have an expression for removal of expired rows, and multiple expressions for automatic move of parts between disks or volumes. When rows in the table expire, ClickHouse deletes all corresponding rows. For parts moving or recompressing, all rows of a part must satisfy the TTL expression criteria. TTL expr [DELETE|RECOMPRESS codec_name1|TO DISK 'xxx'|TO VOLUME 'xxx'][, DELETE|RECOMPRESS codec_name2|TO DISK 'aaa'|TO VOLUME 'bbb'] ... [WHERE conditions] [GROUP BY key_expr [SET v1 = aggr_func(v1) [, v2 = aggr_func(v2) ...]] ]  Type of TTL rule may follow each TTL expression. It affects an action which is to be done once the expression is satisfied (reaches current time): DELETE - delete expired rows (default action);RECOMPRESS codec_name - recompress data part with the codec_name;TO DISK 'aaa' - move part to the disk aaa;TO VOLUME 'bbb' - move part to the disk bbb;GROUP BY - aggregate expired rows. With WHERE clause you may specify which of the expired rows to delete or aggregate (it cannot be applied to moves or recompression). GROUP BY expression must be a prefix of the table primary key. If a column is not part of the GROUP BY expression and is not set explicitly in the SET clause, in result row it contains an occasional value from the grouped rows (as if aggregate function any is applied to it). Examples Creating a table with TTL: CREATE TABLE example_table ( d DateTime, a Int ) ENGINE = MergeTree PARTITION BY toYYYYMM(d) ORDER BY d TTL d + INTERVAL 1 MONTH [DELETE], d + INTERVAL 1 WEEK TO VOLUME 'aaa', d + INTERVAL 2 WEEK TO DISK 'bbb';  Altering TTL of the table: ALTER TABLE example_table MODIFY TTL d + INTERVAL 1 DAY;  Creating a table, where the rows are expired after one month. The expired rows where dates are Mondays are deleted: CREATE TABLE table_with_where ( d DateTime, a Int ) ENGINE = MergeTree PARTITION BY toYYYYMM(d) ORDER BY d TTL d + INTERVAL 1 MONTH DELETE WHERE toDayOfWeek(d) = 1;  Creating a table, where expired rows are recompressed: CREATE TABLE table_for_recompression ( d DateTime, key UInt64, value String ) ENGINE MergeTree() ORDER BY tuple() PARTITION BY key TTL d + INTERVAL 1 MONTH RECOMPRESS CODEC(ZSTD(17)), d + INTERVAL 1 YEAR RECOMPRESS CODEC(LZ4HC(10)) SETTINGS min_rows_for_wide_part = 0, min_bytes_for_wide_part = 0;  Creating a table, where expired rows are aggregated. In result rows x contains the maximum value accross the grouped rows, y — the minimum value, and d — any occasional value from grouped rows. CREATE TABLE table_for_aggregation ( d DateTime, k1 Int, k2 Int, x Int, y Int ) ENGINE = MergeTree ORDER BY (k1, k2) TTL d + INTERVAL 1 MONTH GROUP BY k1, k2 SET x = max(x), y = min(y);  "},{"title":"Removing Expired Data​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#mergetree-removing-expired-data","content":"Data with an expired TTL is removed when ClickHouse merges data parts. When ClickHouse detects that data is expired, it performs an off-schedule merge. To control the frequency of such merges, you can set merge_with_ttl_timeout. If the value is too low, it will perform many off-schedule merges that may consume a lot of resources. If you perform the SELECT query between merges, you may get expired data. To avoid it, use the OPTIMIZE query before SELECT. See Also ttl_only_drop_parts setting "},{"title":"Using Multiple Block Devices for Data Storage​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#table_engine-mergetree-multiple-volumes","content":""},{"title":"Introduction​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#introduction","content":"MergeTree family table engines can store data on multiple block devices. For example, it can be useful when the data of a certain table are implicitly split into “hot” and “cold”. The most recent data is regularly requested but requires only a small amount of space. On the contrary, the fat-tailed historical data is requested rarely. If several disks are available, the “hot” data may be located on fast disks (for example, NVMe SSDs or in memory), while the “cold” data - on relatively slow ones (for example, HDD). Data part is the minimum movable unit for MergeTree-engine tables. The data belonging to one part are stored on one disk. Data parts can be moved between disks in the background (according to user settings) as well as by means of the ALTER queries. "},{"title":"Terms​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#terms","content":"Disk — Block device mounted to the filesystem.Default disk — Disk that stores the path specified in the path server setting.Volume — Ordered set of equal disks (similar to JBOD).Storage policy — Set of volumes and the rules for moving data between them. The names given to the described entities can be found in the system tables, system.storage_policies and system.disks. To apply one of the configured storage policies for a table, use the storage_policy setting of MergeTree-engine family tables. "},{"title":"Configuration​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#table_engine-mergetree-multiple-volumes_configure","content":"Disks, volumes and storage policies should be declared inside the &lt;storage_configuration&gt; tag either in the main file config.xml or in a distinct file in the config.d directory. Configuration structure: &lt;storage_configuration&gt; &lt;disks&gt; &lt;disk_name_1&gt; &lt;!-- disk name --&gt; &lt;path&gt;/mnt/fast_ssd/clickhouse/&lt;/path&gt; &lt;/disk_name_1&gt; &lt;disk_name_2&gt; &lt;path&gt;/mnt/hdd1/clickhouse/&lt;/path&gt; &lt;keep_free_space_bytes&gt;10485760&lt;/keep_free_space_bytes&gt; &lt;/disk_name_2&gt; &lt;disk_name_3&gt; &lt;path&gt;/mnt/hdd2/clickhouse/&lt;/path&gt; &lt;keep_free_space_bytes&gt;10485760&lt;/keep_free_space_bytes&gt; &lt;/disk_name_3&gt; ... &lt;/disks&gt; ... &lt;/storage_configuration&gt;  Tags: &lt;disk_name_N&gt; — Disk name. Names must be different for all disks.path — path under which a server will store data (data and shadow folders), should be terminated with ‘/’.keep_free_space_bytes — the amount of free disk space to be reserved. The order of the disk definition is not important. Storage policies configuration markup: &lt;storage_configuration&gt; ... &lt;policies&gt; &lt;policy_name_1&gt; &lt;volumes&gt; &lt;volume_name_1&gt; &lt;disk&gt;disk_name_from_disks_configuration&lt;/disk&gt; &lt;max_data_part_size_bytes&gt;1073741824&lt;/max_data_part_size_bytes&gt; &lt;/volume_name_1&gt; &lt;volume_name_2&gt; &lt;!-- configuration --&gt; &lt;/volume_name_2&gt; &lt;!-- more volumes --&gt; &lt;/volumes&gt; &lt;move_factor&gt;0.2&lt;/move_factor&gt; &lt;/policy_name_1&gt; &lt;policy_name_2&gt; &lt;!-- configuration --&gt; &lt;/policy_name_2&gt; &lt;!-- more policies --&gt; &lt;/policies&gt; ... &lt;/storage_configuration&gt;  Tags: policy_name_N — Policy name. Policy names must be unique.volume_name_N — Volume name. Volume names must be unique.disk — a disk within a volume.max_data_part_size_bytes — the maximum size of a part that can be stored on any of the volume’s disks. If the a size of a merged part estimated to be bigger than max_data_part_size_bytes then this part will be written to a next volume. Basically this feature allows to keep new/small parts on a hot (SSD) volume and move them to a cold (HDD) volume when they reach large size. Do not use this setting if your policy has only one volume.move_factor — when the amount of available space gets lower than this factor, data automatically starts to move on the next volume if any (by default, 0.1). ClickHouse sorts existing parts by size from largest to smallest (in descending order) and selects parts with the total size that is sufficient to meet the move_factor condition. If the total size of all parts is insufficient, all parts will be moved. prefer_not_to_merge — Disables merging of data parts on this volume. When this setting is enabled, merging data on this volume is not allowed. This allows controlling how ClickHouse works with slow disks. Cofiguration examples: &lt;storage_configuration&gt; ... &lt;policies&gt; &lt;hdd_in_order&gt; &lt;!-- policy name --&gt; &lt;volumes&gt; &lt;single&gt; &lt;!-- volume name --&gt; &lt;disk&gt;disk1&lt;/disk&gt; &lt;disk&gt;disk2&lt;/disk&gt; &lt;/single&gt; &lt;/volumes&gt; &lt;/hdd_in_order&gt; &lt;moving_from_ssd_to_hdd&gt; &lt;volumes&gt; &lt;hot&gt; &lt;disk&gt;fast_ssd&lt;/disk&gt; &lt;max_data_part_size_bytes&gt;1073741824&lt;/max_data_part_size_bytes&gt; &lt;/hot&gt; &lt;cold&gt; &lt;disk&gt;disk1&lt;/disk&gt; &lt;/cold&gt; &lt;/volumes&gt; &lt;move_factor&gt;0.2&lt;/move_factor&gt; &lt;/moving_from_ssd_to_hdd&gt; &lt;small_jbod_with_external_no_merges&gt; &lt;volumes&gt; &lt;main&gt; &lt;disk&gt;jbod1&lt;/disk&gt; &lt;/main&gt; &lt;external&gt; &lt;disk&gt;external&lt;/disk&gt; &lt;prefer_not_to_merge&gt;true&lt;/prefer_not_to_merge&gt; &lt;/external&gt; &lt;/volumes&gt; &lt;/small_jbod_with_external_no_merges&gt; &lt;/policies&gt; ... &lt;/storage_configuration&gt;  In given example, the hdd_in_order policy implements the round-robin approach. Thus this policy defines only one volume (single), the data parts are stored on all its disks in circular order. Such policy can be quite useful if there are several similar disks are mounted to the system, but RAID is not configured. Keep in mind that each individual disk drive is not reliable and you might want to compensate it with replication factor of 3 or more. If there are different kinds of disks available in the system, moving_from_ssd_to_hdd policy can be used instead. The volume hot consists of an SSD disk (fast_ssd), and the maximum size of a part that can be stored on this volume is 1GB. All the parts with the size larger than 1GB will be stored directly on the cold volume, which contains an HDD disk disk1. Also, once the disk fast_ssd gets filled by more than 80%, data will be transferred to the disk1 by a background process. The order of volume enumeration within a storage policy is important. Once a volume is overfilled, data are moved to the next one. The order of disk enumeration is important as well because data are stored on them in turns. When creating a table, one can apply one of the configured storage policies to it: CREATE TABLE table_with_non_default_policy ( EventDate Date, OrderID UInt64, BannerID UInt64, SearchPhrase String ) ENGINE = MergeTree ORDER BY (OrderID, BannerID) PARTITION BY toYYYYMM(EventDate) SETTINGS storage_policy = 'moving_from_ssd_to_hdd'  The default storage policy implies using only one volume, which consists of only one disk given in &lt;path&gt;. You could change storage policy after table creation with [ALTER TABLE ... MODIFY SETTING] query, new policy should include all old disks and volumes with same names. The number of threads performing background moves of data parts can be changed by background_move_pool_size setting. "},{"title":"Details​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#details","content":"In the case of MergeTree tables, data is getting to disk in different ways: As a result of an insert (INSERT query).During background merges and mutations.When downloading from another replica.As a result of partition freezing ALTER TABLE … FREEZE PARTITION. In all these cases except for mutations and partition freezing, a part is stored on a volume and a disk according to the given storage policy: The first volume (in the order of definition) that has enough disk space for storing a part (unreserved_space &gt; current_part_size) and allows for storing parts of a given size (max_data_part_size_bytes &gt; current_part_size) is chosen.Within this volume, that disk is chosen that follows the one, which was used for storing the previous chunk of data, and that has free space more than the part size (unreserved_space - keep_free_space_bytes &gt; current_part_size). Under the hood, mutations and partition freezing make use of hard links. Hard links between different disks are not supported, therefore in such cases the resulting parts are stored on the same disks as the initial ones. In the background, parts are moved between volumes on the basis of the amount of free space (move_factor parameter) according to the order the volumes are declared in the configuration file. Data is never transferred from the last one and into the first one. One may use system tables system.part_log (field type = MOVE_PART) and system.parts (fields path and disk) to monitor background moves. Also, the detailed information can be found in server logs. User can force moving a part or a partition from one volume to another using the query ALTER TABLE … MOVE PART|PARTITION … TO VOLUME|DISK …, all the restrictions for background operations are taken into account. The query initiates a move on its own and does not wait for background operations to be completed. User will get an error message if not enough free space is available or if any of the required conditions are not met. Moving data does not interfere with data replication. Therefore, different storage policies can be specified for the same table on different replicas. After the completion of background merges and mutations, old parts are removed only after a certain amount of time (old_parts_lifetime). During this time, they are not moved to other volumes or disks. Therefore, until the parts are finally removed, they are still taken into account for evaluation of the occupied disk space. User can assign new big parts to different disks of a JBOD volume in a balanced way using the min_bytes_to_rebalance_partition_over_jbod setting. "},{"title":"Using S3 for Data Storage​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#table_engine-mergetree-s3","content":"MergeTree family table engines can store data to S3 using a disk with type s3. This feature is under development and not ready for production. There are known drawbacks such as very low performance. Configuration markup: &lt;storage_configuration&gt; ... &lt;disks&gt; &lt;s3&gt; &lt;type&gt;s3&lt;/type&gt; &lt;endpoint&gt;https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/root-path/&lt;/endpoint&gt; &lt;access_key_id&gt;your_access_key_id&lt;/access_key_id&gt; &lt;secret_access_key&gt;your_secret_access_key&lt;/secret_access_key&gt; &lt;region&gt;&lt;/region&gt; &lt;server_side_encryption_customer_key_base64&gt;your_base64_encoded_customer_key&lt;/server_side_encryption_customer_key_base64&gt; &lt;proxy&gt; &lt;uri&gt;http://proxy1&lt;/uri&gt; &lt;uri&gt;http://proxy2&lt;/uri&gt; &lt;/proxy&gt; &lt;connect_timeout_ms&gt;10000&lt;/connect_timeout_ms&gt; &lt;request_timeout_ms&gt;5000&lt;/request_timeout_ms&gt; &lt;retry_attempts&gt;10&lt;/retry_attempts&gt; &lt;single_read_retries&gt;4&lt;/single_read_retries&gt; &lt;min_bytes_for_seek&gt;1000&lt;/min_bytes_for_seek&gt; &lt;metadata_path&gt;/var/lib/clickhouse/disks/s3/&lt;/metadata_path&gt; &lt;cache_enabled&gt;true&lt;/cache_enabled&gt; &lt;cache_path&gt;/var/lib/clickhouse/disks/s3/cache/&lt;/cache_path&gt; &lt;skip_access_check&gt;false&lt;/skip_access_check&gt; &lt;/s3&gt; &lt;/disks&gt; ... &lt;/storage_configuration&gt;  Required parameters: endpoint — S3 endpoint URL in path or virtual hosted styles. Endpoint URL should contain a bucket and root path to store data.access_key_id — S3 access key id.secret_access_key — S3 secret access key. Optional parameters: region — S3 region name.use_environment_credentials — Reads AWS credentials from the Environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN if they exist. Default value is false.use_insecure_imds_request — If set to true, S3 client will use insecure IMDS request while obtaining credentials from Amazon EC2 metadata. Default value is false.proxy — Proxy configuration for S3 endpoint. Each uri element inside proxy block should contain a proxy URL.connect_timeout_ms — Socket connect timeout in milliseconds. Default value is 10 seconds.request_timeout_ms — Request timeout in milliseconds. Default value is 5 seconds.retry_attempts — Number of retry attempts in case of failed request. Default value is 10.single_read_retries — Number of retry attempts in case of connection drop during read. Default value is 4.min_bytes_for_seek — Minimal number of bytes to use seek operation instead of sequential read. Default value is 1 Mb.metadata_path — Path on local FS to store metadata files for S3. Default value is /var/lib/clickhouse/disks/&lt;disk_name&gt;/.cache_enabled — Allows to cache mark and index files on local FS. Default value is true.cache_path — Path on local FS where to store cached mark and index files. Default value is /var/lib/clickhouse/disks/&lt;disk_name&gt;/cache/.skip_access_check — If true, disk access checks will not be performed on disk start-up. Default value is false.server_side_encryption_customer_key_base64 — If specified, required headers for accessing S3 objects with SSE-C encryption will be set. S3 disk can be configured as main or cold storage: &lt;storage_configuration&gt; ... &lt;disks&gt; &lt;s3&gt; &lt;type&gt;s3&lt;/type&gt; &lt;endpoint&gt;https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/root-path/&lt;/endpoint&gt; &lt;access_key_id&gt;your_access_key_id&lt;/access_key_id&gt; &lt;secret_access_key&gt;your_secret_access_key&lt;/secret_access_key&gt; &lt;/s3&gt; &lt;/disks&gt; &lt;policies&gt; &lt;s3_main&gt; &lt;volumes&gt; &lt;main&gt; &lt;disk&gt;s3&lt;/disk&gt; &lt;/main&gt; &lt;/volumes&gt; &lt;/s3_main&gt; &lt;s3_cold&gt; &lt;volumes&gt; &lt;main&gt; &lt;disk&gt;default&lt;/disk&gt; &lt;/main&gt; &lt;external&gt; &lt;disk&gt;s3&lt;/disk&gt; &lt;/external&gt; &lt;/volumes&gt; &lt;move_factor&gt;0.2&lt;/move_factor&gt; &lt;/s3_cold&gt; &lt;/policies&gt; ... &lt;/storage_configuration&gt;  In case of cold option a data can be moved to S3 if local disk free size will be smaller than move_factor * disk_size or by TTL move rule. "},{"title":"Using Azure Blob Storage for Data Storage​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#table_engine-mergetree-azure-blob-storage","content":"MergeTree family table engines can store data to Azure Blob Storage using a disk with type azure_blob_storage. As of February 2022, this feature is still a fresh addition, so expect that some Azure Blob Storage functionalities might be unimplemented. Configuration markup: &lt;storage_configuration&gt; ... &lt;disks&gt; &lt;blob_storage_disk&gt; &lt;type&gt;azure_blob_storage&lt;/type&gt; &lt;storage_account_url&gt;http://account.blob.core.windows.net&lt;/storage_account_url&gt; &lt;container_name&gt;container&lt;/container_name&gt; &lt;account_name&gt;account&lt;/account_name&gt; &lt;account_key&gt;pass123&lt;/account_key&gt; &lt;metadata_path&gt;/var/lib/clickhouse/disks/blob_storage_disk/&lt;/metadata_path&gt; &lt;cache_enabled&gt;true&lt;/cache_enabled&gt; &lt;cache_path&gt;/var/lib/clickhouse/disks/blob_storage_disk/cache/&lt;/cache_path&gt; &lt;skip_access_check&gt;false&lt;/skip_access_check&gt; &lt;/blob_storage_disk&gt; &lt;/disks&gt; ... &lt;/storage_configuration&gt;  Connection parameters: storage_account_url - Required, Azure Blob Storage account URL, like http://account.blob.core.windows.net or http://azurite1:10000/devstoreaccount1.container_name - Target container name, defaults to default-container.container_already_exists - If set to false, a new container container_name is created in the storage account, if set to true, disk connects to the container directly, and if left unset, disk connects to the account, checks if the container container_name exists, and creates it if it doesn't exist yet. Authentication parameters (the disk will try all available methods and Managed Identity Credential): connection_string - For authentication using a connection string.account_name and account_key - For authentication using Shared Key. Limit parameters (mainly for internal usage): max_single_part_upload_size - Limits the size of a single block upload to Blob Storage.min_bytes_for_seek - Limits the size of a seekable region.max_single_read_retries - Limits the number of attempts to read a chunk of data from Blob Storage.max_single_download_retries - Limits the number of attempts to download a readable buffer from Blob Storage.thread_pool_size - Limits the number of threads with which IDiskRemote is instantiated. Other parameters: metadata_path - Path on local FS to store metadata files for Blob Storage. Default value is /var/lib/clickhouse/disks/&lt;disk_name&gt;/.cache_enabled - Allows to cache mark and index files on local FS. Default value is true.cache_path - Path on local FS where to store cached mark and index files. Default value is /var/lib/clickhouse/disks/&lt;disk_name&gt;/cache/.skip_access_check - If true, disk access checks will not be performed on disk start-up. Default value is false. Examples of working configurations can be found in integration tests directory (see e.g. test_merge_tree_azure_blob_storage or test_azure_blob_storage_zero_copy_replication). "},{"title":"Virtual Columns​","type":1,"pageTitle":"MergeTree","url":"docs/en/engines/table-engines/mergetree-family/mergetree#virtual-columns","content":"_part — Name of a part._part_index — Sequential index of the part in the query result._partition_id — Name of a partition._part_uuid — Unique part identifier (if enabled MergeTree setting assign_part_uuids)._partition_value — Values (a tuple) of a partition by expression._sample_factor — Sample factor (from the query). "},{"title":"Set Table Engine","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/special/set","content":"","keywords":""},{"title":"Limitations and Settings​","type":1,"pageTitle":"Set Table Engine","url":"docs/en/engines/table-engines/special/set#join-limitations-and-settings","content":"When creating a table, the following settings are applied: persistent Original article "},{"title":"MaterializedView Table Engine","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/special/materializedview","content":"MaterializedView Table Engine Used for implementing materialized views (for more information, see CREATE VIEW). For storing data, it uses a different engine that was specified when creating the view. When reading from a table, it just uses that engine. Original article","keywords":""},{"title":"Null Table Engine","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/special/null","content":"Null Table Engine When writing to a Null table, data is ignored. When reading from a Null table, the response is empty. note If you are wondering why this is useful, note that you can create a materialized view on a Null table. So the data written to the table will end up affecting the view, but original raw data will still be discarded. Original article","keywords":""},{"title":"GenerateRandom Table Engine","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/special/generate","content":"","keywords":""},{"title":"Usage in ClickHouse Server​","type":1,"pageTitle":"GenerateRandom Table Engine","url":"docs/en/engines/table-engines/special/generate#usage-in-clickhouse-server","content":"ENGINE = GenerateRandom(random_seed, max_string_length, max_array_length)  The max_array_length and max_string_length parameters specify maximum length of all array columns and strings correspondingly in generated data. Generate table engine supports only SELECT queries. It supports all DataTypes that can be stored in a table except LowCardinality and AggregateFunction. "},{"title":"Example​","type":1,"pageTitle":"GenerateRandom Table Engine","url":"docs/en/engines/table-engines/special/generate#example","content":"1. Set up the generate_engine_table table: CREATE TABLE generate_engine_table (name String, value UInt32) ENGINE = GenerateRandom(1, 5, 3)  2. Query the data: SELECT * FROM generate_engine_table LIMIT 3  ┌─name─┬──────value─┐ │ c4xJ │ 1412771199 │ │ r │ 1791099446 │ │ 7#$ │ 124312908 │ └──────┴────────────┘  "},{"title":"Details of Implementation​","type":1,"pageTitle":"GenerateRandom Table Engine","url":"docs/en/engines/table-engines/special/generate#details-of-implementation","content":"Not supported: ALTERSELECT ... SAMPLEINSERTIndicesReplication Original article "},{"title":"View Table Engine","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/special/view","content":"View Table Engine Used for implementing views (for more information, see the CREATE VIEW query). It does not store data, but only stores the specified SELECT query. When reading from a table, it runs this query (and deletes all unnecessary columns from the query). Original article","keywords":""},{"title":"URL Table Engine","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/special/url","content":"","keywords":""},{"title":"Usage​","type":1,"pageTitle":"URL Table Engine","url":"docs/en/engines/table-engines/special/url#using-the-engine-in-the-clickhouse-server","content":"INSERT and SELECT queries are transformed to POST and GET requests, respectively. For processing POST requests, the remote server must supportChunked transfer encoding. You can limit the maximum number of HTTP GET redirect hops using the max_http_get_redirects setting. "},{"title":"Example​","type":1,"pageTitle":"URL Table Engine","url":"docs/en/engines/table-engines/special/url#example","content":"1. Create a url_engine_table table on the server : CREATE TABLE url_engine_table (word String, value UInt64) ENGINE=URL('http://127.0.0.1:12345/', CSV)  2. Create a basic HTTP server using the standard Python 3 tools and start it: from http.server import BaseHTTPRequestHandler, HTTPServer class CSVHTTPServer(BaseHTTPRequestHandler): def do_GET(self): self.send_response(200) self.send_header('Content-type', 'text/csv') self.end_headers() self.wfile.write(bytes('Hello,1\\nWorld,2\\n', &quot;utf-8&quot;)) if __name__ == &quot;__main__&quot;: server_address = ('127.0.0.1', 12345) HTTPServer(server_address, CSVHTTPServer).serve_forever()  $ python3 server.py  3. Request data: SELECT * FROM url_engine_table  ┌─word──┬─value─┐ │ Hello │ 1 │ │ World │ 2 │ └───────┴───────┘  "},{"title":"Details of Implementation​","type":1,"pageTitle":"URL Table Engine","url":"docs/en/engines/table-engines/special/url#details-of-implementation","content":"Reads and writes can be parallelNot supported: ALTER and SELECT...SAMPLE operations.Indexes.Replication. Original article "},{"title":"Cell Towers","type":0,"sectionRef":"#","url":"docs/en/example-datasets/cell-towers","content":"","keywords":""},{"title":"Get the Dataset​","type":1,"pageTitle":"Cell Towers","url":"docs/en/example-datasets/cell-towers#get-the-dataset","content":"Download the snapshot of the dataset from February 2021: [https://datasets.clickhouse.com/cell_towers.csv.xz] (729 MB). Validate the integrity (optional step): md5sum cell_towers.csv.xz 8cf986f4a0d9f12c6f384a0e9192c908 cell_towers.csv.xz  Decompress it with the following command: xz -d cell_towers.csv.xz  Create a table: CREATE TABLE cell_towers ( radio Enum8('' = 0, 'CDMA' = 1, 'GSM' = 2, 'LTE' = 3, 'NR' = 4, 'UMTS' = 5), mcc UInt16, net UInt16, area UInt16, cell UInt64, unit Int16, lon Float64, lat Float64, range UInt32, samples UInt32, changeable UInt8, created DateTime, updated DateTime, averageSignal UInt8 ) ENGINE = MergeTree ORDER BY (radio, mcc, net, created);  Insert the dataset: clickhouse-client --query &quot;INSERT INTO cell_towers FORMAT CSVWithNames&quot; &lt; cell_towers.csv  "},{"title":"Examples​","type":1,"pageTitle":"Cell Towers","url":"docs/en/example-datasets/cell-towers#examples","content":"A number of cell towers by type: SELECT radio, count() AS c FROM cell_towers GROUP BY radio ORDER BY c DESC ┌─radio─┬────────c─┐ │ UMTS │ 20686487 │ │ LTE │ 12101148 │ │ GSM │ 9931312 │ │ CDMA │ 556344 │ │ NR │ 867 │ └───────┴──────────┘ 5 rows in set. Elapsed: 0.011 sec. Processed 43.28 million rows, 43.28 MB (3.83 billion rows/s., 3.83 GB/s.)  Cell towers by mobile country code (MCC): SELECT mcc, count() FROM cell_towers GROUP BY mcc ORDER BY count() DESC LIMIT 10 ┌─mcc─┬─count()─┐ │ 310 │ 5024650 │ │ 262 │ 2622423 │ │ 250 │ 1953176 │ │ 208 │ 1891187 │ │ 724 │ 1836150 │ │ 404 │ 1729151 │ │ 234 │ 1618924 │ │ 510 │ 1353998 │ │ 440 │ 1343355 │ │ 311 │ 1332798 │ └─────┴─────────┘ 10 rows in set. Elapsed: 0.019 sec. Processed 43.28 million rows, 86.55 MB (2.33 billion rows/s., 4.65 GB/s.)  So, the top countries are: the USA, Germany, and Russia. You may want to create an External Dictionary in ClickHouse to decode these values. "},{"title":"Use case​","type":1,"pageTitle":"Cell Towers","url":"docs/en/example-datasets/cell-towers#use-case","content":"Using pointInPolygon function. Create a table where we will store polygons: CREATE TEMPORARY TABLE moscow (polygon Array(Tuple(Float64, Float64)));  This is a rough shape of Moscow (without &quot;new Moscow&quot;): INSERT INTO moscow VALUES ([(37.84172564285271, 55.78000432402266), (37.8381207618713, 55.775874525970494), (37.83979446823122, 55.775626746008065), (37.84243326983639, 55.77446586811748), (37.84262672750849, 55.771974101091104), (37.84153238623039, 55.77114545193181), (37.841124690460184, 55.76722010265554), (37.84239076983644, 55.76654891107098), (37.842283558197025, 55.76258709833121), (37.8421759312134, 55.758073999993734), (37.84198330422974, 55.75381499999371), (37.8416827275085, 55.749277102484484), (37.84157576190186, 55.74794544108413), (37.83897929098507, 55.74525257875241), (37.83739676451868, 55.74404373042019), (37.838732481460525, 55.74298009816793), (37.841183997352545, 55.743060321833575), (37.84097476190185, 55.73938799999373), (37.84048155819702, 55.73570799999372), (37.840095812164286, 55.73228210777237), (37.83983814285274, 55.73080491981639), (37.83846476321406, 55.729799917464675), (37.83835745269769, 55.72919751082619), (37.838636380279524, 55.72859509486539), (37.8395161005249, 55.727705075632784), (37.83897964285276, 55.722727886185154), (37.83862557539366, 55.72034817326636), (37.83559735744853, 55.71944437307499), (37.835370708803126, 55.71831419154461), (37.83738169402022, 55.71765218986692), (37.83823396494291, 55.71691750159089), (37.838056931213345, 55.71547311301385), (37.836812846557606, 55.71221445615604), (37.83522525396725, 55.709331054395555), (37.83269301586908, 55.70953687463627), (37.829667367706236, 55.70903403789297), (37.83311126588435, 55.70552351822608), (37.83058993121339, 55.70041317726053), (37.82983872750851, 55.69883771404813), (37.82934501586913, 55.69718947487017), (37.828926414016685, 55.69504441658371), (37.82876530422971, 55.69287499999378), (37.82894754100031, 55.690759754047335), (37.827697554878185, 55.68951421135665), (37.82447346292115, 55.68965045405069), (37.83136543914793, 55.68322046195302), (37.833554015869154, 55.67814012759211), (37.83544184655761, 55.67295011628339), (37.837480388885474, 55.6672498719639), (37.838960677246064, 55.66316274139358), (37.83926093121332, 55.66046999999383), (37.839025050262435, 55.65869897264431), (37.83670784390257, 55.65794084879904), (37.835656529083245, 55.65694309303843), (37.83704060449217, 55.65689306460552), (37.83696819873806, 55.65550363526252), (37.83760389616388, 55.65487847246661), (37.83687972750851, 55.65356745541324), (37.83515216004943, 55.65155951234079), (37.83312418518067, 55.64979413590619), (37.82801726983639, 55.64640836412121), (37.820614174591, 55.64164525405531), (37.818908190475426, 55.6421883258084), (37.81717543386075, 55.64112490388471), (37.81690987037274, 55.63916106913107), (37.815099354492155, 55.637925371757085), (37.808769150787356, 55.633798276884455), (37.80100123544311, 55.62873670012244), (37.79598013491824, 55.62554336109055), (37.78634567724606, 55.62033499605651), (37.78334147619623, 55.618768681480326), (37.77746201055901, 55.619855533402706), (37.77527329626457, 55.61909966711279), (37.77801986242668, 55.618770300976294), (37.778212973541216, 55.617257701952106), (37.77784818518065, 55.61574504433011), (37.77016867724609, 55.61148576294007), (37.760191219573976, 55.60599579539028), (37.75338926983641, 55.60227892751446), (37.746329965606634, 55.59920577639331), (37.73939925396728, 55.59631430313617), (37.73273665739439, 55.5935318803559), (37.7299954450912, 55.59350760316188), (37.7268679946899, 55.59469840523759), (37.72626726983634, 55.59229549697373), (37.7262673598022, 55.59081598950582), (37.71897193121335, 55.5877595845419), (37.70871550793456, 55.58393177431724), (37.700497489410374, 55.580917323756644), (37.69204305026244, 55.57778089778455), (37.68544477378839, 55.57815154690915), (37.68391050793454, 55.57472945079756), (37.678803592590306, 55.57328235936491), (37.6743402539673, 55.57255251445782), (37.66813862698363, 55.57216388774464), (37.617927457672096, 55.57505691895805), (37.60443099999999, 55.5757737568051), (37.599683515869145, 55.57749105910326), (37.59754177842709, 55.57796291823627), (37.59625834786988, 55.57906686095235), (37.59501783265684, 55.57746616444403), (37.593090671936025, 55.57671634534502), (37.587018007904, 55.577944600233785), (37.578692203704804, 55.57982895000019), (37.57327546607398, 55.58116294118248), (37.57385012109279, 55.581550362779), (37.57399562266922, 55.5820107079112), (37.5735356072979, 55.58226289171689), (37.57290393054962, 55.582393529795155), (37.57037722355653, 55.581919415056234), (37.5592298306885, 55.584471614867844), (37.54189249206543, 55.58867650795186), (37.5297256269836, 55.59158133551745), (37.517837865081766, 55.59443656218868), (37.51200186508174, 55.59635625174229), (37.506808949737554, 55.59907823904434), (37.49820432275389, 55.6062944994944), (37.494406071441674, 55.60967103463367), (37.494760001358024, 55.61066689753365), (37.49397137107085, 55.61220931698269), (37.49016528606031, 55.613417718449064), (37.48773249206542, 55.61530616333343), (37.47921386508177, 55.622640129112334), (37.470652153442394, 55.62993723476164), (37.46273446298218, 55.6368075123157), (37.46350692265317, 55.64068225239439), (37.46050283203121, 55.640794546982576), (37.457627470916734, 55.64118904154646), (37.450718034393326, 55.64690488145138), (37.44239252645875, 55.65397824729769), (37.434587576721185, 55.66053543155961), (37.43582144975277, 55.661693766520735), (37.43576786245721, 55.662755031737014), (37.430982915344174, 55.664610641628116), (37.428547447097685, 55.66778515273695), (37.42945134592044, 55.668633314343566), (37.42859571562949, 55.66948145750025), (37.4262836402282, 55.670813882451405), (37.418709037048295, 55.6811141674414), (37.41922139651101, 55.68235377885389), (37.419218771842885, 55.68359335082235), (37.417196501327446, 55.684375235224735), (37.41607020370478, 55.68540557585352), (37.415640857147146, 55.68686637150793), (37.414632153442334, 55.68903015131686), (37.413344899475064, 55.690896881757396), (37.41171432275391, 55.69264232162232), (37.40948282275393, 55.69455101638112), (37.40703674603271, 55.69638690385348), (37.39607169577025, 55.70451821283731), (37.38952706878662, 55.70942491932811), (37.387778313491815, 55.71149057784176), (37.39049275399779, 55.71419814298992), (37.385557272491454, 55.7155489617061), (37.38388335714726, 55.71849856042102), (37.378368238098155, 55.7292763261685), (37.37763597123337, 55.730845879211614), (37.37890062088197, 55.73167906388319), (37.37750451918789, 55.734703664681774), (37.375610832015965, 55.734851959522246), (37.3723813571472, 55.74105626086403), (37.37014935714723, 55.746115620904355), (37.36944173016362, 55.750883999993725), (37.36975304365541, 55.76335905525834), (37.37244070571134, 55.76432079697595), (37.3724259757175, 55.76636979670426), (37.369922155757884, 55.76735417953104), (37.369892695770275, 55.76823419316575), (37.370214730163575, 55.782312184391266), (37.370493611114505, 55.78436801120489), (37.37120164550783, 55.78596427165359), (37.37284851456452, 55.7874378183096), (37.37608325135799, 55.7886695054807), (37.3764587460632, 55.78947647305964), (37.37530000265506, 55.79146512926804), (37.38235915344241, 55.79899647809345), (37.384344043655396, 55.80113596939471), (37.38594269577028, 55.80322699999366), (37.38711208598329, 55.804919036911976), (37.3880239841309, 55.806610999993666), (37.38928977249147, 55.81001864976979), (37.39038389947512, 55.81348641242801), (37.39235781481933, 55.81983538336746), (37.393709457672124, 55.82417822811877), (37.394685720901464, 55.82792275755836), (37.39557615344238, 55.830447148154136), (37.39844478226658, 55.83167107969975), (37.40019761214057, 55.83151823557964), (37.400398790382326, 55.83264967594742), (37.39659544313046, 55.83322180909622), (37.39667059524539, 55.83402792148566), (37.39682089947515, 55.83638877400216), (37.39643489154053, 55.83861656112751), (37.3955338994751, 55.84072348043264), (37.392680272491454, 55.84502158126453), (37.39241188227847, 55.84659117913199), (37.392529730163616, 55.84816071336481), (37.39486835714723, 55.85288092980303), (37.39873052645878, 55.859893456073635), (37.40272161111449, 55.86441833633205), (37.40697072750854, 55.867579567544375), (37.410007082016016, 55.868369880337), (37.4120992989502, 55.86920843741314), (37.412668021163924, 55.87055369615854), (37.41482461111453, 55.87170587948249), (37.41862266137694, 55.873183961039565), (37.42413732540892, 55.874879126654704), (37.4312182698669, 55.875614937236705), (37.43111093783558, 55.8762723478417), (37.43332105622856, 55.87706546369396), (37.43385747619623, 55.87790681284802), (37.441303050262405, 55.88027084462084), (37.44747234260555, 55.87942070143253), (37.44716141796871, 55.88072960917233), (37.44769797085568, 55.88121221323979), (37.45204320500181, 55.882080694420715), (37.45673176190186, 55.882346110794586), (37.463383999999984, 55.88252729504517), (37.46682797486874, 55.88294937719063), (37.470014457672086, 55.88361266759345), (37.47751410450743, 55.88546991372396), (37.47860317658232, 55.88534929207307), (37.48165826025772, 55.882563306475106), (37.48316434442331, 55.8815803226785), (37.483831555817645, 55.882427612793315), (37.483182967125686, 55.88372791409729), (37.483092277908824, 55.88495581062434), (37.4855716508179, 55.8875561994203), (37.486440636245746, 55.887827444039566), (37.49014203439328, 55.88897899871799), (37.493210285705544, 55.890208937135604), (37.497512451065035, 55.891342397444696), (37.49780744510645, 55.89174030252967), (37.49940333499519, 55.89239745507079), (37.50018383334346, 55.89339220941865), (37.52421672750851, 55.903869074155224), (37.52977457672118, 55.90564076517974), (37.53503220370484, 55.90661661218259), (37.54042858064267, 55.90714113744566), (37.54320461007303, 55.905645048442985), (37.545686966066306, 55.906608607018505), (37.54743976120755, 55.90788552162358), (37.55796999999999, 55.90901557907218), (37.572711542327866, 55.91059395704873), (37.57942799999998, 55.91073854155573), (37.58502865872187, 55.91009969268444), (37.58739968913264, 55.90794809960554), (37.59131567193598, 55.908713267595054), (37.612687423278814, 55.902866854295375), (37.62348079629517, 55.90041967242986), (37.635797880950896, 55.898141151686396), (37.649487626983664, 55.89639275532968), (37.65619302513125, 55.89572360207488), (37.66294133862307, 55.895295577183965), (37.66874564418033, 55.89505457604897), (37.67375601586915, 55.89254677027454), (37.67744661901856, 55.8947775867987), (37.688347, 55.89450045676125), (37.69480554232789, 55.89422926332761), (37.70107096560668, 55.89322256101114), (37.705962965606716, 55.891763491662616), (37.711885134918205, 55.889110234998974), (37.71682005026245, 55.886577568759876), (37.7199315476074, 55.88458159806678), (37.72234560316464, 55.882281005794134), (37.72364385977171, 55.8809452036196), (37.725371142837474, 55.8809722706006), (37.727870902099546, 55.88037213862385), (37.73394330422971, 55.877941504088696), (37.745339592590376, 55.87208120378722), (37.75525267724611, 55.86703807949492), (37.76919976190188, 55.859821640197474), (37.827835219574, 55.82962968399116), (37.83341438888553, 55.82575289922351), (37.83652584655761, 55.82188784027888), (37.83809213491821, 55.81612575504693), (37.83605359521481, 55.81460347077685), (37.83632178569025, 55.81276696067908), (37.838623105812026, 55.811486181656385), (37.83912198147584, 55.807329380532785), (37.839079078033414, 55.80510270463816), (37.83965844708251, 55.79940712529036), (37.840581150787344, 55.79131399999368), (37.84172564285271, 55.78000432402266)]);  Check how many cell towers are in Moscow: SELECT count() FROM cell_towers WHERE pointInPolygon((lon, lat), (SELECT * FROM moscow)) ┌─count()─┐ │ 310463 │ └─────────┘ 1 rows in set. Elapsed: 0.067 sec. Processed 43.28 million rows, 692.42 MB (645.83 million rows/s., 10.33 GB/s.)  The data is also available for interactive queries in the Playground, example. Although you cannot create temporary tables there. "},{"title":"Merge Table Engine","type":0,"sectionRef":"#","url":"docs/en/engines/table-engines/special/merge","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"Merge Table Engine","url":"docs/en/engines/table-engines/special/merge#creating-a-table","content":"CREATE TABLE ... Engine=Merge(db_name, tables_regexp)  Engine Parameters db_name — Possible values: database name, constant expression that returns a string with a database name, for example, currentDatabase(),REGEXP(expression), where expression is a regular expression to match the DB names. tables_regexp — A regular expression to match the table names in the specified DB or DBs. Regular expressions — re2 (supports a subset of PCRE), case-sensitive. See the notes about escaping symbols in regular expressions in the &quot;match&quot; section. "},{"title":"Usage​","type":1,"pageTitle":"Merge Table Engine","url":"docs/en/engines/table-engines/special/merge#usage","content":"When selecting tables to read, the Merge table itself is not selected, even if it matches the regex. This is to avoid loops. It is possible to create two Merge tables that will endlessly try to read each others' data, but this is not a good idea. The typical way to use the Merge engine is for working with a large number of TinyLog tables as if with a single table. "},{"title":"Examples​","type":1,"pageTitle":"Merge Table Engine","url":"docs/en/engines/table-engines/special/merge#examples","content":"Example 1 Consider two databases ABC_corporate_site and ABC_store. The all_visitors table will contain IDs from the tables visitors in both databases. CREATE TABLE all_visitors (id UInt32) ENGINE=Merge(REGEXP('ABC_*'), 'visitors');  Example 2 Let's say you have an old table WatchLog_old and decided to change partitioning without moving data to a new table WatchLog_new, and you need to see data from both tables. CREATE TABLE WatchLog_old(date Date, UserId Int64, EventType String, Cnt UInt64) ENGINE=MergeTree(date, (UserId, EventType), 8192); INSERT INTO WatchLog_old VALUES ('2018-01-01', 1, 'hit', 3); CREATE TABLE WatchLog_new(date Date, UserId Int64, EventType String, Cnt UInt64) ENGINE=MergeTree PARTITION BY date ORDER BY (UserId, EventType) SETTINGS index_granularity=8192; INSERT INTO WatchLog_new VALUES ('2018-01-02', 2, 'hit', 3); CREATE TABLE WatchLog as WatchLog_old ENGINE=Merge(currentDatabase(), '^WatchLog'); SELECT * FROM WatchLog;  ┌───────date─┬─UserId─┬─EventType─┬─Cnt─┐ │ 2018-01-01 │ 1 │ hit │ 3 │ └────────────┴────────┴───────────┴─────┘ ┌───────date─┬─UserId─┬─EventType─┬─Cnt─┐ │ 2018-01-02 │ 2 │ hit │ 3 │ └────────────┴────────┴───────────┴─────┘  "},{"title":"Virtual Columns​","type":1,"pageTitle":"Merge Table Engine","url":"docs/en/engines/table-engines/special/merge#virtual-columns","content":"_table — Contains the name of the table from which data was read. Type: String. You can set the constant conditions on _table in the WHERE/PREWHERE clause (for example, WHERE _table='xyz'). In this case the read operation is performed only for that tables where the condition on _table is satisfied, so the _table column acts as an index. See Also Virtual columnsmerge table function Original article "},{"title":"Anonymized Web Analytics Data","type":0,"sectionRef":"#","url":"docs/en/example-datasets/metrica","content":"","keywords":""},{"title":"Obtaining Tables from Prepared Partitions​","type":1,"pageTitle":"Anonymized Web Analytics Data","url":"docs/en/example-datasets/metrica#obtaining-tables-from-prepared-partitions","content":"Download and import hits table: curl -O https://datasets.clickhouse.com/hits/partitions/hits_v1.tar tar xvf hits_v1.tar -C /var/lib/clickhouse # path to ClickHouse data directory # check permissions on unpacked data, fix if required sudo service clickhouse-server restart clickhouse-client --query &quot;SELECT COUNT(*) FROM datasets.hits_v1&quot;  Download and import visits: curl -O https://datasets.clickhouse.com/visits/partitions/visits_v1.tar tar xvf visits_v1.tar -C /var/lib/clickhouse # path to ClickHouse data directory # check permissions on unpacked data, fix if required sudo service clickhouse-server restart clickhouse-client --query &quot;SELECT COUNT(*) FROM datasets.visits_v1&quot;  "},{"title":"Obtaining Tables from Compressed TSV File​","type":1,"pageTitle":"Anonymized Web Analytics Data","url":"docs/en/example-datasets/metrica#obtaining-tables-from-compressed-tsv-file","content":"Download and import hits from compressed TSV file: curl https://datasets.clickhouse.com/hits/tsv/hits_v1.tsv.xz | unxz --threads=`nproc` &gt; hits_v1.tsv # Validate the checksum md5sum hits_v1.tsv # Checksum should be equal to: f3631b6295bf06989c1437491f7592cb # now create table clickhouse-client --query &quot;CREATE DATABASE IF NOT EXISTS datasets&quot; # for hits_v1 clickhouse-client --query &quot;CREATE TABLE datasets.hits_v1 ( WatchID UInt64, JavaEnable UInt8, Title String, GoodEvent Int16, EventTime DateTime, EventDate Date, CounterID UInt32, ClientIP UInt32, ClientIP6 FixedString(16), RegionID UInt32, UserID UInt64, CounterClass Int8, OS UInt8, UserAgent UInt8, URL String, Referer String, URLDomain String, RefererDomain String, Refresh UInt8, IsRobot UInt8, RefererCategories Array(UInt16), URLCategories Array(UInt16), URLRegions Array(UInt32), RefererRegions Array(UInt32), ResolutionWidth UInt16, ResolutionHeight UInt16, ResolutionDepth UInt8, FlashMajor UInt8, FlashMinor UInt8, FlashMinor2 String, NetMajor UInt8, NetMinor UInt8, UserAgentMajor UInt16, UserAgentMinor FixedString(2), CookieEnable UInt8, JavascriptEnable UInt8, IsMobile UInt8, MobilePhone UInt8, MobilePhoneModel String, Params String, IPNetworkID UInt32, TraficSourceID Int8, SearchEngineID UInt16, SearchPhrase String, AdvEngineID UInt8, IsArtifical UInt8, WindowClientWidth UInt16, WindowClientHeight UInt16, ClientTimeZone Int16, ClientEventTime DateTime, SilverlightVersion1 UInt8, SilverlightVersion2 UInt8, SilverlightVersion3 UInt32, SilverlightVersion4 UInt16, PageCharset String, CodeVersion UInt32, IsLink UInt8, IsDownload UInt8, IsNotBounce UInt8, FUniqID UInt64, HID UInt32, IsOldCounter UInt8, IsEvent UInt8, IsParameter UInt8, DontCountHits UInt8, WithHash UInt8, HitColor FixedString(1), UTCEventTime DateTime, Age UInt8, Sex UInt8, Income UInt8, Interests UInt16, Robotness UInt8, GeneralInterests Array(UInt16), RemoteIP UInt32, RemoteIP6 FixedString(16), WindowName Int32, OpenerName Int32, HistoryLength Int16, BrowserLanguage FixedString(2), BrowserCountry FixedString(2), SocialNetwork String, SocialAction String, HTTPError UInt16, SendTiming Int32, DNSTiming Int32, ConnectTiming Int32, ResponseStartTiming Int32, ResponseEndTiming Int32, FetchTiming Int32, RedirectTiming Int32, DOMInteractiveTiming Int32, DOMContentLoadedTiming Int32, DOMCompleteTiming Int32, LoadEventStartTiming Int32, LoadEventEndTiming Int32, NSToDOMContentLoadedTiming Int32, FirstPaintTiming Int32, RedirectCount Int8, SocialSourceNetworkID UInt8, SocialSourcePage String, ParamPrice Int64, ParamOrderID String, ParamCurrency FixedString(3), ParamCurrencyID UInt16, GoalsReached Array(UInt32), OpenstatServiceName String, OpenstatCampaignID String, OpenstatAdID String, OpenstatSourceID String, UTMSource String, UTMMedium String, UTMCampaign String, UTMContent String, UTMTerm String, FromTag String, HasGCLID UInt8, RefererHash UInt64, URLHash UInt64, CLID UInt32, YCLID UInt64, ShareService String, ShareURL String, ShareTitle String, ParsedParams Nested(Key1 String, Key2 String, Key3 String, Key4 String, Key5 String, ValueDouble Float64), IslandID FixedString(16), RequestNum UInt32, RequestTry UInt8) ENGINE = MergeTree() PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate, intHash32(UserID)) SAMPLE BY intHash32(UserID) SETTINGS index_granularity = 8192&quot; # for hits_100m_obfuscated clickhouse-client --query=&quot;CREATE TABLE hits_100m_obfuscated (WatchID UInt64, JavaEnable UInt8, Title String, GoodEvent Int16, EventTime DateTime, EventDate Date, CounterID UInt32, ClientIP UInt32, RegionID UInt32, UserID UInt64, CounterClass Int8, OS UInt8, UserAgent UInt8, URL String, Referer String, Refresh UInt8, RefererCategoryID UInt16, RefererRegionID UInt32, URLCategoryID UInt16, URLRegionID UInt32, ResolutionWidth UInt16, ResolutionHeight UInt16, ResolutionDepth UInt8, FlashMajor UInt8, FlashMinor UInt8, FlashMinor2 String, NetMajor UInt8, NetMinor UInt8, UserAgentMajor UInt16, UserAgentMinor FixedString(2), CookieEnable UInt8, JavascriptEnable UInt8, IsMobile UInt8, MobilePhone UInt8, MobilePhoneModel String, Params String, IPNetworkID UInt32, TraficSourceID Int8, SearchEngineID UInt16, SearchPhrase String, AdvEngineID UInt8, IsArtifical UInt8, WindowClientWidth UInt16, WindowClientHeight UInt16, ClientTimeZone Int16, ClientEventTime DateTime, SilverlightVersion1 UInt8, SilverlightVersion2 UInt8, SilverlightVersion3 UInt32, SilverlightVersion4 UInt16, PageCharset String, CodeVersion UInt32, IsLink UInt8, IsDownload UInt8, IsNotBounce UInt8, FUniqID UInt64, OriginalURL String, HID UInt32, IsOldCounter UInt8, IsEvent UInt8, IsParameter UInt8, DontCountHits UInt8, WithHash UInt8, HitColor FixedString(1), LocalEventTime DateTime, Age UInt8, Sex UInt8, Income UInt8, Interests UInt16, Robotness UInt8, RemoteIP UInt32, WindowName Int32, OpenerName Int32, HistoryLength Int16, BrowserLanguage FixedString(2), BrowserCountry FixedString(2), SocialNetwork String, SocialAction String, HTTPError UInt16, SendTiming UInt32, DNSTiming UInt32, ConnectTiming UInt32, ResponseStartTiming UInt32, ResponseEndTiming UInt32, FetchTiming UInt32, SocialSourceNetworkID UInt8, SocialSourcePage String, ParamPrice Int64, ParamOrderID String, ParamCurrency FixedString(3), ParamCurrencyID UInt16, OpenstatServiceName String, OpenstatCampaignID String, OpenstatAdID String, OpenstatSourceID String, UTMSource String, UTMMedium String, UTMCampaign String, UTMContent String, UTMTerm String, FromTag String, HasGCLID UInt8, RefererHash UInt64, URLHash UInt64, CLID UInt32) ENGINE = MergeTree() PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate, intHash32(UserID)) SAMPLE BY intHash32(UserID) SETTINGS index_granularity = 8192&quot; # import data cat hits_v1.tsv | clickhouse-client --query &quot;INSERT INTO datasets.hits_v1 FORMAT TSV&quot; --max_insert_block_size=100000 # optionally you can optimize table clickhouse-client --query &quot;OPTIMIZE TABLE datasets.hits_v1 FINAL&quot; clickhouse-client --query &quot;SELECT COUNT(*) FROM datasets.hits_v1&quot;  Download and import visits from compressed tsv-file: curl https://datasets.clickhouse.com/visits/tsv/visits_v1.tsv.xz | unxz --threads=`nproc` &gt; visits_v1.tsv # Validate the checksum md5sum visits_v1.tsv # Checksum should be equal to: 6dafe1a0f24e59e3fc2d0fed85601de6 # now create table clickhouse-client --query &quot;CREATE DATABASE IF NOT EXISTS datasets&quot; clickhouse-client --query &quot;CREATE TABLE datasets.visits_v1 ( CounterID UInt32, StartDate Date, Sign Int8, IsNew UInt8, VisitID UInt64, UserID UInt64, StartTime DateTime, Duration UInt32, UTCStartTime DateTime, PageViews Int32, Hits Int32, IsBounce UInt8, Referer String, StartURL String, RefererDomain String, StartURLDomain String, EndURL String, LinkURL String, IsDownload UInt8, TraficSourceID Int8, SearchEngineID UInt16, SearchPhrase String, AdvEngineID UInt8, PlaceID Int32, RefererCategories Array(UInt16), URLCategories Array(UInt16), URLRegions Array(UInt32), RefererRegions Array(UInt32), IsYandex UInt8, GoalReachesDepth Int32, GoalReachesURL Int32, GoalReachesAny Int32, SocialSourceNetworkID UInt8, SocialSourcePage String, MobilePhoneModel String, ClientEventTime DateTime, RegionID UInt32, ClientIP UInt32, ClientIP6 FixedString(16), RemoteIP UInt32, RemoteIP6 FixedString(16), IPNetworkID UInt32, SilverlightVersion3 UInt32, CodeVersion UInt32, ResolutionWidth UInt16, ResolutionHeight UInt16, UserAgentMajor UInt16, UserAgentMinor UInt16, WindowClientWidth UInt16, WindowClientHeight UInt16, SilverlightVersion2 UInt8, SilverlightVersion4 UInt16, FlashVersion3 UInt16, FlashVersion4 UInt16, ClientTimeZone Int16, OS UInt8, UserAgent UInt8, ResolutionDepth UInt8, FlashMajor UInt8, FlashMinor UInt8, NetMajor UInt8, NetMinor UInt8, MobilePhone UInt8, SilverlightVersion1 UInt8, Age UInt8, Sex UInt8, Income UInt8, JavaEnable UInt8, CookieEnable UInt8, JavascriptEnable UInt8, IsMobile UInt8, BrowserLanguage UInt16, BrowserCountry UInt16, Interests UInt16, Robotness UInt8, GeneralInterests Array(UInt16), Params Array(String), Goals Nested(ID UInt32, Serial UInt32, EventTime DateTime, Price Int64, OrderID String, CurrencyID UInt32), WatchIDs Array(UInt64), ParamSumPrice Int64, ParamCurrency FixedString(3), ParamCurrencyID UInt16, ClickLogID UInt64, ClickEventID Int32, ClickGoodEvent Int32, ClickEventTime DateTime, ClickPriorityID Int32, ClickPhraseID Int32, ClickPageID Int32, ClickPlaceID Int32, ClickTypeID Int32, ClickResourceID Int32, ClickCost UInt32, ClickClientIP UInt32, ClickDomainID UInt32, ClickURL String, ClickAttempt UInt8, ClickOrderID UInt32, ClickBannerID UInt32, ClickMarketCategoryID UInt32, ClickMarketPP UInt32, ClickMarketCategoryName String, ClickMarketPPName String, ClickAWAPSCampaignName String, ClickPageName String, ClickTargetType UInt16, ClickTargetPhraseID UInt64, ClickContextType UInt8, ClickSelectType Int8, ClickOptions String, ClickGroupBannerID Int32, OpenstatServiceName String, OpenstatCampaignID String, OpenstatAdID String, OpenstatSourceID String, UTMSource String, UTMMedium String, UTMCampaign String, UTMContent String, UTMTerm String, FromTag String, HasGCLID UInt8, FirstVisit DateTime, PredLastVisit Date, LastVisit Date, TotalVisits UInt32, TraficSource Nested(ID Int8, SearchEngineID UInt16, AdvEngineID UInt8, PlaceID UInt16, SocialSourceNetworkID UInt8, Domain String, SearchPhrase String, SocialSourcePage String), Attendance FixedString(16), CLID UInt32, YCLID UInt64, NormalizedRefererHash UInt64, SearchPhraseHash UInt64, RefererDomainHash UInt64, NormalizedStartURLHash UInt64, StartURLDomainHash UInt64, NormalizedEndURLHash UInt64, TopLevelDomain UInt64, URLScheme UInt64, OpenstatServiceNameHash UInt64, OpenstatCampaignIDHash UInt64, OpenstatAdIDHash UInt64, OpenstatSourceIDHash UInt64, UTMSourceHash UInt64, UTMMediumHash UInt64, UTMCampaignHash UInt64, UTMContentHash UInt64, UTMTermHash UInt64, FromHash UInt64, WebVisorEnabled UInt8, WebVisorActivity UInt32, ParsedParams Nested(Key1 String, Key2 String, Key3 String, Key4 String, Key5 String, ValueDouble Float64), Market Nested(Type UInt8, GoalID UInt32, OrderID String, OrderPrice Int64, PP UInt32, DirectPlaceID UInt32, DirectOrderID UInt32, DirectBannerID UInt32, GoodID String, GoodName String, GoodQuantity Int32, GoodPrice Int64), IslandID FixedString(16)) ENGINE = CollapsingMergeTree(Sign) PARTITION BY toYYYYMM(StartDate) ORDER BY (CounterID, StartDate, intHash32(UserID), VisitID) SAMPLE BY intHash32(UserID) SETTINGS index_granularity = 8192&quot; # import data cat visits_v1.tsv | clickhouse-client --query &quot;INSERT INTO datasets.visits_v1 FORMAT TSV&quot; --max_insert_block_size=100000 # optionally you can optimize table clickhouse-client --query &quot;OPTIMIZE TABLE datasets.visits_v1 FINAL&quot; clickhouse-client --query &quot;SELECT COUNT(*) FROM datasets.visits_v1&quot;  "},{"title":"Example Queries​","type":1,"pageTitle":"Anonymized Web Analytics Data","url":"docs/en/example-datasets/metrica#example-queries","content":"The ClickHouse tutorial is based on this web analytics dataset, and the recommended way to get started with this dataset is to go through the tutorial. Additional examples of queries to these tables can be found among stateful tests of ClickHouse (they are named test.hits and test.visits there). "},{"title":"GitHub Events Dataset","type":0,"sectionRef":"#","url":"docs/en/example-datasets/github-events","content":"GitHub Events Dataset Dataset contains all events on GitHub from 2011 to Dec 6 2020, the size is 3.1 billion records. Download size is 75 GB and it will require up to 200 GB space on disk if stored in a table with lz4 compression. Full dataset description, insights, download instruction and interactive queries are posted here.","keywords":""},{"title":"Terabyte of Click Logs from Criteo","type":0,"sectionRef":"#","url":"docs/en/example-datasets/criteo","content":"Terabyte of Click Logs from Criteo Download the data from http://labs.criteo.com/downloads/download-terabyte-click-logs/ Create a table to import the log to: CREATE TABLE criteo_log (date Date, clicked UInt8, int1 Int32, int2 Int32, int3 Int32, int4 Int32, int5 Int32, int6 Int32, int7 Int32, int8 Int32, int9 Int32, int10 Int32, int11 Int32, int12 Int32, int13 Int32, cat1 String, cat2 String, cat3 String, cat4 String, cat5 String, cat6 String, cat7 String, cat8 String, cat9 String, cat10 String, cat11 String, cat12 String, cat13 String, cat14 String, cat15 String, cat16 String, cat17 String, cat18 String, cat19 String, cat20 String, cat21 String, cat22 String, cat23 String, cat24 String, cat25 String, cat26 String) ENGINE = Log Download the data: $ for i in {00..23}; do echo $i; zcat datasets/criteo/day_${i#0}.gz | sed -r 's/^/2000-01-'${i/00/24}'\\t/' | clickhouse-client --host=example-perftest01j --query=&quot;INSERT INTO criteo_log FORMAT TabSeparated&quot;; done Create a table for the converted data: CREATE TABLE criteo ( date Date, clicked UInt8, int1 Int32, int2 Int32, int3 Int32, int4 Int32, int5 Int32, int6 Int32, int7 Int32, int8 Int32, int9 Int32, int10 Int32, int11 Int32, int12 Int32, int13 Int32, icat1 UInt32, icat2 UInt32, icat3 UInt32, icat4 UInt32, icat5 UInt32, icat6 UInt32, icat7 UInt32, icat8 UInt32, icat9 UInt32, icat10 UInt32, icat11 UInt32, icat12 UInt32, icat13 UInt32, icat14 UInt32, icat15 UInt32, icat16 UInt32, icat17 UInt32, icat18 UInt32, icat19 UInt32, icat20 UInt32, icat21 UInt32, icat22 UInt32, icat23 UInt32, icat24 UInt32, icat25 UInt32, icat26 UInt32 ) ENGINE = MergeTree(date, intHash32(icat1), (date, intHash32(icat1)), 8192) Transform data from the raw log and put it in the second table: INSERT INTO criteo SELECT date, clicked, int1, int2, int3, int4, int5, int6, int7, int8, int9, int10, int11, int12, int13, reinterpretAsUInt32(unhex(cat1)) AS icat1, reinterpretAsUInt32(unhex(cat2)) AS icat2, reinterpretAsUInt32(unhex(cat3)) AS icat3, reinterpretAsUInt32(unhex(cat4)) AS icat4, reinterpretAsUInt32(unhex(cat5)) AS icat5, reinterpretAsUInt32(unhex(cat6)) AS icat6, reinterpretAsUInt32(unhex(cat7)) AS icat7, reinterpretAsUInt32(unhex(cat8)) AS icat8, reinterpretAsUInt32(unhex(cat9)) AS icat9, reinterpretAsUInt32(unhex(cat10)) AS icat10, reinterpretAsUInt32(unhex(cat11)) AS icat11, reinterpretAsUInt32(unhex(cat12)) AS icat12, reinterpretAsUInt32(unhex(cat13)) AS icat13, reinterpretAsUInt32(unhex(cat14)) AS icat14, reinterpretAsUInt32(unhex(cat15)) AS icat15, reinterpretAsUInt32(unhex(cat16)) AS icat16, reinterpretAsUInt32(unhex(cat17)) AS icat17, reinterpretAsUInt32(unhex(cat18)) AS icat18, reinterpretAsUInt32(unhex(cat19)) AS icat19, reinterpretAsUInt32(unhex(cat20)) AS icat20, reinterpretAsUInt32(unhex(cat21)) AS icat21, reinterpretAsUInt32(unhex(cat22)) AS icat22, reinterpretAsUInt32(unhex(cat23)) AS icat23, reinterpretAsUInt32(unhex(cat24)) AS icat24, reinterpretAsUInt32(unhex(cat25)) AS icat25, reinterpretAsUInt32(unhex(cat26)) AS icat26 FROM criteo_log; DROP TABLE criteo_log; Original article","keywords":""},{"title":"AMPLab Big Data Benchmark","type":0,"sectionRef":"#","url":"docs/en/example-datasets/amplab-benchmark","content":"AMPLab Big Data Benchmark See https://amplab.cs.berkeley.edu/benchmark/ Sign up for a free account at https://aws.amazon.com. It requires a credit card, email, and phone number. Get a new access key at https://console.aws.amazon.com/iam/home?nc2=h_m_sc#security_credential Run the following in the console: $ sudo apt-get install s3cmd $ mkdir tiny; cd tiny; $ s3cmd sync s3://big-data-benchmark/pavlo/text-deflate/tiny/ . $ cd .. $ mkdir 1node; cd 1node; $ s3cmd sync s3://big-data-benchmark/pavlo/text-deflate/1node/ . $ cd .. $ mkdir 5nodes; cd 5nodes; $ s3cmd sync s3://big-data-benchmark/pavlo/text-deflate/5nodes/ . $ cd .. Run the following ClickHouse queries: CREATE TABLE rankings_tiny ( pageURL String, pageRank UInt32, avgDuration UInt32 ) ENGINE = Log; CREATE TABLE uservisits_tiny ( sourceIP String, destinationURL String, visitDate Date, adRevenue Float32, UserAgent String, cCode FixedString(3), lCode FixedString(6), searchWord String, duration UInt32 ) ENGINE = MergeTree(visitDate, visitDate, 8192); CREATE TABLE rankings_1node ( pageURL String, pageRank UInt32, avgDuration UInt32 ) ENGINE = Log; CREATE TABLE uservisits_1node ( sourceIP String, destinationURL String, visitDate Date, adRevenue Float32, UserAgent String, cCode FixedString(3), lCode FixedString(6), searchWord String, duration UInt32 ) ENGINE = MergeTree(visitDate, visitDate, 8192); CREATE TABLE rankings_5nodes_on_single ( pageURL String, pageRank UInt32, avgDuration UInt32 ) ENGINE = Log; CREATE TABLE uservisits_5nodes_on_single ( sourceIP String, destinationURL String, visitDate Date, adRevenue Float32, UserAgent String, cCode FixedString(3), lCode FixedString(6), searchWord String, duration UInt32 ) ENGINE = MergeTree(visitDate, visitDate, 8192); Go back to the console: $ for i in tiny/rankings/*.deflate; do echo $i; zlib-flate -uncompress &lt; $i | clickhouse-client --host=example-perftest01j --query=&quot;INSERT INTO rankings_tiny FORMAT CSV&quot;; done $ for i in tiny/uservisits/*.deflate; do echo $i; zlib-flate -uncompress &lt; $i | clickhouse-client --host=example-perftest01j --query=&quot;INSERT INTO uservisits_tiny FORMAT CSV&quot;; done $ for i in 1node/rankings/*.deflate; do echo $i; zlib-flate -uncompress &lt; $i | clickhouse-client --host=example-perftest01j --query=&quot;INSERT INTO rankings_1node FORMAT CSV&quot;; done $ for i in 1node/uservisits/*.deflate; do echo $i; zlib-flate -uncompress &lt; $i | clickhouse-client --host=example-perftest01j --query=&quot;INSERT INTO uservisits_1node FORMAT CSV&quot;; done $ for i in 5nodes/rankings/*.deflate; do echo $i; zlib-flate -uncompress &lt; $i | clickhouse-client --host=example-perftest01j --query=&quot;INSERT INTO rankings_5nodes_on_single FORMAT CSV&quot;; done $ for i in 5nodes/uservisits/*.deflate; do echo $i; zlib-flate -uncompress &lt; $i | clickhouse-client --host=example-perftest01j --query=&quot;INSERT INTO uservisits_5nodes_on_single FORMAT CSV&quot;; done Queries for obtaining data samples: SELECT pageURL, pageRank FROM rankings_1node WHERE pageRank &gt; 1000 SELECT substring(sourceIP, 1, 8), sum(adRevenue) FROM uservisits_1node GROUP BY substring(sourceIP, 1, 8) SELECT sourceIP, sum(adRevenue) AS totalRevenue, avg(pageRank) AS pageRank FROM rankings_1node ALL INNER JOIN ( SELECT sourceIP, destinationURL AS pageURL, adRevenue FROM uservisits_1node WHERE (visitDate &gt; '1980-01-01') AND (visitDate &lt; '1980-04-01') ) USING pageURL GROUP BY sourceIP ORDER BY totalRevenue DESC LIMIT 1 Original article","keywords":""},{"title":"Brown University Benchmark","type":0,"sectionRef":"#","url":"docs/en/example-datasets/brown-benchmark","content":"Brown University Benchmark MgBench is a new analytical benchmark for machine-generated log data, Andrew Crotty. Download the data: wget https://datasets.clickhouse.com/mgbench{1..3}.csv.xz Unpack the data: xz -v -d mgbench{1..3}.csv.xz Create tables: CREATE DATABASE mgbench; CREATE TABLE mgbench.logs1 ( log_time DateTime, machine_name LowCardinality(String), machine_group LowCardinality(String), cpu_idle Nullable(Float32), cpu_nice Nullable(Float32), cpu_system Nullable(Float32), cpu_user Nullable(Float32), cpu_wio Nullable(Float32), disk_free Nullable(Float32), disk_total Nullable(Float32), part_max_used Nullable(Float32), load_fifteen Nullable(Float32), load_five Nullable(Float32), load_one Nullable(Float32), mem_buffers Nullable(Float32), mem_cached Nullable(Float32), mem_free Nullable(Float32), mem_shared Nullable(Float32), swap_free Nullable(Float32), bytes_in Nullable(Float32), bytes_out Nullable(Float32) ) ENGINE = MergeTree() ORDER BY (machine_group, machine_name, log_time); CREATE TABLE mgbench.logs2 ( log_time DateTime, client_ip IPv4, request String, status_code UInt16, object_size UInt64 ) ENGINE = MergeTree() ORDER BY log_time; CREATE TABLE mgbench.logs3 ( log_time DateTime64, device_id FixedString(15), device_name LowCardinality(String), device_type LowCardinality(String), device_floor UInt8, event_type LowCardinality(String), event_unit FixedString(1), event_value Nullable(Float32) ) ENGINE = MergeTree() ORDER BY (event_type, log_time); Insert data: clickhouse-client --query &quot;INSERT INTO mgbench.logs1 FORMAT CSVWithNames&quot; &lt; mgbench1.csv clickhouse-client --query &quot;INSERT INTO mgbench.logs2 FORMAT CSVWithNames&quot; &lt; mgbench2.csv clickhouse-client --query &quot;INSERT INTO mgbench.logs3 FORMAT CSVWithNames&quot; &lt; mgbench3.csv Run benchmark queries: -- Q1.1: What is the CPU/network utilization for each web server since midnight? SELECT machine_name, MIN(cpu) AS cpu_min, MAX(cpu) AS cpu_max, AVG(cpu) AS cpu_avg, MIN(net_in) AS net_in_min, MAX(net_in) AS net_in_max, AVG(net_in) AS net_in_avg, MIN(net_out) AS net_out_min, MAX(net_out) AS net_out_max, AVG(net_out) AS net_out_avg FROM ( SELECT machine_name, COALESCE(cpu_user, 0.0) AS cpu, COALESCE(bytes_in, 0.0) AS net_in, COALESCE(bytes_out, 0.0) AS net_out FROM logs1 WHERE machine_name IN ('anansi','aragog','urd') AND log_time &gt;= TIMESTAMP '2017-01-11 00:00:00' ) AS r GROUP BY machine_name; -- Q1.2: Which computer lab machines have been offline in the past day? SELECT machine_name, log_time FROM logs1 WHERE (machine_name LIKE 'cslab%' OR machine_name LIKE 'mslab%') AND load_one IS NULL AND log_time &gt;= TIMESTAMP '2017-01-10 00:00:00' ORDER BY machine_name, log_time; -- Q1.3: What are the hourly average metrics during the past 10 days for a specific workstation? SELECT dt, hr, AVG(load_fifteen) AS load_fifteen_avg, AVG(load_five) AS load_five_avg, AVG(load_one) AS load_one_avg, AVG(mem_free) AS mem_free_avg, AVG(swap_free) AS swap_free_avg FROM ( SELECT CAST(log_time AS DATE) AS dt, EXTRACT(HOUR FROM log_time) AS hr, load_fifteen, load_five, load_one, mem_free, swap_free FROM logs1 WHERE machine_name = 'babbage' AND load_fifteen IS NOT NULL AND load_five IS NOT NULL AND load_one IS NOT NULL AND mem_free IS NOT NULL AND swap_free IS NOT NULL AND log_time &gt;= TIMESTAMP '2017-01-01 00:00:00' ) AS r GROUP BY dt, hr ORDER BY dt, hr; -- Q1.4: Over 1 month, how often was each server blocked on disk I/O? SELECT machine_name, COUNT(*) AS spikes FROM logs1 WHERE machine_group = 'Servers' AND cpu_wio &gt; 0.99 AND log_time &gt;= TIMESTAMP '2016-12-01 00:00:00' AND log_time &lt; TIMESTAMP '2017-01-01 00:00:00' GROUP BY machine_name ORDER BY spikes DESC LIMIT 10; -- Q1.5: Which externally reachable VMs have run low on memory? SELECT machine_name, dt, MIN(mem_free) AS mem_free_min FROM ( SELECT machine_name, CAST(log_time AS DATE) AS dt, mem_free FROM logs1 WHERE machine_group = 'DMZ' AND mem_free IS NOT NULL ) AS r GROUP BY machine_name, dt HAVING MIN(mem_free) &lt; 10000 ORDER BY machine_name, dt; -- Q1.6: What is the total hourly network traffic across all file servers? SELECT dt, hr, SUM(net_in) AS net_in_sum, SUM(net_out) AS net_out_sum, SUM(net_in) + SUM(net_out) AS both_sum FROM ( SELECT CAST(log_time AS DATE) AS dt, EXTRACT(HOUR FROM log_time) AS hr, COALESCE(bytes_in, 0.0) / 1000000000.0 AS net_in, COALESCE(bytes_out, 0.0) / 1000000000.0 AS net_out FROM logs1 WHERE machine_name IN ('allsorts','andes','bigred','blackjack','bonbon', 'cadbury','chiclets','cotton','crows','dove','fireball','hearts','huey', 'lindt','milkduds','milkyway','mnm','necco','nerds','orbit','peeps', 'poprocks','razzles','runts','smarties','smuggler','spree','stride', 'tootsie','trident','wrigley','york') ) AS r GROUP BY dt, hr ORDER BY both_sum DESC LIMIT 10; -- Q2.1: Which requests have caused server errors within the past 2 weeks? SELECT * FROM logs2 WHERE status_code &gt;= 500 AND log_time &gt;= TIMESTAMP '2012-12-18 00:00:00' ORDER BY log_time; -- Q2.2: During a specific 2-week period, was the user password file leaked? SELECT * FROM logs2 WHERE status_code &gt;= 200 AND status_code &lt; 300 AND request LIKE '%/etc/passwd%' AND log_time &gt;= TIMESTAMP '2012-05-06 00:00:00' AND log_time &lt; TIMESTAMP '2012-05-20 00:00:00'; -- Q2.3: What was the average path depth for top-level requests in the past month? SELECT top_level, AVG(LENGTH(request) - LENGTH(REPLACE(request, '/', ''))) AS depth_avg FROM ( SELECT SUBSTRING(request FROM 1 FOR len) AS top_level, request FROM ( SELECT POSITION(SUBSTRING(request FROM 2), '/') AS len, request FROM logs2 WHERE status_code &gt;= 200 AND status_code &lt; 300 AND log_time &gt;= TIMESTAMP '2012-12-01 00:00:00' ) AS r WHERE len &gt; 0 ) AS s WHERE top_level IN ('/about','/courses','/degrees','/events', '/grad','/industry','/news','/people', '/publications','/research','/teaching','/ugrad') GROUP BY top_level ORDER BY top_level; -- Q2.4: During the last 3 months, which clients have made an excessive number of requests? SELECT client_ip, COUNT(*) AS num_requests FROM logs2 WHERE log_time &gt;= TIMESTAMP '2012-10-01 00:00:00' GROUP BY client_ip HAVING COUNT(*) &gt;= 100000 ORDER BY num_requests DESC; -- Q2.5: What are the daily unique visitors? SELECT dt, COUNT(DISTINCT client_ip) FROM ( SELECT CAST(log_time AS DATE) AS dt, client_ip FROM logs2 ) AS r GROUP BY dt ORDER BY dt; -- Q2.6: What are the average and maximum data transfer rates (Gbps)? SELECT AVG(transfer) / 125000000.0 AS transfer_avg, MAX(transfer) / 125000000.0 AS transfer_max FROM ( SELECT log_time, SUM(object_size) AS transfer FROM logs2 GROUP BY log_time ) AS r; -- Q3.1: Did the indoor temperature reach freezing over the weekend? SELECT * FROM logs3 WHERE event_type = 'temperature' AND event_value &lt;= 32.0 AND log_time &gt;= '2019-11-29 17:00:00.000'; -- Q3.4: Over the past 6 months, how frequently were each door opened? SELECT device_name, device_floor, COUNT(*) AS ct FROM logs3 WHERE event_type = 'door_open' AND log_time &gt;= '2019-06-01 00:00:00.000' GROUP BY device_name, device_floor ORDER BY ct DESC; -- Q3.5: Where in the building do large temperature variations occur in winter and summer? WITH temperature AS ( SELECT dt, device_name, device_type, device_floor FROM ( SELECT dt, hr, device_name, device_type, device_floor, AVG(event_value) AS temperature_hourly_avg FROM ( SELECT CAST(log_time AS DATE) AS dt, EXTRACT(HOUR FROM log_time) AS hr, device_name, device_type, device_floor, event_value FROM logs3 WHERE event_type = 'temperature' ) AS r GROUP BY dt, hr, device_name, device_type, device_floor ) AS s GROUP BY dt, device_name, device_type, device_floor HAVING MAX(temperature_hourly_avg) - MIN(temperature_hourly_avg) &gt;= 25.0 ) SELECT DISTINCT device_name, device_type, device_floor, 'WINTER' FROM temperature WHERE dt &gt;= DATE '2018-12-01' AND dt &lt; DATE '2019-03-01' UNION SELECT DISTINCT device_name, device_type, device_floor, 'SUMMER' FROM temperature WHERE dt &gt;= DATE '2019-06-01' AND dt &lt; DATE '2019-09-01'; -- Q3.6: For each device category, what are the monthly power consumption metrics? SELECT yr, mo, SUM(coffee_hourly_avg) AS coffee_monthly_sum, AVG(coffee_hourly_avg) AS coffee_monthly_avg, SUM(printer_hourly_avg) AS printer_monthly_sum, AVG(printer_hourly_avg) AS printer_monthly_avg, SUM(projector_hourly_avg) AS projector_monthly_sum, AVG(projector_hourly_avg) AS projector_monthly_avg, SUM(vending_hourly_avg) AS vending_monthly_sum, AVG(vending_hourly_avg) AS vending_monthly_avg FROM ( SELECT dt, yr, mo, hr, AVG(coffee) AS coffee_hourly_avg, AVG(printer) AS printer_hourly_avg, AVG(projector) AS projector_hourly_avg, AVG(vending) AS vending_hourly_avg FROM ( SELECT CAST(log_time AS DATE) AS dt, EXTRACT(YEAR FROM log_time) AS yr, EXTRACT(MONTH FROM log_time) AS mo, EXTRACT(HOUR FROM log_time) AS hr, CASE WHEN device_name LIKE 'coffee%' THEN event_value END AS coffee, CASE WHEN device_name LIKE 'printer%' THEN event_value END AS printer, CASE WHEN device_name LIKE 'projector%' THEN event_value END AS projector, CASE WHEN device_name LIKE 'vending%' THEN event_value END AS vending FROM logs3 WHERE device_type = 'meter' ) AS r GROUP BY dt, yr, mo, hr ) AS s GROUP BY yr, mo ORDER BY yr, mo; The data is also available for interactive queries in the Playground, example. Original article","keywords":""},{"title":"WikiStat","type":0,"sectionRef":"#","url":"docs/en/example-datasets/wikistat","content":"WikiStat See http://dumps.wikimedia.org/other/pagecounts-raw/ for details. Creating a table: CREATE TABLE wikistat ( date Date, time DateTime, project String, subproject String, path String, hits UInt64, size UInt64 ) ENGINE = MergeTree(date, (path, time), 8192); Loading data: $ for i in {2007..2016}; do for j in {01..12}; do echo $i-$j &gt;&amp;2; curl -sSL &quot;http://dumps.wikimedia.org/other/pagecounts-raw/$i/$i-$j/&quot; | grep -oE 'pagecounts-[0-9]+-[0-9]+\\.gz'; done; done | sort | uniq | tee links.txt $ cat links.txt | while read link; do wget http://dumps.wikimedia.org/other/pagecounts-raw/$(echo $link | sed -r 's/pagecounts-([0-9]{4})([0-9]{2})[0-9]{2}-[0-9]+\\.gz/\\1/')/$(echo $link | sed -r 's/pagecounts-([0-9]{4})([0-9]{2})[0-9]{2}-[0-9]+\\.gz/\\1-\\2/')/$link; done $ ls -1 /opt/wikistat/ | grep gz | while read i; do echo $i; gzip -cd /opt/wikistat/$i | ./wikistat-loader --time=&quot;$(echo -n $i | sed -r 's/pagecounts-([0-9]{4})([0-9]{2})([0-9]{2})-([0-9]{2})([0-9]{2})([0-9]{2})\\.gz/\\1-\\2-\\3 \\4-00-00/')&quot; | clickhouse-client --query=&quot;INSERT INTO wikistat FORMAT TabSeparated&quot;; done Original article","keywords":""},{"title":"OnTime","type":0,"sectionRef":"#","url":"docs/en/example-datasets/ontime","content":"","keywords":""},{"title":"Import from Raw Data​","type":1,"pageTitle":"OnTime","url":"docs/en/example-datasets/ontime#import-from-raw-data","content":"Downloading data: wget --no-check-certificate --continue https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_{1987..2021}_{1..12}.zip  Creating a table: CREATE TABLE `ontime` ( `Year` UInt16, `Quarter` UInt8, `Month` UInt8, `DayofMonth` UInt8, `DayOfWeek` UInt8, `FlightDate` Date, `Reporting_Airline` String, `DOT_ID_Reporting_Airline` Int32, `IATA_CODE_Reporting_Airline` String, `Tail_Number` String, `Flight_Number_Reporting_Airline` String, `OriginAirportID` Int32, `OriginAirportSeqID` Int32, `OriginCityMarketID` Int32, `Origin` FixedString(5), `OriginCityName` String, `OriginState` FixedString(2), `OriginStateFips` String, `OriginStateName` String, `OriginWac` Int32, `DestAirportID` Int32, `DestAirportSeqID` Int32, `DestCityMarketID` Int32, `Dest` FixedString(5), `DestCityName` String, `DestState` FixedString(2), `DestStateFips` String, `DestStateName` String, `DestWac` Int32, `CRSDepTime` Int32, `DepTime` Int32, `DepDelay` Int32, `DepDelayMinutes` Int32, `DepDel15` Int32, `DepartureDelayGroups` String, `DepTimeBlk` String, `TaxiOut` Int32, `WheelsOff` Int32, `WheelsOn` Int32, `TaxiIn` Int32, `CRSArrTime` Int32, `ArrTime` Int32, `ArrDelay` Int32, `ArrDelayMinutes` Int32, `ArrDel15` Int32, `ArrivalDelayGroups` Int32, `ArrTimeBlk` String, `Cancelled` UInt8, `CancellationCode` FixedString(1), `Diverted` UInt8, `CRSElapsedTime` Int32, `ActualElapsedTime` Int32, `AirTime` Nullable(Int32), `Flights` Int32, `Distance` Int32, `DistanceGroup` UInt8, `CarrierDelay` Int32, `WeatherDelay` Int32, `NASDelay` Int32, `SecurityDelay` Int32, `LateAircraftDelay` Int32, `FirstDepTime` String, `TotalAddGTime` String, `LongestAddGTime` String, `DivAirportLandings` String, `DivReachedDest` String, `DivActualElapsedTime` String, `DivArrDelay` String, `DivDistance` String, `Div1Airport` String, `Div1AirportID` Int32, `Div1AirportSeqID` Int32, `Div1WheelsOn` String, `Div1TotalGTime` String, `Div1LongestGTime` String, `Div1WheelsOff` String, `Div1TailNum` String, `Div2Airport` String, `Div2AirportID` Int32, `Div2AirportSeqID` Int32, `Div2WheelsOn` String, `Div2TotalGTime` String, `Div2LongestGTime` String, `Div2WheelsOff` String, `Div2TailNum` String, `Div3Airport` String, `Div3AirportID` Int32, `Div3AirportSeqID` Int32, `Div3WheelsOn` String, `Div3TotalGTime` String, `Div3LongestGTime` String, `Div3WheelsOff` String, `Div3TailNum` String, `Div4Airport` String, `Div4AirportID` Int32, `Div4AirportSeqID` Int32, `Div4WheelsOn` String, `Div4TotalGTime` String, `Div4LongestGTime` String, `Div4WheelsOff` String, `Div4TailNum` String, `Div5Airport` String, `Div5AirportID` Int32, `Div5AirportSeqID` Int32, `Div5WheelsOn` String, `Div5TotalGTime` String, `Div5LongestGTime` String, `Div5WheelsOff` String, `Div5TailNum` String ) ENGINE = MergeTree PARTITION BY Year ORDER BY (IATA_CODE_Reporting_Airline, FlightDate) SETTINGS index_granularity = 8192;  Loading data with multiple threads: ls -1 *.zip | xargs -I{} -P $(nproc) bash -c &quot;echo {}; unzip -cq {} '*.csv' | sed 's/\\.00//g' | clickhouse-client --input_format_with_names_use_header=0 --query='INSERT INTO ontime FORMAT CSVWithNames'&quot;  (if you will have memory shortage or other issues on your server, remove the -P $(nproc) part) "},{"title":"Download of Prepared Partitions​","type":1,"pageTitle":"OnTime","url":"docs/en/example-datasets/ontime#download-of-prepared-partitions","content":"$ curl -O https://datasets.clickhouse.com/ontime/partitions/ontime.tar $ tar xvf ontime.tar -C /var/lib/clickhouse # path to ClickHouse data directory $ # check permissions of unpacked data, fix if required $ sudo service clickhouse-server restart $ clickhouse-client --query &quot;select count(*) from datasets.ontime&quot;  note If you will run the queries described below, you have to use the full table name, datasets.ontime. "},{"title":"Queries​","type":1,"pageTitle":"OnTime","url":"docs/en/example-datasets/ontime#queries","content":"Q0. SELECT avg(c1) FROM ( SELECT Year, Month, count(*) AS c1 FROM ontime GROUP BY Year, Month );  Q1. The number of flights per day from the year 2000 to 2008 SELECT DayOfWeek, count(*) AS c FROM ontime WHERE Year&gt;=2000 AND Year&lt;=2008 GROUP BY DayOfWeek ORDER BY c DESC;  Q2. The number of flights delayed by more than 10 minutes, grouped by the day of the week, for 2000-2008 SELECT DayOfWeek, count(*) AS c FROM ontime WHERE DepDelay&gt;10 AND Year&gt;=2000 AND Year&lt;=2008 GROUP BY DayOfWeek ORDER BY c DESC;  Q3. The number of delays by the airport for 2000-2008 SELECT Origin, count(*) AS c FROM ontime WHERE DepDelay&gt;10 AND Year&gt;=2000 AND Year&lt;=2008 GROUP BY Origin ORDER BY c DESC LIMIT 10;  Q4. The number of delays by carrier for 2007 SELECT IATA_CODE_Reporting_Airline AS Carrier, count(*) FROM ontime WHERE DepDelay&gt;10 AND Year=2007 GROUP BY Carrier ORDER BY count(*) DESC;  Q5. The percentage of delays by carrier for 2007 SELECT Carrier, c, c2, c*100/c2 as c3 FROM ( SELECT IATA_CODE_Reporting_Airline AS Carrier, count(*) AS c FROM ontime WHERE DepDelay&gt;10 AND Year=2007 GROUP BY Carrier ) q JOIN ( SELECT IATA_CODE_Reporting_Airline AS Carrier, count(*) AS c2 FROM ontime WHERE Year=2007 GROUP BY Carrier ) qq USING Carrier ORDER BY c3 DESC;  Better version of the same query: SELECT IATA_CODE_Reporting_Airline AS Carrier, avg(DepDelay&gt;10)*100 AS c3 FROM ontime WHERE Year=2007 GROUP BY Carrier ORDER BY c3 DESC  Q6. The previous request for a broader range of years, 2000-2008 SELECT Carrier, c, c2, c*100/c2 as c3 FROM ( SELECT IATA_CODE_Reporting_Airline AS Carrier, count(*) AS c FROM ontime WHERE DepDelay&gt;10 AND Year&gt;=2000 AND Year&lt;=2008 GROUP BY Carrier ) q JOIN ( SELECT IATA_CODE_Reporting_Airline AS Carrier, count(*) AS c2 FROM ontime WHERE Year&gt;=2000 AND Year&lt;=2008 GROUP BY Carrier ) qq USING Carrier ORDER BY c3 DESC;  Better version of the same query: SELECT IATA_CODE_Reporting_Airline AS Carrier, avg(DepDelay&gt;10)*100 AS c3 FROM ontime WHERE Year&gt;=2000 AND Year&lt;=2008 GROUP BY Carrier ORDER BY c3 DESC;  Q7. Percentage of flights delayed for more than 10 minutes, by year SELECT Year, c1/c2 FROM ( select Year, count(*)*100 as c1 from ontime WHERE DepDelay&gt;10 GROUP BY Year ) q JOIN ( select Year, count(*) as c2 from ontime GROUP BY Year ) qq USING (Year) ORDER BY Year;  Better version of the same query: SELECT Year, avg(DepDelay&gt;10)*100 FROM ontime GROUP BY Year ORDER BY Year;  Q8. The most popular destinations by the number of directly connected cities for various year ranges SELECT DestCityName, uniqExact(OriginCityName) AS u FROM ontime WHERE Year &gt;= 2000 and Year &lt;= 2010 GROUP BY DestCityName ORDER BY u DESC LIMIT 10;  Q9. SELECT Year, count(*) AS c1 FROM ontime GROUP BY Year;  Q10. SELECT min(Year), max(Year), IATA_CODE_Reporting_Airline AS Carrier, count(*) AS cnt, sum(ArrDelayMinutes&gt;30) AS flights_delayed, round(sum(ArrDelayMinutes&gt;30)/count(*),2) AS rate FROM ontime WHERE DayOfWeek NOT IN (6,7) AND OriginState NOT IN ('AK', 'HI', 'PR', 'VI') AND DestState NOT IN ('AK', 'HI', 'PR', 'VI') AND FlightDate &lt; '2010-01-01' GROUP by Carrier HAVING cnt&gt;100000 and max(Year)&gt;1990 ORDER by rate DESC LIMIT 1000;  Bonus: SELECT avg(cnt) FROM ( SELECT Year,Month,count(*) AS cnt FROM ontime WHERE DepDel15=1 GROUP BY Year,Month ); SELECT avg(c1) FROM ( SELECT Year,Month,count(*) AS c1 FROM ontime GROUP BY Year,Month ); SELECT DestCityName, uniqExact(OriginCityName) AS u FROM ontime GROUP BY DestCityName ORDER BY u DESC LIMIT 10; SELECT OriginCityName, DestCityName, count() AS c FROM ontime GROUP BY OriginCityName, DestCityName ORDER BY c DESC LIMIT 10; SELECT OriginCityName, count() AS c FROM ontime GROUP BY OriginCityName ORDER BY c DESC LIMIT 10;  You can also play with the data in Playground, example. This performance test was created by Vadim Tkachenko. See: https://www.percona.com/blog/2009/10/02/analyzing-air-traffic-performance-with-infobright-and-monetdb/https://www.percona.com/blog/2009/10/26/air-traffic-queries-in-luciddb/https://www.percona.com/blog/2009/11/02/air-traffic-queries-in-infinidb-early-alpha/https://www.percona.com/blog/2014/04/21/using-apache-hadoop-and-impala-together-with-mysql-for-data-analysis/https://www.percona.com/blog/2016/01/07/apache-spark-with-air-ontime-performance-data/http://nickmakos.blogspot.ru/2012/08/analyzing-air-traffic-performance-with.html Original article "},{"title":"Recipes Dataset","type":0,"sectionRef":"#","url":"docs/en/example-datasets/recipes","content":"","keywords":""},{"title":"Download and Unpack the Dataset​","type":1,"pageTitle":"Recipes Dataset","url":"docs/en/example-datasets/recipes#download-and-unpack-the-dataset","content":"Go to the download page https://recipenlg.cs.put.poznan.pl/dataset.Accept Terms and Conditions and download zip file.Unpack the zip file with unzip. You will get the full_dataset.csv file. "},{"title":"Create a Table​","type":1,"pageTitle":"Recipes Dataset","url":"docs/en/example-datasets/recipes#create-a-table","content":"Run clickhouse-client and execute the following CREATE query: CREATE TABLE recipes ( title String, ingredients Array(String), directions Array(String), link String, source LowCardinality(String), NER Array(String) ) ENGINE = MergeTree ORDER BY title;  "},{"title":"Insert the Data​","type":1,"pageTitle":"Recipes Dataset","url":"docs/en/example-datasets/recipes#insert-the-data","content":"Run the following command: clickhouse-client --query &quot; INSERT INTO recipes SELECT title, JSONExtract(ingredients, 'Array(String)'), JSONExtract(directions, 'Array(String)'), link, source, JSONExtract(NER, 'Array(String)') FROM input('num UInt32, title String, ingredients String, directions String, link String, source LowCardinality(String), NER String') FORMAT CSVWithNames &quot; --input_format_with_names_use_header 0 --format_csv_allow_single_quote 0 --input_format_allow_errors_num 10 &lt; full_dataset.csv  This is a showcase how to parse custom CSV, as it requires multiple tunes. Explanation: The dataset is in CSV format, but it requires some preprocessing on insertion; we use table function input to perform preprocessing;The structure of CSV file is specified in the argument of the table function input;The field num (row number) is unneeded - we parse it from file and ignore;We use FORMAT CSVWithNames but the header in CSV will be ignored (by command line parameter --input_format_with_names_use_header 0), because the header does not contain the name for the first field;File is using only double quotes to enclose CSV strings; some strings are not enclosed in double quotes, and single quote must not be parsed as the string enclosing - that's why we also add the --format_csv_allow_single_quote 0 parameter;Some strings from CSV cannot parse, because they contain \\M/ sequence at the beginning of the value; the only value starting with backslash in CSV can be \\N that is parsed as SQL NULL. We add --input_format_allow_errors_num 10 parameter and up to ten malformed records can be skipped;There are arrays for ingredients, directions and NER fields; these arrays are represented in unusual form: they are serialized into string as JSON and then placed in CSV - we parse them as String and then use JSONExtract function to transform it to Array. "},{"title":"Validate the Inserted Data​","type":1,"pageTitle":"Recipes Dataset","url":"docs/en/example-datasets/recipes#validate-the-inserted-data","content":"By checking the row count: Query: SELECT count() FROM recipes;  Result: ┌─count()─┐ │ 2231141 │ └─────────┘  "},{"title":"Example Queries​","type":1,"pageTitle":"Recipes Dataset","url":"docs/en/example-datasets/recipes#example-queries","content":""},{"title":"Top Components by the Number of Recipes:​","type":1,"pageTitle":"Recipes Dataset","url":"docs/en/example-datasets/recipes#top-components-by-the-number-of-recipes","content":"In this example we learn how to use arrayJoin function to expand an array into a set of rows. Query: SELECT arrayJoin(NER) AS k, count() AS c FROM recipes GROUP BY k ORDER BY c DESC LIMIT 50  Result: ┌─k────────────────────┬──────c─┐ │ salt │ 890741 │ │ sugar │ 620027 │ │ butter │ 493823 │ │ flour │ 466110 │ │ eggs │ 401276 │ │ onion │ 372469 │ │ garlic │ 358364 │ │ milk │ 346769 │ │ water │ 326092 │ │ vanilla │ 270381 │ │ olive oil │ 197877 │ │ pepper │ 179305 │ │ brown sugar │ 174447 │ │ tomatoes │ 163933 │ │ egg │ 160507 │ │ baking powder │ 148277 │ │ lemon juice │ 146414 │ │ Salt │ 122557 │ │ cinnamon │ 117927 │ │ sour cream │ 116682 │ │ cream cheese │ 114423 │ │ margarine │ 112742 │ │ celery │ 112676 │ │ baking soda │ 110690 │ │ parsley │ 102151 │ │ chicken │ 101505 │ │ onions │ 98903 │ │ vegetable oil │ 91395 │ │ oil │ 85600 │ │ mayonnaise │ 84822 │ │ pecans │ 79741 │ │ nuts │ 78471 │ │ potatoes │ 75820 │ │ carrots │ 75458 │ │ pineapple │ 74345 │ │ soy sauce │ 70355 │ │ black pepper │ 69064 │ │ thyme │ 68429 │ │ mustard │ 65948 │ │ chicken broth │ 65112 │ │ bacon │ 64956 │ │ honey │ 64626 │ │ oregano │ 64077 │ │ ground beef │ 64068 │ │ unsalted butter │ 63848 │ │ mushrooms │ 61465 │ │ Worcestershire sauce │ 59328 │ │ cornstarch │ 58476 │ │ green pepper │ 58388 │ │ Cheddar cheese │ 58354 │ └──────────────────────┴────────┘ 50 rows in set. Elapsed: 0.112 sec. Processed 2.23 million rows, 361.57 MB (19.99 million rows/s., 3.24 GB/s.)  "},{"title":"The Most Complex Recipes with Strawberry​","type":1,"pageTitle":"Recipes Dataset","url":"docs/en/example-datasets/recipes#the-most-complex-recipes-with-strawberry","content":"SELECT title, length(NER), length(directions) FROM recipes WHERE has(NER, 'strawberry') ORDER BY length(directions) DESC LIMIT 10  Result: ┌─title────────────────────────────────────────────────────────────┬─length(NER)─┬─length(directions)─┐ │ Chocolate-Strawberry-Orange Wedding Cake │ 24 │ 126 │ │ Strawberry Cream Cheese Crumble Tart │ 19 │ 47 │ │ Charlotte-Style Ice Cream │ 11 │ 45 │ │ Sinfully Good a Million Layers Chocolate Layer Cake, With Strawb │ 31 │ 45 │ │ Sweetened Berries With Elderflower Sherbet │ 24 │ 44 │ │ Chocolate-Strawberry Mousse Cake │ 15 │ 42 │ │ Rhubarb Charlotte with Strawberries and Rum │ 20 │ 42 │ │ Chef Joey's Strawberry Vanilla Tart │ 7 │ 37 │ │ Old-Fashioned Ice Cream Sundae Cake │ 17 │ 37 │ │ Watermelon Cake │ 16 │ 36 │ └──────────────────────────────────────────────────────────────────┴─────────────┴────────────────────┘ 10 rows in set. Elapsed: 0.215 sec. Processed 2.23 million rows, 1.48 GB (10.35 million rows/s., 6.86 GB/s.)  In this example, we involve has function to filter by array elements and sort by the number of directions. There is a wedding cake that requires the whole 126 steps to produce! Show that directions: Query: SELECT arrayJoin(directions) FROM recipes WHERE title = 'Chocolate-Strawberry-Orange Wedding Cake'  Result: ┌─arrayJoin(directions)───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ Position 1 rack in center and 1 rack in bottom third of oven and preheat to 350F. │ │ Butter one 5-inch-diameter cake pan with 2-inch-high sides, one 8-inch-diameter cake pan with 2-inch-high sides and one 12-inch-diameter cake pan with 2-inch-high sides. │ │ Dust pans with flour; line bottoms with parchment. │ │ Combine 1/3 cup orange juice and 2 ounces unsweetened chocolate in heavy small saucepan. │ │ Stir mixture over medium-low heat until chocolate melts. │ │ Remove from heat. │ │ Gradually mix in 1 2/3 cups orange juice. │ │ Sift 3 cups flour, 2/3 cup cocoa, 2 teaspoons baking soda, 1 teaspoon salt and 1/2 teaspoon baking powder into medium bowl. │ │ using electric mixer, beat 1 cup (2 sticks) butter and 3 cups sugar in large bowl until blended (mixture will look grainy). │ │ Add 4 eggs, 1 at a time, beating to blend after each. │ │ Beat in 1 tablespoon orange peel and 1 tablespoon vanilla extract. │ │ Add dry ingredients alternately with orange juice mixture in 3 additions each, beating well after each addition. │ │ Mix in 1 cup chocolate chips. │ │ Transfer 1 cup plus 2 tablespoons batter to prepared 5-inch pan, 3 cups batter to prepared 8-inch pan and remaining batter (about 6 cups) to 12-inch pan. │ │ Place 5-inch and 8-inch pans on center rack of oven. │ │ Place 12-inch pan on lower rack of oven. │ │ Bake cakes until tester inserted into center comes out clean, about 35 minutes. │ │ Transfer cakes in pans to racks and cool completely. │ │ Mark 4-inch diameter circle on one 6-inch-diameter cardboard cake round. │ │ Cut out marked circle. │ │ Mark 7-inch-diameter circle on one 8-inch-diameter cardboard cake round. │ │ Cut out marked circle. │ │ Mark 11-inch-diameter circle on one 12-inch-diameter cardboard cake round. │ │ Cut out marked circle. │ │ Cut around sides of 5-inch-cake to loosen. │ │ Place 4-inch cardboard over pan. │ │ Hold cardboard and pan together; turn cake out onto cardboard. │ │ Peel off parchment.Wrap cakes on its cardboard in foil. │ │ Repeat turning out, peeling off parchment and wrapping cakes in foil, using 7-inch cardboard for 8-inch cake and 11-inch cardboard for 12-inch cake. │ │ Using remaining ingredients, make 1 more batch of cake batter and bake 3 more cake layers as described above. │ │ Cool cakes in pans. │ │ Cover cakes in pans tightly with foil. │ │ (Can be prepared ahead. │ │ Let stand at room temperature up to 1 day or double-wrap all cake layers and freeze up to 1 week. │ │ Bring cake layers to room temperature before using.) │ │ Place first 12-inch cake on its cardboard on work surface. │ │ Spread 2 3/4 cups ganache over top of cake and all the way to edge. │ │ Spread 2/3 cup jam over ganache, leaving 1/2-inch chocolate border at edge. │ │ Drop 1 3/4 cups white chocolate frosting by spoonfuls over jam. │ │ Gently spread frosting over jam, leaving 1/2-inch chocolate border at edge. │ │ Rub some cocoa powder over second 12-inch cardboard. │ │ Cut around sides of second 12-inch cake to loosen. │ │ Place cardboard, cocoa side down, over pan. │ │ Turn cake out onto cardboard. │ │ Peel off parchment. │ │ Carefully slide cake off cardboard and onto filling on first 12-inch cake. │ │ Refrigerate. │ │ Place first 8-inch cake on its cardboard on work surface. │ │ Spread 1 cup ganache over top all the way to edge. │ │ Spread 1/4 cup jam over, leaving 1/2-inch chocolate border at edge. │ │ Drop 1 cup white chocolate frosting by spoonfuls over jam. │ │ Gently spread frosting over jam, leaving 1/2-inch chocolate border at edge. │ │ Rub some cocoa over second 8-inch cardboard. │ │ Cut around sides of second 8-inch cake to loosen. │ │ Place cardboard, cocoa side down, over pan. │ │ Turn cake out onto cardboard. │ │ Peel off parchment. │ │ Slide cake off cardboard and onto filling on first 8-inch cake. │ │ Refrigerate. │ │ Place first 5-inch cake on its cardboard on work surface. │ │ Spread 1/2 cup ganache over top of cake and all the way to edge. │ │ Spread 2 tablespoons jam over, leaving 1/2-inch chocolate border at edge. │ │ Drop 1/3 cup white chocolate frosting by spoonfuls over jam. │ │ Gently spread frosting over jam, leaving 1/2-inch chocolate border at edge. │ │ Rub cocoa over second 6-inch cardboard. │ │ Cut around sides of second 5-inch cake to loosen. │ │ Place cardboard, cocoa side down, over pan. │ │ Turn cake out onto cardboard. │ │ Peel off parchment. │ │ Slide cake off cardboard and onto filling on first 5-inch cake. │ │ Chill all cakes 1 hour to set filling. │ │ Place 12-inch tiered cake on its cardboard on revolving cake stand. │ │ Spread 2 2/3 cups frosting over top and sides of cake as a first coat. │ │ Refrigerate cake. │ │ Place 8-inch tiered cake on its cardboard on cake stand. │ │ Spread 1 1/4 cups frosting over top and sides of cake as a first coat. │ │ Refrigerate cake. │ │ Place 5-inch tiered cake on its cardboard on cake stand. │ │ Spread 3/4 cup frosting over top and sides of cake as a first coat. │ │ Refrigerate all cakes until first coats of frosting set, about 1 hour. │ │ (Cakes can be made to this point up to 1 day ahead; cover and keep refrigerate.) │ │ Prepare second batch of frosting, using remaining frosting ingredients and following directions for first batch. │ │ Spoon 2 cups frosting into pastry bag fitted with small star tip. │ │ Place 12-inch cake on its cardboard on large flat platter. │ │ Place platter on cake stand. │ │ Using icing spatula, spread 2 1/2 cups frosting over top and sides of cake; smooth top. │ │ Using filled pastry bag, pipe decorative border around top edge of cake. │ │ Refrigerate cake on platter. │ │ Place 8-inch cake on its cardboard on cake stand. │ │ Using icing spatula, spread 1 1/2 cups frosting over top and sides of cake; smooth top. │ │ Using pastry bag, pipe decorative border around top edge of cake. │ │ Refrigerate cake on its cardboard. │ │ Place 5-inch cake on its cardboard on cake stand. │ │ Using icing spatula, spread 3/4 cup frosting over top and sides of cake; smooth top. │ │ Using pastry bag, pipe decorative border around top edge of cake, spooning more frosting into bag if necessary. │ │ Refrigerate cake on its cardboard. │ │ Keep all cakes refrigerated until frosting sets, about 2 hours. │ │ (Can be prepared 2 days ahead. │ │ Cover loosely; keep refrigerated.) │ │ Place 12-inch cake on platter on work surface. │ │ Press 1 wooden dowel straight down into and completely through center of cake. │ │ Mark dowel 1/4 inch above top of frosting. │ │ Remove dowel and cut with serrated knife at marked point. │ │ Cut 4 more dowels to same length. │ │ Press 1 cut dowel back into center of cake. │ │ Press remaining 4 cut dowels into cake, positioning 3 1/2 inches inward from cake edges and spacing evenly. │ │ Place 8-inch cake on its cardboard on work surface. │ │ Press 1 dowel straight down into and completely through center of cake. │ │ Mark dowel 1/4 inch above top of frosting. │ │ Remove dowel and cut with serrated knife at marked point. │ │ Cut 3 more dowels to same length. │ │ Press 1 cut dowel back into center of cake. │ │ Press remaining 3 cut dowels into cake, positioning 2 1/2 inches inward from edges and spacing evenly. │ │ Using large metal spatula as aid, place 8-inch cake on its cardboard atop dowels in 12-inch cake, centering carefully. │ │ Gently place 5-inch cake on its cardboard atop dowels in 8-inch cake, centering carefully. │ │ Using citrus stripper, cut long strips of orange peel from oranges. │ │ Cut strips into long segments. │ │ To make orange peel coils, wrap peel segment around handle of wooden spoon; gently slide peel off handle so that peel keeps coiled shape. │ │ Garnish cake with orange peel coils, ivy or mint sprigs, and some berries. │ │ (Assembled cake can be made up to 8 hours ahead. │ │ Let stand at cool room temperature.) │ │ Remove top and middle cake tiers. │ │ Remove dowels from cakes. │ │ Cut top and middle cakes into slices. │ │ To cut 12-inch cake: Starting 3 inches inward from edge and inserting knife straight down, cut through from top to bottom to make 6-inch-diameter circle in center of cake. │ │ Cut outer portion of cake into slices; cut inner portion into slices and serve with strawberries. │ └─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ 126 rows in set. Elapsed: 0.011 sec. Processed 8.19 thousand rows, 5.34 MB (737.75 thousand rows/s., 480.59 MB/s.)  "},{"title":"Online Playground​","type":1,"pageTitle":"Recipes Dataset","url":"docs/en/example-datasets/recipes#online-playground","content":"The dataset is also available in the Online Playground. Original article "},{"title":"UK Property Price Paid","type":0,"sectionRef":"#","url":"docs/en/example-datasets/uk-price-paid","content":"","keywords":""},{"title":"Download the Dataset​","type":1,"pageTitle":"UK Property Price Paid","url":"docs/en/example-datasets/uk-price-paid#download-dataset","content":"Run the command: wget http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-complete.csv  Download will take about 2 minutes with good internet connection. "},{"title":"Create the Table​","type":1,"pageTitle":"UK Property Price Paid","url":"docs/en/example-datasets/uk-price-paid#create-table","content":"CREATE TABLE uk_price_paid ( price UInt32, date Date, postcode1 LowCardinality(String), postcode2 LowCardinality(String), type Enum8('terraced' = 1, 'semi-detached' = 2, 'detached' = 3, 'flat' = 4, 'other' = 0), is_new UInt8, duration Enum8('freehold' = 1, 'leasehold' = 2, 'unknown' = 0), addr1 String, addr2 String, street LowCardinality(String), locality LowCardinality(String), town LowCardinality(String), district LowCardinality(String), county LowCardinality(String), category UInt8 ) ENGINE = MergeTree ORDER BY (postcode1, postcode2, addr1, addr2);  "},{"title":"Preprocess and Import Data​","type":1,"pageTitle":"UK Property Price Paid","url":"docs/en/example-datasets/uk-price-paid#preprocess-import-data","content":"We will use clickhouse-local tool for data preprocessing and clickhouse-client to upload it. In this example, we define the structure of source data from the CSV file and specify a query to preprocess the data with clickhouse-local. The preprocessing is: splitting the postcode to two different columns postcode1 and postcode2 that is better for storage and queries;coverting the time field to date as it only contains 00:00 time;ignoring the UUid field because we don't need it for analysis;transforming type and duration to more readable Enum fields with function transform;transforming is_new and category fields from single-character string (Y/N and A/B) to UInt8 field with 0 and 1. Preprocessed data is piped directly to clickhouse-client to be inserted into ClickHouse table in streaming fashion. clickhouse-local --input-format CSV --structure ' uuid String, price UInt32, time DateTime, postcode String, a String, b String, c String, addr1 String, addr2 String, street String, locality String, town String, district String, county String, d String, e String ' --query &quot; WITH splitByChar(' ', postcode) AS p SELECT price, toDate(time) AS date, p[1] AS postcode1, p[2] AS postcode2, transform(a, ['T', 'S', 'D', 'F', 'O'], ['terraced', 'semi-detached', 'detached', 'flat', 'other']) AS type, b = 'Y' AS is_new, transform(c, ['F', 'L', 'U'], ['freehold', 'leasehold', 'unknown']) AS duration, addr1, addr2, street, locality, town, district, county, d = 'B' AS category FROM table&quot; --date_time_input_format best_effort &lt; pp-complete.csv | clickhouse-client --query &quot;INSERT INTO uk_price_paid FORMAT TSV&quot;  It will take about 40 seconds. "},{"title":"Validate the Data​","type":1,"pageTitle":"UK Property Price Paid","url":"docs/en/example-datasets/uk-price-paid#validate-data","content":"Query: SELECT count() FROM uk_price_paid;  Result: ┌──count()─┐ │ 26321785 │ └──────────┘  The size of dataset in ClickHouse is just 278 MiB, check it. Query: SELECT formatReadableSize(total_bytes) FROM system.tables WHERE name = 'uk_price_paid';  Result: ┌─formatReadableSize(total_bytes)─┐ │ 278.80 MiB │ └─────────────────────────────────┘  "},{"title":"Run Some Queries​","type":1,"pageTitle":"UK Property Price Paid","url":"docs/en/example-datasets/uk-price-paid#run-queries","content":""},{"title":"Query 1. Average Price Per Year​","type":1,"pageTitle":"UK Property Price Paid","url":"docs/en/example-datasets/uk-price-paid#average-price","content":"Query: SELECT toYear(date) AS year, round(avg(price)) AS price, bar(price, 0, 1000000, 80) FROM uk_price_paid GROUP BY year ORDER BY year;  Result: ┌─year─┬──price─┬─bar(round(avg(price)), 0, 1000000, 80)─┐ │ 1995 │ 67932 │ █████▍ │ │ 1996 │ 71505 │ █████▋ │ │ 1997 │ 78532 │ ██████▎ │ │ 1998 │ 85436 │ ██████▋ │ │ 1999 │ 96037 │ ███████▋ │ │ 2000 │ 107479 │ ████████▌ │ │ 2001 │ 118885 │ █████████▌ │ │ 2002 │ 137941 │ ███████████ │ │ 2003 │ 155889 │ ████████████▍ │ │ 2004 │ 178885 │ ██████████████▎ │ │ 2005 │ 189351 │ ███████████████▏ │ │ 2006 │ 203528 │ ████████████████▎ │ │ 2007 │ 219378 │ █████████████████▌ │ │ 2008 │ 217056 │ █████████████████▎ │ │ 2009 │ 213419 │ █████████████████ │ │ 2010 │ 236109 │ ██████████████████▊ │ │ 2011 │ 232805 │ ██████████████████▌ │ │ 2012 │ 238367 │ ███████████████████ │ │ 2013 │ 256931 │ ████████████████████▌ │ │ 2014 │ 279915 │ ██████████████████████▍ │ │ 2015 │ 297266 │ ███████████████████████▋ │ │ 2016 │ 313201 │ █████████████████████████ │ │ 2017 │ 346097 │ ███████████████████████████▋ │ │ 2018 │ 350116 │ ████████████████████████████ │ │ 2019 │ 351013 │ ████████████████████████████ │ │ 2020 │ 369420 │ █████████████████████████████▌ │ │ 2021 │ 386903 │ ██████████████████████████████▊ │ └──────┴────────┴────────────────────────────────────────┘  "},{"title":"Query 2. Average Price per Year in London​","type":1,"pageTitle":"UK Property Price Paid","url":"docs/en/example-datasets/uk-price-paid#average-price-london","content":"Query: SELECT toYear(date) AS year, round(avg(price)) AS price, bar(price, 0, 2000000, 100) FROM uk_price_paid WHERE town = 'LONDON' GROUP BY year ORDER BY year;  Result: ┌─year─┬───price─┬─bar(round(avg(price)), 0, 2000000, 100)───────────────┐ │ 1995 │ 109116 │ █████▍ │ │ 1996 │ 118667 │ █████▊ │ │ 1997 │ 136518 │ ██████▋ │ │ 1998 │ 152983 │ ███████▋ │ │ 1999 │ 180637 │ █████████ │ │ 2000 │ 215838 │ ██████████▋ │ │ 2001 │ 232994 │ ███████████▋ │ │ 2002 │ 263670 │ █████████████▏ │ │ 2003 │ 278394 │ █████████████▊ │ │ 2004 │ 304666 │ ███████████████▏ │ │ 2005 │ 322875 │ ████████████████▏ │ │ 2006 │ 356191 │ █████████████████▋ │ │ 2007 │ 404054 │ ████████████████████▏ │ │ 2008 │ 420741 │ █████████████████████ │ │ 2009 │ 427753 │ █████████████████████▍ │ │ 2010 │ 480306 │ ████████████████████████ │ │ 2011 │ 496274 │ ████████████████████████▋ │ │ 2012 │ 519442 │ █████████████████████████▊ │ │ 2013 │ 616212 │ ██████████████████████████████▋ │ │ 2014 │ 724154 │ ████████████████████████████████████▏ │ │ 2015 │ 792129 │ ███████████████████████████████████████▌ │ │ 2016 │ 843655 │ ██████████████████████████████████████████▏ │ │ 2017 │ 982642 │ █████████████████████████████████████████████████▏ │ │ 2018 │ 1016835 │ ██████████████████████████████████████████████████▋ │ │ 2019 │ 1042849 │ ████████████████████████████████████████████████████▏ │ │ 2020 │ 1011889 │ ██████████████████████████████████████████████████▌ │ │ 2021 │ 960343 │ ████████████████████████████████████████████████ │ └──────┴─────────┴───────────────────────────────────────────────────────┘  Something happened in 2013. I don't have a clue. Maybe you have a clue what happened in 2020? "},{"title":"Query 3. The Most Expensive Neighborhoods​","type":1,"pageTitle":"UK Property Price Paid","url":"docs/en/example-datasets/uk-price-paid#most-expensive-neighborhoods","content":"Query: SELECT town, district, count() AS c, round(avg(price)) AS price, bar(price, 0, 5000000, 100) FROM uk_price_paid WHERE date &gt;= '2020-01-01' GROUP BY town, district HAVING c &gt;= 100 ORDER BY price DESC LIMIT 100;  Result:  ┌─town─────────────────┬─district───────────────┬────c─┬───price─┬─bar(round(avg(price)), 0, 5000000, 100)────────────────────────────┐ │ LONDON │ CITY OF WESTMINSTER │ 3606 │ 3280239 │ █████████████████████████████████████████████████████████████████▌ │ │ LONDON │ CITY OF LONDON │ 274 │ 3160502 │ ███████████████████████████████████████████████████████████████▏ │ │ LONDON │ KENSINGTON AND CHELSEA │ 2550 │ 2308478 │ ██████████████████████████████████████████████▏ │ │ LEATHERHEAD │ ELMBRIDGE │ 114 │ 1897407 │ █████████████████████████████████████▊ │ │ LONDON │ CAMDEN │ 3033 │ 1805404 │ ████████████████████████████████████ │ │ VIRGINIA WATER │ RUNNYMEDE │ 156 │ 1753247 │ ███████████████████████████████████ │ │ WINDLESHAM │ SURREY HEATH │ 108 │ 1677613 │ █████████████████████████████████▌ │ │ THORNTON HEATH │ CROYDON │ 546 │ 1671721 │ █████████████████████████████████▍ │ │ BARNET │ ENFIELD │ 124 │ 1505840 │ ██████████████████████████████ │ │ COBHAM │ ELMBRIDGE │ 387 │ 1237250 │ ████████████████████████▋ │ │ LONDON │ ISLINGTON │ 2668 │ 1236980 │ ████████████████████████▋ │ │ OXFORD │ SOUTH OXFORDSHIRE │ 321 │ 1220907 │ ████████████████████████▍ │ │ LONDON │ RICHMOND UPON THAMES │ 704 │ 1215551 │ ████████████████████████▎ │ │ LONDON │ HOUNSLOW │ 671 │ 1207493 │ ████████████████████████▏ │ │ ASCOT │ WINDSOR AND MAIDENHEAD │ 407 │ 1183299 │ ███████████████████████▋ │ │ BEACONSFIELD │ BUCKINGHAMSHIRE │ 330 │ 1175615 │ ███████████████████████▌ │ │ RICHMOND │ RICHMOND UPON THAMES │ 874 │ 1110444 │ ██████████████████████▏ │ │ LONDON │ HAMMERSMITH AND FULHAM │ 3086 │ 1053983 │ █████████████████████ │ │ SURBITON │ ELMBRIDGE │ 100 │ 1011800 │ ████████████████████▏ │ │ RADLETT │ HERTSMERE │ 283 │ 1011712 │ ████████████████████▏ │ │ SALCOMBE │ SOUTH HAMS │ 127 │ 1011624 │ ████████████████████▏ │ │ WEYBRIDGE │ ELMBRIDGE │ 655 │ 1007265 │ ████████████████████▏ │ │ ESHER │ ELMBRIDGE │ 485 │ 986581 │ ███████████████████▋ │ │ LEATHERHEAD │ GUILDFORD │ 202 │ 977320 │ ███████████████████▌ │ │ BURFORD │ WEST OXFORDSHIRE │ 111 │ 966893 │ ███████████████████▎ │ │ BROCKENHURST │ NEW FOREST │ 129 │ 956675 │ ███████████████████▏ │ │ HINDHEAD │ WAVERLEY │ 137 │ 953753 │ ███████████████████ │ │ GERRARDS CROSS │ BUCKINGHAMSHIRE │ 419 │ 951121 │ ███████████████████ │ │ EAST MOLESEY │ ELMBRIDGE │ 192 │ 936769 │ ██████████████████▋ │ │ CHALFONT ST GILES │ BUCKINGHAMSHIRE │ 146 │ 925515 │ ██████████████████▌ │ │ LONDON │ TOWER HAMLETS │ 4388 │ 918304 │ ██████████████████▎ │ │ OLNEY │ MILTON KEYNES │ 235 │ 910646 │ ██████████████████▏ │ │ HENLEY-ON-THAMES │ SOUTH OXFORDSHIRE │ 540 │ 902418 │ ██████████████████ │ │ LONDON │ SOUTHWARK │ 3885 │ 892997 │ █████████████████▋ │ │ KINGSTON UPON THAMES │ KINGSTON UPON THAMES │ 960 │ 885969 │ █████████████████▋ │ │ LONDON │ EALING │ 2658 │ 871755 │ █████████████████▍ │ │ CRANBROOK │ TUNBRIDGE WELLS │ 431 │ 862348 │ █████████████████▏ │ │ LONDON │ MERTON │ 2099 │ 859118 │ █████████████████▏ │ │ BELVEDERE │ BEXLEY │ 346 │ 842423 │ ████████████████▋ │ │ GUILDFORD │ WAVERLEY │ 143 │ 841277 │ ████████████████▋ │ │ HARPENDEN │ ST ALBANS │ 657 │ 841216 │ ████████████████▋ │ │ LONDON │ HACKNEY │ 3307 │ 837090 │ ████████████████▋ │ │ LONDON │ WANDSWORTH │ 6566 │ 832663 │ ████████████████▋ │ │ MAIDENHEAD │ BUCKINGHAMSHIRE │ 123 │ 824299 │ ████████████████▍ │ │ KINGS LANGLEY │ DACORUM │ 145 │ 821331 │ ████████████████▍ │ │ BERKHAMSTED │ DACORUM │ 543 │ 818415 │ ████████████████▎ │ │ GREAT MISSENDEN │ BUCKINGHAMSHIRE │ 226 │ 802807 │ ████████████████ │ │ BILLINGSHURST │ CHICHESTER │ 144 │ 797829 │ ███████████████▊ │ │ WOKING │ GUILDFORD │ 176 │ 793494 │ ███████████████▋ │ │ STOCKBRIDGE │ TEST VALLEY │ 178 │ 793269 │ ███████████████▋ │ │ EPSOM │ REIGATE AND BANSTEAD │ 172 │ 791862 │ ███████████████▋ │ │ TONBRIDGE │ TUNBRIDGE WELLS │ 360 │ 787876 │ ███████████████▋ │ │ TEDDINGTON │ RICHMOND UPON THAMES │ 595 │ 786492 │ ███████████████▋ │ │ TWICKENHAM │ RICHMOND UPON THAMES │ 1155 │ 786193 │ ███████████████▋ │ │ LYNDHURST │ NEW FOREST │ 102 │ 785593 │ ███████████████▋ │ │ LONDON │ LAMBETH │ 5228 │ 774574 │ ███████████████▍ │ │ LONDON │ BARNET │ 3955 │ 773259 │ ███████████████▍ │ │ OXFORD │ VALE OF WHITE HORSE │ 353 │ 772088 │ ███████████████▍ │ │ TONBRIDGE │ MAIDSTONE │ 305 │ 770740 │ ███████████████▍ │ │ LUTTERWORTH │ HARBOROUGH │ 538 │ 768634 │ ███████████████▎ │ │ WOODSTOCK │ WEST OXFORDSHIRE │ 140 │ 766037 │ ███████████████▎ │ │ MIDHURST │ CHICHESTER │ 257 │ 764815 │ ███████████████▎ │ │ MARLOW │ BUCKINGHAMSHIRE │ 327 │ 761876 │ ███████████████▏ │ │ LONDON │ NEWHAM │ 3237 │ 761784 │ ███████████████▏ │ │ ALDERLEY EDGE │ CHESHIRE EAST │ 178 │ 757318 │ ███████████████▏ │ │ LUTON │ CENTRAL BEDFORDSHIRE │ 212 │ 754283 │ ███████████████ │ │ PETWORTH │ CHICHESTER │ 154 │ 754220 │ ███████████████ │ │ ALRESFORD │ WINCHESTER │ 219 │ 752718 │ ███████████████ │ │ POTTERS BAR │ WELWYN HATFIELD │ 174 │ 748465 │ ██████████████▊ │ │ HASLEMERE │ CHICHESTER │ 128 │ 746907 │ ██████████████▊ │ │ TADWORTH │ REIGATE AND BANSTEAD │ 502 │ 743252 │ ██████████████▋ │ │ THAMES DITTON │ ELMBRIDGE │ 244 │ 741913 │ ██████████████▋ │ │ REIGATE │ REIGATE AND BANSTEAD │ 581 │ 738198 │ ██████████████▋ │ │ BOURNE END │ BUCKINGHAMSHIRE │ 138 │ 735190 │ ██████████████▋ │ │ SEVENOAKS │ SEVENOAKS │ 1156 │ 730018 │ ██████████████▌ │ │ OXTED │ TANDRIDGE │ 336 │ 729123 │ ██████████████▌ │ │ INGATESTONE │ BRENTWOOD │ 166 │ 728103 │ ██████████████▌ │ │ LONDON │ BRENT │ 2079 │ 720605 │ ██████████████▍ │ │ LONDON │ HARINGEY │ 3216 │ 717780 │ ██████████████▎ │ │ PURLEY │ CROYDON │ 575 │ 716108 │ ██████████████▎ │ │ WELWYN │ WELWYN HATFIELD │ 222 │ 710603 │ ██████████████▏ │ │ RICKMANSWORTH │ THREE RIVERS │ 798 │ 704571 │ ██████████████ │ │ BANSTEAD │ REIGATE AND BANSTEAD │ 401 │ 701293 │ ██████████████ │ │ CHIGWELL │ EPPING FOREST │ 261 │ 701203 │ ██████████████ │ │ PINNER │ HARROW │ 528 │ 698885 │ █████████████▊ │ │ HASLEMERE │ WAVERLEY │ 280 │ 696659 │ █████████████▊ │ │ SLOUGH │ BUCKINGHAMSHIRE │ 396 │ 694917 │ █████████████▊ │ │ WALTON-ON-THAMES │ ELMBRIDGE │ 946 │ 692395 │ █████████████▋ │ │ READING │ SOUTH OXFORDSHIRE │ 318 │ 691988 │ █████████████▋ │ │ NORTHWOOD │ HILLINGDON │ 271 │ 690643 │ █████████████▋ │ │ FELTHAM │ HOUNSLOW │ 763 │ 688595 │ █████████████▋ │ │ ASHTEAD │ MOLE VALLEY │ 303 │ 687923 │ █████████████▋ │ │ BARNET │ BARNET │ 975 │ 686980 │ █████████████▋ │ │ WOKING │ SURREY HEATH │ 283 │ 686669 │ █████████████▋ │ │ MALMESBURY │ WILTSHIRE │ 323 │ 683324 │ █████████████▋ │ │ AMERSHAM │ BUCKINGHAMSHIRE │ 496 │ 680962 │ █████████████▌ │ │ CHISLEHURST │ BROMLEY │ 430 │ 680209 │ █████████████▌ │ │ HYTHE │ FOLKESTONE AND HYTHE │ 490 │ 676908 │ █████████████▌ │ │ MAYFIELD │ WEALDEN │ 101 │ 676210 │ █████████████▌ │ │ ASCOT │ BRACKNELL FOREST │ 168 │ 676004 │ █████████████▌ │ └──────────────────────┴────────────────────────┴──────┴─────────┴────────────────────────────────────────────────────────────────────┘  "},{"title":"Let's Speed Up Queries Using Projections​","type":1,"pageTitle":"UK Property Price Paid","url":"docs/en/example-datasets/uk-price-paid#speedup-with-projections","content":"Projections allow to improve queries speed by storing pre-aggregated data. "},{"title":"Build a Projection​","type":1,"pageTitle":"UK Property Price Paid","url":"docs/en/example-datasets/uk-price-paid#build-projection","content":"Create an aggregate projection by dimensions toYear(date), district, town: ALTER TABLE uk_price_paid ADD PROJECTION projection_by_year_district_town ( SELECT toYear(date), district, town, avg(price), sum(price), count() GROUP BY toYear(date), district, town );  Populate the projection for existing data (without it projection will be created for only newly inserted data): ALTER TABLE uk_price_paid MATERIALIZE PROJECTION projection_by_year_district_town SETTINGS mutations_sync = 1;  "},{"title":"Test Performance​","type":1,"pageTitle":"UK Property Price Paid","url":"docs/en/example-datasets/uk-price-paid#test-performance","content":"Let's run the same 3 queries. Enable projections for selects: SET allow_experimental_projection_optimization = 1;  "},{"title":"Query 1. Average Price Per Year​","type":1,"pageTitle":"UK Property Price Paid","url":"docs/en/example-datasets/uk-price-paid#average-price-projections","content":"Query: SELECT toYear(date) AS year, round(avg(price)) AS price, bar(price, 0, 1000000, 80) FROM uk_price_paid GROUP BY year ORDER BY year ASC;  Result: ┌─year─┬──price─┬─bar(round(avg(price)), 0, 1000000, 80)─┐ │ 1995 │ 67932 │ █████▍ │ │ 1996 │ 71505 │ █████▋ │ │ 1997 │ 78532 │ ██████▎ │ │ 1998 │ 85436 │ ██████▋ │ │ 1999 │ 96037 │ ███████▋ │ │ 2000 │ 107479 │ ████████▌ │ │ 2001 │ 118885 │ █████████▌ │ │ 2002 │ 137941 │ ███████████ │ │ 2003 │ 155889 │ ████████████▍ │ │ 2004 │ 178885 │ ██████████████▎ │ │ 2005 │ 189351 │ ███████████████▏ │ │ 2006 │ 203528 │ ████████████████▎ │ │ 2007 │ 219378 │ █████████████████▌ │ │ 2008 │ 217056 │ █████████████████▎ │ │ 2009 │ 213419 │ █████████████████ │ │ 2010 │ 236109 │ ██████████████████▊ │ │ 2011 │ 232805 │ ██████████████████▌ │ │ 2012 │ 238367 │ ███████████████████ │ │ 2013 │ 256931 │ ████████████████████▌ │ │ 2014 │ 279915 │ ██████████████████████▍ │ │ 2015 │ 297266 │ ███████████████████████▋ │ │ 2016 │ 313201 │ █████████████████████████ │ │ 2017 │ 346097 │ ███████████████████████████▋ │ │ 2018 │ 350116 │ ████████████████████████████ │ │ 2019 │ 351013 │ ████████████████████████████ │ │ 2020 │ 369420 │ █████████████████████████████▌ │ │ 2021 │ 386903 │ ██████████████████████████████▊ │ └──────┴────────┴────────────────────────────────────────┘  "},{"title":"Query 2. Average Price Per Year in London​","type":1,"pageTitle":"UK Property Price Paid","url":"docs/en/example-datasets/uk-price-paid#average-price-london-projections","content":"Query: SELECT toYear(date) AS year, round(avg(price)) AS price, bar(price, 0, 2000000, 100) FROM uk_price_paid WHERE town = 'LONDON' GROUP BY year ORDER BY year ASC;  Result: ┌─year─┬───price─┬─bar(round(avg(price)), 0, 2000000, 100)───────────────┐ │ 1995 │ 109116 │ █████▍ │ │ 1996 │ 118667 │ █████▊ │ │ 1997 │ 136518 │ ██████▋ │ │ 1998 │ 152983 │ ███████▋ │ │ 1999 │ 180637 │ █████████ │ │ 2000 │ 215838 │ ██████████▋ │ │ 2001 │ 232994 │ ███████████▋ │ │ 2002 │ 263670 │ █████████████▏ │ │ 2003 │ 278394 │ █████████████▊ │ │ 2004 │ 304666 │ ███████████████▏ │ │ 2005 │ 322875 │ ████████████████▏ │ │ 2006 │ 356191 │ █████████████████▋ │ │ 2007 │ 404054 │ ████████████████████▏ │ │ 2008 │ 420741 │ █████████████████████ │ │ 2009 │ 427753 │ █████████████████████▍ │ │ 2010 │ 480306 │ ████████████████████████ │ │ 2011 │ 496274 │ ████████████████████████▋ │ │ 2012 │ 519442 │ █████████████████████████▊ │ │ 2013 │ 616212 │ ██████████████████████████████▋ │ │ 2014 │ 724154 │ ████████████████████████████████████▏ │ │ 2015 │ 792129 │ ███████████████████████████████████████▌ │ │ 2016 │ 843655 │ ██████████████████████████████████████████▏ │ │ 2017 │ 982642 │ █████████████████████████████████████████████████▏ │ │ 2018 │ 1016835 │ ██████████████████████████████████████████████████▋ │ │ 2019 │ 1042849 │ ████████████████████████████████████████████████████▏ │ │ 2020 │ 1011889 │ ██████████████████████████████████████████████████▌ │ │ 2021 │ 960343 │ ████████████████████████████████████████████████ │ └──────┴─────────┴───────────────────────────────────────────────────────┘  "},{"title":"Query 3. The Most Expensive Neighborhoods​","type":1,"pageTitle":"UK Property Price Paid","url":"docs/en/example-datasets/uk-price-paid#most-expensive-neighborhoods-projections","content":"The condition (date &gt;= '2020-01-01') needs to be modified to match projection dimension (toYear(date) &gt;= 2020). Query: SELECT town, district, count() AS c, round(avg(price)) AS price, bar(price, 0, 5000000, 100) FROM uk_price_paid WHERE toYear(date) &gt;= 2020 GROUP BY town, district HAVING c &gt;= 100 ORDER BY price DESC LIMIT 100;  Result: ┌─town─────────────────┬─district───────────────┬────c─┬───price─┬─bar(round(avg(price)), 0, 5000000, 100)────────────────────────────┐ │ LONDON │ CITY OF WESTMINSTER │ 3606 │ 3280239 │ █████████████████████████████████████████████████████████████████▌ │ │ LONDON │ CITY OF LONDON │ 274 │ 3160502 │ ███████████████████████████████████████████████████████████████▏ │ │ LONDON │ KENSINGTON AND CHELSEA │ 2550 │ 2308478 │ ██████████████████████████████████████████████▏ │ │ LEATHERHEAD │ ELMBRIDGE │ 114 │ 1897407 │ █████████████████████████████████████▊ │ │ LONDON │ CAMDEN │ 3033 │ 1805404 │ ████████████████████████████████████ │ │ VIRGINIA WATER │ RUNNYMEDE │ 156 │ 1753247 │ ███████████████████████████████████ │ │ WINDLESHAM │ SURREY HEATH │ 108 │ 1677613 │ █████████████████████████████████▌ │ │ THORNTON HEATH │ CROYDON │ 546 │ 1671721 │ █████████████████████████████████▍ │ │ BARNET │ ENFIELD │ 124 │ 1505840 │ ██████████████████████████████ │ │ COBHAM │ ELMBRIDGE │ 387 │ 1237250 │ ████████████████████████▋ │ │ LONDON │ ISLINGTON │ 2668 │ 1236980 │ ████████████████████████▋ │ │ OXFORD │ SOUTH OXFORDSHIRE │ 321 │ 1220907 │ ████████████████████████▍ │ │ LONDON │ RICHMOND UPON THAMES │ 704 │ 1215551 │ ████████████████████████▎ │ │ LONDON │ HOUNSLOW │ 671 │ 1207493 │ ████████████████████████▏ │ │ ASCOT │ WINDSOR AND MAIDENHEAD │ 407 │ 1183299 │ ███████████████████████▋ │ │ BEACONSFIELD │ BUCKINGHAMSHIRE │ 330 │ 1175615 │ ███████████████████████▌ │ │ RICHMOND │ RICHMOND UPON THAMES │ 874 │ 1110444 │ ██████████████████████▏ │ │ LONDON │ HAMMERSMITH AND FULHAM │ 3086 │ 1053983 │ █████████████████████ │ │ SURBITON │ ELMBRIDGE │ 100 │ 1011800 │ ████████████████████▏ │ │ RADLETT │ HERTSMERE │ 283 │ 1011712 │ ████████████████████▏ │ │ SALCOMBE │ SOUTH HAMS │ 127 │ 1011624 │ ████████████████████▏ │ │ WEYBRIDGE │ ELMBRIDGE │ 655 │ 1007265 │ ████████████████████▏ │ │ ESHER │ ELMBRIDGE │ 485 │ 986581 │ ███████████████████▋ │ │ LEATHERHEAD │ GUILDFORD │ 202 │ 977320 │ ███████████████████▌ │ │ BURFORD │ WEST OXFORDSHIRE │ 111 │ 966893 │ ███████████████████▎ │ │ BROCKENHURST │ NEW FOREST │ 129 │ 956675 │ ███████████████████▏ │ │ HINDHEAD │ WAVERLEY │ 137 │ 953753 │ ███████████████████ │ │ GERRARDS CROSS │ BUCKINGHAMSHIRE │ 419 │ 951121 │ ███████████████████ │ │ EAST MOLESEY │ ELMBRIDGE │ 192 │ 936769 │ ██████████████████▋ │ │ CHALFONT ST GILES │ BUCKINGHAMSHIRE │ 146 │ 925515 │ ██████████████████▌ │ │ LONDON │ TOWER HAMLETS │ 4388 │ 918304 │ ██████████████████▎ │ │ OLNEY │ MILTON KEYNES │ 235 │ 910646 │ ██████████████████▏ │ │ HENLEY-ON-THAMES │ SOUTH OXFORDSHIRE │ 540 │ 902418 │ ██████████████████ │ │ LONDON │ SOUTHWARK │ 3885 │ 892997 │ █████████████████▋ │ │ KINGSTON UPON THAMES │ KINGSTON UPON THAMES │ 960 │ 885969 │ █████████████████▋ │ │ LONDON │ EALING │ 2658 │ 871755 │ █████████████████▍ │ │ CRANBROOK │ TUNBRIDGE WELLS │ 431 │ 862348 │ █████████████████▏ │ │ LONDON │ MERTON │ 2099 │ 859118 │ █████████████████▏ │ │ BELVEDERE │ BEXLEY │ 346 │ 842423 │ ████████████████▋ │ │ GUILDFORD │ WAVERLEY │ 143 │ 841277 │ ████████████████▋ │ │ HARPENDEN │ ST ALBANS │ 657 │ 841216 │ ████████████████▋ │ │ LONDON │ HACKNEY │ 3307 │ 837090 │ ████████████████▋ │ │ LONDON │ WANDSWORTH │ 6566 │ 832663 │ ████████████████▋ │ │ MAIDENHEAD │ BUCKINGHAMSHIRE │ 123 │ 824299 │ ████████████████▍ │ │ KINGS LANGLEY │ DACORUM │ 145 │ 821331 │ ████████████████▍ │ │ BERKHAMSTED │ DACORUM │ 543 │ 818415 │ ████████████████▎ │ │ GREAT MISSENDEN │ BUCKINGHAMSHIRE │ 226 │ 802807 │ ████████████████ │ │ BILLINGSHURST │ CHICHESTER │ 144 │ 797829 │ ███████████████▊ │ │ WOKING │ GUILDFORD │ 176 │ 793494 │ ███████████████▋ │ │ STOCKBRIDGE │ TEST VALLEY │ 178 │ 793269 │ ███████████████▋ │ │ EPSOM │ REIGATE AND BANSTEAD │ 172 │ 791862 │ ███████████████▋ │ │ TONBRIDGE │ TUNBRIDGE WELLS │ 360 │ 787876 │ ███████████████▋ │ │ TEDDINGTON │ RICHMOND UPON THAMES │ 595 │ 786492 │ ███████████████▋ │ │ TWICKENHAM │ RICHMOND UPON THAMES │ 1155 │ 786193 │ ███████████████▋ │ │ LYNDHURST │ NEW FOREST │ 102 │ 785593 │ ███████████████▋ │ │ LONDON │ LAMBETH │ 5228 │ 774574 │ ███████████████▍ │ │ LONDON │ BARNET │ 3955 │ 773259 │ ███████████████▍ │ │ OXFORD │ VALE OF WHITE HORSE │ 353 │ 772088 │ ███████████████▍ │ │ TONBRIDGE │ MAIDSTONE │ 305 │ 770740 │ ███████████████▍ │ │ LUTTERWORTH │ HARBOROUGH │ 538 │ 768634 │ ███████████████▎ │ │ WOODSTOCK │ WEST OXFORDSHIRE │ 140 │ 766037 │ ███████████████▎ │ │ MIDHURST │ CHICHESTER │ 257 │ 764815 │ ███████████████▎ │ │ MARLOW │ BUCKINGHAMSHIRE │ 327 │ 761876 │ ███████████████▏ │ │ LONDON │ NEWHAM │ 3237 │ 761784 │ ███████████████▏ │ │ ALDERLEY EDGE │ CHESHIRE EAST │ 178 │ 757318 │ ███████████████▏ │ │ LUTON │ CENTRAL BEDFORDSHIRE │ 212 │ 754283 │ ███████████████ │ │ PETWORTH │ CHICHESTER │ 154 │ 754220 │ ███████████████ │ │ ALRESFORD │ WINCHESTER │ 219 │ 752718 │ ███████████████ │ │ POTTERS BAR │ WELWYN HATFIELD │ 174 │ 748465 │ ██████████████▊ │ │ HASLEMERE │ CHICHESTER │ 128 │ 746907 │ ██████████████▊ │ │ TADWORTH │ REIGATE AND BANSTEAD │ 502 │ 743252 │ ██████████████▋ │ │ THAMES DITTON │ ELMBRIDGE │ 244 │ 741913 │ ██████████████▋ │ │ REIGATE │ REIGATE AND BANSTEAD │ 581 │ 738198 │ ██████████████▋ │ │ BOURNE END │ BUCKINGHAMSHIRE │ 138 │ 735190 │ ██████████████▋ │ │ SEVENOAKS │ SEVENOAKS │ 1156 │ 730018 │ ██████████████▌ │ │ OXTED │ TANDRIDGE │ 336 │ 729123 │ ██████████████▌ │ │ INGATESTONE │ BRENTWOOD │ 166 │ 728103 │ ██████████████▌ │ │ LONDON │ BRENT │ 2079 │ 720605 │ ██████████████▍ │ │ LONDON │ HARINGEY │ 3216 │ 717780 │ ██████████████▎ │ │ PURLEY │ CROYDON │ 575 │ 716108 │ ██████████████▎ │ │ WELWYN │ WELWYN HATFIELD │ 222 │ 710603 │ ██████████████▏ │ │ RICKMANSWORTH │ THREE RIVERS │ 798 │ 704571 │ ██████████████ │ │ BANSTEAD │ REIGATE AND BANSTEAD │ 401 │ 701293 │ ██████████████ │ │ CHIGWELL │ EPPING FOREST │ 261 │ 701203 │ ██████████████ │ │ PINNER │ HARROW │ 528 │ 698885 │ █████████████▊ │ │ HASLEMERE │ WAVERLEY │ 280 │ 696659 │ █████████████▊ │ │ SLOUGH │ BUCKINGHAMSHIRE │ 396 │ 694917 │ █████████████▊ │ │ WALTON-ON-THAMES │ ELMBRIDGE │ 946 │ 692395 │ █████████████▋ │ │ READING │ SOUTH OXFORDSHIRE │ 318 │ 691988 │ █████████████▋ │ │ NORTHWOOD │ HILLINGDON │ 271 │ 690643 │ █████████████▋ │ │ FELTHAM │ HOUNSLOW │ 763 │ 688595 │ █████████████▋ │ │ ASHTEAD │ MOLE VALLEY │ 303 │ 687923 │ █████████████▋ │ │ BARNET │ BARNET │ 975 │ 686980 │ █████████████▋ │ │ WOKING │ SURREY HEATH │ 283 │ 686669 │ █████████████▋ │ │ MALMESBURY │ WILTSHIRE │ 323 │ 683324 │ █████████████▋ │ │ AMERSHAM │ BUCKINGHAMSHIRE │ 496 │ 680962 │ █████████████▌ │ │ CHISLEHURST │ BROMLEY │ 430 │ 680209 │ █████████████▌ │ │ HYTHE │ FOLKESTONE AND HYTHE │ 490 │ 676908 │ █████████████▌ │ │ MAYFIELD │ WEALDEN │ 101 │ 676210 │ █████████████▌ │ │ ASCOT │ BRACKNELL FOREST │ 168 │ 676004 │ █████████████▌ │ └──────────────────────┴────────────────────────┴──────┴─────────┴────────────────────────────────────────────────────────────────────┘  "},{"title":"Summary​","type":1,"pageTitle":"UK Property Price Paid","url":"docs/en/example-datasets/uk-price-paid#summary","content":"All 3 queries work much faster and read fewer rows. Query 1 no projection: 27 rows in set. Elapsed: 0.158 sec. Processed 26.32 million rows, 157.93 MB (166.57 million rows/s., 999.39 MB/s.) projection: 27 rows in set. Elapsed: 0.007 sec. Processed 105.96 thousand rows, 3.33 MB (14.58 million rows/s., 458.13 MB/s.) Query 2 no projection: 27 rows in set. Elapsed: 0.163 sec. Processed 26.32 million rows, 80.01 MB (161.75 million rows/s., 491.64 MB/s.) projection: 27 rows in set. Elapsed: 0.008 sec. Processed 105.96 thousand rows, 3.67 MB (13.29 million rows/s., 459.89 MB/s.) Query 3 no projection: 100 rows in set. Elapsed: 0.069 sec. Processed 26.32 million rows, 62.47 MB (382.13 million rows/s., 906.93 MB/s.) projection: 100 rows in set. Elapsed: 0.029 sec. Processed 8.08 thousand rows, 511.08 KB (276.06 thousand rows/s., 17.47 MB/s.)  "},{"title":"Test It in Playground​","type":1,"pageTitle":"UK Property Price Paid","url":"docs/en/example-datasets/uk-price-paid#playground","content":"The dataset is also available in the Online Playground. "},{"title":"New York Public Library \"What's on the Menu?\" Dataset","type":0,"sectionRef":"#","url":"docs/en/example-datasets/menus","content":"","keywords":""},{"title":"Download the Dataset​","type":1,"pageTitle":"New York Public Library \"What's on the Menu?\" Dataset","url":"docs/en/example-datasets/menus#download-dataset","content":"Run the command: wget https://s3.amazonaws.com/menusdata.nypl.org/gzips/2021_08_01_07_01_17_data.tgz  Replace the link to the up to date link from http://menus.nypl.org/data if needed. Download size is about 35 MB. "},{"title":"Unpack the Dataset​","type":1,"pageTitle":"New York Public Library \"What's on the Menu?\" Dataset","url":"docs/en/example-datasets/menus#unpack-dataset","content":"tar xvf 2021_08_01_07_01_17_data.tgz  Uncompressed size is about 150 MB. The data is normalized consisted of four tables: Menu — Information about menus: the name of the restaurant, the date when menu was seen, etc.Dish — Information about dishes: the name of the dish along with some characteristic.MenuPage — Information about the pages in the menus, because every page belongs to some menu.MenuItem — An item of the menu. A dish along with its price on some menu page: links to dish and menu page. "},{"title":"Create the Tables​","type":1,"pageTitle":"New York Public Library \"What's on the Menu?\" Dataset","url":"docs/en/example-datasets/menus#create-tables","content":"We use Decimal data type to store prices. CREATE TABLE dish ( id UInt32, name String, description String, menus_appeared UInt32, times_appeared Int32, first_appeared UInt16, last_appeared UInt16, lowest_price Decimal64(3), highest_price Decimal64(3) ) ENGINE = MergeTree ORDER BY id; CREATE TABLE menu ( id UInt32, name String, sponsor String, event String, venue String, place String, physical_description String, occasion String, notes String, call_number String, keywords String, language String, date String, location String, location_type String, currency String, currency_symbol String, status String, page_count UInt16, dish_count UInt16 ) ENGINE = MergeTree ORDER BY id; CREATE TABLE menu_page ( id UInt32, menu_id UInt32, page_number UInt16, image_id String, full_height UInt16, full_width UInt16, uuid UUID ) ENGINE = MergeTree ORDER BY id; CREATE TABLE menu_item ( id UInt32, menu_page_id UInt32, price Decimal64(3), high_price Decimal64(3), dish_id UInt32, created_at DateTime, updated_at DateTime, xpos Float64, ypos Float64 ) ENGINE = MergeTree ORDER BY id;  "},{"title":"Import the Data​","type":1,"pageTitle":"New York Public Library \"What's on the Menu?\" Dataset","url":"docs/en/example-datasets/menus#import-data","content":"Upload data into ClickHouse, run: clickhouse-client --format_csv_allow_single_quotes 0 --input_format_null_as_default 0 --query &quot;INSERT INTO dish FORMAT CSVWithNames&quot; &lt; Dish.csv clickhouse-client --format_csv_allow_single_quotes 0 --input_format_null_as_default 0 --query &quot;INSERT INTO menu FORMAT CSVWithNames&quot; &lt; Menu.csv clickhouse-client --format_csv_allow_single_quotes 0 --input_format_null_as_default 0 --query &quot;INSERT INTO menu_page FORMAT CSVWithNames&quot; &lt; MenuPage.csv clickhouse-client --format_csv_allow_single_quotes 0 --input_format_null_as_default 0 --date_time_input_format best_effort --query &quot;INSERT INTO menu_item FORMAT CSVWithNames&quot; &lt; MenuItem.csv  We use CSVWithNames format as the data is represented by CSV with header. We disable format_csv_allow_single_quotes as only double quotes are used for data fields and single quotes can be inside the values and should not confuse the CSV parser. We disable input_format_null_as_default as our data does not have NULL. Otherwise ClickHouse will try to parse \\N sequences and can be confused with \\ in data. The setting date_time_input_format best_effort allows to parse DateTime fields in wide variety of formats. For example, ISO-8601 without seconds like '2000-01-01 01:02' will be recognized. Without this setting only fixed DateTime format is allowed. "},{"title":"Denormalize the Data​","type":1,"pageTitle":"New York Public Library \"What's on the Menu?\" Dataset","url":"docs/en/example-datasets/menus#denormalize-data","content":"Data is presented in multiple tables in normalized form. It means you have to perform JOIN if you want to query, e.g. dish names from menu items. For typical analytical tasks it is way more efficient to deal with pre-JOINed data to avoid doing JOIN every time. It is called &quot;denormalized&quot; data. We will create a table menu_item_denorm where will contain all the data JOINed together: CREATE TABLE menu_item_denorm ENGINE = MergeTree ORDER BY (dish_name, created_at) AS SELECT price, high_price, created_at, updated_at, xpos, ypos, dish.id AS dish_id, dish.name AS dish_name, dish.description AS dish_description, dish.menus_appeared AS dish_menus_appeared, dish.times_appeared AS dish_times_appeared, dish.first_appeared AS dish_first_appeared, dish.last_appeared AS dish_last_appeared, dish.lowest_price AS dish_lowest_price, dish.highest_price AS dish_highest_price, menu.id AS menu_id, menu.name AS menu_name, menu.sponsor AS menu_sponsor, menu.event AS menu_event, menu.venue AS menu_venue, menu.place AS menu_place, menu.physical_description AS menu_physical_description, menu.occasion AS menu_occasion, menu.notes AS menu_notes, menu.call_number AS menu_call_number, menu.keywords AS menu_keywords, menu.language AS menu_language, menu.date AS menu_date, menu.location AS menu_location, menu.location_type AS menu_location_type, menu.currency AS menu_currency, menu.currency_symbol AS menu_currency_symbol, menu.status AS menu_status, menu.page_count AS menu_page_count, menu.dish_count AS menu_dish_count FROM menu_item JOIN dish ON menu_item.dish_id = dish.id JOIN menu_page ON menu_item.menu_page_id = menu_page.id JOIN menu ON menu_page.menu_id = menu.id;  "},{"title":"Validate the Data​","type":1,"pageTitle":"New York Public Library \"What's on the Menu?\" Dataset","url":"docs/en/example-datasets/menus#validate-data","content":"Query: SELECT count() FROM menu_item_denorm;  Result: ┌─count()─┐ │ 1329175 │ └─────────┘  "},{"title":"Run Some Queries​","type":1,"pageTitle":"New York Public Library \"What's on the Menu?\" Dataset","url":"docs/en/example-datasets/menus#run-queries","content":""},{"title":"Averaged historical prices of dishes​","type":1,"pageTitle":"New York Public Library \"What's on the Menu?\" Dataset","url":"docs/en/example-datasets/menus#query-averaged-historical-prices","content":"Query: SELECT round(toUInt32OrZero(extract(menu_date, '^\\\\d{4}')), -1) AS d, count(), round(avg(price), 2), bar(avg(price), 0, 100, 100) FROM menu_item_denorm WHERE (menu_currency = 'Dollars') AND (d &gt; 0) AND (d &lt; 2022) GROUP BY d ORDER BY d ASC;  Result: ┌────d─┬─count()─┬─round(avg(price), 2)─┬─bar(avg(price), 0, 100, 100)─┐ │ 1850 │ 618 │ 1.5 │ █▍ │ │ 1860 │ 1634 │ 1.29 │ █▎ │ │ 1870 │ 2215 │ 1.36 │ █▎ │ │ 1880 │ 3909 │ 1.01 │ █ │ │ 1890 │ 8837 │ 1.4 │ █▍ │ │ 1900 │ 176292 │ 0.68 │ ▋ │ │ 1910 │ 212196 │ 0.88 │ ▊ │ │ 1920 │ 179590 │ 0.74 │ ▋ │ │ 1930 │ 73707 │ 0.6 │ ▌ │ │ 1940 │ 58795 │ 0.57 │ ▌ │ │ 1950 │ 41407 │ 0.95 │ ▊ │ │ 1960 │ 51179 │ 1.32 │ █▎ │ │ 1970 │ 12914 │ 1.86 │ █▋ │ │ 1980 │ 7268 │ 4.35 │ ████▎ │ │ 1990 │ 11055 │ 6.03 │ ██████ │ │ 2000 │ 2467 │ 11.85 │ ███████████▋ │ │ 2010 │ 597 │ 25.66 │ █████████████████████████▋ │ └──────┴─────────┴──────────────────────┴──────────────────────────────┘  Take it with a grain of salt. "},{"title":"Burger Prices​","type":1,"pageTitle":"New York Public Library \"What's on the Menu?\" Dataset","url":"docs/en/example-datasets/menus#query-burger-prices","content":"Query: SELECT round(toUInt32OrZero(extract(menu_date, '^\\\\d{4}')), -1) AS d, count(), round(avg(price), 2), bar(avg(price), 0, 50, 100) FROM menu_item_denorm WHERE (menu_currency = 'Dollars') AND (d &gt; 0) AND (d &lt; 2022) AND (dish_name ILIKE '%burger%') GROUP BY d ORDER BY d ASC;  Result: ┌────d─┬─count()─┬─round(avg(price), 2)─┬─bar(avg(price), 0, 50, 100)───────────┐ │ 1880 │ 2 │ 0.42 │ ▋ │ │ 1890 │ 7 │ 0.85 │ █▋ │ │ 1900 │ 399 │ 0.49 │ ▊ │ │ 1910 │ 589 │ 0.68 │ █▎ │ │ 1920 │ 280 │ 0.56 │ █ │ │ 1930 │ 74 │ 0.42 │ ▋ │ │ 1940 │ 119 │ 0.59 │ █▏ │ │ 1950 │ 134 │ 1.09 │ ██▏ │ │ 1960 │ 272 │ 0.92 │ █▋ │ │ 1970 │ 108 │ 1.18 │ ██▎ │ │ 1980 │ 88 │ 2.82 │ █████▋ │ │ 1990 │ 184 │ 3.68 │ ███████▎ │ │ 2000 │ 21 │ 7.14 │ ██████████████▎ │ │ 2010 │ 6 │ 18.42 │ ████████████████████████████████████▋ │ └──────┴─────────┴──────────────────────┴───────────────────────────────────────┘  "},{"title":"Vodka​","type":1,"pageTitle":"New York Public Library \"What's on the Menu?\" Dataset","url":"docs/en/example-datasets/menus#query-vodka","content":"Query: SELECT round(toUInt32OrZero(extract(menu_date, '^\\\\d{4}')), -1) AS d, count(), round(avg(price), 2), bar(avg(price), 0, 50, 100) FROM menu_item_denorm WHERE (menu_currency IN ('Dollars', '')) AND (d &gt; 0) AND (d &lt; 2022) AND (dish_name ILIKE '%vodka%') GROUP BY d ORDER BY d ASC;  Result: ┌────d─┬─count()─┬─round(avg(price), 2)─┬─bar(avg(price), 0, 50, 100)─┐ │ 1910 │ 2 │ 0 │ │ │ 1920 │ 1 │ 0.3 │ ▌ │ │ 1940 │ 21 │ 0.42 │ ▋ │ │ 1950 │ 14 │ 0.59 │ █▏ │ │ 1960 │ 113 │ 2.17 │ ████▎ │ │ 1970 │ 37 │ 0.68 │ █▎ │ │ 1980 │ 19 │ 2.55 │ █████ │ │ 1990 │ 86 │ 3.6 │ ███████▏ │ │ 2000 │ 2 │ 3.98 │ ███████▊ │ └──────┴─────────┴──────────────────────┴─────────────────────────────┘  To get vodka we have to write ILIKE '%vodka%' and this definitely makes a statement. "},{"title":"Caviar​","type":1,"pageTitle":"New York Public Library \"What's on the Menu?\" Dataset","url":"docs/en/example-datasets/menus#query-caviar","content":"Let's print caviar prices. Also let's print a name of any dish with caviar. Query: SELECT round(toUInt32OrZero(extract(menu_date, '^\\\\d{4}')), -1) AS d, count(), round(avg(price), 2), bar(avg(price), 0, 50, 100), any(dish_name) FROM menu_item_denorm WHERE (menu_currency IN ('Dollars', '')) AND (d &gt; 0) AND (d &lt; 2022) AND (dish_name ILIKE '%caviar%') GROUP BY d ORDER BY d ASC;  Result: ┌────d─┬─count()─┬─round(avg(price), 2)─┬─bar(avg(price), 0, 50, 100)──────┬─any(dish_name)──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ 1090 │ 1 │ 0 │ │ Caviar │ │ 1880 │ 3 │ 0 │ │ Caviar │ │ 1890 │ 39 │ 0.59 │ █▏ │ Butter and caviar │ │ 1900 │ 1014 │ 0.34 │ ▋ │ Anchovy Caviar on Toast │ │ 1910 │ 1588 │ 1.35 │ ██▋ │ 1/1 Brötchen Caviar │ │ 1920 │ 927 │ 1.37 │ ██▋ │ ASTRAKAN CAVIAR │ │ 1930 │ 289 │ 1.91 │ ███▋ │ Astrachan caviar │ │ 1940 │ 201 │ 0.83 │ █▋ │ (SPECIAL) Domestic Caviar Sandwich │ │ 1950 │ 81 │ 2.27 │ ████▌ │ Beluga Caviar │ │ 1960 │ 126 │ 2.21 │ ████▍ │ Beluga Caviar │ │ 1970 │ 105 │ 0.95 │ █▊ │ BELUGA MALOSSOL CAVIAR AMERICAN DRESSING │ │ 1980 │ 12 │ 7.22 │ ██████████████▍ │ Authentic Iranian Beluga Caviar the world's finest black caviar presented in ice garni and a sampling of chilled 100° Russian vodka │ │ 1990 │ 74 │ 14.42 │ ████████████████████████████▋ │ Avocado Salad, Fresh cut avocado with caviare │ │ 2000 │ 3 │ 7.82 │ ███████████████▋ │ Aufgeschlagenes Kartoffelsueppchen mit Forellencaviar │ │ 2010 │ 6 │ 15.58 │ ███████████████████████████████▏ │ &quot;OYSTERS AND PEARLS&quot; &quot;Sabayon&quot; of Pearl Tapioca with Island Creek Oysters and Russian Sevruga Caviar │ └──────┴─────────┴──────────────────────┴──────────────────────────────────┴─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘  At least they have caviar with vodka. Very nice. "},{"title":"Online Playground​","type":1,"pageTitle":"New York Public Library \"What's on the Menu?\" Dataset","url":"docs/en/example-datasets/menus#playground","content":"The data is uploaded to ClickHouse Playground, example. "},{"title":"ClickHouse Playground","type":0,"sectionRef":"#","url":"docs/en/getting-started/playground","content":"","keywords":"clickhouse playground getting started docs"},{"title":"Credentials​","type":1,"pageTitle":"ClickHouse Playground","url":"docs/en/getting-started/playground#credentials","content":"Parameter\tValueHTTPS endpoint\thttps://play.clickhouse.com:443/ Native TCP endpoint\tplay.clickhouse.com:9440 User\texplorer or play Password\t(empty) "},{"title":"Limitations​","type":1,"pageTitle":"ClickHouse Playground","url":"docs/en/getting-started/playground#limitations","content":"The queries are executed as a read-only user. It implies some limitations: DDL queries are not allowedINSERT queries are not allowed The service also have quotas on its usage. "},{"title":"Examples​","type":1,"pageTitle":"ClickHouse Playground","url":"docs/en/getting-started/playground#examples","content":"HTTPS endpoint example with curl: curl &quot;https://play.clickhouse.com/?user=explorer&quot; --data-binary &quot;SELECT 'Play ClickHouse'&quot;  TCP endpoint example with CLI: clickhouse client --secure --host play.clickhouse.com --user explorer  "},{"title":"Crowdsourced air traffic data from The OpenSky Network 2020","type":0,"sectionRef":"#","url":"docs/en/example-datasets/opensky","content":"","keywords":""},{"title":"Download the Dataset​","type":1,"pageTitle":"Crowdsourced air traffic data from The OpenSky Network 2020","url":"docs/en/example-datasets/opensky#download-dataset","content":"Run the command: wget -O- https://zenodo.org/record/5092942 | grep -oP 'https://zenodo.org/record/5092942/files/flightlist_\\d+_\\d+\\.csv\\.gz' | xargs wget  Download will take about 2 minutes with good internet connection. There are 30 files with total size of 4.3 GB. "},{"title":"Create the Table​","type":1,"pageTitle":"Crowdsourced air traffic data from The OpenSky Network 2020","url":"docs/en/example-datasets/opensky#create-table","content":"CREATE TABLE opensky ( callsign String, number String, icao24 String, registration String, typecode String, origin String, destination String, firstseen DateTime, lastseen DateTime, day DateTime, latitude_1 Float64, longitude_1 Float64, altitude_1 Float64, latitude_2 Float64, longitude_2 Float64, altitude_2 Float64 ) ENGINE = MergeTree ORDER BY (origin, destination, callsign);  "},{"title":"Import Data​","type":1,"pageTitle":"Crowdsourced air traffic data from The OpenSky Network 2020","url":"docs/en/example-datasets/opensky#import-data","content":"Upload data into ClickHouse in parallel: ls -1 flightlist_*.csv.gz | xargs -P100 -I{} bash -c 'gzip -c -d &quot;{}&quot; | clickhouse-client --date_time_input_format best_effort --query &quot;INSERT INTO opensky FORMAT CSVWithNames&quot;'  Here we pass the list of files (ls -1 flightlist_*.csv.gz) to xargs for parallel processing.xargs -P100 specifies to use up to 100 parallel workers but as we only have 30 files, the number of workers will be only 30.For every file, xargs will run a script with bash -c. The script has substitution in form of {} and the xargs command will substitute the filename to it (we have asked it for xargs with -I{}).The script will decompress the file (gzip -c -d &quot;{}&quot;) to standard output (-c parameter) and the output is redirected to clickhouse-client.We also asked to parse DateTime fields with extended parser (--date_time_input_format best_effort) to recognize ISO-8601 format with timezone offsets. Finally, clickhouse-client will do insertion. It will read input data in CSVWithNames format. Parallel upload takes 24 seconds. If you don't like parallel upload, here is sequential variant: for file in flightlist_*.csv.gz; do gzip -c -d &quot;$file&quot; | clickhouse-client --date_time_input_format best_effort --query &quot;INSERT INTO opensky FORMAT CSVWithNames&quot;; done  "},{"title":"Validate the Data​","type":1,"pageTitle":"Crowdsourced air traffic data from The OpenSky Network 2020","url":"docs/en/example-datasets/opensky#validate-data","content":"Query: SELECT count() FROM opensky;  Result: ┌──count()─┐ │ 66010819 │ └──────────┘  The size of dataset in ClickHouse is just 2.66 GiB, check it. Query: SELECT formatReadableSize(total_bytes) FROM system.tables WHERE name = 'opensky';  Result: ┌─formatReadableSize(total_bytes)─┐ │ 2.66 GiB │ └─────────────────────────────────┘  "},{"title":"Run Some Queries​","type":1,"pageTitle":"Crowdsourced air traffic data from The OpenSky Network 2020","url":"docs/en/example-datasets/opensky#run-queries","content":"Total distance travelled is 68 billion kilometers. Query: SELECT formatReadableQuantity(sum(geoDistance(longitude_1, latitude_1, longitude_2, latitude_2)) / 1000) FROM opensky;  Result: ┌─formatReadableQuantity(divide(sum(geoDistance(longitude_1, latitude_1, longitude_2, latitude_2)), 1000))─┐ │ 68.72 billion │ └──────────────────────────────────────────────────────────────────────────────────────────────────────────┘  Average flight distance is around 1000 km. Query: SELECT avg(geoDistance(longitude_1, latitude_1, longitude_2, latitude_2)) FROM opensky;  Result: ┌─avg(geoDistance(longitude_1, latitude_1, longitude_2, latitude_2))─┐ │ 1041090.6465708319 │ └────────────────────────────────────────────────────────────────────┘  "},{"title":"Most busy origin airports and the average distance seen​","type":1,"pageTitle":"Crowdsourced air traffic data from The OpenSky Network 2020","url":"docs/en/example-datasets/opensky#busy-airports-average-distance","content":"Query: SELECT origin, count(), round(avg(geoDistance(longitude_1, latitude_1, longitude_2, latitude_2))) AS distance, bar(distance, 0, 10000000, 100) AS bar FROM opensky WHERE origin != '' GROUP BY origin ORDER BY count() DESC LIMIT 100;  Result:  ┌─origin─┬─count()─┬─distance─┬─bar────────────────────────────────────┐ 1. │ KORD │ 745007 │ 1546108 │ ███████████████▍ │ 2. │ KDFW │ 696702 │ 1358721 │ █████████████▌ │ 3. │ KATL │ 667286 │ 1169661 │ ███████████▋ │ 4. │ KDEN │ 582709 │ 1287742 │ ████████████▊ │ 5. │ KLAX │ 581952 │ 2628393 │ ██████████████████████████▎ │ 6. │ KLAS │ 447789 │ 1336967 │ █████████████▎ │ 7. │ KPHX │ 428558 │ 1345635 │ █████████████▍ │ 8. │ KSEA │ 412592 │ 1757317 │ █████████████████▌ │ 9. │ KCLT │ 404612 │ 880355 │ ████████▋ │ 10. │ VIDP │ 363074 │ 1445052 │ ██████████████▍ │ 11. │ EDDF │ 362643 │ 2263960 │ ██████████████████████▋ │ 12. │ KSFO │ 361869 │ 2445732 │ ████████████████████████▍ │ 13. │ KJFK │ 349232 │ 2996550 │ █████████████████████████████▊ │ 14. │ KMSP │ 346010 │ 1287328 │ ████████████▋ │ 15. │ LFPG │ 344748 │ 2206203 │ ██████████████████████ │ 16. │ EGLL │ 341370 │ 3216593 │ ████████████████████████████████▏ │ 17. │ EHAM │ 340272 │ 2116425 │ █████████████████████▏ │ 18. │ KEWR │ 337696 │ 1826545 │ ██████████████████▎ │ 19. │ KPHL │ 320762 │ 1291761 │ ████████████▊ │ 20. │ OMDB │ 308855 │ 2855706 │ ████████████████████████████▌ │ 21. │ UUEE │ 307098 │ 1555122 │ ███████████████▌ │ 22. │ KBOS │ 304416 │ 1621675 │ ████████████████▏ │ 23. │ LEMD │ 291787 │ 1695097 │ ████████████████▊ │ 24. │ YSSY │ 272979 │ 1875298 │ ██████████████████▋ │ 25. │ KMIA │ 265121 │ 1923542 │ ███████████████████▏ │ 26. │ ZGSZ │ 263497 │ 745086 │ ███████▍ │ 27. │ EDDM │ 256691 │ 1361453 │ █████████████▌ │ 28. │ WMKK │ 254264 │ 1626688 │ ████████████████▎ │ 29. │ CYYZ │ 251192 │ 2175026 │ █████████████████████▋ │ 30. │ KLGA │ 248699 │ 1106935 │ ███████████ │ 31. │ VHHH │ 248473 │ 3457658 │ ██████████████████████████████████▌ │ 32. │ RJTT │ 243477 │ 1272744 │ ████████████▋ │ 33. │ KBWI │ 241440 │ 1187060 │ ███████████▋ │ 34. │ KIAD │ 239558 │ 1683485 │ ████████████████▋ │ 35. │ KIAH │ 234202 │ 1538335 │ ███████████████▍ │ 36. │ KFLL │ 223447 │ 1464410 │ ██████████████▋ │ 37. │ KDAL │ 212055 │ 1082339 │ ██████████▋ │ 38. │ KDCA │ 207883 │ 1013359 │ ██████████▏ │ 39. │ LIRF │ 207047 │ 1427965 │ ██████████████▎ │ 40. │ PANC │ 206007 │ 2525359 │ █████████████████████████▎ │ 41. │ LTFJ │ 205415 │ 860470 │ ████████▌ │ 42. │ KDTW │ 204020 │ 1106716 │ ███████████ │ 43. │ VABB │ 201679 │ 1300865 │ █████████████ │ 44. │ OTHH │ 200797 │ 3759544 │ █████████████████████████████████████▌ │ 45. │ KMDW │ 200796 │ 1232551 │ ████████████▎ │ 46. │ KSAN │ 198003 │ 1495195 │ ██████████████▊ │ 47. │ KPDX │ 197760 │ 1269230 │ ████████████▋ │ 48. │ SBGR │ 197624 │ 2041697 │ ████████████████████▍ │ 49. │ VOBL │ 189011 │ 1040180 │ ██████████▍ │ 50. │ LEBL │ 188956 │ 1283190 │ ████████████▋ │ 51. │ YBBN │ 188011 │ 1253405 │ ████████████▌ │ 52. │ LSZH │ 187934 │ 1572029 │ ███████████████▋ │ 53. │ YMML │ 187643 │ 1870076 │ ██████████████████▋ │ 54. │ RCTP │ 184466 │ 2773976 │ ███████████████████████████▋ │ 55. │ KSNA │ 180045 │ 778484 │ ███████▋ │ 56. │ EGKK │ 176420 │ 1694770 │ ████████████████▊ │ 57. │ LOWW │ 176191 │ 1274833 │ ████████████▋ │ 58. │ UUDD │ 176099 │ 1368226 │ █████████████▋ │ 59. │ RKSI │ 173466 │ 3079026 │ ██████████████████████████████▋ │ 60. │ EKCH │ 172128 │ 1229895 │ ████████████▎ │ 61. │ KOAK │ 171119 │ 1114447 │ ███████████▏ │ 62. │ RPLL │ 170122 │ 1440735 │ ██████████████▍ │ 63. │ KRDU │ 167001 │ 830521 │ ████████▎ │ 64. │ KAUS │ 164524 │ 1256198 │ ████████████▌ │ 65. │ KBNA │ 163242 │ 1022726 │ ██████████▏ │ 66. │ KSDF │ 162655 │ 1380867 │ █████████████▋ │ 67. │ ENGM │ 160732 │ 910108 │ █████████ │ 68. │ LIMC │ 160696 │ 1564620 │ ███████████████▋ │ 69. │ KSJC │ 159278 │ 1081125 │ ██████████▋ │ 70. │ KSTL │ 157984 │ 1026699 │ ██████████▎ │ 71. │ UUWW │ 156811 │ 1261155 │ ████████████▌ │ 72. │ KIND │ 153929 │ 987944 │ █████████▊ │ 73. │ ESSA │ 153390 │ 1203439 │ ████████████ │ 74. │ KMCO │ 153351 │ 1508657 │ ███████████████ │ 75. │ KDVT │ 152895 │ 74048 │ ▋ │ 76. │ VTBS │ 152645 │ 2255591 │ ██████████████████████▌ │ 77. │ CYVR │ 149574 │ 2027413 │ ████████████████████▎ │ 78. │ EIDW │ 148723 │ 1503985 │ ███████████████ │ 79. │ LFPO │ 143277 │ 1152964 │ ███████████▌ │ 80. │ EGSS │ 140830 │ 1348183 │ █████████████▍ │ 81. │ KAPA │ 140776 │ 420441 │ ████▏ │ 82. │ KHOU │ 138985 │ 1068806 │ ██████████▋ │ 83. │ KTPA │ 138033 │ 1338223 │ █████████████▍ │ 84. │ KFFZ │ 137333 │ 55397 │ ▌ │ 85. │ NZAA │ 136092 │ 1581264 │ ███████████████▋ │ 86. │ YPPH │ 133916 │ 1271550 │ ████████████▋ │ 87. │ RJBB │ 133522 │ 1805623 │ ██████████████████ │ 88. │ EDDL │ 133018 │ 1265919 │ ████████████▋ │ 89. │ ULLI │ 130501 │ 1197108 │ ███████████▊ │ 90. │ KIWA │ 127195 │ 250876 │ ██▌ │ 91. │ KTEB │ 126969 │ 1189414 │ ███████████▊ │ 92. │ VOMM │ 125616 │ 1127757 │ ███████████▎ │ 93. │ LSGG │ 123998 │ 1049101 │ ██████████▍ │ 94. │ LPPT │ 122733 │ 1779187 │ █████████████████▋ │ 95. │ WSSS │ 120493 │ 3264122 │ ████████████████████████████████▋ │ 96. │ EBBR │ 118539 │ 1579939 │ ███████████████▋ │ 97. │ VTBD │ 118107 │ 661627 │ ██████▌ │ 98. │ KVNY │ 116326 │ 692960 │ ██████▊ │ 99. │ EDDT │ 115122 │ 941740 │ █████████▍ │ 100. │ EFHK │ 114860 │ 1629143 │ ████████████████▎ │ └────────┴─────────┴──────────┴────────────────────────────────────────┘  "},{"title":"Number of flights from three major Moscow airports, weekly​","type":1,"pageTitle":"Crowdsourced air traffic data from The OpenSky Network 2020","url":"docs/en/example-datasets/opensky#flights-from-moscow","content":"Query: SELECT toMonday(day) AS k, count() AS c, bar(c, 0, 10000, 100) AS bar FROM opensky WHERE origin IN ('UUEE', 'UUDD', 'UUWW') GROUP BY k ORDER BY k ASC;  Result:  ┌──────────k─┬────c─┬─bar──────────────────────────────────────────────────────────────────────────┐ 1. │ 2018-12-31 │ 5248 │ ████████████████████████████████████████████████████▍ │ 2. │ 2019-01-07 │ 6302 │ ███████████████████████████████████████████████████████████████ │ 3. │ 2019-01-14 │ 5701 │ █████████████████████████████████████████████████████████ │ 4. │ 2019-01-21 │ 5638 │ ████████████████████████████████████████████████████████▍ │ 5. │ 2019-01-28 │ 5731 │ █████████████████████████████████████████████████████████▎ │ 6. │ 2019-02-04 │ 5683 │ ████████████████████████████████████████████████████████▋ │ 7. │ 2019-02-11 │ 5759 │ █████████████████████████████████████████████████████████▌ │ 8. │ 2019-02-18 │ 5736 │ █████████████████████████████████████████████████████████▎ │ 9. │ 2019-02-25 │ 5873 │ ██████████████████████████████████████████████████████████▋ │ 10. │ 2019-03-04 │ 5965 │ ███████████████████████████████████████████████████████████▋ │ 11. │ 2019-03-11 │ 5900 │ ███████████████████████████████████████████████████████████ │ 12. │ 2019-03-18 │ 5823 │ ██████████████████████████████████████████████████████████▏ │ 13. │ 2019-03-25 │ 5899 │ ██████████████████████████████████████████████████████████▊ │ 14. │ 2019-04-01 │ 6043 │ ████████████████████████████████████████████████████████████▍ │ 15. │ 2019-04-08 │ 6098 │ ████████████████████████████████████████████████████████████▊ │ 16. │ 2019-04-15 │ 6196 │ █████████████████████████████████████████████████████████████▊ │ 17. │ 2019-04-22 │ 6486 │ ████████████████████████████████████████████████████████████████▋ │ 18. │ 2019-04-29 │ 6682 │ ██████████████████████████████████████████████████████████████████▋ │ 19. │ 2019-05-06 │ 6739 │ ███████████████████████████████████████████████████████████████████▍ │ 20. │ 2019-05-13 │ 6600 │ ██████████████████████████████████████████████████████████████████ │ 21. │ 2019-05-20 │ 6575 │ █████████████████████████████████████████████████████████████████▋ │ 22. │ 2019-05-27 │ 6786 │ ███████████████████████████████████████████████████████████████████▋ │ 23. │ 2019-06-03 │ 6872 │ ████████████████████████████████████████████████████████████████████▋ │ 24. │ 2019-06-10 │ 7045 │ ██████████████████████████████████████████████████████████████████████▍ │ 25. │ 2019-06-17 │ 7045 │ ██████████████████████████████████████████████████████████████████████▍ │ 26. │ 2019-06-24 │ 6852 │ ████████████████████████████████████████████████████████████████████▌ │ 27. │ 2019-07-01 │ 7248 │ ████████████████████████████████████████████████████████████████████████▍ │ 28. │ 2019-07-08 │ 7284 │ ████████████████████████████████████████████████████████████████████████▋ │ 29. │ 2019-07-15 │ 7142 │ ███████████████████████████████████████████████████████████████████████▍ │ 30. │ 2019-07-22 │ 7108 │ ███████████████████████████████████████████████████████████████████████ │ 31. │ 2019-07-29 │ 7251 │ ████████████████████████████████████████████████████████████████████████▌ │ 32. │ 2019-08-05 │ 7403 │ ██████████████████████████████████████████████████████████████████████████ │ 33. │ 2019-08-12 │ 7457 │ ██████████████████████████████████████████████████████████████████████████▌ │ 34. │ 2019-08-19 │ 7502 │ ███████████████████████████████████████████████████████████████████████████ │ 35. │ 2019-08-26 │ 7540 │ ███████████████████████████████████████████████████████████████████████████▍ │ 36. │ 2019-09-02 │ 7237 │ ████████████████████████████████████████████████████████████████████████▎ │ 37. │ 2019-09-09 │ 7328 │ █████████████████████████████████████████████████████████████████████████▎ │ 38. │ 2019-09-16 │ 5566 │ ███████████████████████████████████████████████████████▋ │ 39. │ 2019-09-23 │ 7049 │ ██████████████████████████████████████████████████████████████████████▍ │ 40. │ 2019-09-30 │ 6880 │ ████████████████████████████████████████████████████████████████████▋ │ 41. │ 2019-10-07 │ 6518 │ █████████████████████████████████████████████████████████████████▏ │ 42. │ 2019-10-14 │ 6688 │ ██████████████████████████████████████████████████████████████████▊ │ 43. │ 2019-10-21 │ 6667 │ ██████████████████████████████████████████████████████████████████▋ │ 44. │ 2019-10-28 │ 6303 │ ███████████████████████████████████████████████████████████████ │ 45. │ 2019-11-04 │ 6298 │ ██████████████████████████████████████████████████████████████▊ │ 46. │ 2019-11-11 │ 6137 │ █████████████████████████████████████████████████████████████▎ │ 47. │ 2019-11-18 │ 6051 │ ████████████████████████████████████████████████████████████▌ │ 48. │ 2019-11-25 │ 5820 │ ██████████████████████████████████████████████████████████▏ │ 49. │ 2019-12-02 │ 5942 │ ███████████████████████████████████████████████████████████▍ │ 50. │ 2019-12-09 │ 4891 │ ████████████████████████████████████████████████▊ │ 51. │ 2019-12-16 │ 5682 │ ████████████████████████████████████████████████████████▋ │ 52. │ 2019-12-23 │ 6111 │ █████████████████████████████████████████████████████████████ │ 53. │ 2019-12-30 │ 5870 │ ██████████████████████████████████████████████████████████▋ │ 54. │ 2020-01-06 │ 5953 │ ███████████████████████████████████████████████████████████▌ │ 55. │ 2020-01-13 │ 5698 │ ████████████████████████████████████████████████████████▊ │ 56. │ 2020-01-20 │ 5339 │ █████████████████████████████████████████████████████▍ │ 57. │ 2020-01-27 │ 5566 │ ███████████████████████████████████████████████████████▋ │ 58. │ 2020-02-03 │ 5801 │ ██████████████████████████████████████████████████████████ │ 59. │ 2020-02-10 │ 5692 │ ████████████████████████████████████████████████████████▊ │ 60. │ 2020-02-17 │ 5912 │ ███████████████████████████████████████████████████████████ │ 61. │ 2020-02-24 │ 6031 │ ████████████████████████████████████████████████████████████▎ │ 62. │ 2020-03-02 │ 6105 │ █████████████████████████████████████████████████████████████ │ 63. │ 2020-03-09 │ 5823 │ ██████████████████████████████████████████████████████████▏ │ 64. │ 2020-03-16 │ 4659 │ ██████████████████████████████████████████████▌ │ 65. │ 2020-03-23 │ 3720 │ █████████████████████████████████████▏ │ 66. │ 2020-03-30 │ 1720 │ █████████████████▏ │ 67. │ 2020-04-06 │ 849 │ ████████▍ │ 68. │ 2020-04-13 │ 710 │ ███████ │ 69. │ 2020-04-20 │ 725 │ ███████▏ │ 70. │ 2020-04-27 │ 920 │ █████████▏ │ 71. │ 2020-05-04 │ 859 │ ████████▌ │ 72. │ 2020-05-11 │ 1047 │ ██████████▍ │ 73. │ 2020-05-18 │ 1135 │ ███████████▎ │ 74. │ 2020-05-25 │ 1266 │ ████████████▋ │ 75. │ 2020-06-01 │ 1793 │ █████████████████▊ │ 76. │ 2020-06-08 │ 1979 │ ███████████████████▋ │ 77. │ 2020-06-15 │ 2297 │ ██████████████████████▊ │ 78. │ 2020-06-22 │ 2788 │ ███████████████████████████▊ │ 79. │ 2020-06-29 │ 3389 │ █████████████████████████████████▊ │ 80. │ 2020-07-06 │ 3545 │ ███████████████████████████████████▍ │ 81. │ 2020-07-13 │ 3569 │ ███████████████████████████████████▋ │ 82. │ 2020-07-20 │ 3784 │ █████████████████████████████████████▋ │ 83. │ 2020-07-27 │ 3960 │ ███████████████████████████████████████▌ │ 84. │ 2020-08-03 │ 4323 │ ███████████████████████████████████████████▏ │ 85. │ 2020-08-10 │ 4581 │ █████████████████████████████████████████████▋ │ 86. │ 2020-08-17 │ 4791 │ ███████████████████████████████████████████████▊ │ 87. │ 2020-08-24 │ 4928 │ █████████████████████████████████████████████████▎ │ 88. │ 2020-08-31 │ 4687 │ ██████████████████████████████████████████████▋ │ 89. │ 2020-09-07 │ 4643 │ ██████████████████████████████████████████████▍ │ 90. │ 2020-09-14 │ 4594 │ █████████████████████████████████████████████▊ │ 91. │ 2020-09-21 │ 4478 │ ████████████████████████████████████████████▋ │ 92. │ 2020-09-28 │ 4382 │ ███████████████████████████████████████████▋ │ 93. │ 2020-10-05 │ 4261 │ ██████████████████████████████████████████▌ │ 94. │ 2020-10-12 │ 4243 │ ██████████████████████████████████████████▍ │ 95. │ 2020-10-19 │ 3941 │ ███████████████████████████████████████▍ │ 96. │ 2020-10-26 │ 3616 │ ████████████████████████████████████▏ │ 97. │ 2020-11-02 │ 3586 │ ███████████████████████████████████▋ │ 98. │ 2020-11-09 │ 3403 │ ██████████████████████████████████ │ 99. │ 2020-11-16 │ 3336 │ █████████████████████████████████▎ │ 100. │ 2020-11-23 │ 3230 │ ████████████████████████████████▎ │ 101. │ 2020-11-30 │ 3183 │ ███████████████████████████████▋ │ 102. │ 2020-12-07 │ 3285 │ ████████████████████████████████▋ │ 103. │ 2020-12-14 │ 3367 │ █████████████████████████████████▋ │ 104. │ 2020-12-21 │ 3748 │ █████████████████████████████████████▍ │ 105. │ 2020-12-28 │ 3986 │ ███████████████████████████████████████▋ │ 106. │ 2021-01-04 │ 3906 │ ███████████████████████████████████████ │ 107. │ 2021-01-11 │ 3425 │ ██████████████████████████████████▎ │ 108. │ 2021-01-18 │ 3144 │ ███████████████████████████████▍ │ 109. │ 2021-01-25 │ 3115 │ ███████████████████████████████▏ │ 110. │ 2021-02-01 │ 3285 │ ████████████████████████████████▋ │ 111. │ 2021-02-08 │ 3321 │ █████████████████████████████████▏ │ 112. │ 2021-02-15 │ 3475 │ ██████████████████████████████████▋ │ 113. │ 2021-02-22 │ 3549 │ ███████████████████████████████████▍ │ 114. │ 2021-03-01 │ 3755 │ █████████████████████████████████████▌ │ 115. │ 2021-03-08 │ 3080 │ ██████████████████████████████▋ │ 116. │ 2021-03-15 │ 3789 │ █████████████████████████████████████▊ │ 117. │ 2021-03-22 │ 3804 │ ██████████████████████████████████████ │ 118. │ 2021-03-29 │ 4238 │ ██████████████████████████████████████████▍ │ 119. │ 2021-04-05 │ 4307 │ ███████████████████████████████████████████ │ 120. │ 2021-04-12 │ 4225 │ ██████████████████████████████████████████▎ │ 121. │ 2021-04-19 │ 4391 │ ███████████████████████████████████████████▊ │ 122. │ 2021-04-26 │ 4868 │ ████████████████████████████████████████████████▋ │ 123. │ 2021-05-03 │ 4977 │ █████████████████████████████████████████████████▋ │ 124. │ 2021-05-10 │ 5164 │ ███████████████████████████████████████████████████▋ │ 125. │ 2021-05-17 │ 4986 │ █████████████████████████████████████████████████▋ │ 126. │ 2021-05-24 │ 5024 │ ██████████████████████████████████████████████████▏ │ 127. │ 2021-05-31 │ 4824 │ ████████████████████████████████████████████████▏ │ 128. │ 2021-06-07 │ 5652 │ ████████████████████████████████████████████████████████▌ │ 129. │ 2021-06-14 │ 5613 │ ████████████████████████████████████████████████████████▏ │ 130. │ 2021-06-21 │ 6061 │ ████████████████████████████████████████████████████████████▌ │ 131. │ 2021-06-28 │ 2554 │ █████████████████████████▌ │ └────────────┴──────┴──────────────────────────────────────────────────────────────────────────────┘  "},{"title":"Online Playground​","type":1,"pageTitle":"Crowdsourced air traffic data from The OpenSky Network 2020","url":"docs/en/example-datasets/opensky#playground","content":"You can test other queries to this data set using the interactive resource Online Playground. For example, like this. However, please note that you cannot create temporary tables here. "},{"title":"Installation","type":0,"sectionRef":"#","url":"docs/en/getting-started/install","content":"","keywords":"clickhouse install installation docs"},{"title":"System Requirements​","type":1,"pageTitle":"Installation","url":"docs/en/getting-started/install#system-requirements","content":"ClickHouse can run on any Linux, FreeBSD, or Mac OS X with x86_64, AArch64, or PowerPC64LE CPU architecture. Official pre-built binaries are typically compiled for x86_64 and leverage SSE 4.2 instruction set, so unless otherwise stated usage of CPU that supports it becomes an additional system requirement. Here’s the command to check if current CPU has support for SSE 4.2: $ grep -q sse4_2 /proc/cpuinfo &amp;&amp; echo &quot;SSE 4.2 supported&quot; || echo &quot;SSE 4.2 not supported&quot;  To run ClickHouse on processors that do not support SSE 4.2 or have AArch64 or PowerPC64LE architecture, you should build ClickHouse from sources with proper configuration adjustments. "},{"title":"Available Installation Options​","type":1,"pageTitle":"Installation","url":"docs/en/getting-started/install#available-installation-options","content":""},{"title":"From DEB Packages​","type":1,"pageTitle":"Installation","url":"docs/en/getting-started/install#install-from-deb-packages","content":"It is recommended to use official pre-compiled deb packages for Debian or Ubuntu. Run these commands to install packages: sudo apt-get install apt-transport-https ca-certificates dirmngr sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv E0C56BD4 echo &quot;deb https://repo.clickhouse.com/deb/stable/ main/&quot; | sudo tee \\ /etc/apt/sources.list.d/clickhouse.list sudo apt-get update sudo apt-get install -y clickhouse-server clickhouse-client sudo service clickhouse-server start clickhouse-client # or &quot;clickhouse-client --password&quot; if you set up a password.  Deprecated Method for installing deb-packages sudo apt-get install apt-transport-https ca-certificates dirmngr sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv E0C56BD4 echo &quot;deb https://repo.clickhouse.com/deb/stable/ main/&quot; | sudo tee \\ /etc/apt/sources.list.d/clickhouse.list sudo apt-get update sudo apt-get install -y clickhouse-server clickhouse-client sudo service clickhouse-server start clickhouse-client # or &quot;clickhouse-client --password&quot; if you set up a password.  You can replace stable with lts or testing to use different release trains based on your needs. You can also download and install packages manually from here. Packages​ clickhouse-common-static — Installs ClickHouse compiled binary files.clickhouse-server — Creates a symbolic link for clickhouse-server and installs the default server configuration.clickhouse-client — Creates a symbolic link for clickhouse-client and other client-related tools. and installs client configuration files.clickhouse-common-static-dbg — Installs ClickHouse compiled binary files with debug info. info If you need to install specific version of ClickHouse you have to install all packages with the same version:sudo apt-get install clickhouse-server=21.8.5.7 clickhouse-client=21.8.5.7 clickhouse-common-static=21.8.5.7 "},{"title":"From RPM Packages​","type":1,"pageTitle":"Installation","url":"docs/en/getting-started/install#from-rpm-packages","content":"It is recommended to use official pre-compiled rpm packages for CentOS, RedHat, and all other rpm-based Linux distributions. First, you need to add the official repository: sudo yum install -y yum-utils sudo yum-config-manager --add-repo https://packages.clickhouse.com/rpm/clickhouse.repo sudo yum install -y clickhouse-server clickhouse-client sudo /etc/init.d/clickhouse-server start clickhouse-client # or &quot;clickhouse-client --password&quot; if you set up a password.  Deprecated Method for installing rpm-packages sudo yum install yum-utils sudo rpm --import https://repo.clickhouse.com/CLICKHOUSE-KEY.GPG sudo yum-config-manager --add-repo https://repo.clickhouse.com/rpm/clickhouse.repo sudo yum install clickhouse-server clickhouse-client sudo /etc/init.d/clickhouse-server start clickhouse-client # or &quot;clickhouse-client --password&quot; if you set up a password.  If you want to use the most recent version, replace stable with testing (this is recommended for your testing environments). prestable is sometimes also available. Then run these commands to install packages: sudo yum install clickhouse-server clickhouse-client  You can also download and install packages manually from here. "},{"title":"From Tgz Archives​","type":1,"pageTitle":"Installation","url":"docs/en/getting-started/install#from-tgz-archives","content":"It is recommended to use official pre-compiled tgz archives for all Linux distributions, where installation of deb or rpm packages is not possible. The required version can be downloaded with curl or wget from repository https://packages.clickhouse.com/tgz/. After that downloaded archives should be unpacked and installed with installation scripts. Example for the latest stable version: LATEST_VERSION=$(curl -s https://packages.clickhouse.com/tgz/stable/ | \\ grep -Eo '[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+' | sort -V -r | head -n 1) export LATEST_VERSION curl -O &quot;https://packages.clickhouse.com/tgz/stable/clickhouse-common-static-$LATEST_VERSION.tgz&quot; curl -O &quot;https://packages.clickhouse.com/tgz/stable/clickhouse-common-static-dbg-$LATEST_VERSION.tgz&quot; curl -O &quot;https://packages.clickhouse.com/tgz/stable/clickhouse-server-$LATEST_VERSION.tgz&quot; curl -O &quot;https://packages.clickhouse.com/tgz/stable/clickhouse-client-$LATEST_VERSION.tgz&quot; tar -xzvf &quot;clickhouse-common-static-$LATEST_VERSION.tgz&quot; sudo &quot;clickhouse-common-static-$LATEST_VERSION/install/doinst.sh&quot; tar -xzvf &quot;clickhouse-common-static-dbg-$LATEST_VERSION.tgz&quot; sudo &quot;clickhouse-common-static-dbg-$LATEST_VERSION/install/doinst.sh&quot; tar -xzvf &quot;clickhouse-server-$LATEST_VERSION.tgz&quot; sudo &quot;clickhouse-server-$LATEST_VERSION/install/doinst.sh&quot; sudo /etc/init.d/clickhouse-server start tar -xzvf &quot;clickhouse-client-$LATEST_VERSION.tgz&quot; sudo &quot;clickhouse-client-$LATEST_VERSION/install/doinst.sh&quot;  Deprecated Method for installing tgz archives export LATEST_VERSION=$(curl -s https://repo.clickhouse.com/tgz/stable/ | \\ grep -Eo '[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+' | sort -V -r | head -n 1) curl -O https://repo.clickhouse.com/tgz/stable/clickhouse-common-static-$LATEST_VERSION.tgz curl -O https://repo.clickhouse.com/tgz/stable/clickhouse-common-static-dbg-$LATEST_VERSION.tgz curl -O https://repo.clickhouse.com/tgz/stable/clickhouse-server-$LATEST_VERSION.tgz curl -O https://repo.clickhouse.com/tgz/stable/clickhouse-client-$LATEST_VERSION.tgz tar -xzvf clickhouse-common-static-$LATEST_VERSION.tgz sudo clickhouse-common-static-$LATEST_VERSION/install/doinst.sh tar -xzvf clickhouse-common-static-dbg-$LATEST_VERSION.tgz sudo clickhouse-common-static-dbg-$LATEST_VERSION/install/doinst.sh tar -xzvf clickhouse-server-$LATEST_VERSION.tgz sudo clickhouse-server-$LATEST_VERSION/install/doinst.sh sudo /etc/init.d/clickhouse-server start tar -xzvf clickhouse-client-$LATEST_VERSION.tgz sudo clickhouse-client-$LATEST_VERSION/install/doinst.sh  For production environments, it’s recommended to use the latest stable-version. You can find its number on GitHub page https://github.com/ClickHouse/ClickHouse/tags with postfix -stable. "},{"title":"From Docker Image​","type":1,"pageTitle":"Installation","url":"docs/en/getting-started/install#from-docker-image","content":"To run ClickHouse inside Docker follow the guide on Docker Hub. Those images use official deb packages inside. "},{"title":"Single Binary​","type":1,"pageTitle":"Installation","url":"docs/en/getting-started/install#from-single-binary","content":"You can install ClickHouse on Linux using a single portable binary from the latest commit of the master branch: [https://builds.clickhouse.com/master/amd64/clickhouse]. curl -O 'https://builds.clickhouse.com/master/amd64/clickhouse' &amp;&amp; chmod a+x clickhouse sudo ./clickhouse install  "},{"title":"From Precompiled Binaries for Non-Standard Environments​","type":1,"pageTitle":"Installation","url":"docs/en/getting-started/install#from-binaries-non-linux","content":"For non-Linux operating systems and for AArch64 CPU arhitecture, ClickHouse builds are provided as a cross-compiled binary from the latest commit of the master branch (with a few hours delay). MacOS x86_64 curl -O 'https://builds.clickhouse.com/master/macos/clickhouse' &amp;&amp; chmod a+x ./clickhouse MacOS Aarch64 (Apple Silicon) curl -O 'https://builds.clickhouse.com/master/macos-aarch64/clickhouse' &amp;&amp; chmod a+x ./clickhouse FreeBSD x86_64 curl -O 'https://builds.clickhouse.com/master/freebsd/clickhouse' &amp;&amp; chmod a+x ./clickhouse Linux AArch64 curl -O 'https://builds.clickhouse.com/master/aarch64/clickhouse' &amp;&amp; chmod a+x ./clickhouse  Run sudo ./clickhouse install to install ClickHouse system-wide (also with needed configuration files, configuring users etc.). Then run clickhouse start commands to start the clickhouse-server and clickhouse-client to connect to it. Use the clickhouse client to connect to the server, or clickhouse local to process local data. "},{"title":"From Sources​","type":1,"pageTitle":"Installation","url":"docs/en/getting-started/install#from-sources","content":"To manually compile ClickHouse, follow the instructions for Linux or Mac OS X. You can compile packages and install them or use programs without installing packages. Also by building manually you can disable SSE 4.2 requirement or build for AArch64 CPUs.  Client: programs/clickhouse-client Server: programs/clickhouse-server  You’ll need to create a data and metadata folders and chown them for the desired user. Their paths can be changed in server config (src/programs/server/config.xml), by default they are:  /var/lib/clickhouse/data/default/ /var/lib/clickhouse/metadata/default/  On Gentoo, you can just use emerge clickhouse to install ClickHouse from sources. "},{"title":"Launch​","type":1,"pageTitle":"Installation","url":"docs/en/getting-started/install#launch","content":"To start the server as a daemon, run: $ sudo clickhouse start  There are also another ways to run ClickHouse: $ sudo service clickhouse-server start  If you do not have service command, run as $ sudo /etc/init.d/clickhouse-server start  If you have systemctl command, run as $ sudo systemctl start clickhouse-server.service  See the logs in the /var/log/clickhouse-server/ directory. If the server does not start, check the configurations in the file /etc/clickhouse-server/config.xml. You can also manually launch the server from the console: $ clickhouse-server --config-file=/etc/clickhouse-server/config.xml  In this case, the log will be printed to the console, which is convenient during development. If the configuration file is in the current directory, you do not need to specify the --config-file parameter. By default, it uses ./config.xml. ClickHouse supports access restriction settings. They are located in the users.xml file (next to config.xml). By default, access is allowed from anywhere for the default user, without a password. See user/default/networks. For more information, see the section “Configuration Files”. After launching server, you can use the command-line client to connect to it: $ clickhouse-client  By default, it connects to localhost:9000 on behalf of the user default without a password. It can also be used to connect to a remote server using --host argument. The terminal must use UTF-8 encoding. For more information, see the section “Command-line client”. Example: $ ./clickhouse-client ClickHouse client version 0.0.18749. Connecting to localhost:9000. Connected to ClickHouse server version 0.0.18749. :) SELECT 1 SELECT 1 ┌─1─┐ │ 1 │ └───┘ 1 rows in set. Elapsed: 0.003 sec. :)  Congratulations, the system works! To continue experimenting, you can download one of the test data sets or go through tutorial. Original article "},{"title":"Star Schema Benchmark","type":0,"sectionRef":"#","url":"docs/en/example-datasets/star-schema","content":"Star Schema Benchmark Compiling dbgen: $ git clone git@github.com:vadimtk/ssb-dbgen.git $ cd ssb-dbgen $ make Generating data: warning With -s 100 dbgen generates 600 million rows (67 GB), while while -s 1000 it generates 6 billion rows (which takes a lot of time) $ ./dbgen -s 1000 -T c $ ./dbgen -s 1000 -T l $ ./dbgen -s 1000 -T p $ ./dbgen -s 1000 -T s $ ./dbgen -s 1000 -T d Creating tables in ClickHouse: CREATE TABLE customer ( C_CUSTKEY UInt32, C_NAME String, C_ADDRESS String, C_CITY LowCardinality(String), C_NATION LowCardinality(String), C_REGION LowCardinality(String), C_PHONE String, C_MKTSEGMENT LowCardinality(String) ) ENGINE = MergeTree ORDER BY (C_CUSTKEY); CREATE TABLE lineorder ( LO_ORDERKEY UInt32, LO_LINENUMBER UInt8, LO_CUSTKEY UInt32, LO_PARTKEY UInt32, LO_SUPPKEY UInt32, LO_ORDERDATE Date, LO_ORDERPRIORITY LowCardinality(String), LO_SHIPPRIORITY UInt8, LO_QUANTITY UInt8, LO_EXTENDEDPRICE UInt32, LO_ORDTOTALPRICE UInt32, LO_DISCOUNT UInt8, LO_REVENUE UInt32, LO_SUPPLYCOST UInt32, LO_TAX UInt8, LO_COMMITDATE Date, LO_SHIPMODE LowCardinality(String) ) ENGINE = MergeTree PARTITION BY toYear(LO_ORDERDATE) ORDER BY (LO_ORDERDATE, LO_ORDERKEY); CREATE TABLE part ( P_PARTKEY UInt32, P_NAME String, P_MFGR LowCardinality(String), P_CATEGORY LowCardinality(String), P_BRAND LowCardinality(String), P_COLOR LowCardinality(String), P_TYPE LowCardinality(String), P_SIZE UInt8, P_CONTAINER LowCardinality(String) ) ENGINE = MergeTree ORDER BY P_PARTKEY; CREATE TABLE supplier ( S_SUPPKEY UInt32, S_NAME String, S_ADDRESS String, S_CITY LowCardinality(String), S_NATION LowCardinality(String), S_REGION LowCardinality(String), S_PHONE String ) ENGINE = MergeTree ORDER BY S_SUPPKEY; Inserting data: $ clickhouse-client --query &quot;INSERT INTO customer FORMAT CSV&quot; &lt; customer.tbl $ clickhouse-client --query &quot;INSERT INTO part FORMAT CSV&quot; &lt; part.tbl $ clickhouse-client --query &quot;INSERT INTO supplier FORMAT CSV&quot; &lt; supplier.tbl $ clickhouse-client --query &quot;INSERT INTO lineorder FORMAT CSV&quot; &lt; lineorder.tbl Converting “star schema” to denormalized “flat schema”: SET max_memory_usage = 20000000000; CREATE TABLE lineorder_flat ENGINE = MergeTree PARTITION BY toYear(LO_ORDERDATE) ORDER BY (LO_ORDERDATE, LO_ORDERKEY) AS SELECT l.LO_ORDERKEY AS LO_ORDERKEY, l.LO_LINENUMBER AS LO_LINENUMBER, l.LO_CUSTKEY AS LO_CUSTKEY, l.LO_PARTKEY AS LO_PARTKEY, l.LO_SUPPKEY AS LO_SUPPKEY, l.LO_ORDERDATE AS LO_ORDERDATE, l.LO_ORDERPRIORITY AS LO_ORDERPRIORITY, l.LO_SHIPPRIORITY AS LO_SHIPPRIORITY, l.LO_QUANTITY AS LO_QUANTITY, l.LO_EXTENDEDPRICE AS LO_EXTENDEDPRICE, l.LO_ORDTOTALPRICE AS LO_ORDTOTALPRICE, l.LO_DISCOUNT AS LO_DISCOUNT, l.LO_REVENUE AS LO_REVENUE, l.LO_SUPPLYCOST AS LO_SUPPLYCOST, l.LO_TAX AS LO_TAX, l.LO_COMMITDATE AS LO_COMMITDATE, l.LO_SHIPMODE AS LO_SHIPMODE, c.C_NAME AS C_NAME, c.C_ADDRESS AS C_ADDRESS, c.C_CITY AS C_CITY, c.C_NATION AS C_NATION, c.C_REGION AS C_REGION, c.C_PHONE AS C_PHONE, c.C_MKTSEGMENT AS C_MKTSEGMENT, s.S_NAME AS S_NAME, s.S_ADDRESS AS S_ADDRESS, s.S_CITY AS S_CITY, s.S_NATION AS S_NATION, s.S_REGION AS S_REGION, s.S_PHONE AS S_PHONE, p.P_NAME AS P_NAME, p.P_MFGR AS P_MFGR, p.P_CATEGORY AS P_CATEGORY, p.P_BRAND AS P_BRAND, p.P_COLOR AS P_COLOR, p.P_TYPE AS P_TYPE, p.P_SIZE AS P_SIZE, p.P_CONTAINER AS P_CONTAINER FROM lineorder AS l INNER JOIN customer AS c ON c.C_CUSTKEY = l.LO_CUSTKEY INNER JOIN supplier AS s ON s.S_SUPPKEY = l.LO_SUPPKEY INNER JOIN part AS p ON p.P_PARTKEY = l.LO_PARTKEY; Running the queries: Q1.1 SELECT sum(LO_EXTENDEDPRICE * LO_DISCOUNT) AS revenue FROM lineorder_flat WHERE toYear(LO_ORDERDATE) = 1993 AND LO_DISCOUNT BETWEEN 1 AND 3 AND LO_QUANTITY &lt; 25; Q1.2 SELECT sum(LO_EXTENDEDPRICE * LO_DISCOUNT) AS revenue FROM lineorder_flat WHERE toYYYYMM(LO_ORDERDATE) = 199401 AND LO_DISCOUNT BETWEEN 4 AND 6 AND LO_QUANTITY BETWEEN 26 AND 35; Q1.3 SELECT sum(LO_EXTENDEDPRICE * LO_DISCOUNT) AS revenue FROM lineorder_flat WHERE toISOWeek(LO_ORDERDATE) = 6 AND toYear(LO_ORDERDATE) = 1994 AND LO_DISCOUNT BETWEEN 5 AND 7 AND LO_QUANTITY BETWEEN 26 AND 35; Q2.1 SELECT sum(LO_REVENUE), toYear(LO_ORDERDATE) AS year, P_BRAND FROM lineorder_flat WHERE P_CATEGORY = 'MFGR#12' AND S_REGION = 'AMERICA' GROUP BY year, P_BRAND ORDER BY year, P_BRAND; Q2.2 SELECT sum(LO_REVENUE), toYear(LO_ORDERDATE) AS year, P_BRAND FROM lineorder_flat WHERE P_BRAND &gt;= 'MFGR#2221' AND P_BRAND &lt;= 'MFGR#2228' AND S_REGION = 'ASIA' GROUP BY year, P_BRAND ORDER BY year, P_BRAND; Q2.3 SELECT sum(LO_REVENUE), toYear(LO_ORDERDATE) AS year, P_BRAND FROM lineorder_flat WHERE P_BRAND = 'MFGR#2239' AND S_REGION = 'EUROPE' GROUP BY year, P_BRAND ORDER BY year, P_BRAND; Q3.1 SELECT C_NATION, S_NATION, toYear(LO_ORDERDATE) AS year, sum(LO_REVENUE) AS revenue FROM lineorder_flat WHERE C_REGION = 'ASIA' AND S_REGION = 'ASIA' AND year &gt;= 1992 AND year &lt;= 1997 GROUP BY C_NATION, S_NATION, year ORDER BY year ASC, revenue DESC; Q3.2 SELECT C_CITY, S_CITY, toYear(LO_ORDERDATE) AS year, sum(LO_REVENUE) AS revenue FROM lineorder_flat WHERE C_NATION = 'UNITED STATES' AND S_NATION = 'UNITED STATES' AND year &gt;= 1992 AND year &lt;= 1997 GROUP BY C_CITY, S_CITY, year ORDER BY year ASC, revenue DESC; Q3.3 SELECT C_CITY, S_CITY, toYear(LO_ORDERDATE) AS year, sum(LO_REVENUE) AS revenue FROM lineorder_flat WHERE (C_CITY = 'UNITED KI1' OR C_CITY = 'UNITED KI5') AND (S_CITY = 'UNITED KI1' OR S_CITY = 'UNITED KI5') AND year &gt;= 1992 AND year &lt;= 1997 GROUP BY C_CITY, S_CITY, year ORDER BY year ASC, revenue DESC; Q3.4 SELECT C_CITY, S_CITY, toYear(LO_ORDERDATE) AS year, sum(LO_REVENUE) AS revenue FROM lineorder_flat WHERE (C_CITY = 'UNITED KI1' OR C_CITY = 'UNITED KI5') AND (S_CITY = 'UNITED KI1' OR S_CITY = 'UNITED KI5') AND toYYYYMM(LO_ORDERDATE) = 199712 GROUP BY C_CITY, S_CITY, year ORDER BY year ASC, revenue DESC; Q4.1 SELECT toYear(LO_ORDERDATE) AS year, C_NATION, sum(LO_REVENUE - LO_SUPPLYCOST) AS profit FROM lineorder_flat WHERE C_REGION = 'AMERICA' AND S_REGION = 'AMERICA' AND (P_MFGR = 'MFGR#1' OR P_MFGR = 'MFGR#2') GROUP BY year, C_NATION ORDER BY year ASC, C_NATION ASC; Q4.2 SELECT toYear(LO_ORDERDATE) AS year, S_NATION, P_CATEGORY, sum(LO_REVENUE - LO_SUPPLYCOST) AS profit FROM lineorder_flat WHERE C_REGION = 'AMERICA' AND S_REGION = 'AMERICA' AND (year = 1997 OR year = 1998) AND (P_MFGR = 'MFGR#1' OR P_MFGR = 'MFGR#2') GROUP BY year, S_NATION, P_CATEGORY ORDER BY year ASC, S_NATION ASC, P_CATEGORY ASC; Q4.3 SELECT toYear(LO_ORDERDATE) AS year, S_CITY, P_BRAND, sum(LO_REVENUE - LO_SUPPLYCOST) AS profit FROM lineorder_flat WHERE S_NATION = 'UNITED STATES' AND (year = 1997 OR year = 1998) AND P_CATEGORY = 'MFGR#14' GROUP BY year, S_CITY, P_BRAND ORDER BY year ASC, S_CITY ASC, P_BRAND ASC; Original article","keywords":""},{"title":"Interfaces","type":0,"sectionRef":"#","url":"docs/en/interfaces/","content":"Interfaces ClickHouse provides three network interfaces (they can be optionally wrapped in TLS for additional security): HTTP, which is documented and easy to use directly.Native TCP, which has less overhead.gRPC. In most cases it is recommended to use an appropriate tool or library instead of interacting with those directly. The following are officially supported by ClickHouse: Command-line clientJDBC driverODBC driverC++ client library There are also a wide range of third-party libraries for working with ClickHouse: Client librariesIntegrationsVisual interfaces","keywords":"clickhouse network interfaces http tcp grpc command-line client jdbc odbc driver"},{"title":"C++ Client Library","type":0,"sectionRef":"#","url":"docs/en/interfaces/cpp","content":"C++ Client Library See README at clickhouse-cpp repository. Original article","keywords":""},{"title":"ODBC Driver","type":0,"sectionRef":"#","url":"docs/en/interfaces/odbc","content":"ODBC Driver Use the official ODBC driver for accessing ClickHouse as a data source. Original article","keywords":""},{"title":"gRPC Interface","type":0,"sectionRef":"#","url":"docs/en/interfaces/grpc","content":"","keywords":""},{"title":"Introduction​","type":1,"pageTitle":"gRPC Interface","url":"docs/en/interfaces/grpc#grpc-interface-introduction","content":"ClickHouse supports gRPC interface. It is an open source remote procedure call system that uses HTTP/2 and Protocol Buffers. The implementation of gRPC in ClickHouse supports: SSL; authentication; sessions; compression; parallel queries through the same channel; cancellation of queries; getting progress and logs; external tables. The specification of the interface is described in clickhouse_grpc.proto. "},{"title":"gRPC Configuration​","type":1,"pageTitle":"gRPC Interface","url":"docs/en/interfaces/grpc#grpc-interface-configuration","content":"To use the gRPC interface set grpc_port in the main server configuration. Other configuration options see in the following example: &lt;grpc_port&gt;9100&lt;/grpc_port&gt; &lt;grpc&gt; &lt;enable_ssl&gt;false&lt;/enable_ssl&gt; &lt;!-- The following two files are used only if SSL is enabled --&gt; &lt;ssl_cert_file&gt;/path/to/ssl_cert_file&lt;/ssl_cert_file&gt; &lt;ssl_key_file&gt;/path/to/ssl_key_file&lt;/ssl_key_file&gt; &lt;!-- Whether server requests client for a certificate --&gt; &lt;ssl_require_client_auth&gt;false&lt;/ssl_require_client_auth&gt; &lt;!-- The following file is used only if ssl_require_client_auth=true --&gt; &lt;ssl_ca_cert_file&gt;/path/to/ssl_ca_cert_file&lt;/ssl_ca_cert_file&gt; &lt;!-- Default compression algorithm (applied if client doesn't specify another algorithm, see result_compression in QueryInfo). Supported algorithms: none, deflate, gzip, stream_gzip --&gt; &lt;compression&gt;deflate&lt;/compression&gt; &lt;!-- Default compression level (applied if client doesn't specify another level, see result_compression in QueryInfo). Supported levels: none, low, medium, high --&gt; &lt;compression_level&gt;medium&lt;/compression_level&gt; &lt;!-- Send/receive message size limits in bytes. -1 means unlimited --&gt; &lt;max_send_message_size&gt;-1&lt;/max_send_message_size&gt; &lt;max_receive_message_size&gt;-1&lt;/max_receive_message_size&gt; &lt;!-- Enable if you want to get detailed logs --&gt; &lt;verbose_logs&gt;false&lt;/verbose_logs&gt; &lt;/grpc&gt;  "},{"title":"Built-in Client​","type":1,"pageTitle":"gRPC Interface","url":"docs/en/interfaces/grpc#grpc-client","content":"You can write a client in any of the programming languages supported by gRPC using the provided specification. Or you can use a built-in Python client. It is placed in utils/grpc-client/clickhouse-grpc-client.py in the repository. The built-in client requires grpcio and grpcio-tools Python modules. The client supports the following arguments: --help – Shows a help message and exits.--host HOST, -h HOST – A server name. Default value: localhost. You can use IPv4 or IPv6 addresses also.--port PORT – A port to connect to. This port should be enabled in the ClickHouse server configuration (see grpc_port). Default value: 9100.--user USER_NAME, -u USER_NAME – A user name. Default value: default.--password PASSWORD – A password. Default value: empty string.--query QUERY, -q QUERY – A query to process when using non-interactive mode.--database DATABASE, -d DATABASE – A default database. If not specified, the current database set in the server settings is used (default by default).--format OUTPUT_FORMAT, -f OUTPUT_FORMAT – A result output format. Default value for interactive mode: PrettyCompact.--debug – Enables showing debug information. To run the client in an interactive mode call it without --query argument. In a batch mode query data can be passed via stdin. Client Usage Example In the following example a table is created and loaded with data from a CSV file. Then the content of the table is queried. ./clickhouse-grpc-client.py -q &quot;CREATE TABLE grpc_example_table (id UInt32, text String) ENGINE = MergeTree() ORDER BY id;&quot; echo &quot;0,Input data for&quot; &gt; a.txt ; echo &quot;1,gRPC protocol example&quot; &gt;&gt; a.txt cat a.txt | ./clickhouse-grpc-client.py -q &quot;INSERT INTO grpc_example_table FORMAT CSV&quot; ./clickhouse-grpc-client.py --format PrettyCompact -q &quot;SELECT * FROM grpc_example_table;&quot;  Result: ┌─id─┬─text──────────────────┐ │ 0 │ Input data for │ │ 1 │ gRPC protocol example │ └────┴───────────────────────┘  "},{"title":"Third-Party Interfaces","type":0,"sectionRef":"#","url":"docs/en/interfaces/third-party/","content":"Third-Party Interfaces This is a collection of links to third-party tools that provide some sort of interface to ClickHouse. It can be either visual interface, command-line interface or an API: Client librariesIntegrationsGUIProxies note Generic tools that support common API like ODBC or JDBC usually can work with ClickHouse as well, but are not listed here because there are way too many of them.","keywords":""},{"title":"JDBC Driver","type":0,"sectionRef":"#","url":"docs/en/interfaces/jdbc","content":"JDBC Driver Use the official JDBC driver (and Java client) to access ClickHouse from your Java applications. Third-party drivers: ClickHouse-Native-JDBCclickhouse4j Original article","keywords":""},{"title":"Command-line Client","type":0,"sectionRef":"#","url":"docs/en/interfaces/cli","content":"","keywords":""},{"title":"Usage​","type":1,"pageTitle":"Command-line Client","url":"docs/en/interfaces/cli#cli_usage","content":"The client can be used in interactive and non-interactive (batch) mode. To use batch mode, specify the ‘query’ parameter, or send data to ‘stdin’ (it verifies that ‘stdin’ is not a terminal), or both. Similar to the HTTP interface, when using the ‘query’ parameter and sending data to ‘stdin’, the request is a concatenation of the ‘query’ parameter, a line feed, and the data in ‘stdin’. This is convenient for large INSERT queries. Example of using the client to insert data: $ echo -ne &quot;1, 'some text', '2016-08-14 00:00:00'\\n2, 'some more text', '2016-08-14 00:00:01'&quot; | clickhouse-client --database=test --query=&quot;INSERT INTO test FORMAT CSV&quot;; $ cat &lt;&lt;_EOF | clickhouse-client --database=test --query=&quot;INSERT INTO test FORMAT CSV&quot;; 3, 'some text', '2016-08-14 00:00:00' 4, 'some more text', '2016-08-14 00:00:01' _EOF $ cat file.csv | clickhouse-client --database=test --query=&quot;INSERT INTO test FORMAT CSV&quot;;  In batch mode, the default data format is TabSeparated. You can set the format in the FORMAT clause of the query. By default, you can only process a single query in batch mode. To make multiple queries from a “script,” use the --multiquery parameter. This works for all queries except INSERT. Query results are output consecutively without additional separators. Similarly, to process a large number of queries, you can run ‘clickhouse-client’ for each query. Note that it may take tens of milliseconds to launch the ‘clickhouse-client’ program. In interactive mode, you get a command line where you can enter queries. If ‘multiline’ is not specified (the default): To run the query, press Enter. The semicolon is not necessary at the end of the query. To enter a multiline query, enter a backslash \\ before the line feed. After you press Enter, you will be asked to enter the next line of the query. If multiline is specified: To run a query, end it with a semicolon and press Enter. If the semicolon was omitted at the end of the entered line, you will be asked to enter the next line of the query. Only a single query is run, so everything after the semicolon is ignored. You can specify \\G instead of or after the semicolon. This indicates Vertical format. In this format, each value is printed on a separate line, which is convenient for wide tables. This unusual feature was added for compatibility with the MySQL CLI. The command line is based on ‘replxx’ (similar to ‘readline’). In other words, it uses the familiar keyboard shortcuts and keeps a history. The history is written to ~/.clickhouse-client-history. By default, the format used is PrettyCompact. You can change the format in the FORMAT clause of the query, or by specifying \\G at the end of the query, using the --format or --vertical argument in the command line, or using the client configuration file. To exit the client, press Ctrl+D, or enter one of the following instead of a query: “exit”, “quit”, “logout”, “exit;”, “quit;”, “logout;”, “q”, “Q”, “:q” When processing a query, the client shows: Progress, which is updated no more than 10 times per second (by default). For quick queries, the progress might not have time to be displayed.The formatted query after parsing, for debugging.The result in the specified format.The number of lines in the result, the time passed, and the average speed of query processing. You can cancel a long query by pressing Ctrl+C. However, you will still need to wait for a little for the server to abort the request. It is not possible to cancel a query at certain stages. If you do not wait and press Ctrl+C a second time, the client will exit. The command-line client allows passing external data (external temporary tables) for querying. For more information, see the section “External data for query processing”. "},{"title":"Queries with Parameters​","type":1,"pageTitle":"Command-line Client","url":"docs/en/interfaces/cli#cli-queries-with-parameters","content":"You can create a query with parameters and pass values to them from client application. This allows to avoid formatting query with specific dynamic values on client side. For example: $ clickhouse-client --param_parName=&quot;[1, 2]&quot; -q &quot;SELECT * FROM table WHERE a = {parName:Array(UInt16)}&quot;  Query Syntax​ Format a query as usual, then place the values that you want to pass from the app parameters to the query in braces in the following format: {&lt;name&gt;:&lt;data type&gt;}  name — Placeholder identifier. In the console client it should be used in app parameters as --param_&lt;name&gt; = value.data type — Data type of the app parameter value. For example, a data structure like (integer, ('string', integer)) can have the Tuple(UInt8, Tuple(String, UInt8)) data type (you can also use another integer types). It's also possible to pass table, database, column names as a parameter, in that case you would need to use Identifier as a data type. Example​ $ clickhouse-client --param_tuple_in_tuple=&quot;(10, ('dt', 10))&quot; -q &quot;SELECT * FROM table WHERE val = {tuple_in_tuple:Tuple(UInt8, Tuple(String, UInt8))}&quot; $ clickhouse-client --param_tbl=&quot;numbers&quot; --param_db=&quot;system&quot; --param_col=&quot;number&quot; --query &quot;SELECT {col:Identifier} FROM {db:Identifier}.{tbl:Identifier} LIMIT 10&quot;  "},{"title":"Configuring​","type":1,"pageTitle":"Command-line Client","url":"docs/en/interfaces/cli#interfaces_cli_configuration","content":"You can pass parameters to clickhouse-client (all parameters have a default value) using: From the Command Line Command-line options override the default values and settings in configuration files. Configuration files. Settings in the configuration files override the default values. "},{"title":"Command Line Options​","type":1,"pageTitle":"Command-line Client","url":"docs/en/interfaces/cli#command-line-options","content":"--host, -h – The server name, ‘localhost’ by default. You can use either the name or the IPv4 or IPv6 address.--port – The port to connect to. Default value: 9000. Note that the HTTP interface and the native interface use different ports.--user, -u – The username. Default value: default.--password – The password. Default value: empty string.--query, -q – The query to process when using non-interactive mode. You must specify either query or queries-file option.--queries-file, -qf – file path with queries to execute. You must specify either query or queries-file option.--database, -d – Select the current default database. Default value: the current database from the server settings (‘default’ by default).--multiline, -m – If specified, allow multiline queries (do not send the query on Enter).--multiquery, -n – If specified, allow processing multiple queries separated by semicolons.--format, -f – Use the specified default format to output the result.--vertical, -E – If specified, use the Vertical format by default to output the result. This is the same as –format=Vertical. In this format, each value is printed on a separate line, which is helpful when displaying wide tables.--time, -t – If specified, print the query execution time to ‘stderr’ in non-interactive mode.--stacktrace – If specified, also print the stack trace if an exception occurs.--config-file – The name of the configuration file.--secure – If specified, will connect to server over secure connection.--history_file — Path to a file containing command history.--param_&lt;name&gt; — Value for a query with parameters.--hardware-utilization — Print hardware utilization information in progress bar.--print-profile-events – Print ProfileEvents packets.--profile-events-delay-ms – Delay between printing ProfileEvents packets (-1 - print only totals, 0 - print every single packet). Since version 20.5, clickhouse-client has automatic syntax highlighting (always enabled). "},{"title":"Configuration Files​","type":1,"pageTitle":"Command-line Client","url":"docs/en/interfaces/cli#configuration_files","content":"clickhouse-client uses the first existing file of the following: Defined in the --config-file parameter../clickhouse-client.xml~/.clickhouse-client/config.xml/etc/clickhouse-client/config.xml Example of a config file: &lt;config&gt; &lt;user&gt;username&lt;/user&gt; &lt;password&gt;password&lt;/password&gt; &lt;secure&gt;False&lt;/secure&gt; &lt;/config&gt;  "},{"title":"Query ID Format​","type":1,"pageTitle":"Command-line Client","url":"docs/en/interfaces/cli#query-id-format","content":"In interactive mode clickhouse-client shows query ID for every query. By default, the ID is formatted like this: Query id: 927f137d-00f1-4175-8914-0dd066365e96  A custom format may be specified in a configuration file inside a query_id_formats tag. {query_id} placeholder in the format string is replaced with the ID of a query. Several format strings are allowed inside the tag. This feature can be used to generate URLs to facilitate profiling of queries. Example &lt;config&gt; &lt;query_id_formats&gt; &lt;speedscope&gt;http://speedscope-host/#profileURL=qp%3Fid%3D{query_id}&lt;/speedscope&gt; &lt;/query_id_formats&gt; &lt;/config&gt;  If the configuration above is applied, the ID of a query is shown in the following format: speedscope:http://speedscope-host/#profileURL=qp%3Fid%3Dc8ecc783-e753-4b38-97f1-42cddfb98b7d  "},{"title":"MySQL Interface","type":0,"sectionRef":"#","url":"docs/en/interfaces/mysql","content":"MySQL Interface ClickHouse supports MySQL wire protocol. It can be enabled by mysql_port setting in configuration file: &lt;mysql_port&gt;9004&lt;/mysql_port&gt; Example of connecting using command-line tool mysql: $ mysql --protocol tcp -u default -P 9004 Output if a connection succeeded: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 4 Server version: 20.2.1.1-ClickHouse Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql&gt; For compatibility with all MySQL clients, it is recommended to specify user password with double SHA1 in configuration file. If user password is specified using SHA256, some clients won’t be able to authenticate (mysqljs and old versions of command-line tool MySQL and MariaDB). Restrictions: prepared queries are not supported some data types are sent as strings To cancel a long query use KILL QUERY connection_id statement (it is replaced with KILL QUERY WHERE query_id = connection_id while proceeding). For example: $ mysql --protocol tcp -h mysql_server -P 9004 default -u default --password=123 -e &quot;KILL QUERY 123456;&quot; Original article","keywords":""},{"title":"Native Interface (TCP)","type":0,"sectionRef":"#","url":"docs/en/interfaces/tcp","content":"Native Interface (TCP) The native protocol is used in the command-line client, for inter-server communication during distributed query processing, and also in other C++ programs. Unfortunately, native ClickHouse protocol does not have formal specification yet, but it can be reverse-engineered from ClickHouse source code (starting around here) and/or by intercepting and analyzing TCP traffic. Original article","keywords":""},{"title":"Operations","type":0,"sectionRef":"#","url":"docs/en/operations/","content":"Operations ClickHouse operations manual consists of the following major sections: RequirementsMonitoringTroubleshootingUsage RecommendationsUpdate ProcedureAccess RightsData BackupConfiguration FilesQuotasSystem TablesServer Configuration ParametersHow To Test Your Hardware With ClickHouseSettingsUtilities Original article","keywords":""},{"title":"Visual Interfaces from Third-party Developers","type":0,"sectionRef":"#","url":"docs/en/interfaces/third-party/gui","content":"","keywords":""},{"title":"Open-Source​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"docs/en/interfaces/third-party/gui#open-source","content":""},{"title":"Tabix​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"docs/en/interfaces/third-party/gui#tabix","content":"Web interface for ClickHouse in the Tabix project. Features: Works with ClickHouse directly from the browser, without the need to install additional software.Query editor with syntax highlighting.Auto-completion of commands.Tools for graphical analysis of query execution.Colour scheme options. Tabix documentation. "},{"title":"HouseOps​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"docs/en/interfaces/third-party/gui#houseops","content":"HouseOps is a UI/IDE for OSX, Linux and Windows. Features: Query builder with syntax highlighting. View the response in a table or JSON view.Export query results as CSV or JSON.List of processes with descriptions. Write mode. Ability to stop (KILL) a process.Database graph. Shows all tables and their columns with additional information.A quick view of the column size.Server configuration. The following features are planned for development: Database management.User management.Real-time data analysis.Cluster monitoring.Cluster management.Monitoring replicated and Kafka tables. "},{"title":"LightHouse​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"docs/en/interfaces/third-party/gui#lighthouse","content":"LightHouse is a lightweight web interface for ClickHouse. Features: Table list with filtering and metadata.Table preview with filtering and sorting.Read-only queries execution. "},{"title":"Redash​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"docs/en/interfaces/third-party/gui#redash","content":"Redash is a platform for data visualization. Supports for multiple data sources including ClickHouse, Redash can join results of queries from different data sources into one final dataset. Features: Powerful editor of queries.Database explorer.Visualization tools, that allow you to represent data in different forms. "},{"title":"Grafana​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"docs/en/interfaces/third-party/gui#grafana","content":"Grafana is a platform for monitoring and visualization. &quot;Grafana allows you to query, visualize, alert on and understand your metrics no matter where they are stored. Create, explore, and share dashboards with your team and foster a data driven culture. Trusted and loved by the community&quot; — grafana.com. ClickHouse datasource plugin provides a support for ClickHouse as a backend database. "},{"title":"DBeaver​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"docs/en/interfaces/third-party/gui#dbeaver","content":"DBeaver - universal desktop database client with ClickHouse support. Features: Query development with syntax highlight and autocompletion.Table list with filters and metadata search.Table data preview.Full-text search. By default, DBeaver does not connect using a session (the CLI for example does). If you require session support (for example to set settings for your session), edit the driver connection properties and set session_id to a random string (it uses the http connection under the hood). Then you can use any setting from the query window. "},{"title":"clickhouse-cli​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"docs/en/interfaces/third-party/gui#clickhouse-cli","content":"clickhouse-cli is an alternative command-line client for ClickHouse, written in Python 3. Features: Autocompletion.Syntax highlighting for the queries and data output.Pager support for the data output.Custom PostgreSQL-like commands. "},{"title":"clickhouse-flamegraph​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"docs/en/interfaces/third-party/gui#clickhouse-flamegraph","content":"clickhouse-flamegraph is a specialized tool to visualize the system.trace_log as flamegraph. "},{"title":"clickhouse-plantuml​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"docs/en/interfaces/third-party/gui#clickhouse-plantuml","content":"cickhouse-plantuml is a script to generate PlantUML diagram of tables’ schemes. "},{"title":"xeus-clickhouse​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"docs/en/interfaces/third-party/gui#xeus-clickhouse","content":"xeus-clickhouse is a Jupyter kernal for ClickHouse, which supports query CH data using SQL in Jupyter. "},{"title":"MindsDB Studio​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"docs/en/interfaces/third-party/gui#mindsdb","content":"MindsDB is an open-source AI layer for databases including ClickHouse that allows you to effortlessly develop, train and deploy state-of-the-art machine learning models. MindsDB Studio(GUI) allows you to train new models from database, interpret predictions made by the model, identify potential data biases, and evaluate and visualize model accuracy using the Explainable AI function to adapt and tune your Machine Learning models faster. "},{"title":"DBM​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"docs/en/interfaces/third-party/gui#dbm","content":"DBM DBM is a visual management tool for ClickHouse! Features: Support query history (pagination, clear all, etc.)Support selected sql clauses querySupport terminating querySupport table management (metadata, delete, preview)Support database management (delete, create)Support custom querySupport multiple data sources management(connection test, monitoring)Support monitor (processor, connection, query)Support migrate data "},{"title":"Bytebase​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"docs/en/interfaces/third-party/gui#bytebase","content":"Bytebase is a web-based, open source schema change and version control tool for teams. It supports various databases including ClickHouse. Features: Schema review between developers and DBAs.Database-as-Code, version control the schema in VCS such GitLab and trigger the deployment upon code commit.Streamlined deployment with per-environment policy.Full migration history.Schema drift detection.Backup and restore.RBAC. "},{"title":"Zeppelin-Interpreter-for-ClickHouse​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"docs/en/interfaces/third-party/gui#zeppelin-interpreter-for-clickhouse","content":"Zeppelin-Interpreter-for-ClickHouse is a Zeppelin interpreter for ClickHouse. Compared with JDBC interpreter, it can provide better timeout control for long running queries. "},{"title":"Commercial​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"docs/en/interfaces/third-party/gui#commercial","content":""},{"title":"DataGrip​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"docs/en/interfaces/third-party/gui#datagrip","content":"DataGrip is a database IDE from JetBrains with dedicated support for ClickHouse. It is also embedded in other IntelliJ-based tools: PyCharm, IntelliJ IDEA, GoLand, PhpStorm and others. Features: Very fast code completion.ClickHouse syntax highlighting.Support for features specific to ClickHouse, for example, nested columns, table engines.Data Editor.Refactorings.Search and Navigation. "},{"title":"Yandex DataLens​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"docs/en/interfaces/third-party/gui#yandex-datalens","content":"Yandex DataLens is a service of data visualization and analytics. Features: Wide range of available visualizations, from simple bar charts to complex dashboards.Dashboards could be made publicly available.Support for multiple data sources including ClickHouse.Storage for materialized data based on ClickHouse. DataLens is available for free for low-load projects, even for commercial use. DataLens documentation.Tutorial on visualizing data from a ClickHouse database. "},{"title":"Holistics Software​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"docs/en/interfaces/third-party/gui#holistics-software","content":"Holistics is a full-stack data platform and business intelligence tool. Features: Automated email, Slack and Google Sheet schedules of reports.SQL editor with visualizations, version control, auto-completion, reusable query components and dynamic filters.Embedded analytics of reports and dashboards via iframe.Data preparation and ETL capabilities.SQL data modelling support for relational mapping of data. "},{"title":"Looker​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"docs/en/interfaces/third-party/gui#looker","content":"Looker is a data platform and business intelligence tool with support for 50+ database dialects including ClickHouse. Looker is available as a SaaS platform and self-hosted. Users can use Looker via the browser to explore data, build visualizations and dashboards, schedule reports, and share their insights with colleagues. Looker provides a rich set of tools to embed these features in other applications, and an API to integrate data with other applications. Features: Easy and agile development using LookML, a language which supports curatedData Modeling to support report writers and end-users.Powerful workflow integration via Looker’s Data Actions. How to configure ClickHouse in Looker. "},{"title":"SeekTable​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"docs/en/interfaces/third-party/gui#seektable","content":"SeekTable is a self-service BI tool for data exploration and operational reporting. It is available both as a cloud service and a self-hosted version. Reports from SeekTable may be embedded into any web-app. Features: Business users-friendly reports builder.Powerful report parameters for SQL filtering and report-specific query customizations.Can connect to ClickHouse both with a native TCP/IP endpoint and a HTTP(S) interface (2 different drivers).It is possible to use all power of ClickHouse SQL dialect in dimensions/measures definitions.Web API for automated reports generation.Supports reports development flow with account data backup/restore; data models (cubes) / reports configuration is a human-readable XML and can be stored under version control system. SeekTable is free for personal/individual usage. How to configure ClickHouse connection in SeekTable. "},{"title":"Chadmin​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"docs/en/interfaces/third-party/gui#chadmin","content":"Chadmin is a simple UI where you can visualize your currently running queries on your ClickHouse cluster and info about them and kill them if you want. "},{"title":"TABLUM.IO​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"docs/en/interfaces/third-party/gui#tablum_io","content":"TABLUM.IO — an online query and analytics tool for ETL and visualization. It allows connecting to ClickHouse, query data via a versatile SQL console as well as to load data from static files and 3rd party services. TABLUM.IO can visualize data results as charts and tables. Features: ETL: data loading from popular databases, local and remote files, API invocations.Versatile SQL console with syntax highlight and visual query builder.Data visualization as charts and tables.Data materialization and sub-queries.Data reporting to Slack, Telegram or email.Data pipelining via proprietary API.Data export in JSON, CSV, SQL, HTML formats.Web-based interface. TABLUM.IO can be run as a self-hosted solution (as a docker image) or in the cloud. License: commercial product with 3-month free period. Try it out for free in the cloud. Learn more about the product at TABLUM.IO Original article "},{"title":"Client Libraries from Third-party Developers","type":0,"sectionRef":"#","url":"docs/en/interfaces/third-party/client-libraries","content":"Client Libraries from Third-party Developers warning ClickHouse Inc does not maintain the libraries listed below and hasn’t done any extensive testing to ensure their quality. Python infi.clickhouse_ormclickhouse-driverclickhouse-clientaiochclientasynch PHP smi2/phpclickhouse8bitov/clickhouse-php-clientbozerkins/clickhouse-clientsimpod/clickhouse-clientseva-code/php-click-house-clientSeasClick C++ clientone-ckglushkovds/phpclickhouse-laravelkolya7k ClickHouse PHP extension Go clickhousego-clickhousechconnmailrugo-clickhousegolang-clickhouse Swift ClickHouseNIOClickHouseVapor ORM NodeJs clickhouse (NodeJs)node-clickhousenestjs-clickhouseclickhouse-client Perl perl-DBD-ClickHouseHTTP-ClickHouseAnyEvent-ClickHouse Ruby ClickHouse (Ruby)clickhouse-activerecord Rust Klickhouse R clickhouse-rRClickHouse Java clickhouse-client-javaclickhouse-client Scala clickhouse-scala-client Kotlin AORM C# Octonica.ClickHouseClientClickHouse.AdoClickHouse.ClientClickHouse.Net Elixir clickhousexpillar Nim nim-clickhouse Haskell hdbc-clickhouse Original article","keywords":""},{"title":"Access Control and Account Management","type":0,"sectionRef":"#","url":"docs/en/operations/access-rights","content":"","keywords":""},{"title":"Usage​","type":1,"pageTitle":"Access Control and Account Management","url":"docs/en/operations/access-rights#access-control-usage","content":"By default, the ClickHouse server provides the default user account which is not allowed using SQL-driven access control and account management but has all the rights and permissions. The default user account is used in any cases when the username is not defined, for example, at login from client or in distributed queries. In distributed query processing a default user account is used, if the configuration of the server or cluster does not specify the user and password properties. If you just started using ClickHouse, consider the following scenario: Enable SQL-driven access control and account management for the default user.Log in to the default user account and create all the required users. Don’t forget to create an administrator account (GRANT ALL ON *.* TO admin_user_account WITH GRANT OPTION).Restrict permissions for the default user and disable SQL-driven access control and account management for it. "},{"title":"Properties of Current Solution​","type":1,"pageTitle":"Access Control and Account Management","url":"docs/en/operations/access-rights#access-control-properties","content":"You can grant permissions for databases and tables even if they do not exist.If a table was deleted, all the privileges that correspond to this table are not revoked. This means that even if you create a new table with the same name later, all the privileges remain valid. To revoke privileges corresponding to the deleted table, you need to execute, for example, the REVOKE ALL PRIVILEGES ON db.table FROM ALL query.There are no lifetime settings for privileges. "},{"title":"User Account​","type":1,"pageTitle":"Access Control and Account Management","url":"docs/en/operations/access-rights#user-account-management","content":"A user account is an access entity that allows to authorize someone in ClickHouse. A user account contains: Identification information.Privileges that define a scope of queries the user can execute.Hosts allowed to connect to the ClickHouse server.Assigned and default roles.Settings with their constraints applied by default at user login.Assigned settings profiles. Privileges can be granted to a user account by the GRANT query or by assigning roles. To revoke privileges from a user, ClickHouse provides the REVOKE query. To list privileges for a user, use the SHOW GRANTS statement. Management queries: CREATE USERALTER USERDROP USERSHOW CREATE USERSHOW USERS "},{"title":"Settings Applying​","type":1,"pageTitle":"Access Control and Account Management","url":"docs/en/operations/access-rights#access-control-settings-applying","content":"Settings can be configured differently: for a user account, in its granted roles and in settings profiles. At user login, if a setting is configured for different access entities, the value and constraints of this setting are applied as follows (from higher to lower priority): User account settings.The settings of default roles of the user account. If a setting is configured in some roles, then order of the setting application is undefined.The settings from settings profiles assigned to a user or to its default roles. If a setting is configured in some profiles, then order of setting application is undefined.Settings applied to all the server by default or from the default profile. "},{"title":"Role​","type":1,"pageTitle":"Access Control and Account Management","url":"docs/en/operations/access-rights#role-management","content":"Role is a container for access entities that can be granted to a user account. Role contains: PrivilegesSettings and constraintsList of assigned roles Management queries: CREATE ROLEALTER ROLEDROP ROLESET ROLESET DEFAULT ROLESHOW CREATE ROLESHOW ROLES Privileges can be granted to a role by the GRANT query. To revoke privileges from a role ClickHouse provides the REVOKE query. "},{"title":"Row Policy​","type":1,"pageTitle":"Access Control and Account Management","url":"docs/en/operations/access-rights#row-policy-management","content":"Row policy is a filter that defines which of the rows are available to a user or a role. Row policy contains filters for one particular table, as well as a list of roles and/or users which should use this row policy. warning Row policies makes sense only for users with readonly access. If user can modify table or copy partitions between tables, it defeats the restrictions of row policies. Management queries: CREATE ROW POLICYALTER ROW POLICYDROP ROW POLICYSHOW CREATE ROW POLICYSHOW POLICIES "},{"title":"Settings Profile​","type":1,"pageTitle":"Access Control and Account Management","url":"docs/en/operations/access-rights#settings-profiles-management","content":"Settings profile is a collection of settings. Settings profile contains settings and constraints, as well as a list of roles and/or users to which this profile is applied. Management queries: CREATE SETTINGS PROFILEALTER SETTINGS PROFILEDROP SETTINGS PROFILESHOW CREATE SETTINGS PROFILESHOW PROFILES "},{"title":"Quota​","type":1,"pageTitle":"Access Control and Account Management","url":"docs/en/operations/access-rights#quotas-management","content":"Quota limits resource usage. See Quotas. Quota contains a set of limits for some durations, as well as a list of roles and/or users which should use this quota. Management queries: CREATE QUOTAALTER QUOTADROP QUOTASHOW CREATE QUOTASHOW QUOTASHOW QUOTAS "},{"title":"Enabling SQL-driven Access Control and Account Management​","type":1,"pageTitle":"Access Control and Account Management","url":"docs/en/operations/access-rights#enabling-access-control","content":"Setup a directory for configurations storage. ClickHouse stores access entity configurations in the folder set in the access_control_path server configuration parameter. Enable SQL-driven access control and account management for at least one user account. By default, SQL-driven access control and account management is disabled for all users. You need to configure at least one user in the users.xml configuration file and set the value of the access_management setting to 1. Original article "},{"title":"Proxy Servers from Third-party Developers","type":0,"sectionRef":"#","url":"docs/en/interfaces/third-party/proxy","content":"","keywords":""},{"title":"chproxy​","type":1,"pageTitle":"Proxy Servers from Third-party Developers","url":"docs/en/interfaces/third-party/proxy#chproxy","content":"chproxy, is an HTTP proxy and load balancer for ClickHouse database. Features: Per-user routing and response caching.Flexible limits.Automatic SSL certificate renewal. Implemented in Go. "},{"title":"KittenHouse​","type":1,"pageTitle":"Proxy Servers from Third-party Developers","url":"docs/en/interfaces/third-party/proxy#kittenhouse","content":"KittenHouse is designed to be a local proxy between ClickHouse and application server in case it’s impossible or inconvenient to buffer INSERT data on your application side. Features: In-memory and on-disk data buffering.Per-table routing.Load-balancing and health checking. Implemented in Go. "},{"title":"ClickHouse-Bulk​","type":1,"pageTitle":"Proxy Servers from Third-party Developers","url":"docs/en/interfaces/third-party/proxy#clickhouse-bulk","content":"ClickHouse-Bulk is a simple ClickHouse insert collector. Features: Group requests and send by threshold or interval.Multiple remote servers.Basic authentication. Implemented in Go. Original article "},{"title":"Data Backup","type":0,"sectionRef":"#","url":"docs/en/operations/backup","content":"","keywords":""},{"title":"Duplicating Source Data Somewhere Else​","type":1,"pageTitle":"Data Backup","url":"docs/en/operations/backup#duplicating-source-data-somewhere-else","content":"Often data that is ingested into ClickHouse is delivered through some sort of persistent queue, such as Apache Kafka. In this case it is possible to configure an additional set of subscribers that will read the same data stream while it is being written to ClickHouse and store it in cold storage somewhere. Most companies already have some default recommended cold storage, which could be an object store or a distributed filesystem like HDFS. "},{"title":"Filesystem Snapshots​","type":1,"pageTitle":"Data Backup","url":"docs/en/operations/backup#filesystem-snapshots","content":"Some local filesystems provide snapshot functionality (for example, ZFS), but they might not be the best choice for serving live queries. A possible solution is to create additional replicas with this kind of filesystem and exclude them from the Distributed tables that are used for SELECT queries. Snapshots on such replicas will be out of reach of any queries that modify data. As a bonus, these replicas might have special hardware configurations with more disks attached per server, which would be cost-effective. "},{"title":"clickhouse-copier​","type":1,"pageTitle":"Data Backup","url":"docs/en/operations/backup#clickhouse-copier","content":"clickhouse-copier is a versatile tool that was initially created to re-shard petabyte-sized tables. It can also be used for backup and restore purposes because it reliably copies data between ClickHouse tables and clusters. For smaller volumes of data, a simple INSERT INTO ... SELECT ... to remote tables might work as well. "},{"title":"Manipulations with Parts​","type":1,"pageTitle":"Data Backup","url":"docs/en/operations/backup#manipulations-with-parts","content":"ClickHouse allows using the ALTER TABLE ... FREEZE PARTITION ... query to create a local copy of table partitions. This is implemented using hardlinks to the /var/lib/clickhouse/shadow/ folder, so it usually does not consume extra disk space for old data. The created copies of files are not handled by ClickHouse server, so you can just leave them there: you will have a simple backup that does not require any additional external system, but it will still be prone to hardware issues. For this reason, it’s better to remotely copy them to another location and then remove the local copies. Distributed filesystems and object stores are still a good options for this, but normal attached file servers with a large enough capacity might work as well (in this case the transfer will occur via the network filesystem or maybe rsync). Data can be restored from backup using the ALTER TABLE ... ATTACH PARTITION ... For more information about queries related to partition manipulations, see the ALTER documentation. A third-party tool is available to automate this approach: clickhouse-backup. Original article "},{"title":"External User Authenticators and Directories","type":0,"sectionRef":"#","url":"docs/en/operations/external-authenticators/","content":"External User Authenticators and Directories ClickHouse supports authenticating and managing users using external services. The following external authenticators and directories are supported: LDAP Authenticator and DirectoryKerberos AuthenticatorSSL X.509 authentication Original article","keywords":""},{"title":"Integration Libraries from Third-party Developers","type":0,"sectionRef":"#","url":"docs/en/interfaces/third-party/integrations","content":"","keywords":""},{"title":"Infrastructure Products​","type":1,"pageTitle":"Integration Libraries from Third-party Developers","url":"docs/en/interfaces/third-party/integrations#infrastructure-products","content":"Relational database management systems MySQL mysql2chProxySQLclickhouse-mysql-data-readerhorgh-replicator PostgreSQL clickhousedb_fdwinfi.clickhouse_fdw (uses infi.clickhouse_orm)pg2chclickhouse_fdw MSSQL ClickHouseMigrator Message queues Kafka clickhouse_sinker (uses Go client)stream-loader-clickhouse Stream processing Flink flink-clickhouse-sink Object storages S3 clickhouse-backup Container orchestration Kubernetes clickhouse-operator Configuration management puppet innogames/clickhousemfedotov/clickhouse Monitoring Graphite graphousecarbon-clickhousegraphite-clickhousegraphite-ch-optimizer - optimizes staled partitions in *GraphiteMergeTree if rules from rollup configuration could be applied Grafana clickhouse-grafana Prometheus clickhouse_exporterPromHouseclickhouse_exporter (uses Go client) Nagios check_clickhousecheck_clickhouse.py Zabbix clickhouse-zabbix-template Sematext clickhouse integration Logging rsyslog omclickhouse fluentd loghouse (for Kubernetes) logagent logagent output-plugin-clickhouse Geo MaxMind clickhouse-maxmind-geoip AutoML MindsDB MindsDB - Predictive AI layer for ClickHouse database. "},{"title":"Programming Language Ecosystems​","type":1,"pageTitle":"Integration Libraries from Third-party Developers","url":"docs/en/interfaces/third-party/integrations#programming-language-ecosystems","content":"Python SQLAlchemy sqlalchemy-clickhouse (uses infi.clickhouse_orm) pandas pandahouse PHP Doctrine dbal-clickhouse R dplyr RClickHouse (uses clickhouse-cpp) Java Hadoop clickhouse-hdfs-loader (uses JDBC) Scala Akka clickhouse-scala-client C# ADO.NET ClickHouse.AdoClickHouse.ClientClickHouse.NetClickHouse.Net.Migrations Elixir Ecto clickhouse_ecto Ruby Ruby on Rails activecubeActiveRecord GraphQL activecube-graphql Original article "},{"title":"Cache Types","type":0,"sectionRef":"#","url":"docs/en/operations/caches","content":"Cache Types When performing queries, ClichHouse uses different caches. Main cache types: mark_cache — Cache of marks used by table engines of the MergeTree family.uncompressed_cache — Cache of uncompressed data used by table engines of the MergeTree family. Additional cache types: DNS cache.Regexp cache.Compiled expressions cache.Avro format schemas cache.Dictionaries data cache. Indirectly used: OS page cache. To drop cache, use SYSTEM DROP ... CACHE statements. Original article","keywords":""},{"title":"Configuration Files","type":0,"sectionRef":"#","url":"docs/en/operations/configuration-files","content":"","keywords":""},{"title":"Override​","type":1,"pageTitle":"Configuration Files","url":"docs/en/operations/configuration-files#override","content":"Some settings specified in the main configuration file can be overridden in other configuration files: The replace or remove attributes can be specified for the elements of these configuration files.If neither is specified, it combines the contents of elements recursively, replacing values of duplicate children.If replace is specified, it replaces the entire element with the specified one.If remove is specified, it deletes the element. You can also declare attributes as coming from environment variables by using from_env=&quot;VARIABLE_NAME&quot;: &lt;clickhouse&gt; &lt;macros&gt; &lt;replica from_env=&quot;REPLICA&quot; /&gt; &lt;layer from_env=&quot;LAYER&quot; /&gt; &lt;shard from_env=&quot;SHARD&quot; /&gt; &lt;/macros&gt; &lt;/clickhouse&gt;  "},{"title":"Substitution​","type":1,"pageTitle":"Configuration Files","url":"docs/en/operations/configuration-files#substitution","content":"The config can also define “substitutions”. If an element has the incl attribute, the corresponding substitution from the file will be used as the value. By default, the path to the file with substitutions is /etc/metrika.xml. This can be changed in the include_from element in the server config. The substitution values are specified in /clickhouse/substitution_name elements in this file. If a substitution specified in incl does not exist, it is recorded in the log. To prevent ClickHouse from logging missing substitutions, specify the optional=&quot;true&quot; attribute (for example, settings for macros). If you want to replace an entire element with a substitution use include as element name. XML substitution example: &lt;clickhouse&gt; &lt;!-- Appends XML subtree found at `/profiles-in-zookeeper` ZK path to `&lt;profiles&gt;` element. --&gt; &lt;profiles from_zk=&quot;/profiles-in-zookeeper&quot; /&gt; &lt;users&gt; &lt;!-- Replaces `include` element with the subtree found at `/users-in-zookeeper` ZK path. --&gt; &lt;include from_zk=&quot;/users-in-zookeeper&quot; /&gt; &lt;include from_zk=&quot;/other-users-in-zookeeper&quot; /&gt; &lt;/users&gt; &lt;/clickhouse&gt;  Substitutions can also be performed from ZooKeeper. To do this, specify the attribute from_zk = &quot;/path/to/node&quot;. The element value is replaced with the contents of the node at /path/to/node in ZooKeeper. You can also put an entire XML subtree on the ZooKeeper node and it will be fully inserted into the source element. "},{"title":"User Settings​","type":1,"pageTitle":"Configuration Files","url":"docs/en/operations/configuration-files#user-settings","content":"The config.xml file can specify a separate config with user settings, profiles, and quotas. The relative path to this config is set in the users_config element. By default, it is users.xml. If users_config is omitted, the user settings, profiles, and quotas are specified directly in config.xml. Users configuration can be splitted into separate files similar to config.xml and config.d/. Directory name is defined as users_config setting without .xml postfix concatenated with .d. Directory users.d is used by default, as users_config defaults to users.xml. Note that configuration files are first merged taking into account Override settings and includes are processed after that. "},{"title":"XML example​","type":1,"pageTitle":"Configuration Files","url":"docs/en/operations/configuration-files#example","content":"For example, you can have separate config file for each user like this: $ cat /etc/clickhouse-server/users.d/alice.xml  &lt;clickhouse&gt; &lt;users&gt; &lt;alice&gt; &lt;profile&gt;analytics&lt;/profile&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;password_sha256_hex&gt;...&lt;/password_sha256_hex&gt; &lt;quota&gt;analytics&lt;/quota&gt; &lt;/alice&gt; &lt;/users&gt; &lt;/clickhouse&gt;  "},{"title":"YAML examples​","type":1,"pageTitle":"Configuration Files","url":"docs/en/operations/configuration-files#example","content":"Here you can see default config written in YAML: config.yaml.example. There are some differences between YAML and XML formats in terms of ClickHouse configurations. Here are some tips for writing a configuration in YAML format. You should use a Scalar node to write a key-value pair: key: value  To create a node, containing other nodes you should use a Map: map_key: key1: val1 key2: val2 key3: val3  To create a list of values or nodes assigned to one tag you should use a Sequence: seq_key: - val1 - val2 - key1: val3 - map: key2: val4 key3: val5  If you want to write an attribute for a Sequence or Map node, you should use a @ prefix before the attribute key. Note, that @ is reserved by YAML standard, so you should also to wrap it into double quotes: map: &quot;@attr1&quot;: value1 &quot;@attr2&quot;: value2 key: 123  From that Map we will get these XML nodes: &lt;map attr1=&quot;value1&quot; attr2=&quot;value2&quot;&gt; &lt;key&gt;123&lt;/key&gt; &lt;/map&gt;  You can also set attributes for Sequence: seq: - &quot;@attr1&quot;: value1 - &quot;@attr2&quot;: value2 - 123 - abc  So, we can get YAML config equal to this XML one: &lt;seq attr1=&quot;value1&quot; attr2=&quot;value2&quot;&gt;123&lt;/seq&gt; &lt;seq attr1=&quot;value1&quot; attr2=&quot;value2&quot;&gt;abc&lt;/seq&gt;  "},{"title":"Implementation Details​","type":1,"pageTitle":"Configuration Files","url":"docs/en/operations/configuration-files#implementation-details","content":"For each config file, the server also generates file-preprocessed.xml files when starting. These files contain all the completed substitutions and overrides, and they are intended for informational use. If ZooKeeper substitutions were used in the config files but ZooKeeper is not available on the server start, the server loads the configuration from the preprocessed file. The server tracks changes in config files, as well as files and ZooKeeper nodes that were used when performing substitutions and overrides, and reloads the settings for users and clusters on the fly. This means that you can modify the cluster, users, and their settings without restarting the server. Original article "},{"title":"SSL X.509 certificate authentication","type":0,"sectionRef":"#","url":"docs/en/operations/external-authenticators/ssl-x509","content":"SSL X.509 certificate authentication SSL 'strict' option enables mandatory certificate validation for the incoming connections. In this case, only connections with trusted certificates can be established. Connections with untrusted certificates will be rejected. Thus, certificate validation allows to uniquely authenticate an incoming connection. Common Name field of the certificate is used to identify connected user. This allows to associate multiple certificates with the same user. Additionally, reissuing and revoking of the certificates does not affect the ClickHouse configuration. To enable SSL certificate authentication, a list of Common Name's for each ClickHouse user must be sspecified in the settings file config.xml : Example &lt;clickhouse&gt; &lt;!- ... --&gt; &lt;users&gt; &lt;user_name&gt; &lt;certificates&gt; &lt;common_name&gt;host.domain.com:example_user&lt;/common_name&gt; &lt;common_name&gt;host.domain.com:example_user_dev&lt;/common_name&gt; &lt;!-- More names --&gt; &lt;/certificates&gt; &lt;!-- Other settings --&gt; &lt;/user_name&gt; &lt;/users&gt; &lt;/clickhouse&gt; For the SSL chain of trust to work correctly, it is also important to make sure that the caConfig parameter is configured properly.","keywords":""},{"title":"Kerberos","type":0,"sectionRef":"#","url":"docs/en/operations/external-authenticators/kerberos","content":"","keywords":""},{"title":"Enabling Kerberos in ClickHouse​","type":1,"pageTitle":"Kerberos","url":"docs/en/operations/external-authenticators/kerberos#enabling-kerberos-in-clickhouse","content":"To enable Kerberos, one should include kerberos section in config.xml. This section may contain additional parameters. Parameters:​ principal - canonical service principal name that will be acquired and used when accepting security contexts. This parameter is optional, if omitted, the default principal will be used. realm - a realm, that will be used to restrict authentication to only those requests whose initiator's realm matches it. This parameter is optional, if omitted, no additional filtering by realm will be applied. Example (goes into config.xml): &lt;clickhouse&gt; &lt;!- ... --&gt; &lt;kerberos /&gt; &lt;/clickhouse&gt;  With principal specification: &lt;clickhouse&gt; &lt;!- ... --&gt; &lt;kerberos&gt; &lt;principal&gt;HTTP/clickhouse.example.com@EXAMPLE.COM&lt;/principal&gt; &lt;/kerberos&gt; &lt;/clickhouse&gt;  With filtering by realm: &lt;clickhouse&gt; &lt;!- ... --&gt; &lt;kerberos&gt; &lt;realm&gt;EXAMPLE.COM&lt;/realm&gt; &lt;/kerberos&gt; &lt;/clickhouse&gt;  warning You can define only one kerberos section. The presence of multiple kerberos sections will force ClickHouse to disable Kerberos authentication. warning principal and realm sections cannot be specified at the same time. The presence of both principal and realm sections will force ClickHouse to disable Kerberos authentication. "},{"title":"Kerberos as an external authenticator for existing users​","type":1,"pageTitle":"Kerberos","url":"docs/en/operations/external-authenticators/kerberos#kerberos-as-an-external-authenticator-for-existing-users","content":"Kerberos can be used as a method for verifying the identity of locally defined users (users defined in users.xml or in local access control paths). Currently, only requests over the HTTP interface can be kerberized (via GSS-SPNEGO mechanism). Kerberos principal name format usually follows this pattern: primary/instance@REALM The /instance part may occur zero or more times. The primary part of the canonical principal name of the initiator is expected to match the kerberized user name for authentication to succeed. "},{"title":"Enabling Kerberos in users.xml​","type":1,"pageTitle":"Kerberos","url":"docs/en/operations/external-authenticators/kerberos#enabling-kerberos-in-users-xml","content":"In order to enable Kerberos authentication for the user, specify kerberos section instead of password or similar sections in the user definition. Parameters: realm - a realm that will be used to restrict authentication to only those requests whose initiator's realm matches it. This parameter is optional, if omitted, no additional filtering by realm will be applied. Example (goes into users.xml): &lt;clickhouse&gt; &lt;!- ... --&gt; &lt;users&gt; &lt;!- ... --&gt; &lt;my_user&gt; &lt;!- ... --&gt; &lt;kerberos&gt; &lt;realm&gt;EXAMPLE.COM&lt;/realm&gt; &lt;/kerberos&gt; &lt;/my_user&gt; &lt;/users&gt; &lt;/clickhouse&gt;  warning Note that Kerberos authentication cannot be used alongside with any other authentication mechanism. The presence of any other sections like password alongside kerberos will force ClickHouse to shutdown. Reminder Note, that now, once user my_user uses kerberos, Kerberos must be enabled in the main config.xml file as described previously. "},{"title":"Enabling Kerberos using SQL​","type":1,"pageTitle":"Kerberos","url":"docs/en/operations/external-authenticators/kerberos#enabling-kerberos-using-sql","content":"When SQL-driven Access Control and Account Management is enabled in ClickHouse, users identified by Kerberos can also be created using SQL statements. CREATE USER my_user IDENTIFIED WITH kerberos REALM 'EXAMPLE.COM'  ...or, without filtering by realm: CREATE USER my_user IDENTIFIED WITH kerberos  "},{"title":"Monitoring","type":0,"sectionRef":"#","url":"docs/en/operations/monitoring","content":"","keywords":""},{"title":"Resource Utilization​","type":1,"pageTitle":"Monitoring","url":"docs/en/operations/monitoring#resource-utilization","content":"ClickHouse does not monitor the state of hardware resources by itself. It is highly recommended to set up monitoring for: Load and temperature on processors. You can use dmesg, turbostat or other instruments. Utilization of storage system, RAM and network. "},{"title":"ClickHouse Server Metrics​","type":1,"pageTitle":"Monitoring","url":"docs/en/operations/monitoring#clickhouse-server-metrics","content":"ClickHouse server has embedded instruments for self-state monitoring. To track server events use server logs. See the logger section of the configuration file. ClickHouse collects: Different metrics of how the server uses computational resources.Common statistics on query processing. You can find metrics in the system.metrics, system.events, and system.asynchronous_metrics tables. You can configure ClickHouse to export metrics to Graphite. See the Graphite section in the ClickHouse server configuration file. Before configuring export of metrics, you should set up Graphite by following their official guide. You can configure ClickHouse to export metrics to Prometheus. See the Prometheus section in the ClickHouse server configuration file. Before configuring export of metrics, you should set up Prometheus by following their official guide. Additionally, you can monitor server availability through the HTTP API. Send the HTTP GET request to /ping. If the server is available, it responds with 200 OK. To monitor servers in a cluster configuration, you should set the max_replica_delay_for_distributed_queries parameter and use the HTTP resource /replicas_status. A request to /replicas_status returns 200 OK if the replica is available and is not delayed behind the other replicas. If a replica is delayed, it returns 503 HTTP_SERVICE_UNAVAILABLE with information about the gap. "},{"title":"Optimizing Performance","type":0,"sectionRef":"#","url":"docs/en/operations/optimizing-performance/","content":"Optimizing Performance Sampling query profiler","keywords":""},{"title":"[pre-production] ClickHouse Keeper","type":0,"sectionRef":"#","url":"docs/en/operations/clickhouse-keeper","content":"","keywords":""},{"title":"Implementation details​","type":1,"pageTitle":"[pre-production] ClickHouse Keeper","url":"docs/en/operations/clickhouse-keeper#implementation-details","content":"ZooKeeper is one of the first well-known open-source coordination systems. It's implemented in Java, has quite a simple and powerful data model. ZooKeeper's coordination algorithm called ZAB (ZooKeeper Atomic Broadcast) doesn't provide linearizability guarantees for reads, because each ZooKeeper node serves reads locally. Unlike ZooKeeper ClickHouse Keeper is written in C++ and uses RAFT algorithm implementation. This algorithm allows to have linearizability for reads and writes, has several open-source implementations in different languages. By default, ClickHouse Keeper provides the same guarantees as ZooKeeper (linearizable writes, non-linearizable reads). It has a compatible client-server protocol, so any standard ZooKeeper client can be used to interact with ClickHouse Keeper. Snapshots and logs have an incompatible format with ZooKeeper, but clickhouse-keeper-converter tool allows to convert ZooKeeper data to ClickHouse Keeper snapshot. Interserver protocol in ClickHouse Keeper is also incompatible with ZooKeeper so mixed ZooKeeper / ClickHouse Keeper cluster is impossible. ClickHouse Keeper supports Access Control List (ACL) the same way as ZooKeeper does. ClickHouse Keeper supports the same set of permissions and has the identical built-in schemes: world, auth, digest, host and ip. Digest authentication scheme uses pair username:password. Password is encoded in Base64. note External integrations are not supported. "},{"title":"Configuration​","type":1,"pageTitle":"[pre-production] ClickHouse Keeper","url":"docs/en/operations/clickhouse-keeper#configuration","content":"ClickHouse Keeper can be used as a standalone replacement for ZooKeeper or as an internal part of the ClickHouse server, but in both cases configuration is almost the same .xml file. The main ClickHouse Keeper configuration tag is &lt;keeper_server&gt;. Keeper configuration has the following parameters: tcp_port — Port for a client to connect (default for ZooKeeper is 2181).tcp_port_secure — Secure port for an SSL connection between client and keeper-server.server_id — Unique server id, each participant of the ClickHouse Keeper cluster must have a unique number (1, 2, 3, and so on).log_storage_path — Path to coordination logs, better to store logs on the non-busy device (same for ZooKeeper).snapshot_storage_path — Path to coordination snapshots. Other common parameters are inherited from the ClickHouse server config (listen_host, logger, and so on). Internal coordination settings are located in &lt;keeper_server&gt;.&lt;coordination_settings&gt; section: operation_timeout_ms — Timeout for a single client operation (ms) (default: 10000).min_session_timeout_ms — Min timeout for client session (ms) (default: 10000).session_timeout_ms — Max timeout for client session (ms) (default: 100000).dead_session_check_period_ms — How often ClickHouse Keeper check dead sessions and remove them (ms) (default: 500).heart_beat_interval_ms — How often a ClickHouse Keeper leader will send heartbeats to followers (ms) (default: 500).election_timeout_lower_bound_ms — If the follower didn't receive heartbeats from the leader in this interval, then it can initiate leader election (default: 1000).election_timeout_upper_bound_ms — If the follower didn't receive heartbeats from the leader in this interval, then it must initiate leader election (default: 2000).rotate_log_storage_interval — How many log records to store in a single file (default: 100000).reserved_log_items — How many coordination log records to store before compaction (default: 100000).snapshot_distance — How often ClickHouse Keeper will create new snapshots (in the number of records in logs) (default: 100000).snapshots_to_keep — How many snapshots to keep (default: 3).stale_log_gap — Threshold when leader considers follower as stale and sends the snapshot to it instead of logs (default: 10000).fresh_log_gap — When node became fresh (default: 200).max_requests_batch_size - Max size of batch in requests count before it will be sent to RAFT (default: 100).force_sync — Call fsync on each write to coordination log (default: true).quorum_reads — Execute read requests as writes through whole RAFT consensus with similar speed (default: false).raft_logs_level — Text logging level about coordination (trace, debug, and so on) (default: system default).auto_forwarding — Allow to forward write requests from followers to the leader (default: true).shutdown_timeout — Wait to finish internal connections and shutdown (ms) (default: 5000).startup_timeout — If the server doesn't connect to other quorum participants in the specified timeout it will terminate (ms) (default: 30000).four_letter_word_white_list — White list of 4lw commands (default: &quot;conf,cons,crst,envi,ruok,srst,srvr,stat,wchc,wchs,dirs,mntr,isro&quot;). Quorum configuration is located in &lt;keeper_server&gt;.&lt;raft_configuration&gt; section and contain servers description. The only parameter for the whole quorum is secure, which enables encrypted connection for communication between quorum participants. The parameter can be set true if SSL connection is required for internal communication between nodes, or left unspecified otherwise. The main parameters for each &lt;server&gt; are: id — Server identifier in a quorum.hostname — Hostname where this server is placed.port — Port where this server listens for connections. Examples of configuration for quorum with three nodes can be found in integration tests with test_keeper_ prefix. Example configuration for server #1: &lt;keeper_server&gt; &lt;tcp_port&gt;2181&lt;/tcp_port&gt; &lt;server_id&gt;1&lt;/server_id&gt; &lt;log_storage_path&gt;/var/lib/clickhouse/coordination/log&lt;/log_storage_path&gt; &lt;snapshot_storage_path&gt;/var/lib/clickhouse/coordination/snapshots&lt;/snapshot_storage_path&gt; &lt;coordination_settings&gt; &lt;operation_timeout_ms&gt;10000&lt;/operation_timeout_ms&gt; &lt;session_timeout_ms&gt;30000&lt;/session_timeout_ms&gt; &lt;raft_logs_level&gt;trace&lt;/raft_logs_level&gt; &lt;/coordination_settings&gt; &lt;raft_configuration&gt; &lt;server&gt; &lt;id&gt;1&lt;/id&gt; &lt;hostname&gt;zoo1&lt;/hostname&gt; &lt;port&gt;9444&lt;/port&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;2&lt;/id&gt; &lt;hostname&gt;zoo2&lt;/hostname&gt; &lt;port&gt;9444&lt;/port&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;3&lt;/id&gt; &lt;hostname&gt;zoo3&lt;/hostname&gt; &lt;port&gt;9444&lt;/port&gt; &lt;/server&gt; &lt;/raft_configuration&gt; &lt;/keeper_server&gt;  "},{"title":"How to run​","type":1,"pageTitle":"[pre-production] ClickHouse Keeper","url":"docs/en/operations/clickhouse-keeper#how-to-run","content":"ClickHouse Keeper is bundled into the ClickHouse server package, just add configuration of &lt;keeper_server&gt; and start ClickHouse server as always. If you want to run standalone ClickHouse Keeper you can start it in a similar way with: clickhouse-keeper --config /etc/your_path_to_config/config.xml  If you don't have the symlink (clickhouse-keeper) you can create it or specify keeper as argument: clickhouse keeper --config /etc/your_path_to_config/config.xml  "},{"title":"Four Letter Word Commands​","type":1,"pageTitle":"[pre-production] ClickHouse Keeper","url":"docs/en/operations/clickhouse-keeper#four-letter-word-commands","content":"ClickHouse Keeper also provides 4lw commands which are almost the same with Zookeeper. Each command is composed of four letters such as mntr, stat etc. There are some more interesting commands: stat gives some general information about the server and connected clients, while srvr and cons give extended details on server and connections respectively. The 4lw commands has a white list configuration four_letter_word_white_list which has default value &quot;conf,cons,crst,envi,ruok,srst,srvr,stat,wchc,wchs,dirs,mntr,isro&quot;. You can issue the commands to ClickHouse Keeper via telnet or nc, at the client port. echo mntr | nc localhost 9181  Bellow is the detailed 4lw commands: ruok: Tests if server is running in a non-error state. The server will respond with imok if it is running. Otherwise it will not respond at all. A response of &quot;imok&quot; does not necessarily indicate that the server has joined the quorum, just that the server process is active and bound to the specified client port. Use &quot;stat&quot; for details on state wrt quorum and client connection information. imok  mntr: Outputs a list of variables that could be used for monitoring the health of the cluster. zk_version v21.11.1.1-prestable-7a4a0b0edef0ad6e0aa662cd3b90c3f4acf796e7 zk_avg_latency 0 zk_max_latency 0 zk_min_latency 0 zk_packets_received 68 zk_packets_sent 68 zk_num_alive_connections 1 zk_outstanding_requests 0 zk_server_state leader zk_znode_count 4 zk_watch_count 1 zk_ephemerals_count 0 zk_approximate_data_size 723 zk_open_file_descriptor_count 310 zk_max_file_descriptor_count 10240 zk_followers 0 zk_synced_followers 0  srvr: Lists full details for the server. ClickHouse Keeper version: v21.11.1.1-prestable-7a4a0b0edef0ad6e0aa662cd3b90c3f4acf796e7 Latency min/avg/max: 0/0/0 Received: 2 Sent : 2 Connections: 1 Outstanding: 0 Zxid: 34 Mode: leader Node count: 4  stat: Lists brief details for the server and connected clients. ClickHouse Keeper version: v21.11.1.1-prestable-7a4a0b0edef0ad6e0aa662cd3b90c3f4acf796e7 Clients: 192.168.1.1:52852(recved=0,sent=0) 192.168.1.1:52042(recved=24,sent=48) Latency min/avg/max: 0/0/0 Received: 4 Sent : 4 Connections: 1 Outstanding: 0 Zxid: 36 Mode: leader Node count: 4  srst: Reset server statistics. The command will affect the result of srvr, mntr and stat. Server stats reset.  conf: Print details about serving configuration. server_id=1 tcp_port=2181 four_letter_word_white_list=* log_storage_path=./coordination/logs snapshot_storage_path=./coordination/snapshots max_requests_batch_size=100 session_timeout_ms=30000 operation_timeout_ms=10000 dead_session_check_period_ms=500 heart_beat_interval_ms=500 election_timeout_lower_bound_ms=1000 election_timeout_upper_bound_ms=2000 reserved_log_items=1000000000000000 snapshot_distance=10000 auto_forwarding=true shutdown_timeout=5000 startup_timeout=240000 raft_logs_level=information snapshots_to_keep=3 rotate_log_storage_interval=100000 stale_log_gap=10000 fresh_log_gap=200 max_requests_batch_size=100 quorum_reads=false force_sync=false compress_logs=true compress_snapshots_with_zstd_format=true configuration_change_tries_count=20  cons: List full connection/session details for all clients connected to this server. Includes information on numbers of packets received/sent, session id, operation latencies, last operation performed, etc...  192.168.1.1:52163(recved=0,sent=0,sid=0xffffffffffffffff,lop=NA,est=1636454787393,to=30000,lzxid=0xffffffffffffffff,lresp=0,llat=0,minlat=0,avglat=0,maxlat=0) 192.168.1.1:52042(recved=9,sent=18,sid=0x0000000000000001,lop=List,est=1636454739887,to=30000,lcxid=0x0000000000000005,lzxid=0x0000000000000005,lresp=1636454739892,llat=0,minlat=0,avglat=0,maxlat=0)  crst: Reset connection/session statistics for all connections. Connection stats reset.  envi: Print details about serving environment Environment: clickhouse.keeper.version=v21.11.1.1-prestable-7a4a0b0edef0ad6e0aa662cd3b90c3f4acf796e7 host.name=ZBMAC-C02D4054M.local os.name=Darwin os.arch=x86_64 os.version=19.6.0 cpu.count=12 user.name=root user.home=/Users/JackyWoo/ user.dir=/Users/JackyWoo/project/jd/clickhouse/cmake-build-debug/programs/ user.tmp=/var/folders/b4/smbq5mfj7578f2jzwn602tt40000gn/T/  dirs: Shows the total size of snapshot and log files in bytes snapshot_dir_size: 0 log_dir_size: 3875  isro: Tests if server is running in read-only mode. The server will respond with &quot;ro&quot; if in read-only mode or &quot;rw&quot; if not in read-only mode. rw  wchs: Lists brief information on watches for the server. 1 connections watching 1 paths Total watches:1  wchc: Lists detailed information on watches for the server, by session. This outputs a list of sessions (connections) with associated watches (paths). Note, depending on the number of watches this operation may be expensive (ie impact server performance), use it carefully. 0x0000000000000001 /clickhouse/task_queue/ddl  wchp: Lists detailed information on watches for the server, by path. This outputs a list of paths (znodes) with associated sessions. Note, depending on the number of watches this operation may be expensive (i. e. impact server performance), use it carefully. /clickhouse/task_queue/ddl 0x0000000000000001  dump: Lists the outstanding sessions and ephemeral nodes. This only works on the leader. Sessions dump (2): 0x0000000000000001 0x0000000000000002 Sessions with Ephemerals (1): 0x0000000000000001 /clickhouse/task_queue/ddl  "},{"title":"[experimental] Migration from ZooKeeper​","type":1,"pageTitle":"[pre-production] ClickHouse Keeper","url":"docs/en/operations/clickhouse-keeper#migration-from-zookeeper","content":"Seamlessly migration from ZooKeeper to ClickHouse Keeper is impossible you have to stop your ZooKeeper cluster, convert data and start ClickHouse Keeper. clickhouse-keeper-converter tool allows converting ZooKeeper logs and snapshots to ClickHouse Keeper snapshot. It works only with ZooKeeper &gt; 3.4. Steps for migration: Stop all ZooKeeper nodes. Optional, but recommended: find ZooKeeper leader node, start and stop it again. It will force ZooKeeper to create a consistent snapshot. Run clickhouse-keeper-converter on a leader, for example: clickhouse-keeper-converter --zookeeper-logs-dir /var/lib/zookeeper/version-2 --zookeeper-snapshots-dir /var/lib/zookeeper/version-2 --output-dir /path/to/clickhouse/keeper/snapshots  Copy snapshot to ClickHouse server nodes with a configured keeper or start ClickHouse Keeper instead of ZooKeeper. The snapshot must persist on all nodes, otherwise, empty nodes can be faster and one of them can become a leader. Original article "},{"title":"Sampling Query Profiler","type":0,"sectionRef":"#","url":"docs/en/operations/optimizing-performance/sampling-query-profiler","content":"","keywords":""},{"title":"Example​","type":1,"pageTitle":"Sampling Query Profiler","url":"docs/en/operations/optimizing-performance/sampling-query-profiler#example","content":"In this example we: Filtering trace_log data by a query identifier and the current date. Aggregating by stack trace. Using introspection functions, we will get a report of: Names of symbols and corresponding source code functions.Source code locations of these functions. SELECT count(), arrayStringConcat(arrayMap(x -&gt; concat(demangle(addressToSymbol(x)), '\\n ', addressToLine(x)), trace), '\\n') AS sym FROM system.trace_log WHERE (query_id = 'ebca3574-ad0a-400a-9cbc-dca382f5998c') AND (event_date = today()) GROUP BY trace ORDER BY count() DESC LIMIT 10  {% include &quot;examples/sampling_query_profiler_result.txt&quot; %}  "},{"title":"Storing details for connecting to external sources in configuration files","type":0,"sectionRef":"#","url":"docs/en/operations/named-collections","content":"","keywords":""},{"title":"Named connections for accessing S3.​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"docs/en/operations/named-collections#named-connections-for-accessing-s3","content":"The description of parameters see s3 Table Function. Example of configuration: &lt;clickhouse&gt; &lt;named_collections&gt; &lt;s3_mydata&gt; &lt;access_key_id&gt;AKIAIOSFODNN7EXAMPLE&lt;/access_key_id&gt; &lt;secret_access_key&gt; wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&lt;/secret_access_key&gt; &lt;format&gt;CSV&lt;/format&gt; &lt;url&gt;https://s3.us-east-1.amazonaws.com/yourbucket/mydata/&lt;/url&gt; &lt;/s3_mydata&gt; &lt;/named_collections&gt; &lt;/clickhouse&gt;  "},{"title":"Example of using named connections with the s3 function​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"docs/en/operations/named-collections#example-of-using-named-connections-with-the-s3-function","content":"INSERT INTO FUNCTION s3(s3_mydata, filename = 'test_file.tsv.gz', format = 'TSV', structure = 'number UInt64', compression_method = 'gzip') SELECT * FROM numbers(10000); SELECT count() FROM s3(s3_mydata, filename = 'test_file.tsv.gz') ┌─count()─┐ │ 10000 │ └─────────┘ 1 rows in set. Elapsed: 0.279 sec. Processed 10.00 thousand rows, 90.00 KB (35.78 thousand rows/s., 322.02 KB/s.)  "},{"title":"Example of using named connections with an S3 table​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"docs/en/operations/named-collections#example-of-using-named-connections-with-an-s3-table","content":"CREATE TABLE s3_engine_table (number Int64) ENGINE=S3(s3_mydata, url='https://s3.us-east-1.amazonaws.com/yourbucket/mydata/test_file.tsv.gz', format = 'TSV') SETTINGS input_format_with_names_use_header = 0; SELECT * FROM s3_engine_table LIMIT 3; ┌─number─┐ │ 0 │ │ 1 │ │ 2 │ └────────┘  "},{"title":"Named connections for accessing MySQL database​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"docs/en/operations/named-collections#named-connections-for-accessing-mysql-database","content":"The description of parameters see mysql. Example of configuration: &lt;clickhouse&gt; &lt;named_collections&gt; &lt;mymysql&gt; &lt;user&gt;myuser&lt;/user&gt; &lt;password&gt;mypass&lt;/password&gt; &lt;host&gt;127.0.0.1&lt;/host&gt; &lt;port&gt;3306&lt;/port&gt; &lt;database&gt;test&lt;/database&gt; &lt;connection_pool_size&gt;8&lt;/connection_pool_size&gt; &lt;on_duplicate_clause&gt;1&lt;/on_duplicate_clause&gt; &lt;replace_query&gt;1&lt;/replace_query&gt; &lt;/mymysql&gt; &lt;/named_collections&gt; &lt;/clickhouse&gt;  "},{"title":"Example of using named connections with the mysql function​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"docs/en/operations/named-collections#example-of-using-named-connections-with-the-mysql-function","content":"SELECT count() FROM mysql(mymysql, table = 'test'); ┌─count()─┐ │ 3 │ └─────────┘  "},{"title":"Example of using named connections with an MySQL table​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"docs/en/operations/named-collections#example-of-using-named-connections-with-an-mysql-table","content":"CREATE TABLE mytable(A Int64) ENGINE = MySQL(mymysql, table = 'test', connection_pool_size=3, replace_query=0); SELECT count() FROM mytable; ┌─count()─┐ │ 3 │ └─────────┘  "},{"title":"Example of using named connections with database with engine MySQL​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"docs/en/operations/named-collections#example-of-using-named-connections-with-database-with-engine-mysql","content":"CREATE DATABASE mydatabase ENGINE = MySQL(mymysql); SHOW TABLES FROM mydatabase; ┌─name───┐ │ source │ │ test │ └────────┘  "},{"title":"Example of using named connections with an external dictionary with source MySQL​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"docs/en/operations/named-collections#example-of-using-named-connections-with-an-external-dictionary-with-source-mysql","content":"CREATE DICTIONARY dict (A Int64, B String) PRIMARY KEY A SOURCE(MYSQL(NAME mymysql TABLE 'source')) LIFETIME(MIN 1 MAX 2) LAYOUT(HASHED()); SELECT dictGet('dict', 'B', 2); ┌─dictGet('dict', 'B', 2)─┐ │ two │ └─────────────────────────┘  "},{"title":"Named connections for accessing PostgreSQL database​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"docs/en/operations/named-collections#named-connections-for-accessing-postgresql-database","content":"The description of parameters see postgresql. Example of configuration: &lt;clickhouse&gt; &lt;named_collections&gt; &lt;mypg&gt; &lt;user&gt;pguser&lt;/user&gt; &lt;password&gt;jw8s0F4&lt;/password&gt; &lt;host&gt;127.0.0.1&lt;/host&gt; &lt;port&gt;5432&lt;/port&gt; &lt;database&gt;test&lt;/database&gt; &lt;schema&gt;test_schema&lt;/schema&gt; &lt;connection_pool_size&gt;8&lt;/connection_pool_size&gt; &lt;/mypg&gt; &lt;/named_collections&gt; &lt;/clickhouse&gt;  "},{"title":"Example of using named connections with the postgresql function​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"docs/en/operations/named-collections#example-of-using-named-connections-with-the-postgresql-function","content":"SELECT * FROM postgresql(mypg, table = 'test'); ┌─a─┬─b───┐ │ 2 │ two │ │ 1 │ one │ └───┴─────┘ SELECT * FROM postgresql(mypg, table = 'test', schema = 'public'); ┌─a─┐ │ 1 │ │ 2 │ │ 3 │ └───┘  "},{"title":"Example of using named connections with database with engine PostgreSQL​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"docs/en/operations/named-collections#example-of-using-named-connections-with-database-with-engine-postgresql","content":"CREATE TABLE mypgtable (a Int64) ENGINE = PostgreSQL(mypg, table = 'test', schema = 'public'); SELECT * FROM mypgtable; ┌─a─┐ │ 1 │ │ 2 │ │ 3 │ └───┘  "},{"title":"Example of using named connections with database with engine PostgreSQL​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"docs/en/operations/named-collections#example-of-using-named-connections-with-database-with-engine-postgresql-1","content":"CREATE DATABASE mydatabase ENGINE = PostgreSQL(mypg); SHOW TABLES FROM mydatabase ┌─name─┐ │ test │ └──────┘  "},{"title":"Example of using named connections with an external dictionary with source POSTGRESQL​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"docs/en/operations/named-collections#example-of-using-named-connections-with-an-external-dictionary-with-source-postgresql","content":"CREATE DICTIONARY dict (a Int64, b String) PRIMARY KEY a SOURCE(POSTGRESQL(NAME mypg TABLE test)) LIFETIME(MIN 1 MAX 2) LAYOUT(HASHED()); SELECT dictGet('dict', 'b', 2); ┌─dictGet('dict', 'b', 2)─┐ │ two │ └─────────────────────────┘  "},{"title":"[experimental] OpenTelemetry Support","type":0,"sectionRef":"#","url":"docs/en/operations/opentelemetry","content":"","keywords":""},{"title":"Supplying Trace Context to ClickHouse​","type":1,"pageTitle":"[experimental] OpenTelemetry Support","url":"docs/en/operations/opentelemetry#supplying-trace-context-to-clickhouse","content":"ClickHouse accepts trace context HTTP headers, as described by the W3C recommendation. It also accepts trace context over a native protocol that is used for communication between ClickHouse servers or between the client and server. For manual testing, trace context headers conforming to the Trace Context recommendation can be supplied to clickhouse-client using --opentelemetry-traceparent and --opentelemetry-tracestate flags. If no parent trace context is supplied or the provided trace context does not comply with W3C standard above, ClickHouse can start a new trace, with probability controlled by the opentelemetry_start_trace_probability setting. "},{"title":"Propagating the Trace Context​","type":1,"pageTitle":"[experimental] OpenTelemetry Support","url":"docs/en/operations/opentelemetry#propagating-the-trace-context","content":"The trace context is propagated to downstream services in the following cases: Queries to remote ClickHouse servers, such as when using Distributed table engine. url table function. Trace context information is sent in HTTP headers. "},{"title":"Tracing the ClickHouse Itself​","type":1,"pageTitle":"[experimental] OpenTelemetry Support","url":"docs/en/operations/opentelemetry#tracing-the-clickhouse-itself","content":"ClickHouse creates trace spans for each query and some of the query execution stages, such as query planning or distributed queries. To be useful, the tracing information has to be exported to a monitoring system that supports OpenTelemetry, such as Jaeger or Prometheus. ClickHouse avoids a dependency on a particular monitoring system, instead only providing the tracing data through a system table. OpenTelemetry trace span information required by the standard is stored in the system.opentelemetry_span_log table. The table must be enabled in the server configuration, see the opentelemetry_span_log element in the default config file config.xml. It is enabled by default. The tags or attributes are saved as two parallel arrays, containing the keys and values. Use ARRAY JOIN to work with them. "},{"title":"Integration with monitoring systems​","type":1,"pageTitle":"[experimental] OpenTelemetry Support","url":"docs/en/operations/opentelemetry#integration-with-monitoring-systems","content":"At the moment, there is no ready tool that can export the tracing data from ClickHouse to a monitoring system. For testing, it is possible to setup the export using a materialized view with the URL engine over the system.opentelemetry_span_log table, which would push the arriving log data to an HTTP endpoint of a trace collector. For example, to push the minimal span data to a Zipkin instance running at http://localhost:9411, in Zipkin v2 JSON format: CREATE MATERIALIZED VIEW default.zipkin_spans ENGINE = URL('http://127.0.0.1:9411/api/v2/spans', 'JSONEachRow') SETTINGS output_format_json_named_tuples_as_objects = 1, output_format_json_array_of_rows = 1 AS SELECT lower(hex(trace_id)) AS traceId, case when parent_span_id = 0 then '' else lower(hex(parent_span_id)) end AS parentId, lower(hex(span_id)) AS id, operation_name AS name, start_time_us AS timestamp, finish_time_us - start_time_us AS duration, cast(tuple('clickhouse'), 'Tuple(serviceName text)') AS localEndpoint, cast(tuple( attribute.values[indexOf(attribute.names, 'db.statement')]), 'Tuple(&quot;db.statement&quot; text)') AS tags FROM system.opentelemetry_span_log  In case of any errors, the part of the log data for which the error has occurred will be silently lost. Check the server log for error messages if the data does not arrive. Original article "},{"title":"HTTP Interface","type":0,"sectionRef":"#","url":"docs/en/interfaces/http","content":"","keywords":""},{"title":"Compression​","type":1,"pageTitle":"HTTP Interface","url":"docs/en/interfaces/http#compression","content":"You can use compression to reduce network traffic when transmitting a large amount of data or for creating dumps that are immediately compressed. You can use the internal ClickHouse compression format when transmitting data. The compressed data has a non-standard format, and you need clickhouse-compressor program to work with it. It is installed with the clickhouse-client package. To increase the efficiency of data insertion, you can disable server-side checksum verification by using the http_native_compression_disable_checksumming_on_decompress setting. If you specify compress=1 in the URL, the server will compress the data it sends to you. If you specify decompress=1 in the URL, the server will decompress the data which you pass in the POST method. You can also choose to use HTTP compression. ClickHouse supports the following compression methods: gzipbrdeflatexz To send a compressed POST request, append the request header Content-Encoding: compression_method. In order for ClickHouse to compress the response, enable compression with enable_http_compression setting and append Accept-Encoding: compression_method header to the request. You can configure the data compression level in the http_zlib_compression_level setting for all compression methods. info Some HTTP clients might decompress data from the server by default (with gzip and deflate) and you might get decompressed data even if you use the compression settings correctly. Examples # Sending compressed data to the server $ echo &quot;SELECT 1&quot; | gzip -c | \\ curl -sS --data-binary @- -H 'Content-Encoding: gzip' 'http://localhost:8123/'  # Receiving compressed data archive from the server $ curl -vsS &quot;http://localhost:8123/?enable_http_compression=1&quot; \\ -H 'Accept-Encoding: gzip' --output result.gz -d 'SELECT number FROM system.numbers LIMIT 3' $ zcat result.gz 0 1 2  # Receiving compressed data from the server and using the gunzip to receive decompressed data $ curl -sS &quot;http://localhost:8123/?enable_http_compression=1&quot; \\ -H 'Accept-Encoding: gzip' -d 'SELECT number FROM system.numbers LIMIT 3' | gunzip - 0 1 2  "},{"title":"Default Database​","type":1,"pageTitle":"HTTP Interface","url":"docs/en/interfaces/http#default-database","content":"You can use the ‘database’ URL parameter or the ‘X-ClickHouse-Database’ header to specify the default database. $ echo 'SELECT number FROM numbers LIMIT 10' | curl 'http://localhost:8123/?database=system' --data-binary @- 0 1 2 3 4 5 6 7 8 9  By default, the database that is registered in the server settings is used as the default database. By default, this is the database called ‘default’. Alternatively, you can always specify the database using a dot before the table name. The username and password can be indicated in one of three ways: Using HTTP Basic Authentication. Example: $ echo 'SELECT 1' | curl 'http://user:password@localhost:8123/' -d @-  In the ‘user’ and ‘password’ URL parameters. Example: $ echo 'SELECT 1' | curl 'http://localhost:8123/?user=user&amp;password=password' -d @-  Using ‘X-ClickHouse-User’ and ‘X-ClickHouse-Key’ headers. Example: $ echo 'SELECT 1' | curl -H 'X-ClickHouse-User: user' -H 'X-ClickHouse-Key: password' 'http://localhost:8123/' -d @-  If the user name is not specified, the default name is used. If the password is not specified, the empty password is used. You can also use the URL parameters to specify any settings for processing a single query or entire profiles of settings. Example:http://localhost:8123/?profile=web&amp;max_rows_to_read=1000000000&amp;query=SELECT+1 For more information, see the Settings section. $ echo 'SELECT number FROM system.numbers LIMIT 10' | curl 'http://localhost:8123/?' --data-binary @- 0 1 2 3 4 5 6 7 8 9  For information about other parameters, see the section “SET”. Similarly, you can use ClickHouse sessions in the HTTP protocol. To do this, you need to add the session_id GET parameter to the request. You can use any string as the session ID. By default, the session is terminated after 60 seconds of inactivity. To change this timeout, modify the default_session_timeout setting in the server configuration, or add the session_timeout GET parameter to the request. To check the session status, use the session_check=1 parameter. Only one query at a time can be executed within a single session. You can receive information about the progress of a query in X-ClickHouse-Progress response headers. To do this, enable send_progress_in_http_headers. Example of the header sequence: X-ClickHouse-Progress: {&quot;read_rows&quot;:&quot;2752512&quot;,&quot;read_bytes&quot;:&quot;240570816&quot;,&quot;total_rows_to_read&quot;:&quot;8880128&quot;} X-ClickHouse-Progress: {&quot;read_rows&quot;:&quot;5439488&quot;,&quot;read_bytes&quot;:&quot;482285394&quot;,&quot;total_rows_to_read&quot;:&quot;8880128&quot;} X-ClickHouse-Progress: {&quot;read_rows&quot;:&quot;8783786&quot;,&quot;read_bytes&quot;:&quot;819092887&quot;,&quot;total_rows_to_read&quot;:&quot;8880128&quot;}  Possible header fields: read_rows — Number of rows read.read_bytes — Volume of data read in bytes.total_rows_to_read — Total number of rows to be read.written_rows — Number of rows written.written_bytes — Volume of data written in bytes. Running requests do not stop automatically if the HTTP connection is lost. Parsing and data formatting are performed on the server-side, and using the network might be ineffective. The optional ‘query_id’ parameter can be passed as the query ID (any string). For more information, see the section “Settings, replace_running_query”. The optional ‘quota_key’ parameter can be passed as the quota key (any string). For more information, see the section “Quotas”. The HTTP interface allows passing external data (external temporary tables) for querying. For more information, see the section “External data for query processing”. "},{"title":"Response Buffering​","type":1,"pageTitle":"HTTP Interface","url":"docs/en/interfaces/http#response-buffering","content":"You can enable response buffering on the server-side. The buffer_size and wait_end_of_query URL parameters are provided for this purpose. buffer_size determines the number of bytes in the result to buffer in the server memory. If a result body is larger than this threshold, the buffer is written to the HTTP channel, and the remaining data is sent directly to the HTTP channel. To ensure that the entire response is buffered, set wait_end_of_query=1. In this case, the data that is not stored in memory will be buffered in a temporary server file. Example: $ curl -sS 'http://localhost:8123/?max_result_bytes=4000000&amp;buffer_size=3000000&amp;wait_end_of_query=1' -d 'SELECT toUInt8(number) FROM system.numbers LIMIT 9000000 FORMAT RowBinary'  Use buffering to avoid situations where a query processing error occurred after the response code and HTTP headers were sent to the client. In this situation, an error message is written at the end of the response body, and on the client-side, the error can only be detected at the parsing stage. "},{"title":"Queries with Parameters​","type":1,"pageTitle":"HTTP Interface","url":"docs/en/interfaces/http#cli-queries-with-parameters","content":"You can create a query with parameters and pass values for them from the corresponding HTTP request parameters. For more information, see Queries with Parameters for CLI. "},{"title":"Example​","type":1,"pageTitle":"HTTP Interface","url":"docs/en/interfaces/http#example","content":"$ curl -sS &quot;&lt;address&gt;?param_id=2&amp;param_phrase=test&quot; -d &quot;SELECT * FROM table WHERE int_column = {id:UInt8} and string_column = {phrase:String}&quot;  "},{"title":"Predefined HTTP Interface​","type":1,"pageTitle":"HTTP Interface","url":"docs/en/interfaces/http#predefined_http_interface","content":"ClickHouse supports specific queries through the HTTP interface. For example, you can write data to a table as follows: $ echo '(4),(5),(6)' | curl 'http://localhost:8123/?query=INSERT%20INTO%20t%20VALUES' --data-binary @-  ClickHouse also supports Predefined HTTP Interface which can help you more easily integrate with third-party tools like Prometheus exporter. Example: First of all, add this section to server configuration file: &lt;http_handlers&gt; &lt;rule&gt; &lt;url&gt;/predefined_query&lt;/url&gt; &lt;methods&gt;POST,GET&lt;/methods&gt; &lt;handler&gt; &lt;type&gt;predefined_query_handler&lt;/type&gt; &lt;query&gt;SELECT * FROM system.metrics LIMIT 5 FORMAT Template SETTINGS format_template_resultset = 'prometheus_template_output_format_resultset', format_template_row = 'prometheus_template_output_format_row', format_template_rows_between_delimiter = '\\n'&lt;/query&gt; &lt;/handler&gt; &lt;/rule&gt; &lt;rule&gt;...&lt;/rule&gt; &lt;rule&gt;...&lt;/rule&gt; &lt;/http_handlers&gt;  You can now request the URL directly for data in the Prometheus format: $ curl -v 'http://localhost:8123/predefined_query' * Trying ::1... * Connected to localhost (::1) port 8123 (#0) &gt; GET /predefined_query HTTP/1.1 &gt; Host: localhost:8123 &gt; User-Agent: curl/7.47.0 &gt; Accept: */* &gt; &lt; HTTP/1.1 200 OK &lt; Date: Tue, 28 Apr 2020 08:52:56 GMT &lt; Connection: Keep-Alive &lt; Content-Type: text/plain; charset=UTF-8 &lt; X-ClickHouse-Server-Display-Name: i-mloy5trc &lt; Transfer-Encoding: chunked &lt; X-ClickHouse-Query-Id: 96fe0052-01e6-43ce-b12a-6b7370de6e8a &lt; X-ClickHouse-Format: Template &lt; X-ClickHouse-Timezone: Asia/Shanghai &lt; Keep-Alive: timeout=3 &lt; X-ClickHouse-Summary: {&quot;read_rows&quot;:&quot;0&quot;,&quot;read_bytes&quot;:&quot;0&quot;,&quot;written_rows&quot;:&quot;0&quot;,&quot;written_bytes&quot;:&quot;0&quot;,&quot;total_rows_to_read&quot;:&quot;0&quot;} &lt; # HELP &quot;Query&quot; &quot;Number of executing queries&quot; # TYPE &quot;Query&quot; counter &quot;Query&quot; 1 # HELP &quot;Merge&quot; &quot;Number of executing background merges&quot; # TYPE &quot;Merge&quot; counter &quot;Merge&quot; 0 # HELP &quot;PartMutation&quot; &quot;Number of mutations (ALTER DELETE/UPDATE)&quot; # TYPE &quot;PartMutation&quot; counter &quot;PartMutation&quot; 0 # HELP &quot;ReplicatedFetch&quot; &quot;Number of data parts being fetched from replica&quot; # TYPE &quot;ReplicatedFetch&quot; counter &quot;ReplicatedFetch&quot; 0 # HELP &quot;ReplicatedSend&quot; &quot;Number of data parts being sent to replicas&quot; # TYPE &quot;ReplicatedSend&quot; counter &quot;ReplicatedSend&quot; 0 * Connection #0 to host localhost left intact * Connection #0 to host localhost left intact  As you can see from the example if http_handlers is configured in the config.xml file and http_handlers can contain many rules. ClickHouse will match the HTTP requests received to the predefined type in rule and the first matched runs the handler. Then ClickHouse will execute the corresponding predefined query if the match is successful. Now rule can configure method, headers, url, handler: method is responsible for matching the method part of the HTTP request. method fully conforms to the definition of method in the HTTP protocol. It is an optional configuration. If it is not defined in the configuration file, it does not match the method portion of the HTTP request. url is responsible for matching the URL part of the HTTP request. It is compatible with RE2’s regular expressions. It is an optional configuration. If it is not defined in the configuration file, it does not match the URL portion of the HTTP request. headers are responsible for matching the header part of the HTTP request. It is compatible with RE2’s regular expressions. It is an optional configuration. If it is not defined in the configuration file, it does not match the header portion of the HTTP request. handler contains the main processing part. Now handler can configure type, status, content_type, response_content, query, query_param_name.type currently supports three types: predefined_query_handler, dynamic_query_handler, static. query — use with predefined_query_handler type, executes query when the handler is called. query_param_name — use with dynamic_query_handler type, extracts and executes the value corresponding to the query_param_name value in HTTP request params. status — use with static type, response status code. content_type — use with static type, response content-type. response_content — use with static type, response content sent to client, when using the prefix ‘file://’ or ‘config://’, find the content from the file or configuration sends to client. Next are the configuration methods for different type. "},{"title":"predefined_query_handler​","type":1,"pageTitle":"HTTP Interface","url":"docs/en/interfaces/http#predefined_query_handler","content":"predefined_query_handler supports setting Settings and query_params values. You can configure query in the type of predefined_query_handler. query value is a predefined query of predefined_query_handler, which is executed by ClickHouse when an HTTP request is matched and the result of the query is returned. It is a must configuration. The following example defines the values of max_threads and max_final_threads settings, then queries the system table to check whether these settings were set successfully. warning To keep the default handlers such as query, play, ping, add the &lt;defaults/&gt; rule. Example: &lt;http_handlers&gt; &lt;rule&gt; &lt;url&gt;&lt;![CDATA[/query_param_with_url/\\w+/(?P&lt;name_1&gt;[^/]+)(/(?P&lt;name_2&gt;[^/]+))?]]&gt;&lt;/url&gt; &lt;methods&gt;GET&lt;/methods&gt; &lt;headers&gt; &lt;XXX&gt;TEST_HEADER_VALUE&lt;/XXX&gt; &lt;PARAMS_XXX&gt;&lt;![CDATA[(?P&lt;name_1&gt;[^/]+)(/(?P&lt;name_2&gt;[^/]+))?]]&gt;&lt;/PARAMS_XXX&gt; &lt;/headers&gt; &lt;handler&gt; &lt;type&gt;predefined_query_handler&lt;/type&gt; &lt;query&gt;SELECT value FROM system.settings WHERE name = {name_1:String}&lt;/query&gt; &lt;query&gt;SELECT name, value FROM system.settings WHERE name = {name_2:String}&lt;/query&gt; &lt;/handler&gt; &lt;/rule&gt; &lt;defaults/&gt; &lt;/http_handlers&gt;  $ curl -H 'XXX:TEST_HEADER_VALUE' -H 'PARAMS_XXX:max_threads' 'http://localhost:8123/query_param_with_url/1/max_threads/max_final_threads?max_threads=1&amp;max_final_threads=2' 1 max_final_threads 2  warning In one predefined_query_handler only supports one query of an insert type. "},{"title":"dynamic_query_handler​","type":1,"pageTitle":"HTTP Interface","url":"docs/en/interfaces/http#dynamic_query_handler","content":"In dynamic_query_handler, the query is written in the form of param of the HTTP request. The difference is that in predefined_query_handler, the query is written in the configuration file. You can configure query_param_name in dynamic_query_handler. ClickHouse extracts and executes the value corresponding to the query_param_name value in the URL of the HTTP request. The default value of query_param_name is /query . It is an optional configuration. If there is no definition in the configuration file, the param is not passed in. To experiment with this functionality, the example defines the values of max_threads and max_final_threads and queries whether the settings were set successfully. Example: &lt;http_handlers&gt; &lt;rule&gt; &lt;headers&gt; &lt;XXX&gt;TEST_HEADER_VALUE_DYNAMIC&lt;/XXX&gt; &lt;/headers&gt; &lt;handler&gt; &lt;type&gt;dynamic_query_handler&lt;/type&gt; &lt;query_param_name&gt;query_param&lt;/query_param_name&gt; &lt;/handler&gt; &lt;/rule&gt; &lt;defaults/&gt; &lt;/http_handlers&gt;  $ curl -H 'XXX:TEST_HEADER_VALUE_DYNAMIC' 'http://localhost:8123/own?max_threads=1&amp;max_final_threads=2&amp;param_name_1=max_threads&amp;param_name_2=max_final_threads&amp;query_param=SELECT%20name,value%20FROM%20system.settings%20where%20name%20=%20%7Bname_1:String%7D%20OR%20name%20=%20%7Bname_2:String%7D' max_threads 1 max_final_threads 2  "},{"title":"static​","type":1,"pageTitle":"HTTP Interface","url":"docs/en/interfaces/http#static","content":"static can return content_type, status and response_content. response_content can return the specified content. Example: Return a message. &lt;http_handlers&gt; &lt;rule&gt; &lt;methods&gt;GET&lt;/methods&gt; &lt;headers&gt;&lt;XXX&gt;xxx&lt;/XXX&gt;&lt;/headers&gt; &lt;url&gt;/hi&lt;/url&gt; &lt;handler&gt; &lt;type&gt;static&lt;/type&gt; &lt;status&gt;402&lt;/status&gt; &lt;content_type&gt;text/html; charset=UTF-8&lt;/content_type&gt; &lt;response_content&gt;Say Hi!&lt;/response_content&gt; &lt;/handler&gt; &lt;/rule&gt; &lt;defaults/&gt; &lt;/http_handlers&gt;  $ curl -vv -H 'XXX:xxx' 'http://localhost:8123/hi' * Trying ::1... * Connected to localhost (::1) port 8123 (#0) &gt; GET /hi HTTP/1.1 &gt; Host: localhost:8123 &gt; User-Agent: curl/7.47.0 &gt; Accept: */* &gt; XXX:xxx &gt; &lt; HTTP/1.1 402 Payment Required &lt; Date: Wed, 29 Apr 2020 03:51:26 GMT &lt; Connection: Keep-Alive &lt; Content-Type: text/html; charset=UTF-8 &lt; Transfer-Encoding: chunked &lt; Keep-Alive: timeout=3 &lt; X-ClickHouse-Summary: {&quot;read_rows&quot;:&quot;0&quot;,&quot;read_bytes&quot;:&quot;0&quot;,&quot;written_rows&quot;:&quot;0&quot;,&quot;written_bytes&quot;:&quot;0&quot;,&quot;total_rows_to_read&quot;:&quot;0&quot;} &lt; * Connection #0 to host localhost left intact Say Hi!%  Find the content from the configuration send to client. &lt;get_config_static_handler&gt;&lt;![CDATA[&lt;html ng-app=&quot;SMI2&quot;&gt;&lt;head&gt;&lt;base href=&quot;http://ui.tabix.io/&quot;&gt;&lt;/head&gt;&lt;body&gt;&lt;div ui-view=&quot;&quot; class=&quot;content-ui&quot;&gt;&lt;/div&gt;&lt;script src=&quot;http://loader.tabix.io/master.js&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;]]&gt;&lt;/get_config_static_handler&gt; &lt;http_handlers&gt; &lt;rule&gt; &lt;methods&gt;GET&lt;/methods&gt; &lt;headers&gt;&lt;XXX&gt;xxx&lt;/XXX&gt;&lt;/headers&gt; &lt;url&gt;/get_config_static_handler&lt;/url&gt; &lt;handler&gt; &lt;type&gt;static&lt;/type&gt; &lt;response_content&gt;config://get_config_static_handler&lt;/response_content&gt; &lt;/handler&gt; &lt;/rule&gt; &lt;/http_handlers&gt;  $ curl -v -H 'XXX:xxx' 'http://localhost:8123/get_config_static_handler' * Trying ::1... * Connected to localhost (::1) port 8123 (#0) &gt; GET /get_config_static_handler HTTP/1.1 &gt; Host: localhost:8123 &gt; User-Agent: curl/7.47.0 &gt; Accept: */* &gt; XXX:xxx &gt; &lt; HTTP/1.1 200 OK &lt; Date: Wed, 29 Apr 2020 04:01:24 GMT &lt; Connection: Keep-Alive &lt; Content-Type: text/plain; charset=UTF-8 &lt; Transfer-Encoding: chunked &lt; Keep-Alive: timeout=3 &lt; X-ClickHouse-Summary: {&quot;read_rows&quot;:&quot;0&quot;,&quot;read_bytes&quot;:&quot;0&quot;,&quot;written_rows&quot;:&quot;0&quot;,&quot;written_bytes&quot;:&quot;0&quot;,&quot;total_rows_to_read&quot;:&quot;0&quot;} &lt; * Connection #0 to host localhost left intact &lt;html ng-app=&quot;SMI2&quot;&gt;&lt;head&gt;&lt;base href=&quot;http://ui.tabix.io/&quot;&gt;&lt;/head&gt;&lt;body&gt;&lt;div ui-view=&quot;&quot; class=&quot;content-ui&quot;&gt;&lt;/div&gt;&lt;script src=&quot;http://loader.tabix.io/master.js&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;%  Find the content from the file send to client. &lt;http_handlers&gt; &lt;rule&gt; &lt;methods&gt;GET&lt;/methods&gt; &lt;headers&gt;&lt;XXX&gt;xxx&lt;/XXX&gt;&lt;/headers&gt; &lt;url&gt;/get_absolute_path_static_handler&lt;/url&gt; &lt;handler&gt; &lt;type&gt;static&lt;/type&gt; &lt;content_type&gt;text/html; charset=UTF-8&lt;/content_type&gt; &lt;response_content&gt;file:///absolute_path_file.html&lt;/response_content&gt; &lt;/handler&gt; &lt;/rule&gt; &lt;rule&gt; &lt;methods&gt;GET&lt;/methods&gt; &lt;headers&gt;&lt;XXX&gt;xxx&lt;/XXX&gt;&lt;/headers&gt; &lt;url&gt;/get_relative_path_static_handler&lt;/url&gt; &lt;handler&gt; &lt;type&gt;static&lt;/type&gt; &lt;content_type&gt;text/html; charset=UTF-8&lt;/content_type&gt; &lt;response_content&gt;file://./relative_path_file.html&lt;/response_content&gt; &lt;/handler&gt; &lt;/rule&gt; &lt;/http_handlers&gt;  $ user_files_path='/var/lib/clickhouse/user_files' $ sudo echo &quot;&lt;html&gt;&lt;body&gt;Relative Path File&lt;/body&gt;&lt;/html&gt;&quot; &gt; $user_files_path/relative_path_file.html $ sudo echo &quot;&lt;html&gt;&lt;body&gt;Absolute Path File&lt;/body&gt;&lt;/html&gt;&quot; &gt; $user_files_path/absolute_path_file.html $ curl -vv -H 'XXX:xxx' 'http://localhost:8123/get_absolute_path_static_handler' * Trying ::1... * Connected to localhost (::1) port 8123 (#0) &gt; GET /get_absolute_path_static_handler HTTP/1.1 &gt; Host: localhost:8123 &gt; User-Agent: curl/7.47.0 &gt; Accept: */* &gt; XXX:xxx &gt; &lt; HTTP/1.1 200 OK &lt; Date: Wed, 29 Apr 2020 04:18:16 GMT &lt; Connection: Keep-Alive &lt; Content-Type: text/html; charset=UTF-8 &lt; Transfer-Encoding: chunked &lt; Keep-Alive: timeout=3 &lt; X-ClickHouse-Summary: {&quot;read_rows&quot;:&quot;0&quot;,&quot;read_bytes&quot;:&quot;0&quot;,&quot;written_rows&quot;:&quot;0&quot;,&quot;written_bytes&quot;:&quot;0&quot;,&quot;total_rows_to_read&quot;:&quot;0&quot;} &lt; &lt;html&gt;&lt;body&gt;Absolute Path File&lt;/body&gt;&lt;/html&gt; * Connection #0 to host localhost left intact $ curl -vv -H 'XXX:xxx' 'http://localhost:8123/get_relative_path_static_handler' * Trying ::1... * Connected to localhost (::1) port 8123 (#0) &gt; GET /get_relative_path_static_handler HTTP/1.1 &gt; Host: localhost:8123 &gt; User-Agent: curl/7.47.0 &gt; Accept: */* &gt; XXX:xxx &gt; &lt; HTTP/1.1 200 OK &lt; Date: Wed, 29 Apr 2020 04:18:31 GMT &lt; Connection: Keep-Alive &lt; Content-Type: text/html; charset=UTF-8 &lt; Transfer-Encoding: chunked &lt; Keep-Alive: timeout=3 &lt; X-ClickHouse-Summary: {&quot;read_rows&quot;:&quot;0&quot;,&quot;read_bytes&quot;:&quot;0&quot;,&quot;written_rows&quot;:&quot;0&quot;,&quot;written_bytes&quot;:&quot;0&quot;,&quot;total_rows_to_read&quot;:&quot;0&quot;} &lt; &lt;html&gt;&lt;body&gt;Relative Path File&lt;/body&gt;&lt;/html&gt; * Connection #0 to host localhost left intact  "},{"title":"New York Taxi Data","type":0,"sectionRef":"#","url":"docs/en/example-datasets/nyc-taxi","content":"","keywords":""},{"title":"How to Import the Raw Data​","type":1,"pageTitle":"New York Taxi Data","url":"docs/en/example-datasets/nyc-taxi#how-to-import-the-raw-data","content":"See https://github.com/toddwschneider/nyc-taxi-data and http://tech.marksblogg.com/billion-nyc-taxi-rides-redshift.html for the description of a dataset and instructions for downloading. Downloading will result in about 227 GB of uncompressed data in CSV files. The download takes about an hour over a 1 Gbit connection (parallel downloading from s3.amazonaws.com recovers at least half of a 1 Gbit channel). Some of the files might not download fully. Check the file sizes and re-download any that seem doubtful. Some of the files might contain invalid rows. You can fix them as follows: sed -E '/(.*,){18,}/d' data/yellow_tripdata_2010-02.csv &gt; data/yellow_tripdata_2010-02.csv_ sed -E '/(.*,){18,}/d' data/yellow_tripdata_2010-03.csv &gt; data/yellow_tripdata_2010-03.csv_ mv data/yellow_tripdata_2010-02.csv_ data/yellow_tripdata_2010-02.csv mv data/yellow_tripdata_2010-03.csv_ data/yellow_tripdata_2010-03.csv  Then the data must be pre-processed in PostgreSQL. This will create selections of points in the polygons (to match points on the map with the boroughs of New York City) and combine all the data into a single denormalized flat table by using a JOIN. To do this, you will need to install PostgreSQL with PostGIS support. Be careful when running initialize_database.sh and manually re-check that all the tables were created correctly. It takes about 20-30 minutes to process each month’s worth of data in PostgreSQL, for a total of about 48 hours. You can check the number of downloaded rows as follows: $ time psql nyc-taxi-data -c &quot;SELECT count(*) FROM trips;&quot; ## Count 1298979494 (1 row) real 7m9.164s  (This is slightly more than 1.1 billion rows reported by Mark Litwintschik in a series of blog posts.) The data in PostgreSQL uses 370 GB of space. Exporting the data from PostgreSQL: COPY ( SELECT trips.id, trips.vendor_id, trips.pickup_datetime, trips.dropoff_datetime, trips.store_and_fwd_flag, trips.rate_code_id, trips.pickup_longitude, trips.pickup_latitude, trips.dropoff_longitude, trips.dropoff_latitude, trips.passenger_count, trips.trip_distance, trips.fare_amount, trips.extra, trips.mta_tax, trips.tip_amount, trips.tolls_amount, trips.ehail_fee, trips.improvement_surcharge, trips.total_amount, trips.payment_type, trips.trip_type, trips.pickup, trips.dropoff, cab_types.type cab_type, weather.precipitation_tenths_of_mm rain, weather.snow_depth_mm, weather.snowfall_mm, weather.max_temperature_tenths_degrees_celsius max_temp, weather.min_temperature_tenths_degrees_celsius min_temp, weather.average_wind_speed_tenths_of_meters_per_second wind, pick_up.gid pickup_nyct2010_gid, pick_up.ctlabel pickup_ctlabel, pick_up.borocode pickup_borocode, pick_up.boroname pickup_boroname, pick_up.ct2010 pickup_ct2010, pick_up.boroct2010 pickup_boroct2010, pick_up.cdeligibil pickup_cdeligibil, pick_up.ntacode pickup_ntacode, pick_up.ntaname pickup_ntaname, pick_up.puma pickup_puma, drop_off.gid dropoff_nyct2010_gid, drop_off.ctlabel dropoff_ctlabel, drop_off.borocode dropoff_borocode, drop_off.boroname dropoff_boroname, drop_off.ct2010 dropoff_ct2010, drop_off.boroct2010 dropoff_boroct2010, drop_off.cdeligibil dropoff_cdeligibil, drop_off.ntacode dropoff_ntacode, drop_off.ntaname dropoff_ntaname, drop_off.puma dropoff_puma FROM trips LEFT JOIN cab_types ON trips.cab_type_id = cab_types.id LEFT JOIN central_park_weather_observations_raw weather ON weather.date = trips.pickup_datetime::date LEFT JOIN nyct2010 pick_up ON pick_up.gid = trips.pickup_nyct2010_gid LEFT JOIN nyct2010 drop_off ON drop_off.gid = trips.dropoff_nyct2010_gid ) TO '/opt/milovidov/nyc-taxi-data/trips.tsv';  The data snapshot is created at a speed of about 50 MB per second. While creating the snapshot, PostgreSQL reads from the disk at a speed of about 28 MB per second. This takes about 5 hours. The resulting TSV file is 590612904969 bytes. Create a temporary table in ClickHouse: CREATE TABLE trips ( trip_id UInt32, vendor_id String, pickup_datetime DateTime, dropoff_datetime Nullable(DateTime), store_and_fwd_flag Nullable(FixedString(1)), rate_code_id Nullable(UInt8), pickup_longitude Nullable(Float64), pickup_latitude Nullable(Float64), dropoff_longitude Nullable(Float64), dropoff_latitude Nullable(Float64), passenger_count Nullable(UInt8), trip_distance Nullable(Float64), fare_amount Nullable(Float32), extra Nullable(Float32), mta_tax Nullable(Float32), tip_amount Nullable(Float32), tolls_amount Nullable(Float32), ehail_fee Nullable(Float32), improvement_surcharge Nullable(Float32), total_amount Nullable(Float32), payment_type Nullable(String), trip_type Nullable(UInt8), pickup Nullable(String), dropoff Nullable(String), cab_type Nullable(String), precipitation Nullable(UInt8), snow_depth Nullable(UInt8), snowfall Nullable(UInt8), max_temperature Nullable(UInt8), min_temperature Nullable(UInt8), average_wind_speed Nullable(UInt8), pickup_nyct2010_gid Nullable(UInt8), pickup_ctlabel Nullable(String), pickup_borocode Nullable(UInt8), pickup_boroname Nullable(String), pickup_ct2010 Nullable(String), pickup_boroct2010 Nullable(String), pickup_cdeligibil Nullable(FixedString(1)), pickup_ntacode Nullable(String), pickup_ntaname Nullable(String), pickup_puma Nullable(String), dropoff_nyct2010_gid Nullable(UInt8), dropoff_ctlabel Nullable(String), dropoff_borocode Nullable(UInt8), dropoff_boroname Nullable(String), dropoff_ct2010 Nullable(String), dropoff_boroct2010 Nullable(String), dropoff_cdeligibil Nullable(String), dropoff_ntacode Nullable(String), dropoff_ntaname Nullable(String), dropoff_puma Nullable(String) ) ENGINE = Log;  It is needed for converting fields to more correct data types and, if possible, to eliminate NULLs. $ time clickhouse-client --query=&quot;INSERT INTO trips FORMAT TabSeparated&quot; &lt; trips.tsv real 75m56.214s  Data is read at a speed of 112-140 Mb/second. Loading data into a Log type table in one stream took 76 minutes. The data in this table uses 142 GB. (Importing data directly from Postgres is also possible using COPY ... TO PROGRAM.) Unfortunately, all the fields associated with the weather (precipitation…average_wind_speed) were filled with NULL. Because of this, we will remove them from the final data set. To start, we’ll create a table on a single server. Later we will make the table distributed. Create and populate a summary table: CREATE TABLE trips_mergetree ENGINE = MergeTree(pickup_date, pickup_datetime, 8192) AS SELECT trip_id, CAST(vendor_id AS Enum8('1' = 1, '2' = 2, 'CMT' = 3, 'VTS' = 4, 'DDS' = 5, 'B02512' = 10, 'B02598' = 11, 'B02617' = 12, 'B02682' = 13, 'B02764' = 14)) AS vendor_id, toDate(pickup_datetime) AS pickup_date, ifNull(pickup_datetime, toDateTime(0)) AS pickup_datetime, toDate(dropoff_datetime) AS dropoff_date, ifNull(dropoff_datetime, toDateTime(0)) AS dropoff_datetime, assumeNotNull(store_and_fwd_flag) IN ('Y', '1', '2') AS store_and_fwd_flag, assumeNotNull(rate_code_id) AS rate_code_id, assumeNotNull(pickup_longitude) AS pickup_longitude, assumeNotNull(pickup_latitude) AS pickup_latitude, assumeNotNull(dropoff_longitude) AS dropoff_longitude, assumeNotNull(dropoff_latitude) AS dropoff_latitude, assumeNotNull(passenger_count) AS passenger_count, assumeNotNull(trip_distance) AS trip_distance, assumeNotNull(fare_amount) AS fare_amount, assumeNotNull(extra) AS extra, assumeNotNull(mta_tax) AS mta_tax, assumeNotNull(tip_amount) AS tip_amount, assumeNotNull(tolls_amount) AS tolls_amount, assumeNotNull(ehail_fee) AS ehail_fee, assumeNotNull(improvement_surcharge) AS improvement_surcharge, assumeNotNull(total_amount) AS total_amount, CAST((assumeNotNull(payment_type) AS pt) IN ('CSH', 'CASH', 'Cash', 'CAS', 'Cas', '1') ? 'CSH' : (pt IN ('CRD', 'Credit', 'Cre', 'CRE', 'CREDIT', '2') ? 'CRE' : (pt IN ('NOC', 'No Charge', 'No', '3') ? 'NOC' : (pt IN ('DIS', 'Dispute', 'Dis', '4') ? 'DIS' : 'UNK'))) AS Enum8('CSH' = 1, 'CRE' = 2, 'UNK' = 0, 'NOC' = 3, 'DIS' = 4)) AS payment_type_, assumeNotNull(trip_type) AS trip_type, ifNull(toFixedString(unhex(pickup), 25), toFixedString('', 25)) AS pickup, ifNull(toFixedString(unhex(dropoff), 25), toFixedString('', 25)) AS dropoff, CAST(assumeNotNull(cab_type) AS Enum8('yellow' = 1, 'green' = 2, 'uber' = 3)) AS cab_type, assumeNotNull(pickup_nyct2010_gid) AS pickup_nyct2010_gid, toFloat32(ifNull(pickup_ctlabel, '0')) AS pickup_ctlabel, assumeNotNull(pickup_borocode) AS pickup_borocode, CAST(assumeNotNull(pickup_boroname) AS Enum8('Manhattan' = 1, 'Queens' = 4, 'Brooklyn' = 3, '' = 0, 'Bronx' = 2, 'Staten Island' = 5)) AS pickup_boroname, toFixedString(ifNull(pickup_ct2010, '000000'), 6) AS pickup_ct2010, toFixedString(ifNull(pickup_boroct2010, '0000000'), 7) AS pickup_boroct2010, CAST(assumeNotNull(ifNull(pickup_cdeligibil, ' ')) AS Enum8(' ' = 0, 'E' = 1, 'I' = 2)) AS pickup_cdeligibil, toFixedString(ifNull(pickup_ntacode, '0000'), 4) AS pickup_ntacode, CAST(assumeNotNull(pickup_ntaname) AS Enum16('' = 0, 'Airport' = 1, 'Allerton-Pelham Gardens' = 2, 'Annadale-Huguenot-Prince\\'s Bay-Eltingville' = 3, 'Arden Heights' = 4, 'Astoria' = 5, 'Auburndale' = 6, 'Baisley Park' = 7, 'Bath Beach' = 8, 'Battery Park City-Lower Manhattan' = 9, 'Bay Ridge' = 10, 'Bayside-Bayside Hills' = 11, 'Bedford' = 12, 'Bedford Park-Fordham North' = 13, 'Bellerose' = 14, 'Belmont' = 15, 'Bensonhurst East' = 16, 'Bensonhurst West' = 17, 'Borough Park' = 18, 'Breezy Point-Belle Harbor-Rockaway Park-Broad Channel' = 19, 'Briarwood-Jamaica Hills' = 20, 'Brighton Beach' = 21, 'Bronxdale' = 22, 'Brooklyn Heights-Cobble Hill' = 23, 'Brownsville' = 24, 'Bushwick North' = 25, 'Bushwick South' = 26, 'Cambria Heights' = 27, 'Canarsie' = 28, 'Carroll Gardens-Columbia Street-Red Hook' = 29, 'Central Harlem North-Polo Grounds' = 30, 'Central Harlem South' = 31, 'Charleston-Richmond Valley-Tottenville' = 32, 'Chinatown' = 33, 'Claremont-Bathgate' = 34, 'Clinton' = 35, 'Clinton Hill' = 36, 'Co-op City' = 37, 'College Point' = 38, 'Corona' = 39, 'Crotona Park East' = 40, 'Crown Heights North' = 41, 'Crown Heights South' = 42, 'Cypress Hills-City Line' = 43, 'DUMBO-Vinegar Hill-Downtown Brooklyn-Boerum Hill' = 44, 'Douglas Manor-Douglaston-Little Neck' = 45, 'Dyker Heights' = 46, 'East Concourse-Concourse Village' = 47, 'East Elmhurst' = 48, 'East Flatbush-Farragut' = 49, 'East Flushing' = 50, 'East Harlem North' = 51, 'East Harlem South' = 52, 'East New York' = 53, 'East New York (Pennsylvania Ave)' = 54, 'East Tremont' = 55, 'East Village' = 56, 'East Williamsburg' = 57, 'Eastchester-Edenwald-Baychester' = 58, 'Elmhurst' = 59, 'Elmhurst-Maspeth' = 60, 'Erasmus' = 61, 'Far Rockaway-Bayswater' = 62, 'Flatbush' = 63, 'Flatlands' = 64, 'Flushing' = 65, 'Fordham South' = 66, 'Forest Hills' = 67, 'Fort Greene' = 68, 'Fresh Meadows-Utopia' = 69, 'Ft. Totten-Bay Terrace-Clearview' = 70, 'Georgetown-Marine Park-Bergen Beach-Mill Basin' = 71, 'Glen Oaks-Floral Park-New Hyde Park' = 72, 'Glendale' = 73, 'Gramercy' = 74, 'Grasmere-Arrochar-Ft. Wadsworth' = 75, 'Gravesend' = 76, 'Great Kills' = 77, 'Greenpoint' = 78, 'Grymes Hill-Clifton-Fox Hills' = 79, 'Hamilton Heights' = 80, 'Hammels-Arverne-Edgemere' = 81, 'Highbridge' = 82, 'Hollis' = 83, 'Homecrest' = 84, 'Hudson Yards-Chelsea-Flatiron-Union Square' = 85, 'Hunters Point-Sunnyside-West Maspeth' = 86, 'Hunts Point' = 87, 'Jackson Heights' = 88, 'Jamaica' = 89, 'Jamaica Estates-Holliswood' = 90, 'Kensington-Ocean Parkway' = 91, 'Kew Gardens' = 92, 'Kew Gardens Hills' = 93, 'Kingsbridge Heights' = 94, 'Laurelton' = 95, 'Lenox Hill-Roosevelt Island' = 96, 'Lincoln Square' = 97, 'Lindenwood-Howard Beach' = 98, 'Longwood' = 99, 'Lower East Side' = 100, 'Madison' = 101, 'Manhattanville' = 102, 'Marble Hill-Inwood' = 103, 'Mariner\\'s Harbor-Arlington-Port Ivory-Graniteville' = 104, 'Maspeth' = 105, 'Melrose South-Mott Haven North' = 106, 'Middle Village' = 107, 'Midtown-Midtown South' = 108, 'Midwood' = 109, 'Morningside Heights' = 110, 'Morrisania-Melrose' = 111, 'Mott Haven-Port Morris' = 112, 'Mount Hope' = 113, 'Murray Hill' = 114, 'Murray Hill-Kips Bay' = 115, 'New Brighton-Silver Lake' = 116, 'New Dorp-Midland Beach' = 117, 'New Springville-Bloomfield-Travis' = 118, 'North Corona' = 119, 'North Riverdale-Fieldston-Riverdale' = 120, 'North Side-South Side' = 121, 'Norwood' = 122, 'Oakland Gardens' = 123, 'Oakwood-Oakwood Beach' = 124, 'Ocean Hill' = 125, 'Ocean Parkway South' = 126, 'Old Astoria' = 127, 'Old Town-Dongan Hills-South Beach' = 128, 'Ozone Park' = 129, 'Park Slope-Gowanus' = 130, 'Parkchester' = 131, 'Pelham Bay-Country Club-City Island' = 132, 'Pelham Parkway' = 133, 'Pomonok-Flushing Heights-Hillcrest' = 134, 'Port Richmond' = 135, 'Prospect Heights' = 136, 'Prospect Lefferts Gardens-Wingate' = 137, 'Queens Village' = 138, 'Queensboro Hill' = 139, 'Queensbridge-Ravenswood-Long Island City' = 140, 'Rego Park' = 141, 'Richmond Hill' = 142, 'Ridgewood' = 143, 'Rikers Island' = 144, 'Rosedale' = 145, 'Rossville-Woodrow' = 146, 'Rugby-Remsen Village' = 147, 'Schuylerville-Throgs Neck-Edgewater Park' = 148, 'Seagate-Coney Island' = 149, 'Sheepshead Bay-Gerritsen Beach-Manhattan Beach' = 150, 'SoHo-TriBeCa-Civic Center-Little Italy' = 151, 'Soundview-Bruckner' = 152, 'Soundview-Castle Hill-Clason Point-Harding Park' = 153, 'South Jamaica' = 154, 'South Ozone Park' = 155, 'Springfield Gardens North' = 156, 'Springfield Gardens South-Brookville' = 157, 'Spuyten Duyvil-Kingsbridge' = 158, 'St. Albans' = 159, 'Stapleton-Rosebank' = 160, 'Starrett City' = 161, 'Steinway' = 162, 'Stuyvesant Heights' = 163, 'Stuyvesant Town-Cooper Village' = 164, 'Sunset Park East' = 165, 'Sunset Park West' = 166, 'Todt Hill-Emerson Hill-Heartland Village-Lighthouse Hill' = 167, 'Turtle Bay-East Midtown' = 168, 'University Heights-Morris Heights' = 169, 'Upper East Side-Carnegie Hill' = 170, 'Upper West Side' = 171, 'Van Cortlandt Village' = 172, 'Van Nest-Morris Park-Westchester Square' = 173, 'Washington Heights North' = 174, 'Washington Heights South' = 175, 'West Brighton' = 176, 'West Concourse' = 177, 'West Farms-Bronx River' = 178, 'West New Brighton-New Brighton-St. George' = 179, 'West Village' = 180, 'Westchester-Unionport' = 181, 'Westerleigh' = 182, 'Whitestone' = 183, 'Williamsbridge-Olinville' = 184, 'Williamsburg' = 185, 'Windsor Terrace' = 186, 'Woodhaven' = 187, 'Woodlawn-Wakefield' = 188, 'Woodside' = 189, 'Yorkville' = 190, 'park-cemetery-etc-Bronx' = 191, 'park-cemetery-etc-Brooklyn' = 192, 'park-cemetery-etc-Manhattan' = 193, 'park-cemetery-etc-Queens' = 194, 'park-cemetery-etc-Staten Island' = 195)) AS pickup_ntaname, toUInt16(ifNull(pickup_puma, '0')) AS pickup_puma, assumeNotNull(dropoff_nyct2010_gid) AS dropoff_nyct2010_gid, toFloat32(ifNull(dropoff_ctlabel, '0')) AS dropoff_ctlabel, assumeNotNull(dropoff_borocode) AS dropoff_borocode, CAST(assumeNotNull(dropoff_boroname) AS Enum8('Manhattan' = 1, 'Queens' = 4, 'Brooklyn' = 3, '' = 0, 'Bronx' = 2, 'Staten Island' = 5)) AS dropoff_boroname, toFixedString(ifNull(dropoff_ct2010, '000000'), 6) AS dropoff_ct2010, toFixedString(ifNull(dropoff_boroct2010, '0000000'), 7) AS dropoff_boroct2010, CAST(assumeNotNull(ifNull(dropoff_cdeligibil, ' ')) AS Enum8(' ' = 0, 'E' = 1, 'I' = 2)) AS dropoff_cdeligibil, toFixedString(ifNull(dropoff_ntacode, '0000'), 4) AS dropoff_ntacode, CAST(assumeNotNull(dropoff_ntaname) AS Enum16('' = 0, 'Airport' = 1, 'Allerton-Pelham Gardens' = 2, 'Annadale-Huguenot-Prince\\'s Bay-Eltingville' = 3, 'Arden Heights' = 4, 'Astoria' = 5, 'Auburndale' = 6, 'Baisley Park' = 7, 'Bath Beach' = 8, 'Battery Park City-Lower Manhattan' = 9, 'Bay Ridge' = 10, 'Bayside-Bayside Hills' = 11, 'Bedford' = 12, 'Bedford Park-Fordham North' = 13, 'Bellerose' = 14, 'Belmont' = 15, 'Bensonhurst East' = 16, 'Bensonhurst West' = 17, 'Borough Park' = 18, 'Breezy Point-Belle Harbor-Rockaway Park-Broad Channel' = 19, 'Briarwood-Jamaica Hills' = 20, 'Brighton Beach' = 21, 'Bronxdale' = 22, 'Brooklyn Heights-Cobble Hill' = 23, 'Brownsville' = 24, 'Bushwick North' = 25, 'Bushwick South' = 26, 'Cambria Heights' = 27, 'Canarsie' = 28, 'Carroll Gardens-Columbia Street-Red Hook' = 29, 'Central Harlem North-Polo Grounds' = 30, 'Central Harlem South' = 31, 'Charleston-Richmond Valley-Tottenville' = 32, 'Chinatown' = 33, 'Claremont-Bathgate' = 34, 'Clinton' = 35, 'Clinton Hill' = 36, 'Co-op City' = 37, 'College Point' = 38, 'Corona' = 39, 'Crotona Park East' = 40, 'Crown Heights North' = 41, 'Crown Heights South' = 42, 'Cypress Hills-City Line' = 43, 'DUMBO-Vinegar Hill-Downtown Brooklyn-Boerum Hill' = 44, 'Douglas Manor-Douglaston-Little Neck' = 45, 'Dyker Heights' = 46, 'East Concourse-Concourse Village' = 47, 'East Elmhurst' = 48, 'East Flatbush-Farragut' = 49, 'East Flushing' = 50, 'East Harlem North' = 51, 'East Harlem South' = 52, 'East New York' = 53, 'East New York (Pennsylvania Ave)' = 54, 'East Tremont' = 55, 'East Village' = 56, 'East Williamsburg' = 57, 'Eastchester-Edenwald-Baychester' = 58, 'Elmhurst' = 59, 'Elmhurst-Maspeth' = 60, 'Erasmus' = 61, 'Far Rockaway-Bayswater' = 62, 'Flatbush' = 63, 'Flatlands' = 64, 'Flushing' = 65, 'Fordham South' = 66, 'Forest Hills' = 67, 'Fort Greene' = 68, 'Fresh Meadows-Utopia' = 69, 'Ft. Totten-Bay Terrace-Clearview' = 70, 'Georgetown-Marine Park-Bergen Beach-Mill Basin' = 71, 'Glen Oaks-Floral Park-New Hyde Park' = 72, 'Glendale' = 73, 'Gramercy' = 74, 'Grasmere-Arrochar-Ft. Wadsworth' = 75, 'Gravesend' = 76, 'Great Kills' = 77, 'Greenpoint' = 78, 'Grymes Hill-Clifton-Fox Hills' = 79, 'Hamilton Heights' = 80, 'Hammels-Arverne-Edgemere' = 81, 'Highbridge' = 82, 'Hollis' = 83, 'Homecrest' = 84, 'Hudson Yards-Chelsea-Flatiron-Union Square' = 85, 'Hunters Point-Sunnyside-West Maspeth' = 86, 'Hunts Point' = 87, 'Jackson Heights' = 88, 'Jamaica' = 89, 'Jamaica Estates-Holliswood' = 90, 'Kensington-Ocean Parkway' = 91, 'Kew Gardens' = 92, 'Kew Gardens Hills' = 93, 'Kingsbridge Heights' = 94, 'Laurelton' = 95, 'Lenox Hill-Roosevelt Island' = 96, 'Lincoln Square' = 97, 'Lindenwood-Howard Beach' = 98, 'Longwood' = 99, 'Lower East Side' = 100, 'Madison' = 101, 'Manhattanville' = 102, 'Marble Hill-Inwood' = 103, 'Mariner\\'s Harbor-Arlington-Port Ivory-Graniteville' = 104, 'Maspeth' = 105, 'Melrose South-Mott Haven North' = 106, 'Middle Village' = 107, 'Midtown-Midtown South' = 108, 'Midwood' = 109, 'Morningside Heights' = 110, 'Morrisania-Melrose' = 111, 'Mott Haven-Port Morris' = 112, 'Mount Hope' = 113, 'Murray Hill' = 114, 'Murray Hill-Kips Bay' = 115, 'New Brighton-Silver Lake' = 116, 'New Dorp-Midland Beach' = 117, 'New Springville-Bloomfield-Travis' = 118, 'North Corona' = 119, 'North Riverdale-Fieldston-Riverdale' = 120, 'North Side-South Side' = 121, 'Norwood' = 122, 'Oakland Gardens' = 123, 'Oakwood-Oakwood Beach' = 124, 'Ocean Hill' = 125, 'Ocean Parkway South' = 126, 'Old Astoria' = 127, 'Old Town-Dongan Hills-South Beach' = 128, 'Ozone Park' = 129, 'Park Slope-Gowanus' = 130, 'Parkchester' = 131, 'Pelham Bay-Country Club-City Island' = 132, 'Pelham Parkway' = 133, 'Pomonok-Flushing Heights-Hillcrest' = 134, 'Port Richmond' = 135, 'Prospect Heights' = 136, 'Prospect Lefferts Gardens-Wingate' = 137, 'Queens Village' = 138, 'Queensboro Hill' = 139, 'Queensbridge-Ravenswood-Long Island City' = 140, 'Rego Park' = 141, 'Richmond Hill' = 142, 'Ridgewood' = 143, 'Rikers Island' = 144, 'Rosedale' = 145, 'Rossville-Woodrow' = 146, 'Rugby-Remsen Village' = 147, 'Schuylerville-Throgs Neck-Edgewater Park' = 148, 'Seagate-Coney Island' = 149, 'Sheepshead Bay-Gerritsen Beach-Manhattan Beach' = 150, 'SoHo-TriBeCa-Civic Center-Little Italy' = 151, 'Soundview-Bruckner' = 152, 'Soundview-Castle Hill-Clason Point-Harding Park' = 153, 'South Jamaica' = 154, 'South Ozone Park' = 155, 'Springfield Gardens North' = 156, 'Springfield Gardens South-Brookville' = 157, 'Spuyten Duyvil-Kingsbridge' = 158, 'St. Albans' = 159, 'Stapleton-Rosebank' = 160, 'Starrett City' = 161, 'Steinway' = 162, 'Stuyvesant Heights' = 163, 'Stuyvesant Town-Cooper Village' = 164, 'Sunset Park East' = 165, 'Sunset Park West' = 166, 'Todt Hill-Emerson Hill-Heartland Village-Lighthouse Hill' = 167, 'Turtle Bay-East Midtown' = 168, 'University Heights-Morris Heights' = 169, 'Upper East Side-Carnegie Hill' = 170, 'Upper West Side' = 171, 'Van Cortlandt Village' = 172, 'Van Nest-Morris Park-Westchester Square' = 173, 'Washington Heights North' = 174, 'Washington Heights South' = 175, 'West Brighton' = 176, 'West Concourse' = 177, 'West Farms-Bronx River' = 178, 'West New Brighton-New Brighton-St. George' = 179, 'West Village' = 180, 'Westchester-Unionport' = 181, 'Westerleigh' = 182, 'Whitestone' = 183, 'Williamsbridge-Olinville' = 184, 'Williamsburg' = 185, 'Windsor Terrace' = 186, 'Woodhaven' = 187, 'Woodlawn-Wakefield' = 188, 'Woodside' = 189, 'Yorkville' = 190, 'park-cemetery-etc-Bronx' = 191, 'park-cemetery-etc-Brooklyn' = 192, 'park-cemetery-etc-Manhattan' = 193, 'park-cemetery-etc-Queens' = 194, 'park-cemetery-etc-Staten Island' = 195)) AS dropoff_ntaname, toUInt16(ifNull(dropoff_puma, '0')) AS dropoff_puma FROM trips  This takes 3030 seconds at a speed of about 428,000 rows per second. To load it faster, you can create the table with the Log engine instead of MergeTree. In this case, the download works faster than 200 seconds. The table uses 126 GB of disk space. SELECT formatReadableSize(sum(bytes)) FROM system.parts WHERE table = 'trips_mergetree' AND active  ┌─formatReadableSize(sum(bytes))─┐ │ 126.18 GiB │ └────────────────────────────────┘  Among other things, you can run the OPTIMIZE query on MergeTree. But it’s not required since everything will be fine without it. "},{"title":"Download of Prepared Partitions​","type":1,"pageTitle":"New York Taxi Data","url":"docs/en/example-datasets/nyc-taxi#download-of-prepared-partitions","content":"$ curl -O https://datasets.clickhouse.com/trips_mergetree/partitions/trips_mergetree.tar $ tar xvf trips_mergetree.tar -C /var/lib/clickhouse # path to ClickHouse data directory $ # check permissions of unpacked data, fix if required $ sudo service clickhouse-server restart $ clickhouse-client --query &quot;select count(*) from datasets.trips_mergetree&quot;  info If you will run the queries described below, you have to use the full table name, datasets.trips_mergetree. "},{"title":"Results on Single Server​","type":1,"pageTitle":"New York Taxi Data","url":"docs/en/example-datasets/nyc-taxi#results-on-single-server","content":"Q1: SELECT cab_type, count(*) FROM trips_mergetree GROUP BY cab_type  0.490 seconds. Q2: SELECT passenger_count, avg(total_amount) FROM trips_mergetree GROUP BY passenger_count  1.224 seconds. Q3: SELECT passenger_count, toYear(pickup_date) AS year, count(*) FROM trips_mergetree GROUP BY passenger_count, year  2.104 seconds. Q4: SELECT passenger_count, toYear(pickup_date) AS year, round(trip_distance) AS distance, count(*) FROM trips_mergetree GROUP BY passenger_count, year, distance ORDER BY year, count(*) DESC  3.593 seconds. The following server was used: Two Intel(R) Xeon(R) CPU E5-2650 v2 @ 2.60GHz, 16 physical cores total, 128 GiB RAM, 8x6 TB HD on hardware RAID-5 Execution time is the best of three runs. But starting from the second run, queries read data from the file system cache. No further caching occurs: the data is read out and processed in each run. Creating a table on three servers: On each server: CREATE TABLE default.trips_mergetree_third ( trip_id UInt32, vendor_id Enum8('1' = 1, '2' = 2, 'CMT' = 3, 'VTS' = 4, 'DDS' = 5, 'B02512' = 10, 'B02598' = 11, 'B02617' = 12, 'B02682' = 13, 'B02764' = 14), pickup_date Date, pickup_datetime DateTime, dropoff_date Date, dropoff_datetime DateTime, store_and_fwd_flag UInt8, rate_code_id UInt8, pickup_longitude Float64, pickup_latitude Float64, dropoff_longitude Float64, dropoff_latitude Float64, passenger_count UInt8, trip_distance Float64, fare_amount Float32, extra Float32, mta_tax Float32, tip_amount Float32, tolls_amount Float32, ehail_fee Float32, improvement_surcharge Float32, total_amount Float32, payment_type_ Enum8('UNK' = 0, 'CSH' = 1, 'CRE' = 2, 'NOC' = 3, 'DIS' = 4), trip_type UInt8, pickup FixedString(25), dropoff FixedString(25), cab_type Enum8('yellow' = 1, 'green' = 2, 'uber' = 3), pickup_nyct2010_gid UInt8, pickup_ctlabel Float32, pickup_borocode UInt8, pickup_boroname Enum8('' = 0, 'Manhattan' = 1, 'Bronx' = 2, 'Brooklyn' = 3, 'Queens' = 4, 'Staten Island' = 5), pickup_ct2010 FixedString(6), pickup_boroct2010 FixedString(7), pickup_cdeligibil Enum8(' ' = 0, 'E' = 1, 'I' = 2), pickup_ntacode FixedString(4), pickup_ntaname Enum16('' = 0, 'Airport' = 1, 'Allerton-Pelham Gardens' = 2, 'Annadale-Huguenot-Prince\\'s Bay-Eltingville' = 3, 'Arden Heights' = 4, 'Astoria' = 5, 'Auburndale' = 6, 'Baisley Park' = 7, 'Bath Beach' = 8, 'Battery Park City-Lower Manhattan' = 9, 'Bay Ridge' = 10, 'Bayside-Bayside Hills' = 11, 'Bedford' = 12, 'Bedford Park-Fordham North' = 13, 'Bellerose' = 14, 'Belmont' = 15, 'Bensonhurst East' = 16, 'Bensonhurst West' = 17, 'Borough Park' = 18, 'Breezy Point-Belle Harbor-Rockaway Park-Broad Channel' = 19, 'Briarwood-Jamaica Hills' = 20, 'Brighton Beach' = 21, 'Bronxdale' = 22, 'Brooklyn Heights-Cobble Hill' = 23, 'Brownsville' = 24, 'Bushwick North' = 25, 'Bushwick South' = 26, 'Cambria Heights' = 27, 'Canarsie' = 28, 'Carroll Gardens-Columbia Street-Red Hook' = 29, 'Central Harlem North-Polo Grounds' = 30, 'Central Harlem South' = 31, 'Charleston-Richmond Valley-Tottenville' = 32, 'Chinatown' = 33, 'Claremont-Bathgate' = 34, 'Clinton' = 35, 'Clinton Hill' = 36, 'Co-op City' = 37, 'College Point' = 38, 'Corona' = 39, 'Crotona Park East' = 40, 'Crown Heights North' = 41, 'Crown Heights South' = 42, 'Cypress Hills-City Line' = 43, 'DUMBO-Vinegar Hill-Downtown Brooklyn-Boerum Hill' = 44, 'Douglas Manor-Douglaston-Little Neck' = 45, 'Dyker Heights' = 46, 'East Concourse-Concourse Village' = 47, 'East Elmhurst' = 48, 'East Flatbush-Farragut' = 49, 'East Flushing' = 50, 'East Harlem North' = 51, 'East Harlem South' = 52, 'East New York' = 53, 'East New York (Pennsylvania Ave)' = 54, 'East Tremont' = 55, 'East Village' = 56, 'East Williamsburg' = 57, 'Eastchester-Edenwald-Baychester' = 58, 'Elmhurst' = 59, 'Elmhurst-Maspeth' = 60, 'Erasmus' = 61, 'Far Rockaway-Bayswater' = 62, 'Flatbush' = 63, 'Flatlands' = 64, 'Flushing' = 65, 'Fordham South' = 66, 'Forest Hills' = 67, 'Fort Greene' = 68, 'Fresh Meadows-Utopia' = 69, 'Ft. Totten-Bay Terrace-Clearview' = 70, 'Georgetown-Marine Park-Bergen Beach-Mill Basin' = 71, 'Glen Oaks-Floral Park-New Hyde Park' = 72, 'Glendale' = 73, 'Gramercy' = 74, 'Grasmere-Arrochar-Ft. Wadsworth' = 75, 'Gravesend' = 76, 'Great Kills' = 77, 'Greenpoint' = 78, 'Grymes Hill-Clifton-Fox Hills' = 79, 'Hamilton Heights' = 80, 'Hammels-Arverne-Edgemere' = 81, 'Highbridge' = 82, 'Hollis' = 83, 'Homecrest' = 84, 'Hudson Yards-Chelsea-Flatiron-Union Square' = 85, 'Hunters Point-Sunnyside-West Maspeth' = 86, 'Hunts Point' = 87, 'Jackson Heights' = 88, 'Jamaica' = 89, 'Jamaica Estates-Holliswood' = 90, 'Kensington-Ocean Parkway' = 91, 'Kew Gardens' = 92, 'Kew Gardens Hills' = 93, 'Kingsbridge Heights' = 94, 'Laurelton' = 95, 'Lenox Hill-Roosevelt Island' = 96, 'Lincoln Square' = 97, 'Lindenwood-Howard Beach' = 98, 'Longwood' = 99, 'Lower East Side' = 100, 'Madison' = 101, 'Manhattanville' = 102, 'Marble Hill-Inwood' = 103, 'Mariner\\'s Harbor-Arlington-Port Ivory-Graniteville' = 104, 'Maspeth' = 105, 'Melrose South-Mott Haven North' = 106, 'Middle Village' = 107, 'Midtown-Midtown South' = 108, 'Midwood' = 109, 'Morningside Heights' = 110, 'Morrisania-Melrose' = 111, 'Mott Haven-Port Morris' = 112, 'Mount Hope' = 113, 'Murray Hill' = 114, 'Murray Hill-Kips Bay' = 115, 'New Brighton-Silver Lake' = 116, 'New Dorp-Midland Beach' = 117, 'New Springville-Bloomfield-Travis' = 118, 'North Corona' = 119, 'North Riverdale-Fieldston-Riverdale' = 120, 'North Side-South Side' = 121, 'Norwood' = 122, 'Oakland Gardens' = 123, 'Oakwood-Oakwood Beach' = 124, 'Ocean Hill' = 125, 'Ocean Parkway South' = 126, 'Old Astoria' = 127, 'Old Town-Dongan Hills-South Beach' = 128, 'Ozone Park' = 129, 'Park Slope-Gowanus' = 130, 'Parkchester' = 131, 'Pelham Bay-Country Club-City Island' = 132, 'Pelham Parkway' = 133, 'Pomonok-Flushing Heights-Hillcrest' = 134, 'Port Richmond' = 135, 'Prospect Heights' = 136, 'Prospect Lefferts Gardens-Wingate' = 137, 'Queens Village' = 138, 'Queensboro Hill' = 139, 'Queensbridge-Ravenswood-Long Island City' = 140, 'Rego Park' = 141, 'Richmond Hill' = 142, 'Ridgewood' = 143, 'Rikers Island' = 144, 'Rosedale' = 145, 'Rossville-Woodrow' = 146, 'Rugby-Remsen Village' = 147, 'Schuylerville-Throgs Neck-Edgewater Park' = 148, 'Seagate-Coney Island' = 149, 'Sheepshead Bay-Gerritsen Beach-Manhattan Beach' = 150, 'SoHo-TriBeCa-Civic Center-Little Italy' = 151, 'Soundview-Bruckner' = 152, 'Soundview-Castle Hill-Clason Point-Harding Park' = 153, 'South Jamaica' = 154, 'South Ozone Park' = 155, 'Springfield Gardens North' = 156, 'Springfield Gardens South-Brookville' = 157, 'Spuyten Duyvil-Kingsbridge' = 158, 'St. Albans' = 159, 'Stapleton-Rosebank' = 160, 'Starrett City' = 161, 'Steinway' = 162, 'Stuyvesant Heights' = 163, 'Stuyvesant Town-Cooper Village' = 164, 'Sunset Park East' = 165, 'Sunset Park West' = 166, 'Todt Hill-Emerson Hill-Heartland Village-Lighthouse Hill' = 167, 'Turtle Bay-East Midtown' = 168, 'University Heights-Morris Heights' = 169, 'Upper East Side-Carnegie Hill' = 170, 'Upper West Side' = 171, 'Van Cortlandt Village' = 172, 'Van Nest-Morris Park-Westchester Square' = 173, 'Washington Heights North' = 174, 'Washington Heights South' = 175, 'West Brighton' = 176, 'West Concourse' = 177, 'West Farms-Bronx River' = 178, 'West New Brighton-New Brighton-St. George' = 179, 'West Village' = 180, 'Westchester-Unionport' = 181, 'Westerleigh' = 182, 'Whitestone' = 183, 'Williamsbridge-Olinville' = 184, 'Williamsburg' = 185, 'Windsor Terrace' = 186, 'Woodhaven' = 187, 'Woodlawn-Wakefield' = 188, 'Woodside' = 189, 'Yorkville' = 190, 'park-cemetery-etc-Bronx' = 191, 'park-cemetery-etc-Brooklyn' = 192, 'park-cemetery-etc-Manhattan' = 193, 'park-cemetery-etc-Queens' = 194, 'park-cemetery-etc-Staten Island' = 195), pickup_puma UInt16, dropoff_nyct2010_gid UInt8, dropoff_ctlabel Float32, dropoff_borocode UInt8, dropoff_boroname Enum8('' = 0, 'Manhattan' = 1, 'Bronx' = 2, 'Brooklyn' = 3, 'Queens' = 4, 'Staten Island' = 5), dropoff_ct2010 FixedString(6), dropoff_boroct2010 FixedString(7), dropoff_cdeligibil Enum8(' ' = 0, 'E' = 1, 'I' = 2), dropoff_ntacode FixedString(4), dropoff_ntaname Enum16('' = 0, 'Airport' = 1, 'Allerton-Pelham Gardens' = 2, 'Annadale-Huguenot-Prince\\'s Bay-Eltingville' = 3, 'Arden Heights' = 4, 'Astoria' = 5, 'Auburndale' = 6, 'Baisley Park' = 7, 'Bath Beach' = 8, 'Battery Park City-Lower Manhattan' = 9, 'Bay Ridge' = 10, 'Bayside-Bayside Hills' = 11, 'Bedford' = 12, 'Bedford Park-Fordham North' = 13, 'Bellerose' = 14, 'Belmont' = 15, 'Bensonhurst East' = 16, 'Bensonhurst West' = 17, 'Borough Park' = 18, 'Breezy Point-Belle Harbor-Rockaway Park-Broad Channel' = 19, 'Briarwood-Jamaica Hills' = 20, 'Brighton Beach' = 21, 'Bronxdale' = 22, 'Brooklyn Heights-Cobble Hill' = 23, 'Brownsville' = 24, 'Bushwick North' = 25, 'Bushwick South' = 26, 'Cambria Heights' = 27, 'Canarsie' = 28, 'Carroll Gardens-Columbia Street-Red Hook' = 29, 'Central Harlem North-Polo Grounds' = 30, 'Central Harlem South' = 31, 'Charleston-Richmond Valley-Tottenville' = 32, 'Chinatown' = 33, 'Claremont-Bathgate' = 34, 'Clinton' = 35, 'Clinton Hill' = 36, 'Co-op City' = 37, 'College Point' = 38, 'Corona' = 39, 'Crotona Park East' = 40, 'Crown Heights North' = 41, 'Crown Heights South' = 42, 'Cypress Hills-City Line' = 43, 'DUMBO-Vinegar Hill-Downtown Brooklyn-Boerum Hill' = 44, 'Douglas Manor-Douglaston-Little Neck' = 45, 'Dyker Heights' = 46, 'East Concourse-Concourse Village' = 47, 'East Elmhurst' = 48, 'East Flatbush-Farragut' = 49, 'East Flushing' = 50, 'East Harlem North' = 51, 'East Harlem South' = 52, 'East New York' = 53, 'East New York (Pennsylvania Ave)' = 54, 'East Tremont' = 55, 'East Village' = 56, 'East Williamsburg' = 57, 'Eastchester-Edenwald-Baychester' = 58, 'Elmhurst' = 59, 'Elmhurst-Maspeth' = 60, 'Erasmus' = 61, 'Far Rockaway-Bayswater' = 62, 'Flatbush' = 63, 'Flatlands' = 64, 'Flushing' = 65, 'Fordham South' = 66, 'Forest Hills' = 67, 'Fort Greene' = 68, 'Fresh Meadows-Utopia' = 69, 'Ft. Totten-Bay Terrace-Clearview' = 70, 'Georgetown-Marine Park-Bergen Beach-Mill Basin' = 71, 'Glen Oaks-Floral Park-New Hyde Park' = 72, 'Glendale' = 73, 'Gramercy' = 74, 'Grasmere-Arrochar-Ft. Wadsworth' = 75, 'Gravesend' = 76, 'Great Kills' = 77, 'Greenpoint' = 78, 'Grymes Hill-Clifton-Fox Hills' = 79, 'Hamilton Heights' = 80, 'Hammels-Arverne-Edgemere' = 81, 'Highbridge' = 82, 'Hollis' = 83, 'Homecrest' = 84, 'Hudson Yards-Chelsea-Flatiron-Union Square' = 85, 'Hunters Point-Sunnyside-West Maspeth' = 86, 'Hunts Point' = 87, 'Jackson Heights' = 88, 'Jamaica' = 89, 'Jamaica Estates-Holliswood' = 90, 'Kensington-Ocean Parkway' = 91, 'Kew Gardens' = 92, 'Kew Gardens Hills' = 93, 'Kingsbridge Heights' = 94, 'Laurelton' = 95, 'Lenox Hill-Roosevelt Island' = 96, 'Lincoln Square' = 97, 'Lindenwood-Howard Beach' = 98, 'Longwood' = 99, 'Lower East Side' = 100, 'Madison' = 101, 'Manhattanville' = 102, 'Marble Hill-Inwood' = 103, 'Mariner\\'s Harbor-Arlington-Port Ivory-Graniteville' = 104, 'Maspeth' = 105, 'Melrose South-Mott Haven North' = 106, 'Middle Village' = 107, 'Midtown-Midtown South' = 108, 'Midwood' = 109, 'Morningside Heights' = 110, 'Morrisania-Melrose' = 111, 'Mott Haven-Port Morris' = 112, 'Mount Hope' = 113, 'Murray Hill' = 114, 'Murray Hill-Kips Bay' = 115, 'New Brighton-Silver Lake' = 116, 'New Dorp-Midland Beach' = 117, 'New Springville-Bloomfield-Travis' = 118, 'North Corona' = 119, 'North Riverdale-Fieldston-Riverdale' = 120, 'North Side-South Side' = 121, 'Norwood' = 122, 'Oakland Gardens' = 123, 'Oakwood-Oakwood Beach' = 124, 'Ocean Hill' = 125, 'Ocean Parkway South' = 126, 'Old Astoria' = 127, 'Old Town-Dongan Hills-South Beach' = 128, 'Ozone Park' = 129, 'Park Slope-Gowanus' = 130, 'Parkchester' = 131, 'Pelham Bay-Country Club-City Island' = 132, 'Pelham Parkway' = 133, 'Pomonok-Flushing Heights-Hillcrest' = 134, 'Port Richmond' = 135, 'Prospect Heights' = 136, 'Prospect Lefferts Gardens-Wingate' = 137, 'Queens Village' = 138, 'Queensboro Hill' = 139, 'Queensbridge-Ravenswood-Long Island City' = 140, 'Rego Park' = 141, 'Richmond Hill' = 142, 'Ridgewood' = 143, 'Rikers Island' = 144, 'Rosedale' = 145, 'Rossville-Woodrow' = 146, 'Rugby-Remsen Village' = 147, 'Schuylerville-Throgs Neck-Edgewater Park' = 148, 'Seagate-Coney Island' = 149, 'Sheepshead Bay-Gerritsen Beach-Manhattan Beach' = 150, 'SoHo-TriBeCa-Civic Center-Little Italy' = 151, 'Soundview-Bruckner' = 152, 'Soundview-Castle Hill-Clason Point-Harding Park' = 153, 'South Jamaica' = 154, 'South Ozone Park' = 155, 'Springfield Gardens North' = 156, 'Springfield Gardens South-Brookville' = 157, 'Spuyten Duyvil-Kingsbridge' = 158, 'St. Albans' = 159, 'Stapleton-Rosebank' = 160, 'Starrett City' = 161, 'Steinway' = 162, 'Stuyvesant Heights' = 163, 'Stuyvesant Town-Cooper Village' = 164, 'Sunset Park East' = 165, 'Sunset Park West' = 166, 'Todt Hill-Emerson Hill-Heartland Village-Lighthouse Hill' = 167, 'Turtle Bay-East Midtown' = 168, 'University Heights-Morris Heights' = 169, 'Upper East Side-Carnegie Hill' = 170, 'Upper West Side' = 171, 'Van Cortlandt Village' = 172, 'Van Nest-Morris Park-Westchester Square' = 173, 'Washington Heights North' = 174, 'Washington Heights South' = 175, 'West Brighton' = 176, 'West Concourse' = 177, 'West Farms-Bronx River' = 178, 'West New Brighton-New Brighton-St. George' = 179, 'West Village' = 180, 'Westchester-Unionport' = 181, 'Westerleigh' = 182, 'Whitestone' = 183, 'Williamsbridge-Olinville' = 184, 'Williamsburg' = 185, 'Windsor Terrace' = 186, 'Woodhaven' = 187, 'Woodlawn-Wakefield' = 188, 'Woodside' = 189, 'Yorkville' = 190, 'park-cemetery-etc-Bronx' = 191, 'park-cemetery-etc-Brooklyn' = 192, 'park-cemetery-etc-Manhattan' = 193, 'park-cemetery-etc-Queens' = 194, 'park-cemetery-etc-Staten Island' = 195), dropoff_puma UInt16) ENGINE = MergeTree(pickup_date, pickup_datetime, 8192)  On the source server: CREATE TABLE trips_mergetree_x3 AS trips_mergetree_third ENGINE = Distributed(perftest, default, trips_mergetree_third, rand())  The following query redistributes data: INSERT INTO trips_mergetree_x3 SELECT * FROM trips_mergetree  This takes 2454 seconds. On three servers: Q1: 0.212 seconds. Q2: 0.438 seconds. Q3: 0.733 seconds. Q4: 1.241 seconds. No surprises here, since the queries are scaled linearly. We also have the results from a cluster of 140 servers: Q1: 0.028 sec. Q2: 0.043 sec. Q3: 0.051 sec. Q4: 0.072 sec. In this case, the query processing time is determined above all by network latency. We ran queries using a client located in a different datacenter than where the cluster was located, which added about 20 ms of latency. "},{"title":"Summary​","type":1,"pageTitle":"New York Taxi Data","url":"docs/en/example-datasets/nyc-taxi#summary","content":"servers\tQ1\tQ2\tQ3\tQ41, E5-2650v2\t0.490\t1.224\t2.104\t3.593 3, E5-2650v2\t0.212\t0.438\t0.733\t1.241 1, AWS c5n.4xlarge\t0.249\t1.279\t1.738\t3.527 1, AWS c5n.9xlarge\t0.130\t0.584\t0.777\t1.811 3, AWS c5n.9xlarge\t0.057\t0.231\t0.285\t0.641 140, E5-2650v2\t0.028\t0.043\t0.051\t0.072 Original article "},{"title":"How to Test Your Hardware with ClickHouse","type":0,"sectionRef":"#","url":"docs/en/operations/performance-test","content":"","keywords":""},{"title":"Automated Run​","type":1,"pageTitle":"How to Test Your Hardware with ClickHouse","url":"docs/en/operations/performance-test#automated-run","content":"You can run benchmark with a single script. Download the script. wget https://raw.githubusercontent.com/ClickHouse/ClickHouse/master/benchmark/hardware.sh  Run the script. chmod a+x ./hardware.sh ./hardware.sh  Copy the output and send it to feedback@clickhouse.com All the results are published here: https://clickhouse.com/benchmark/hardware/ "},{"title":"Manual Run​","type":1,"pageTitle":"How to Test Your Hardware with ClickHouse","url":"docs/en/operations/performance-test#manual-run","content":"Alternatively you can perform benchmark in the following steps. ssh to the server and download the binary with wget: # For amd64: wget https://builds.clickhouse.com/master/amd64/clickhouse # For aarch64: wget https://builds.clickhouse.com/master/aarch64/clickhouse # For powerpc64le: wget https://builds.clickhouse.com/master/powerpc64le/clickhouse # For freebsd: wget https://builds.clickhouse.com/master/freebsd/clickhouse # For freebsd-aarch64: wget https://builds.clickhouse.com/master/freebsd-aarch64/clickhouse # For freebsd-powerpc64le: wget https://builds.clickhouse.com/master/freebsd-powerpc64le/clickhouse # For macos: wget https://builds.clickhouse.com/master/macos/clickhouse # For macos-aarch64: wget https://builds.clickhouse.com/master/macos-aarch64/clickhouse # Then do: chmod a+x clickhouse  Download benchmark files: wget https://raw.githubusercontent.com/ClickHouse/ClickHouse/master/benchmark/clickhouse/benchmark-new.sh chmod a+x benchmark-new.sh wget https://raw.githubusercontent.com/ClickHouse/ClickHouse/master/benchmark/clickhouse/queries.sql  Download the web analytics dataset (“hits” table containing 100 million rows). wget https://datasets.clickhouse.com/hits/partitions/hits_100m_obfuscated_v1.tar.xz tar xvf hits_100m_obfuscated_v1.tar.xz -C . mv hits_100m_obfuscated_v1/* .  Run the server: ./clickhouse server  Check the data: ssh to the server in another terminal ./clickhouse client --query &quot;SELECT count() FROM hits_100m_obfuscated&quot; 100000000  Run the benchmark: ./benchmark-new.sh hits_100m_obfuscated  Send the numbers and the info about your hardware configuration to feedback@clickhouse.com All the results are published here: https://clickhouse.com/benchmark/hardware/ "},{"title":"Requirements","type":0,"sectionRef":"#","url":"docs/en/operations/requirements","content":"","keywords":""},{"title":"CPU​","type":1,"pageTitle":"Requirements","url":"docs/en/operations/requirements#cpu","content":"For installation from prebuilt deb packages, use a CPU with x86_64 architecture and support for SSE 4.2 instructions. To run ClickHouse with processors that do not support SSE 4.2 or have AArch64 or PowerPC64LE architecture, you should build ClickHouse from sources. ClickHouse implements parallel data processing and uses all the hardware resources available. When choosing a processor, take into account that ClickHouse works more efficiently at configurations with a large number of cores but a lower clock rate than at configurations with fewer cores and a higher clock rate. For example, 16 cores with 2600 MHz is preferable to 8 cores with 3600 MHz. It is recommended to use Turbo Boost and hyper-threading technologies. It significantly improves performance with a typical workload. "},{"title":"RAM​","type":1,"pageTitle":"Requirements","url":"docs/en/operations/requirements#ram","content":"We recommend using a minimum of 4GB of RAM to perform non-trivial queries. The ClickHouse server can run with a much smaller amount of RAM, but it requires memory for processing queries. The required volume of RAM depends on: The complexity of queries.The amount of data that is processed in queries. To calculate the required volume of RAM, you should estimate the size of temporary data for GROUP BY, DISTINCT, JOIN and other operations you use. ClickHouse can use external memory for temporary data. See GROUP BY in External Memory for details. "},{"title":"Swap File​","type":1,"pageTitle":"Requirements","url":"docs/en/operations/requirements#swap-file","content":"Disable the swap file for production environments. "},{"title":"Storage Subsystem​","type":1,"pageTitle":"Requirements","url":"docs/en/operations/requirements#storage-subsystem","content":"You need to have 2GB of free disk space to install ClickHouse. The volume of storage required for your data should be calculated separately. Assessment should include: Estimation of the data volume. You can take a sample of the data and get the average size of a row from it. Then multiply the value by the number of rows you plan to store. The data compression coefficient. To estimate the data compression coefficient, load a sample of your data into ClickHouse, and compare the actual size of the data with the size of the table stored. For example, clickstream data is usually compressed by 6-10 times. To calculate the final volume of data to be stored, apply the compression coefficient to the estimated data volume. If you plan to store data in several replicas, then multiply the estimated volume by the number of replicas. "},{"title":"Network​","type":1,"pageTitle":"Requirements","url":"docs/en/operations/requirements#network","content":"If possible, use networks of 10G or higher class. The network bandwidth is critical for processing distributed queries with a large amount of intermediate data. Besides, network speed affects replication processes. "},{"title":"Software​","type":1,"pageTitle":"Requirements","url":"docs/en/operations/requirements#software","content":"ClickHouse is developed primarily for the Linux family of operating systems. The recommended Linux distribution is Ubuntu. The tzdata package should be installed in the system. ClickHouse can also work in other operating system families. See details in the install guide section of the documentation. "},{"title":"Quotas","type":0,"sectionRef":"#","url":"docs/en/operations/quotas","content":"Quotas Quotas allow you to limit resource usage over a period of time or track the use of resources. Quotas are set up in the user config, which is usually ‘users.xml’. The system also has a feature for limiting the complexity of a single query. See the section Restrictions on query complexity. In contrast to query complexity restrictions, quotas: Place restrictions on a set of queries that can be run over a period of time, instead of limiting a single query.Account for resources spent on all remote servers for distributed query processing. Let’s look at the section of the ‘users.xml’ file that defines quotas. &lt;!-- Quotas --&gt; &lt;quotas&gt; &lt;!-- Quota name. --&gt; &lt;default&gt; &lt;!-- Restrictions for a time period. You can set many intervals with different restrictions. --&gt; &lt;interval&gt; &lt;!-- Length of the interval. --&gt; &lt;duration&gt;3600&lt;/duration&gt; &lt;!-- Unlimited. Just collect data for the specified time interval. --&gt; &lt;queries&gt;0&lt;/queries&gt; &lt;query_selects&gt;0&lt;/query_selects&gt; &lt;query_inserts&gt;0&lt;/query_inserts&gt; &lt;errors&gt;0&lt;/errors&gt; &lt;result_rows&gt;0&lt;/result_rows&gt; &lt;read_rows&gt;0&lt;/read_rows&gt; &lt;execution_time&gt;0&lt;/execution_time&gt; &lt;/interval&gt; &lt;/default&gt; By default, the quota tracks resource consumption for each hour, without limiting usage. The resource consumption calculated for each interval is output to the server log after each request. &lt;statbox&gt; &lt;!-- Restrictions for a time period. You can set many intervals with different restrictions. --&gt; &lt;interval&gt; &lt;!-- Length of the interval. --&gt; &lt;duration&gt;3600&lt;/duration&gt; &lt;queries&gt;1000&lt;/queries&gt; &lt;query_selects&gt;100&lt;/query_selects&gt; &lt;query_inserts&gt;100&lt;/query_inserts&gt; &lt;errors&gt;100&lt;/errors&gt; &lt;result_rows&gt;1000000000&lt;/result_rows&gt; &lt;read_rows&gt;100000000000&lt;/read_rows&gt; &lt;execution_time&gt;900&lt;/execution_time&gt; &lt;/interval&gt; &lt;interval&gt; &lt;duration&gt;86400&lt;/duration&gt; &lt;queries&gt;10000&lt;/queries&gt; &lt;query_selects&gt;10000&lt;/query_selects&gt; &lt;query_inserts&gt;10000&lt;/query_inserts&gt; &lt;errors&gt;1000&lt;/errors&gt; &lt;result_rows&gt;5000000000&lt;/result_rows&gt; &lt;read_rows&gt;500000000000&lt;/read_rows&gt; &lt;execution_time&gt;7200&lt;/execution_time&gt; &lt;/interval&gt; &lt;/statbox&gt; For the ‘statbox’ quota, restrictions are set for every hour and for every 24 hours (86,400 seconds). The time interval is counted, starting from an implementation-defined fixed moment in time. In other words, the 24-hour interval does not necessarily begin at midnight. When the interval ends, all collected values are cleared. For the next hour, the quota calculation starts over. Here are the amounts that can be restricted: queries – The total number of requests. query_selects – The total number of select requests. query_inserts – The total number of insert requests. errors – The number of queries that threw an exception. result_rows – The total number of rows given as a result. read_rows – The total number of source rows read from tables for running the query on all remote servers. execution_time – The total query execution time, in seconds (wall time). If the limit is exceeded for at least one time interval, an exception is thrown with a text about which restriction was exceeded, for which interval, and when the new interval begins (when queries can be sent again). Quotas can use the “quota key” feature to report on resources for multiple keys independently. Here is an example of this: &lt;!-- For the global reports designer. --&gt; &lt;web_global&gt; &lt;!-- keyed – The quota_key &quot;key&quot; is passed in the query parameter, and the quota is tracked separately for each key value. For example, you can pass a username as the key, so the quota will be counted separately for each username. Using keys makes sense only if quota_key is transmitted by the program, not by a user. You can also write &lt;keyed_by_ip /&gt;, so the IP address is used as the quota key. (But keep in mind that users can change the IPv6 address fairly easily.) --&gt; &lt;keyed /&gt; The quota is assigned to users in the ‘users’ section of the config. See the section “Access rights”. For distributed query processing, the accumulated amounts are stored on the requestor server. So if the user goes to another server, the quota there will “start over”. When the server is restarted, quotas are reset. Original article","keywords":""},{"title":"Formats for Input and Output Data","type":0,"sectionRef":"#","url":"docs/en/interfaces/formats","content":"","keywords":""},{"title":"TabSeparated​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#tabseparated","content":"In TabSeparated format, data is written by row. Each row contains values separated by tabs. Each value is followed by a tab, except the last value in the row, which is followed by a line feed. Strictly Unix line feeds are assumed everywhere. The last row also must contain a line feed at the end. Values are written in text format, without enclosing quotation marks, and with special characters escaped. This format is also available under the name TSV. The TabSeparated format is convenient for processing data using custom programs and scripts. It is used by default in the HTTP interface, and in the command-line client’s batch mode. This format also allows transferring data between different DBMSs. For example, you can get a dump from MySQL and upload it to ClickHouse, or vice versa. The TabSeparated format supports outputting total values (when using WITH TOTALS) and extreme values (when ‘extremes’ is set to 1). In these cases, the total values and extremes are output after the main data. The main result, total values, and extremes are separated from each other by an empty line. Example: SELECT EventDate, count() AS c FROM test.hits GROUP BY EventDate WITH TOTALS ORDER BY EventDate FORMAT TabSeparated  2014-03-17 1406958 2014-03-18 1383658 2014-03-19 1405797 2014-03-20 1353623 2014-03-21 1245779 2014-03-22 1031592 2014-03-23 1046491 1970-01-01 8873898 2014-03-17 1031592 2014-03-23 1406958  "},{"title":"Data Formatting​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#data-formatting","content":"Integer numbers are written in decimal form. Numbers can contain an extra “+” character at the beginning (ignored when parsing, and not recorded when formatting). Non-negative numbers can’t contain the negative sign. When reading, it is allowed to parse an empty string as a zero, or (for signed types) a string consisting of just a minus sign as a zero. Numbers that do not fit into the corresponding data type may be parsed as a different number, without an error message. Floating-point numbers are written in decimal form. The dot is used as the decimal separator. Exponential entries are supported, as are ‘inf’, ‘+inf’, ‘-inf’, and ‘nan’. An entry of floating-point numbers may begin or end with a decimal point. During formatting, accuracy may be lost on floating-point numbers. During parsing, it is not strictly required to read the nearest machine-representable number. Dates are written in YYYY-MM-DD format and parsed in the same format, but with any characters as separators. Dates with times are written in the format YYYY-MM-DD hh:mm:ss and parsed in the same format, but with any characters as separators. This all occurs in the system time zone at the time the client or server starts (depending on which of them formats data). For dates with times, daylight saving time is not specified. So if a dump has times during daylight saving time, the dump does not unequivocally match the data, and parsing will select one of the two times. During a read operation, incorrect dates and dates with times can be parsed with natural overflow or as null dates and times, without an error message. As an exception, parsing dates with times is also supported in Unix timestamp format, if it consists of exactly 10 decimal digits. The result is not time zone-dependent. The formats YYYY-MM-DD hh:mm:ss and NNNNNNNNNN are differentiated automatically. Strings are output with backslash-escaped special characters. The following escape sequences are used for output: \\b, \\f, \\r, \\n, \\t, \\0, \\', \\\\. Parsing also supports the sequences \\a, \\v, and \\xHH (hex escape sequences) and any \\c sequences, where c is any character (these sequences are converted to c). Thus, reading data supports formats where a line feed can be written as \\n or \\, or as a line feed. For example, the string Hello world with a line feed between the words instead of space can be parsed in any of the following variations: Hello\\nworld Hello\\ world  The second variant is supported because MySQL uses it when writing tab-separated dumps. The minimum set of characters that you need to escape when passing data in TabSeparated format: tab, line feed (LF) and backslash. Only a small set of symbols are escaped. You can easily stumble onto a string value that your terminal will ruin in output. Arrays are written as a list of comma-separated values in square brackets. Number items in the array are formatted as normally. Date and DateTime types are written in single quotes. Strings are written in single quotes with the same escaping rules as above. NULL is formatted according to setting format_tsv_null_representation (default value is \\N). If setting input_format_tsv_empty_as_default is enabled, empty input fields are replaced with default values. For complex default expressions input_format_defaults_for_omitted_fields must be enabled too. Each element of Nested structures is represented as array. In input data, ENUM values can be represented as names or as ids. First, we try to match the input value to the ENUM name. If we fail and the input value is a number, we try to match this number to ENUM id. If input data contains only ENUM ids, it's recommended to enable the setting input_format_tsv_enum_as_number to optimize ENUM parsing. For example: CREATE TABLE nestedt ( `id` UInt8, `aux` Nested( a UInt8, b String ) ) ENGINE = TinyLog  INSERT INTO nestedt Values ( 1, [1], ['a'])  SELECT * FROM nestedt FORMAT TSV  1 [1] ['a']  "},{"title":"TabSeparatedRaw​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#tabseparatedraw","content":"Differs from TabSeparated format in that the rows are written without escaping. When parsing with this format, tabs or linefeeds are not allowed in each field. This format is also available under the name TSVRaw. "},{"title":"TabSeparatedWithNames​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#tabseparatedwithnames","content":"Differs from the TabSeparated format in that the column names are written in the first row. During parsing, the first row is expected to contain the column names. You can use column names to determine their position and to check their correctness. If setting input_format_with_names_use_header is set to 1, the columns from input data will be mapped to the columns from the table by their names, columns with unknown names will be skipped if setting input_format_skip_unknown_fields is set to 1. Otherwise, the first row will be skipped. This format is also available under the name TSVWithNames. "},{"title":"TabSeparatedWithNamesAndTypes​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#tabseparatedwithnamesandtypes","content":"Differs from the TabSeparated format in that the column names are written to the first row, while the column types are in the second row. The first row with names is processed the same way as in TabSeparatedWithNames format. If setting input_format_with_types_use_header is set to 1, the types from input data will be compared with the types of the corresponding columns from the table. Otherwise, the second row will be skipped. This format is also available under the name TSVWithNamesAndTypes. "},{"title":"TabSeparatedRawWithNames​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#tabseparatedrawwithnames","content":"Differs from TabSeparatedWithNames format in that the rows are written without escaping. When parsing with this format, tabs or linefeeds are not allowed in each field. This format is also available under the name TSVRawWithNames. "},{"title":"TabSeparatedRawWithNamesAndTypes​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#tabseparatedrawwithnamesandtypes","content":"Differs from TabSeparatedWithNamesAndTypes format in that the rows are written without escaping. When parsing with this format, tabs or linefeeds are not allowed in each field. This format is also available under the name TSVRawWithNamesAndNames. "},{"title":"Template​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#format-template","content":"This format allows specifying a custom format string with placeholders for values with a specified escaping rule. It uses settings format_template_resultset, format_template_row, format_template_rows_between_delimiter and some settings of other formats (e.g. output_format_json_quote_64bit_integers when using JSON escaping, see further) Setting format_template_row specifies path to file, which contains format string for rows with the following syntax: delimiter_1${column_1:serializeAs_1}delimiter_2${column_2:serializeAs_2} ... delimiter_N, where delimiter_i is a delimiter between values ($ symbol can be escaped as $$),column_i is a name or index of a column whose values are to be selected or inserted (if empty, then column will be skipped),serializeAs_i is an escaping rule for the column values. The following escaping rules are supported: CSV, JSON, XML (similarly to the formats of the same names)Escaped (similarly to TSV)Quoted (similarly to Values)Raw (without escaping, similarly to TSVRaw)None (no escaping rule, see further) If an escaping rule is omitted, then None will be used. XML is suitable only for output. So, for the following format string:  `Search phrase: ${SearchPhrase:Quoted}, count: ${c:Escaped}, ad price: $$${price:JSON};`  the values of SearchPhrase, c and price columns, which are escaped as Quoted, Escaped and JSON will be printed (for select) or will be expected (for insert) between Search phrase:, , count:, , ad price: $ and ; delimiters respectively. For example: Search phrase: 'bathroom interior design', count: 2166, ad price: $3; The format_template_rows_between_delimiter setting specifies delimiter between rows, which is printed (or expected) after every row except the last one (\\n by default) Setting format_template_resultset specifies the path to file, which contains a format string for resultset. Format string for resultset has the same syntax as a format string for row and allows to specify a prefix, a suffix and a way to print some additional information. It contains the following placeholders instead of column names: data is the rows with data in format_template_row format, separated by format_template_rows_between_delimiter. This placeholder must be the first placeholder in the format string.totals is the row with total values in format_template_row format (when using WITH TOTALS)min is the row with minimum values in format_template_row format (when extremes are set to 1)max is the row with maximum values in format_template_row format (when extremes are set to 1)rows is the total number of output rowsrows_before_limit is the minimal number of rows there would have been without LIMIT. Output only if the query contains LIMIT. If the query contains GROUP BY, rows_before_limit_at_least is the exact number of rows there would have been without a LIMIT.time is the request execution time in secondsrows_read is the number of rows has been readbytes_read is the number of bytes (uncompressed) has been read The placeholders data, totals, min and max must not have escaping rule specified (or None must be specified explicitly). The remaining placeholders may have any escaping rule specified. If the format_template_resultset setting is an empty string, ${data} is used as default value. For insert queries format allows skipping some columns or some fields if prefix or suffix (see example). Select example: SELECT SearchPhrase, count() AS c FROM test.hits GROUP BY SearchPhrase ORDER BY c DESC LIMIT 5 FORMAT Template SETTINGS format_template_resultset = '/some/path/resultset.format', format_template_row = '/some/path/row.format', format_template_rows_between_delimiter = '\\n '  /some/path/resultset.format: &lt;!DOCTYPE HTML&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Search phrases&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;table border=&quot;1&quot;&gt; &lt;caption&gt;Search phrases&lt;/caption&gt; &lt;tr&gt; &lt;th&gt;Search phrase&lt;/th&gt; &lt;th&gt;Count&lt;/th&gt; &lt;/tr&gt; ${data} &lt;/table&gt; &lt;table border=&quot;1&quot;&gt; &lt;caption&gt;Max&lt;/caption&gt; ${max} &lt;/table&gt; &lt;b&gt;Processed ${rows_read:XML} rows in ${time:XML} sec&lt;/b&gt; &lt;/body&gt; &lt;/html&gt;  /some/path/row.format: &lt;tr&gt; &lt;td&gt;${0:XML}&lt;/td&gt; &lt;td&gt;${1:XML}&lt;/td&gt; &lt;/tr&gt;  Result: &lt;!DOCTYPE HTML&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Search phrases&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;table border=&quot;1&quot;&gt; &lt;caption&gt;Search phrases&lt;/caption&gt; &lt;tr&gt; &lt;th&gt;Search phrase&lt;/th&gt; &lt;th&gt;Count&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;8267016&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;bathroom interior design&lt;/td&gt; &lt;td&gt;2166&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;clickhouse&lt;/td&gt; &lt;td&gt;1655&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;spring 2014 fashion&lt;/td&gt; &lt;td&gt;1549&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;freeform photos&lt;/td&gt; &lt;td&gt;1480&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;table border=&quot;1&quot;&gt; &lt;caption&gt;Max&lt;/caption&gt; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;8873898&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;b&gt;Processed 3095973 rows in 0.1569913 sec&lt;/b&gt; &lt;/body&gt; &lt;/html&gt;  Insert example: Some header Page views: 5, User id: 4324182021466249494, Useless field: hello, Duration: 146, Sign: -1 Page views: 6, User id: 4324182021466249494, Useless field: world, Duration: 185, Sign: 1 Total rows: 2  INSERT INTO UserActivity FORMAT Template SETTINGS format_template_resultset = '/some/path/resultset.format', format_template_row = '/some/path/row.format'  /some/path/resultset.format: Some header\\n${data}\\nTotal rows: ${:CSV}\\n  /some/path/row.format: Page views: ${PageViews:CSV}, User id: ${UserID:CSV}, Useless field: ${:CSV}, Duration: ${Duration:CSV}, Sign: ${Sign:CSV}  PageViews, UserID, Duration and Sign inside placeholders are names of columns in the table. Values after Useless field in rows and after \\nTotal rows: in suffix will be ignored. All delimiters in the input data must be strictly equal to delimiters in specified format strings. "},{"title":"TemplateIgnoreSpaces​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#templateignorespaces","content":"This format is suitable only for input. Similar to Template, but skips whitespace characters between delimiters and values in the input stream. However, if format strings contain whitespace characters, these characters will be expected in the input stream. Also allows to specify empty placeholders (${} or ${:None}) to split some delimiter into separate parts to ignore spaces between them. Such placeholders are used only for skipping whitespace characters. It’s possible to read JSON using this format, if values of columns have the same order in all rows. For example, the following request can be used for inserting data from output example of format JSON: INSERT INTO table_name FORMAT TemplateIgnoreSpaces SETTINGS format_template_resultset = '/some/path/resultset.format', format_template_row = '/some/path/row.format', format_template_rows_between_delimiter = ','  /some/path/resultset.format: {${}&quot;meta&quot;${}:${:JSON},${}&quot;data&quot;${}:${}[${data}]${},${}&quot;totals&quot;${}:${:JSON},${}&quot;extremes&quot;${}:${:JSON},${}&quot;rows&quot;${}:${:JSON},${}&quot;rows_before_limit_at_least&quot;${}:${:JSON}${}}  /some/path/row.format: {${}&quot;SearchPhrase&quot;${}:${}${phrase:JSON}${},${}&quot;c&quot;${}:${}${cnt:JSON}${}}  "},{"title":"TSKV​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#tskv","content":"Similar to TabSeparated, but outputs a value in name=value format. Names are escaped the same way as in TabSeparated format, and the = symbol is also escaped. SearchPhrase= count()=8267016 SearchPhrase=bathroom interior design count()=2166 SearchPhrase=clickhouse count()=1655 SearchPhrase=2014 spring fashion count()=1549 SearchPhrase=freeform photos count()=1480 SearchPhrase=angelina jolie count()=1245 SearchPhrase=omsk count()=1112 SearchPhrase=photos of dog breeds count()=1091 SearchPhrase=curtain designs count()=1064 SearchPhrase=baku count()=1000  NULL is formatted as \\N. SELECT * FROM t_null FORMAT TSKV  x=1 y=\\N  When there is a large number of small columns, this format is ineffective, and there is generally no reason to use it. Nevertheless, it is no worse than JSONEachRow in terms of efficiency. Both data output and parsing are supported in this format. For parsing, any order is supported for the values of different columns. It is acceptable for some values to be omitted – they are treated as equal to their default values. In this case, zeros and blank rows are used as default values. Complex values that could be specified in the table are not supported as defaults. Parsing allows the presence of the additional field tskv without the equal sign or a value. This field is ignored. "},{"title":"CSV​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#csv","content":"Comma Separated Values format (RFC). When formatting, rows are enclosed in double-quotes. A double quote inside a string is output as two double quotes in a row. There are no other rules for escaping characters. Date and date-time are enclosed in double-quotes. Numbers are output without quotes. Values are separated by a delimiter character, which is , by default. The delimiter character is defined in the setting format_csv_delimiter. Rows are separated using the Unix line feed (LF). Arrays are serialized in CSV as follows: first, the array is serialized to a string as in TabSeparated format, and then the resulting string is output to CSV in double-quotes. Tuples in CSV format are serialized as separate columns (that is, their nesting in the tuple is lost). $ clickhouse-client --format_csv_delimiter=&quot;|&quot; --query=&quot;INSERT INTO test.csv FORMAT CSV&quot; &lt; data.csv  *By default, the delimiter is ,. See the format_csv_delimiter setting for more information. When parsing, all values can be parsed either with or without quotes. Both double and single quotes are supported. Rows can also be arranged without quotes. In this case, they are parsed up to the delimiter character or line feed (CR or LF). In violation of the RFC, when parsing rows without quotes, the leading and trailing spaces and tabs are ignored. For the line feed, Unix (LF), Windows (CR LF) and Mac OS Classic (CR LF) types are all supported. If setting input_format_csv_empty_as_default is enabled, empty unquoted input values are replaced with default values. For complex default expressions input_format_defaults_for_omitted_fields must be enabled too. NULL is formatted according to setting format_csv_null_representation (default value is \\N). In input data, ENUM values can be represented as names or as ids. First, we try to match the input value to the ENUM name. If we fail and the input value is a number, we try to match this number to ENUM id. If input data contains only ENUM ids, it's recommended to enable the setting input_format_csv_enum_as_number to optimize ENUM parsing. The CSV format supports the output of totals and extremes the same way as TabSeparated. "},{"title":"CSVWithNames​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#csvwithnames","content":"Also prints the header row with column names, similar to TabSeparatedWithNames. "},{"title":"CSVWithNamesAndTypes​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#csvwithnamesandtypes","content":"Also prints two header rows with column names and types, similar to TabSeparatedWithNamesAndTypes. "},{"title":"CustomSeparated​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#format-customseparated","content":"Similar to Template, but it prints or reads all names and types of columns and uses escaping rule from format_custom_escaping_rule setting and delimiters from format_custom_field_delimiter, format_custom_row_before_delimiter, format_custom_row_after_delimiter, format_custom_row_between_delimiter, format_custom_result_before_delimiter and format_custom_result_after_delimiter settings, not from format strings. There is also CustomSeparatedIgnoreSpaces format, which is similar to TemplateIgnoreSpaces. "},{"title":"CustomSeparatedWithNames​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#customseparatedwithnames","content":"Also prints the header row with column names, similar to TabSeparatedWithNames. "},{"title":"CustomSeparatedWithNamesAndTypes​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#customseparatedwithnamesandtypes","content":"Also prints two header rows with column names and types, similar to TabSeparatedWithNamesAndTypes. "},{"title":"JSON​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#json","content":"Outputs data in JSON format. Besides data tables, it also outputs column names and types, along with some additional information: the total number of output rows, and the number of rows that could have been output if there weren’t a LIMIT. Example: SELECT SearchPhrase, count() AS c FROM test.hits GROUP BY SearchPhrase WITH TOTALS ORDER BY c DESC LIMIT 5 FORMAT JSON  { &quot;meta&quot;: [ { &quot;name&quot;: &quot;'hello'&quot;, &quot;type&quot;: &quot;String&quot; }, { &quot;name&quot;: &quot;multiply(42, number)&quot;, &quot;type&quot;: &quot;UInt64&quot; }, { &quot;name&quot;: &quot;range(5)&quot;, &quot;type&quot;: &quot;Array(UInt8)&quot; } ], &quot;data&quot;: [ { &quot;'hello'&quot;: &quot;hello&quot;, &quot;multiply(42, number)&quot;: &quot;0&quot;, &quot;range(5)&quot;: [0,1,2,3,4] }, { &quot;'hello'&quot;: &quot;hello&quot;, &quot;multiply(42, number)&quot;: &quot;42&quot;, &quot;range(5)&quot;: [0,1,2,3,4] }, { &quot;'hello'&quot;: &quot;hello&quot;, &quot;multiply(42, number)&quot;: &quot;84&quot;, &quot;range(5)&quot;: [0,1,2,3,4] } ], &quot;rows&quot;: 3, &quot;rows_before_limit_at_least&quot;: 3 }  The JSON is compatible with JavaScript. To ensure this, some characters are additionally escaped: the slash / is escaped as \\/; alternative line breaks U+2028 and U+2029, which break some browsers, are escaped as \\uXXXX. ASCII control characters are escaped: backspace, form feed, line feed, carriage return, and horizontal tab are replaced with \\b, \\f, \\n, \\r, \\t , as well as the remaining bytes in the 00-1F range using \\uXXXX sequences. Invalid UTF-8 sequences are changed to the replacement character � so the output text will consist of valid UTF-8 sequences. For compatibility with JavaScript, Int64 and UInt64 integers are enclosed in double-quotes by default. To remove the quotes, you can set the configuration parameter output_format_json_quote_64bit_integers to 0. rows – The total number of output rows. rows_before_limit_at_least The minimal number of rows there would have been without LIMIT. Output only if the query contains LIMIT. If the query contains GROUP BY, rows_before_limit_at_least is the exact number of rows there would have been without a LIMIT. totals – Total values (when using WITH TOTALS). extremes – Extreme values (when extremes are set to 1). This format is only appropriate for outputting a query result, but not for parsing (retrieving data to insert in a table). ClickHouse supports NULL, which is displayed as null in the JSON output. To enable +nan, -nan, +inf, -inf values in output, set the output_format_json_quote_denormals to 1. See Also JSONEachRow formatoutput_format_json_array_of_rows setting "},{"title":"JSONStrings​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#jsonstrings","content":"Differs from JSON only in that data fields are output in strings, not in typed JSON values. Example: { &quot;meta&quot;: [ { &quot;name&quot;: &quot;'hello'&quot;, &quot;type&quot;: &quot;String&quot; }, { &quot;name&quot;: &quot;multiply(42, number)&quot;, &quot;type&quot;: &quot;UInt64&quot; }, { &quot;name&quot;: &quot;range(5)&quot;, &quot;type&quot;: &quot;Array(UInt8)&quot; } ], &quot;data&quot;: [ { &quot;'hello'&quot;: &quot;hello&quot;, &quot;multiply(42, number)&quot;: &quot;0&quot;, &quot;range(5)&quot;: &quot;[0,1,2,3,4]&quot; }, { &quot;'hello'&quot;: &quot;hello&quot;, &quot;multiply(42, number)&quot;: &quot;42&quot;, &quot;range(5)&quot;: &quot;[0,1,2,3,4]&quot; }, { &quot;'hello'&quot;: &quot;hello&quot;, &quot;multiply(42, number)&quot;: &quot;84&quot;, &quot;range(5)&quot;: &quot;[0,1,2,3,4]&quot; } ], &quot;rows&quot;: 3, &quot;rows_before_limit_at_least&quot;: 3 }  "},{"title":"JSONAsString​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#jsonasstring","content":"In this format, a single JSON object is interpreted as a single value. If the input has several JSON objects (comma separated), they are interpreted as separate rows. If the input data is enclosed in square brackets, it is interpreted as an array of JSONs. This format can only be parsed for table with a single field of type String. The remaining columns must be set to DEFAULT or MATERIALIZED, or omitted. Once you collect whole JSON object to string you can use JSON functions to process it. Examples Query: DROP TABLE IF EXISTS json_as_string; CREATE TABLE json_as_string (json String) ENGINE = Memory; INSERT INTO json_as_string (json) FORMAT JSONAsString {&quot;foo&quot;:{&quot;bar&quot;:{&quot;x&quot;:&quot;y&quot;},&quot;baz&quot;:1}},{},{&quot;any json stucture&quot;:1} SELECT * FROM json_as_string;  Result: ┌─json──────────────────────────────┐ │ {&quot;foo&quot;:{&quot;bar&quot;:{&quot;x&quot;:&quot;y&quot;},&quot;baz&quot;:1}} │ │ {} │ │ {&quot;any json stucture&quot;:1} │ └───────────────────────────────────┘  An array of JSON objects Query: CREATE TABLE json_square_brackets (field String) ENGINE = Memory; INSERT INTO json_square_brackets FORMAT JSONAsString [{&quot;id&quot;: 1, &quot;name&quot;: &quot;name1&quot;}, {&quot;id&quot;: 2, &quot;name&quot;: &quot;name2&quot;}]; SELECT * FROM json_square_brackets;  Result: ┌─field──────────────────────┐ │ {&quot;id&quot;: 1, &quot;name&quot;: &quot;name1&quot;} │ │ {&quot;id&quot;: 2, &quot;name&quot;: &quot;name2&quot;} │ └────────────────────────────┘  "},{"title":"JSONCompact​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#jsoncompact","content":""},{"title":"JSONCompactStrings​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#jsoncompactstrings","content":"Differs from JSON only in that data rows are output in arrays, not in objects. Example: // JSONCompact { &quot;meta&quot;: [ { &quot;name&quot;: &quot;'hello'&quot;, &quot;type&quot;: &quot;String&quot; }, { &quot;name&quot;: &quot;multiply(42, number)&quot;, &quot;type&quot;: &quot;UInt64&quot; }, { &quot;name&quot;: &quot;range(5)&quot;, &quot;type&quot;: &quot;Array(UInt8)&quot; } ], &quot;data&quot;: [ [&quot;hello&quot;, &quot;0&quot;, [0,1,2,3,4]], [&quot;hello&quot;, &quot;42&quot;, [0,1,2,3,4]], [&quot;hello&quot;, &quot;84&quot;, [0,1,2,3,4]] ], &quot;rows&quot;: 3, &quot;rows_before_limit_at_least&quot;: 3 }  // JSONCompactStrings { &quot;meta&quot;: [ { &quot;name&quot;: &quot;'hello'&quot;, &quot;type&quot;: &quot;String&quot; }, { &quot;name&quot;: &quot;multiply(42, number)&quot;, &quot;type&quot;: &quot;UInt64&quot; }, { &quot;name&quot;: &quot;range(5)&quot;, &quot;type&quot;: &quot;Array(UInt8)&quot; } ], &quot;data&quot;: [ [&quot;hello&quot;, &quot;0&quot;, &quot;[0,1,2,3,4]&quot;], [&quot;hello&quot;, &quot;42&quot;, &quot;[0,1,2,3,4]&quot;], [&quot;hello&quot;, &quot;84&quot;, &quot;[0,1,2,3,4]&quot;] ], &quot;rows&quot;: 3, &quot;rows_before_limit_at_least&quot;: 3 }  "},{"title":"JSONEachRow​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#jsoneachrow","content":""},{"title":"JSONStringsEachRow​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#jsonstringseachrow","content":""},{"title":"JSONCompactEachRow​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#jsoncompacteachrow","content":""},{"title":"JSONCompactStringsEachRow​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#jsoncompactstringseachrow","content":"When using these formats, ClickHouse outputs rows as separated, newline-delimited JSON values, but the data as a whole is not valid JSON. {&quot;some_int&quot;:42,&quot;some_str&quot;:&quot;hello&quot;,&quot;some_tuple&quot;:[1,&quot;a&quot;]} // JSONEachRow [42,&quot;hello&quot;,[1,&quot;a&quot;]] // JSONCompactEachRow [&quot;42&quot;,&quot;hello&quot;,&quot;(2,'a')&quot;] // JSONCompactStringsEachRow  When inserting the data, you should provide a separate JSON value for each row. "},{"title":"JSONEachRowWithProgress​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#jsoneachrowwithprogress","content":""},{"title":"JSONStringsEachRowWithProgress​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#jsonstringseachrowwithprogress","content":"Differs from JSONEachRow/JSONStringsEachRow in that ClickHouse will also yield progress information as JSON values. {&quot;row&quot;:{&quot;'hello'&quot;:&quot;hello&quot;,&quot;multiply(42, number)&quot;:&quot;0&quot;,&quot;range(5)&quot;:[0,1,2,3,4]}} {&quot;row&quot;:{&quot;'hello'&quot;:&quot;hello&quot;,&quot;multiply(42, number)&quot;:&quot;42&quot;,&quot;range(5)&quot;:[0,1,2,3,4]}} {&quot;row&quot;:{&quot;'hello'&quot;:&quot;hello&quot;,&quot;multiply(42, number)&quot;:&quot;84&quot;,&quot;range(5)&quot;:[0,1,2,3,4]}} {&quot;progress&quot;:{&quot;read_rows&quot;:&quot;3&quot;,&quot;read_bytes&quot;:&quot;24&quot;,&quot;written_rows&quot;:&quot;0&quot;,&quot;written_bytes&quot;:&quot;0&quot;,&quot;total_rows_to_read&quot;:&quot;3&quot;}}  "},{"title":"JSONCompactEachRowWithNames​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#jsoncompacteachrowwithnames","content":"Differs from JSONCompactEachRow format in that it also prints the header row with column names, similar to TabSeparatedWithNames. "},{"title":"JSONCompactEachRowWithNamesAndTypes​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#jsoncompacteachrowwithnamesandtypes","content":"Differs from JSONCompactEachRow format in that it also prints two header rows with column names and types, similar to TabSeparatedWithNamesAndTypes. "},{"title":"JSONCompactStringsEachRowWithNames​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#jsoncompactstringseachrowwithnames","content":"Differs from JSONCompactStringsEachRow in that in that it also prints the header row with column names, similar to TabSeparatedWithNames. "},{"title":"JSONCompactStringsEachRowWithNamesAndTypes​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#jsoncompactstringseachrowwithnamesandtypes","content":"Differs from JSONCompactStringsEachRow in that it also prints two header rows with column names and types, similar to TabSeparatedWithNamesAndTypes. [&quot;'hello'&quot;, &quot;multiply(42, number)&quot;, &quot;range(5)&quot;] [&quot;String&quot;, &quot;UInt64&quot;, &quot;Array(UInt8)&quot;] [&quot;hello&quot;, &quot;0&quot;, [0,1,2,3,4]] [&quot;hello&quot;, &quot;42&quot;, [0,1,2,3,4]] [&quot;hello&quot;, &quot;84&quot;, [0,1,2,3,4]]  "},{"title":"Inserting Data​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#inserting-data","content":"INSERT INTO UserActivity FORMAT JSONEachRow {&quot;PageViews&quot;:5, &quot;UserID&quot;:&quot;4324182021466249494&quot;, &quot;Duration&quot;:146,&quot;Sign&quot;:-1} {&quot;UserID&quot;:&quot;4324182021466249494&quot;,&quot;PageViews&quot;:6,&quot;Duration&quot;:185,&quot;Sign&quot;:1}  ClickHouse allows: Any order of key-value pairs in the object.Omitting some values. ClickHouse ignores spaces between elements and commas after the objects. You can pass all the objects in one line. You do not have to separate them with line breaks. Omitted values processing ClickHouse substitutes omitted values with the default values for the corresponding data types. If DEFAULT expr is specified, ClickHouse uses different substitution rules depending on the input_format_defaults_for_omitted_fields setting. Consider the following table: CREATE TABLE IF NOT EXISTS example_table ( x UInt32, a DEFAULT x * 2 ) ENGINE = Memory;  If input_format_defaults_for_omitted_fields = 0, then the default value for x and a equals 0 (as the default value for the UInt32 data type).If input_format_defaults_for_omitted_fields = 1, then the default value for x equals 0, but the default value of a equals x * 2. warning When inserting data with input_format_defaults_for_omitted_fields = 1, ClickHouse consumes more computational resources, compared to insertion with input_format_defaults_for_omitted_fields = 0. "},{"title":"Selecting Data​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#selecting-data","content":"Consider the UserActivity table as an example: ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐ │ 4324182021466249494 │ 5 │ 146 │ -1 │ │ 4324182021466249494 │ 6 │ 185 │ 1 │ └─────────────────────┴───────────┴──────────┴──────┘  The query SELECT * FROM UserActivity FORMAT JSONEachRow returns: {&quot;UserID&quot;:&quot;4324182021466249494&quot;,&quot;PageViews&quot;:5,&quot;Duration&quot;:146,&quot;Sign&quot;:-1} {&quot;UserID&quot;:&quot;4324182021466249494&quot;,&quot;PageViews&quot;:6,&quot;Duration&quot;:185,&quot;Sign&quot;:1}  Unlike the JSON format, there is no substitution of invalid UTF-8 sequences. Values are escaped in the same way as for JSON. info Any set of bytes can be output in the strings. Use the JSONEachRow format if you are sure that the data in the table can be formatted as JSON without losing any information. "},{"title":"Usage of Nested Structures​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#jsoneachrow-nested","content":"If you have a table with Nested data type columns, you can insert JSON data with the same structure. Enable this feature with the input_format_import_nested_json setting. For example, consider the following table: CREATE TABLE json_each_row_nested (n Nested (s String, i Int32) ) ENGINE = Memory  As you can see in the Nested data type description, ClickHouse treats each component of the nested structure as a separate column (n.s and n.i for our table). You can insert data in the following way: INSERT INTO json_each_row_nested FORMAT JSONEachRow {&quot;n.s&quot;: [&quot;abc&quot;, &quot;def&quot;], &quot;n.i&quot;: [1, 23]}  To insert data as a hierarchical JSON object, set input_format_import_nested_json=1. { &quot;n&quot;: { &quot;s&quot;: [&quot;abc&quot;, &quot;def&quot;], &quot;i&quot;: [1, 23] } }  Without this setting, ClickHouse throws an exception. SELECT name, value FROM system.settings WHERE name = 'input_format_import_nested_json'  ┌─name────────────────────────────┬─value─┐ │ input_format_import_nested_json │ 0 │ └─────────────────────────────────┴───────┘  INSERT INTO json_each_row_nested FORMAT JSONEachRow {&quot;n&quot;: {&quot;s&quot;: [&quot;abc&quot;, &quot;def&quot;], &quot;i&quot;: [1, 23]}}  Code: 117. DB::Exception: Unknown field found while parsing JSONEachRow format: n: (at row 1)  SET input_format_import_nested_json=1 INSERT INTO json_each_row_nested FORMAT JSONEachRow {&quot;n&quot;: {&quot;s&quot;: [&quot;abc&quot;, &quot;def&quot;], &quot;i&quot;: [1, 23]}} SELECT * FROM json_each_row_nested  ┌─n.s───────────┬─n.i────┐ │ ['abc','def'] │ [1,23] │ └───────────────┴────────┘  "},{"title":"Native​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#native","content":"The most efficient format. Data is written and read by blocks in binary format. For each block, the number of rows, number of columns, column names and types, and parts of columns in this block are recorded one after another. In other words, this format is “columnar” – it does not convert columns to rows. This is the format used in the native interface for interaction between servers, for using the command-line client, and for C++ clients. You can use this format to quickly generate dumps that can only be read by the ClickHouse DBMS. It does not make sense to work with this format yourself. "},{"title":"Null​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#null","content":"Nothing is output. However, the query is processed, and when using the command-line client, data is transmitted to the client. This is used for tests, including performance testing. Obviously, this format is only appropriate for output, not for parsing. "},{"title":"Pretty​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#pretty","content":"Outputs data as Unicode-art tables, also using ANSI-escape sequences for setting colours in the terminal. A full grid of the table is drawn, and each row occupies two lines in the terminal. Each result block is output as a separate table. This is necessary so that blocks can be output without buffering results (buffering would be necessary in order to pre-calculate the visible width of all the values). NULL is output as ᴺᵁᴸᴸ. Example (shown for the PrettyCompact format): SELECT * FROM t_null  ┌─x─┬────y─┐ │ 1 │ ᴺᵁᴸᴸ │ └───┴──────┘  Rows are not escaped in Pretty* formats. Example is shown for the PrettyCompact format: SELECT 'String with \\'quotes\\' and \\t character' AS Escaping_test  ┌─Escaping_test────────────────────────┐ │ String with 'quotes' and character │ └──────────────────────────────────────┘  To avoid dumping too much data to the terminal, only the first 10,000 rows are printed. If the number of rows is greater than or equal to 10,000, the message “Showed first 10 000” is printed. This format is only appropriate for outputting a query result, but not for parsing (retrieving data to insert in a table). The Pretty format supports outputting total values (when using WITH TOTALS) and extremes (when ‘extremes’ is set to 1). In these cases, total values and extreme values are output after the main data, in separate tables. Example (shown for the PrettyCompact format): SELECT EventDate, count() AS c FROM test.hits GROUP BY EventDate WITH TOTALS ORDER BY EventDate FORMAT PrettyCompact  ┌──EventDate─┬───────c─┐ │ 2014-03-17 │ 1406958 │ │ 2014-03-18 │ 1383658 │ │ 2014-03-19 │ 1405797 │ │ 2014-03-20 │ 1353623 │ │ 2014-03-21 │ 1245779 │ │ 2014-03-22 │ 1031592 │ │ 2014-03-23 │ 1046491 │ └────────────┴─────────┘ Totals: ┌──EventDate─┬───────c─┐ │ 1970-01-01 │ 8873898 │ └────────────┴─────────┘ Extremes: ┌──EventDate─┬───────c─┐ │ 2014-03-17 │ 1031592 │ │ 2014-03-23 │ 1406958 │ └────────────┴─────────┘  "},{"title":"PrettyCompact​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#prettycompact","content":"Differs from Pretty in that the grid is drawn between rows and the result is more compact. This format is used by default in the command-line client in interactive mode. "},{"title":"PrettyCompactMonoBlock​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#prettycompactmonoblock","content":"Differs from PrettyCompact in that up to 10,000 rows are buffered, then output as a single table, not by blocks. "},{"title":"PrettyNoEscapes​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#prettynoescapes","content":"Differs from Pretty in that ANSI-escape sequences aren’t used. This is necessary for displaying this format in a browser, as well as for using the ‘watch’ command-line utility. Example: $ watch -n1 &quot;clickhouse-client --query='SELECT event, value FROM system.events FORMAT PrettyCompactNoEscapes'&quot;  You can use the HTTP interface for displaying in the browser. "},{"title":"PrettyCompactNoEscapes​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#prettycompactnoescapes","content":"The same as the previous setting. "},{"title":"PrettySpaceNoEscapes​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#prettyspacenoescapes","content":"The same as the previous setting. "},{"title":"PrettySpace​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#prettyspace","content":"Differs from PrettyCompact in that whitespace (space characters) is used instead of the grid. "},{"title":"RowBinary​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#rowbinary","content":"Formats and parses data by row in binary format. Rows and values are listed consecutively, without separators. This format is less efficient than the Native format since it is row-based. Integers use fixed-length little-endian representation. For example, UInt64 uses 8 bytes. DateTime is represented as UInt32 containing the Unix timestamp as the value. Date is represented as a UInt16 object that contains the number of days since 1970-01-01 as the value. String is represented as a varint length (unsigned LEB128), followed by the bytes of the string. FixedString is represented simply as a sequence of bytes. Array is represented as a varint length (unsigned LEB128), followed by successive elements of the array. For NULL support, an additional byte containing 1 or 0 is added before each Nullable value. If 1, then the value is NULL and this byte is interpreted as a separate value. If 0, the value after the byte is not NULL. "},{"title":"RowBinaryWithNames​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#rowbinarywithnames","content":"Similar to RowBinary, but with added header: LEB128-encoded number of columns (N)N Strings specifying column names "},{"title":"RowBinaryWithNamesAndTypes​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#rowbinarywithnamesandtypes","content":"Similar to RowBinary, but with added header: LEB128-encoded number of columns (N)N Strings specifying column namesN Strings specifying column types "},{"title":"Values​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#data-format-values","content":"Prints every row in brackets. Rows are separated by commas. There is no comma after the last row. The values inside the brackets are also comma-separated. Numbers are output in a decimal format without quotes. Arrays are output in square brackets. Strings, dates, and dates with times are output in quotes. Escaping rules and parsing are similar to the TabSeparated format. During formatting, extra spaces aren’t inserted, but during parsing, they are allowed and skipped (except for spaces inside array values, which are not allowed). NULL is represented as NULL. The minimum set of characters that you need to escape when passing data in Values ​​format: single quotes and backslashes. This is the format that is used in INSERT INTO t VALUES ..., but you can also use it for formatting query results. See also: input_format_values_interpret_expressions and input_format_values_deduce_templates_of_expressions settings. "},{"title":"Vertical​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#vertical","content":"Prints each value on a separate line with the column name specified. This format is convenient for printing just one or a few rows if each row consists of a large number of columns. NULL is output as ᴺᵁᴸᴸ. Example: SELECT * FROM t_null FORMAT Vertical  Row 1: ────── x: 1 y: ᴺᵁᴸᴸ  Rows are not escaped in Vertical format: SELECT 'string with \\'quotes\\' and \\t with some special \\n characters' AS test FORMAT Vertical  Row 1: ────── test: string with 'quotes' and with some special characters  This format is only appropriate for outputting a query result, but not for parsing (retrieving data to insert in a table). "},{"title":"XML​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#xml","content":"XML format is suitable only for output, not for parsing. Example: &lt;?xml version='1.0' encoding='UTF-8' ?&gt; &lt;result&gt; &lt;meta&gt; &lt;columns&gt; &lt;column&gt; &lt;name&gt;SearchPhrase&lt;/name&gt; &lt;type&gt;String&lt;/type&gt; &lt;/column&gt; &lt;column&gt; &lt;name&gt;count()&lt;/name&gt; &lt;type&gt;UInt64&lt;/type&gt; &lt;/column&gt; &lt;/columns&gt; &lt;/meta&gt; &lt;data&gt; &lt;row&gt; &lt;SearchPhrase&gt;&lt;/SearchPhrase&gt; &lt;field&gt;8267016&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;SearchPhrase&gt;bathroom interior design&lt;/SearchPhrase&gt; &lt;field&gt;2166&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;SearchPhrase&gt;clickhouse&lt;/SearchPhrase&gt; &lt;field&gt;1655&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;SearchPhrase&gt;2014 spring fashion&lt;/SearchPhrase&gt; &lt;field&gt;1549&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;SearchPhrase&gt;freeform photos&lt;/SearchPhrase&gt; &lt;field&gt;1480&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;SearchPhrase&gt;angelina jolie&lt;/SearchPhrase&gt; &lt;field&gt;1245&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;SearchPhrase&gt;omsk&lt;/SearchPhrase&gt; &lt;field&gt;1112&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;SearchPhrase&gt;photos of dog breeds&lt;/SearchPhrase&gt; &lt;field&gt;1091&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;SearchPhrase&gt;curtain designs&lt;/SearchPhrase&gt; &lt;field&gt;1064&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;SearchPhrase&gt;baku&lt;/SearchPhrase&gt; &lt;field&gt;1000&lt;/field&gt; &lt;/row&gt; &lt;/data&gt; &lt;rows&gt;10&lt;/rows&gt; &lt;rows_before_limit_at_least&gt;141137&lt;/rows_before_limit_at_least&gt; &lt;/result&gt;  If the column name does not have an acceptable format, just ‘field’ is used as the element name. In general, the XML structure follows the JSON structure. Just as for JSON, invalid UTF-8 sequences are changed to the replacement character � so the output text will consist of valid UTF-8 sequences. In string values, the characters &lt; and &amp; are escaped as &lt; and &amp;. Arrays are output as &lt;array&gt;&lt;elem&gt;Hello&lt;/elem&gt;&lt;elem&gt;World&lt;/elem&gt;...&lt;/array&gt;,and tuples as &lt;tuple&gt;&lt;elem&gt;Hello&lt;/elem&gt;&lt;elem&gt;World&lt;/elem&gt;...&lt;/tuple&gt;. "},{"title":"CapnProto​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#capnproto","content":"CapnProto is a binary message format similar to Protocol Buffers and Thrift, but not like JSON or MessagePack. CapnProto messages are strictly typed and not self-describing, meaning they need an external schema description. The schema is applied on the fly and cached for each query. See also Format Schema. "},{"title":"Data Types Matching​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#data_types-matching-capnproto","content":"The table below shows supported data types and how they match ClickHouse data types in INSERT and SELECT queries. CapnProto data type (INSERT)\tClickHouse data type\tCapnProto data type (SELECT)UINT8, BOOL\tUInt8\tUINT8 INT8\tInt8\tINT8 UINT16\tUInt16, Date\tUINT16 INT16\tInt16\tINT16 UINT32\tUInt32, DateTime\tUINT32 INT32\tInt32\tINT32 UINT64\tUInt64\tUINT64 INT64\tInt64, DateTime64\tINT64 FLOAT32\tFloat32\tFLOAT32 FLOAT64\tFloat64\tFLOAT64 TEXT, DATA\tString, FixedString\tTEXT, DATA union(T, Void), union(Void, T)\tNullable(T)\tunion(T, Void), union(Void, T) ENUM\tEnum(8|16)\tENUM LIST\tArray\tLIST STRUCT\tTuple\tSTRUCT For working with Enum in CapnProto format use the format_capn_proto_enum_comparising_mode setting. Arrays can be nested and can have a value of the Nullable type as an argument. Tuple type also can be nested. "},{"title":"Inserting and Selecting Data​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#inserting-and-selecting-data-capnproto","content":"You can insert CapnProto data from a file into ClickHouse table by the following command: $ cat capnproto_messages.bin | clickhouse-client --query &quot;INSERT INTO test.hits FORMAT CapnProto SETTINGS format_schema = 'schema:Message'&quot;  Where schema.capnp looks like this: struct Message { SearchPhrase @0 :Text; c @1 :Uint64; }  You can select data from a ClickHouse table and save them into some file in the CapnProto format by the following command: $ clickhouse-client --query = &quot;SELECT * FROM test.hits FORMAT CapnProto SETTINGS format_schema = 'schema:Message'&quot;  "},{"title":"Protobuf​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#protobuf","content":"Protobuf - is a Protocol Buffers format. This format requires an external format schema. The schema is cached between queries. ClickHouse supports both proto2 and proto3 syntaxes. Repeated/optional/required fields are supported. Usage examples: SELECT * FROM test.table FORMAT Protobuf SETTINGS format_schema = 'schemafile:MessageType'  cat protobuf_messages.bin | clickhouse-client --query &quot;INSERT INTO test.table FORMAT Protobuf SETTINGS format_schema='schemafile:MessageType'&quot;  where the file schemafile.proto looks like this: syntax = &quot;proto3&quot;; message MessageType { string name = 1; string surname = 2; uint32 birthDate = 3; repeated string phoneNumbers = 4; };  To find the correspondence between table columns and fields of Protocol Buffers’ message type ClickHouse compares their names. This comparison is case-insensitive and the characters _ (underscore) and . (dot) are considered as equal. If types of a column and a field of Protocol Buffers’ message are different the necessary conversion is applied. Nested messages are supported. For example, for the field z in the following message type message MessageType { message XType { message YType { int32 z; }; repeated YType y; }; XType x; };  ClickHouse tries to find a column named x.y.z (or x_y_z or X.y_Z and so on). Nested messages are suitable to input or output a nested data structures. Default values defined in a protobuf schema like this syntax = &quot;proto2&quot;; message MessageType { optional int32 result_per_page = 3 [default = 10]; }  are not applied; the table defaults are used instead of them. ClickHouse inputs and outputs protobuf messages in the length-delimited format. It means before every message should be written its length as a varint. See also how to read/write length-delimited protobuf messages in popular languages. "},{"title":"ProtobufSingle​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#protobufsingle","content":"Same as Protobuf but for storing/parsing single Protobuf message without length delimiters. "},{"title":"Avro​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#data-format-avro","content":"Apache Avro is a row-oriented data serialization framework developed within Apache’s Hadoop project. ClickHouse Avro format supports reading and writing Avro data files. "},{"title":"Data Types Matching​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#data_types-matching","content":"The table below shows supported data types and how they match ClickHouse data types in INSERT and SELECT queries. Avro data type INSERT\tClickHouse data type\tAvro data type SELECTboolean, int, long, float, double\tInt(8|16|32), UInt(8|16|32)\tint boolean, int, long, float, double\tInt64, UInt64\tlong boolean, int, long, float, double\tFloat32\tfloat boolean, int, long, float, double\tFloat64\tdouble bytes, string, fixed, enum\tString\tbytes or string * bytes, string, fixed\tFixedString(N)\tfixed(N) enum\tEnum(8|16)\tenum array(T)\tArray(T)\tarray(T) union(null, T), union(T, null)\tNullable(T)\tunion(null, T) null\tNullable(Nothing)\tnull int (date) **\tDate\tint (date) ** long (timestamp-millis) **\tDateTime64(3)\tlong (timestamp-millis) * long (timestamp-micros) **\tDateTime64(6)\tlong (timestamp-micros) * * bytes is default, controlled by output_format_avro_string_column_pattern** Avro logical types Unsupported Avro data types: record (non-root), map Unsupported Avro logical data types: time-millis, time-micros, duration "},{"title":"Inserting Data​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#inserting-data-1","content":"To insert data from an Avro file into ClickHouse table: $ cat file.avro | clickhouse-client --query=&quot;INSERT INTO {some_table} FORMAT Avro&quot;  The root schema of input Avro file must be of record type. To find the correspondence between table columns and fields of Avro schema ClickHouse compares their names. This comparison is case-sensitive. Unused fields are skipped. Data types of ClickHouse table columns can differ from the corresponding fields of the Avro data inserted. When inserting data, ClickHouse interprets data types according to the table above and then casts the data to corresponding column type. "},{"title":"Selecting Data​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#selecting-data-1","content":"To select data from ClickHouse table into an Avro file: $ clickhouse-client --query=&quot;SELECT * FROM {some_table} FORMAT Avro&quot; &gt; file.avro  Column names must: start with [A-Za-z_]subsequently contain only [A-Za-z0-9_] Output Avro file compression and sync interval can be configured with output_format_avro_codec and output_format_avro_sync_interval respectively. "},{"title":"AvroConfluent​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#data-format-avro-confluent","content":"AvroConfluent supports decoding single-object Avro messages commonly used with Kafka and Confluent Schema Registry. Each Avro message embeds a schema id that can be resolved to the actual schema with help of the Schema Registry. Schemas are cached once resolved. Schema Registry URL is configured with format_avro_schema_registry_url. "},{"title":"Data Types Matching​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#data_types-matching-1","content":"Same as Avro. "},{"title":"Usage​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#usage","content":"To quickly verify schema resolution you can use kafkacat with clickhouse-local: $ kafkacat -b kafka-broker -C -t topic1 -o beginning -f '%s' -c 3 | clickhouse-local --input-format AvroConfluent --format_avro_schema_registry_url 'http://schema-registry' -S &quot;field1 Int64, field2 String&quot; -q 'select * from table' 1 a 2 b 3 c  To use AvroConfluent with Kafka: CREATE TABLE topic1_stream ( field1 String, field2 String ) ENGINE = Kafka() SETTINGS kafka_broker_list = 'kafka-broker', kafka_topic_list = 'topic1', kafka_group_name = 'group1', kafka_format = 'AvroConfluent'; SET format_avro_schema_registry_url = 'http://schema-registry'; SELECT * FROM topic1_stream;  warning Setting format_avro_schema_registry_url needs to be configured in users.xml to maintain it’s value after a restart. Also you can use the format_avro_schema_registry_url setting of the Kafka table engine. "},{"title":"Parquet​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#data-format-parquet","content":"Apache Parquet is a columnar storage format widespread in the Hadoop ecosystem. ClickHouse supports read and write operations for this format. "},{"title":"Data Types Matching​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#data_types-matching-2","content":"The table below shows supported data types and how they match ClickHouse data types in INSERT and SELECT queries. Parquet data type (INSERT)\tClickHouse data type\tParquet data type (SELECT)UINT8, BOOL\tUInt8\tUINT8 INT8\tInt8\tINT8 UINT16\tUInt16\tUINT16 INT16\tInt16\tINT16 UINT32\tUInt32\tUINT32 INT32\tInt32\tINT32 UINT64\tUInt64\tUINT64 INT64\tInt64\tINT64 FLOAT, HALF_FLOAT\tFloat32\tFLOAT DOUBLE\tFloat64\tDOUBLE DATE32\tDate\tUINT16 DATE64, TIMESTAMP\tDateTime\tUINT32 STRING, BINARY\tString\tBINARY —\tFixedString\tBINARY DECIMAL\tDecimal\tDECIMAL LIST\tArray\tLIST STRUCT\tTuple\tSTRUCT MAP\tMap\tMAP Arrays can be nested and can have a value of the Nullable type as an argument. Tuple and Map types also can be nested. ClickHouse supports configurable precision of Decimal type. The INSERT query treats the Parquet DECIMAL type as the ClickHouse Decimal128 type. Unsupported Parquet data types: TIME32, FIXED_SIZE_BINARY, JSON, UUID, ENUM. Data types of ClickHouse table columns can differ from the corresponding fields of the Parquet data inserted. When inserting data, ClickHouse interprets data types according to the table above and then cast the data to that data type which is set for the ClickHouse table column. "},{"title":"Inserting and Selecting Data​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#inserting-and-selecting-data","content":"You can insert Parquet data from a file into ClickHouse table by the following command: $ cat {filename} | clickhouse-client --query=&quot;INSERT INTO {some_table} FORMAT Parquet&quot;  To insert data into Nested columns as an array of structs values you must switch on the input_format_parquet_import_nested setting. You can select data from a ClickHouse table and save them into some file in the Parquet format by the following command: $ clickhouse-client --query=&quot;SELECT * FROM {some_table} FORMAT Parquet&quot; &gt; {some_file.pq}  To exchange data with Hadoop, you can use HDFS table engine. "},{"title":"Arrow​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#data-format-arrow","content":"Apache Arrow comes with two built-in columnar storage formats. ClickHouse supports read and write operations for these formats. Arrow is Apache Arrow’s &quot;file mode&quot; format. It is designed for in-memory random access. "},{"title":"Data Types Matching​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#data_types-matching-arrow","content":"The table below shows supported data types and how they match ClickHouse data types in INSERT and SELECT queries. Arrow data type (INSERT)\tClickHouse data type\tArrow data type (SELECT)UINT8, BOOL\tUInt8\tUINT8 INT8\tInt8\tINT8 UINT16\tUInt16\tUINT16 INT16\tInt16\tINT16 UINT32\tUInt32\tUINT32 INT32\tInt32\tINT32 UINT64\tUInt64\tUINT64 INT64\tInt64\tINT64 FLOAT, HALF_FLOAT\tFloat32\tFLOAT32 DOUBLE\tFloat64\tFLOAT64 DATE32\tDate\tUINT16 DATE64, TIMESTAMP\tDateTime\tUINT32 STRING, BINARY\tString\tBINARY STRING, BINARY\tFixedString\tBINARY DECIMAL\tDecimal\tDECIMAL DECIMAL256\tDecimal256\tDECIMAL256 LIST\tArray\tLIST STRUCT\tTuple\tSTRUCT MAP\tMap\tMAP Arrays can be nested and can have a value of the Nullable type as an argument. Tuple and Map types also can be nested. The DICTIONARY type is supported for INSERT queries, and for SELECT queries there is an output_format_arrow_low_cardinality_as_dictionary setting that allows to output LowCardinality type as a DICTIONARY type. ClickHouse supports configurable precision of the Decimal type. The INSERT query treats the Arrow DECIMAL type as the ClickHouse Decimal128 type. Unsupported Arrow data types: TIME32, FIXED_SIZE_BINARY, JSON, UUID, ENUM. The data types of ClickHouse table columns do not have to match the corresponding Arrow data fields. When inserting data, ClickHouse interprets data types according to the table above and then casts the data to the data type set for the ClickHouse table column. "},{"title":"Inserting Data​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#inserting-data-arrow","content":"You can insert Arrow data from a file into ClickHouse table by the following command: $ cat filename.arrow | clickhouse-client --query=&quot;INSERT INTO some_table FORMAT Arrow&quot;  To insert data into Nested columns as an array of structs values you must switch on the input_format_arrow_import_nested setting. "},{"title":"Selecting Data​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#selecting-data-arrow","content":"You can select data from a ClickHouse table and save them into some file in the Arrow format by the following command: $ clickhouse-client --query=&quot;SELECT * FROM {some_table} FORMAT Arrow&quot; &gt; {filename.arrow}  "},{"title":"ArrowStream​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#data-format-arrow-stream","content":"ArrowStream is Apache Arrow’s “stream mode” format. It is designed for in-memory stream processing. "},{"title":"ORC​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#data-format-orc","content":"Apache ORC is a columnar storage format widespread in the Hadoop ecosystem. "},{"title":"Data Types Matching​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#data_types-matching-3","content":"The table below shows supported data types and how they match ClickHouse data types in INSERT and SELECT queries. ORC data type (INSERT)\tClickHouse data type\tORC data type (SELECT)UINT8, BOOL\tUInt8\tUINT8 INT8\tInt8\tINT8 UINT16\tUInt16\tUINT16 INT16\tInt16\tINT16 UINT32\tUInt32\tUINT32 INT32\tInt32\tINT32 UINT64\tUInt64\tUINT64 INT64\tInt64\tINT64 FLOAT, HALF_FLOAT\tFloat32\tFLOAT DOUBLE\tFloat64\tDOUBLE DATE32\tDate\tDATE32 DATE64, TIMESTAMP\tDateTime\tTIMESTAMP STRING, BINARY\tString\tBINARY DECIMAL\tDecimal\tDECIMAL LIST\tArray\tLIST STRUCT\tTuple\tSTRUCT MAP\tMap\tMAP Arrays can be nested and can have a value of the Nullable type as an argument. Tuple and Map types also can be nested. ClickHouse supports configurable precision of the Decimal type. The INSERT query treats the ORC DECIMAL type as the ClickHouse Decimal128 type. Unsupported ORC data types: TIME32, FIXED_SIZE_BINARY, JSON, UUID, ENUM. The data types of ClickHouse table columns do not have to match the corresponding ORC data fields. When inserting data, ClickHouse interprets data types according to the table above and then casts the data to the data type set for the ClickHouse table column. "},{"title":"Inserting Data​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#inserting-data-2","content":"You can insert ORC data from a file into ClickHouse table by the following command: $ cat filename.orc | clickhouse-client --query=&quot;INSERT INTO some_table FORMAT ORC&quot;  To insert data into Nested columns as an array of structs values you must switch on the input_format_orc_import_nested setting. "},{"title":"Selecting Data​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#selecting-data-2","content":"You can select data from a ClickHouse table and save them into some file in the ORC format by the following command: $ clickhouse-client --query=&quot;SELECT * FROM {some_table} FORMAT ORC&quot; &gt; {filename.orc}  To exchange data with Hadoop, you can use HDFS table engine. "},{"title":"LineAsString​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#lineasstring","content":"In this format, every line of input data is interpreted as a single string value. This format can only be parsed for table with a single field of type String. The remaining columns must be set to DEFAULT or MATERIALIZED, or omitted. Example Query: DROP TABLE IF EXISTS line_as_string; CREATE TABLE line_as_string (field String) ENGINE = Memory; INSERT INTO line_as_string FORMAT LineAsString &quot;I love apple&quot;, &quot;I love banana&quot;, &quot;I love orange&quot;; SELECT * FROM line_as_string;  Result: ┌─field─────────────────────────────────────────────┐ │ &quot;I love apple&quot;, &quot;I love banana&quot;, &quot;I love orange&quot;; │ └───────────────────────────────────────────────────┘  "},{"title":"Regexp​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#data-format-regexp","content":"Each line of imported data is parsed according to the regular expression. When working with the Regexp format, you can use the following settings: format_regexp — String. Contains regular expression in the re2 format. format_regexp_escaping_rule — String. The following escaping rules are supported: CSV (similarly to CSV)JSON (similarly to JSONEachRow)Escaped (similarly to TSV)Quoted (similarly to Values)Raw (extracts subpatterns as a whole, no escaping rules, similarly to TSVRaw) format_regexp_skip_unmatched — UInt8. Defines the need to throw an exeption in case the format_regexp expression does not match the imported data. Can be set to 0 or 1. Usage The regular expression from format_regexp setting is applied to every line of imported data. The number of subpatterns in the regular expression must be equal to the number of columns in imported dataset. Lines of the imported data must be separated by newline character '\\n' or DOS-style newline &quot;\\r\\n&quot;. The content of every matched subpattern is parsed with the method of corresponding data type, according to format_regexp_escaping_rule setting. If the regular expression does not match the line and format_regexp_skip_unmatched is set to 1, the line is silently skipped. If format_regexp_skip_unmatched is set to 0, exception is thrown. Example Consider the file data.tsv: id: 1 array: [1,2,3] string: str1 date: 2020-01-01 id: 2 array: [1,2,3] string: str2 date: 2020-01-02 id: 3 array: [1,2,3] string: str3 date: 2020-01-03  and the table: CREATE TABLE imp_regex_table (id UInt32, array Array(UInt32), string String, date Date) ENGINE = Memory;  Import command: $ cat data.tsv | clickhouse-client --query &quot;INSERT INTO imp_regex_table FORMAT Regexp SETTINGS format_regexp='id: (.+?) array: (.+?) string: (.+?) date: (.+?)', format_regexp_escaping_rule='Escaped', format_regexp_skip_unmatched=0;&quot;  Query: SELECT * FROM imp_regex_table;  Result: ┌─id─┬─array───┬─string─┬───────date─┐ │ 1 │ [1,2,3] │ str1 │ 2020-01-01 │ │ 2 │ [1,2,3] │ str2 │ 2020-01-02 │ │ 3 │ [1,2,3] │ str3 │ 2020-01-03 │ └────┴─────────┴────────┴────────────┘  "},{"title":"Format Schema​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#formatschema","content":"The file name containing the format schema is set by the setting format_schema. It’s required to set this setting when it is used one of the formats Cap'n Proto and Protobuf. The format schema is a combination of a file name and the name of a message type in this file, delimited by a colon, e.g. schemafile.proto:MessageType. If the file has the standard extension for the format (for example, .proto for Protobuf), it can be omitted and in this case, the format schema looks like schemafile:MessageType. If you input or output data via the client in the interactive mode, the file name specified in the format schema can contain an absolute path or a path relative to the current directory on the client. If you use the client in the batch mode, the path to the schema must be relative due to security reasons. If you input or output data via the HTTP interface the file name specified in the format schema should be located in the directory specified in format_schema_pathin the server configuration. "},{"title":"Skipping Errors​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#skippingerrors","content":"Some formats such as CSV, TabSeparated, TSKV, JSONEachRow, Template, CustomSeparated and Protobuf can skip broken row if parsing error occurred and continue parsing from the beginning of next row. See input_format_allow_errors_num andinput_format_allow_errors_ratio settings. Limitations: In case of parsing error JSONEachRow skips all data until the new line (or EOF), so rows must be delimited by \\n to count errors correctly.Template and CustomSeparated use delimiter after the last column and delimiter between rows to find the beginning of next row, so skipping errors works only if at least one of them is not empty. "},{"title":"RawBLOB​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#rawblob","content":"In this format, all input data is read to a single value. It is possible to parse only a table with a single field of type String or similar. The result is output in binary format without delimiters and escaping. If more than one value is output, the format is ambiguous, and it will be impossible to read the data back. Below is a comparison of the formats RawBLOB and TabSeparatedRaw.RawBLOB: data is output in binary format, no escaping;there are no delimiters between values;no newline at the end of each value.[TabSeparatedRaw] (#tabseparatedraw):data is output without escaping;the rows contain values separated by tabs;there is a line feed after the last value in every row. The following is a comparison of the RawBLOB and RowBinary formats.RawBLOB: String fields are output without being prefixed by length.RowBinary:String fields are represented as length in varint format (unsigned [LEB128] (https://en.wikipedia.org/wiki/LEB128)), followed by the bytes of the string. When an empty data is passed to the RawBLOB input, ClickHouse throws an exception: Code: 108. DB::Exception: No data to insert  Example $ clickhouse-client --query &quot;CREATE TABLE {some_table} (a String) ENGINE = Memory;&quot; $ cat {filename} | clickhouse-client --query=&quot;INSERT INTO {some_table} FORMAT RawBLOB&quot; $ clickhouse-client --query &quot;SELECT * FROM {some_table} FORMAT RawBLOB&quot; | md5sum  Result: f9725a22f9191e064120d718e26862a9 -  "},{"title":"MsgPack​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#msgpack","content":"ClickHouse supports reading and writing MessagePack data files. "},{"title":"Data Types Matching​","type":1,"pageTitle":"Formats for Input and Output Data","url":"docs/en/interfaces/formats#data-types-matching-msgpack","content":"MessagePack data type (INSERT)\tClickHouse data type\tMessagePack data type (SELECT)uint N, positive fixint\tUIntN\tuint N int N\tIntN\tint N bool\tUInt8\tuint 8 fixstr, str 8, str 16, str 32, bin 8, bin 16, bin 32\tString\tbin 8, bin 16, bin 32 fixstr, str 8, str 16, str 32, bin 8, bin 16, bin 32\tFixedString\tbin 8, bin 16, bin 32 float 32\tFloat32\tfloat 32 float 64\tFloat64\tfloat 64 uint 16\tDate\tuint 16 uint 32\tDateTime\tuint 32 uint 64\tDateTime64\tuint 64 fixarray, array 16, array 32\tArray\tfixarray, array 16, array 32 fixmap, map 16, map 32\tMap\tfixmap, map 16, map 32 Example: Writing to a file &quot;.msgpk&quot;: $ clickhouse-client --query=&quot;CREATE TABLE msgpack (array Array(UInt8)) ENGINE = Memory;&quot; $ clickhouse-client --query=&quot;INSERT INTO msgpack VALUES ([0, 1, 2, 3, 42, 253, 254, 255]), ([255, 254, 253, 42, 3, 2, 1, 0])&quot;; $ clickhouse-client --query=&quot;SELECT * FROM msgpack FORMAT MsgPack&quot; &gt; tmp_msgpack.msgpk;  "},{"title":"Server Configuration Parameters","type":0,"sectionRef":"#","url":"docs/en/operations/server-configuration-parameters/","content":"Server Configuration Parameters This section contains descriptions of server settings that cannot be changed at the session or query level. These settings are stored in the config.xml file on the ClickHouse server. Other settings are described in the “Settings” section. Before studying the settings, read the Configuration files section and note the use of substitutions (the incl and optional attributes). Original article","keywords":""},{"title":"Settings","type":0,"sectionRef":"#","url":"docs/en/operations/settings/index","content":"","keywords":""},{"title":"Custom Settings​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/index#custom_settings","content":"In addition to the common settings, users can define custom settings. A custom setting name must begin with one of predefined prefixes. The list of these prefixes must be declared in the custom_settings_prefixes parameter in the server configuration file. &lt;custom_settings_prefixes&gt;custom_&lt;/custom_settings_prefixes&gt;  To define a custom setting use SET command: SET custom_a = 123;  To get the current value of a custom setting use getSetting() function: SELECT getSetting('custom_a');  See Also Server Configuration Settings Original article "},{"title":"Constraints on Settings","type":0,"sectionRef":"#","url":"docs/en/operations/settings/constraints-on-settings","content":"Constraints on Settings The constraints on settings can be defined in the profiles section of the user.xml configuration file and prohibit users from changing some of the settings with the SET query. The constraints are defined as the following: &lt;profiles&gt; &lt;user_name&gt; &lt;constraints&gt; &lt;setting_name_1&gt; &lt;min&gt;lower_boundary&lt;/min&gt; &lt;/setting_name_1&gt; &lt;setting_name_2&gt; &lt;max&gt;upper_boundary&lt;/max&gt; &lt;/setting_name_2&gt; &lt;setting_name_3&gt; &lt;min&gt;lower_boundary&lt;/min&gt; &lt;max&gt;upper_boundary&lt;/max&gt; &lt;/setting_name_3&gt; &lt;setting_name_4&gt; &lt;readonly/&gt; &lt;/setting_name_4&gt; &lt;/constraints&gt; &lt;/user_name&gt; &lt;/profiles&gt; If the user tries to violate the constraints an exception is thrown and the setting isn’t changed. There are supported three types of constraints: min, max, readonly. The min and max constraints specify upper and lower boundaries for a numeric setting and can be used in combination. The readonly constraint specifies that the user cannot change the corresponding setting at all. Example: Let users.xml includes lines: &lt;profiles&gt; &lt;default&gt; &lt;max_memory_usage&gt;10000000000&lt;/max_memory_usage&gt; &lt;force_index_by_date&gt;0&lt;/force_index_by_date&gt; ... &lt;constraints&gt; &lt;max_memory_usage&gt; &lt;min&gt;5000000000&lt;/min&gt; &lt;max&gt;20000000000&lt;/max&gt; &lt;/max_memory_usage&gt; &lt;force_index_by_date&gt; &lt;readonly/&gt; &lt;/force_index_by_date&gt; &lt;/constraints&gt; &lt;/default&gt; &lt;/profiles&gt; The following queries all throw exceptions: SET max_memory_usage=20000000001; SET max_memory_usage=4999999999; SET force_index_by_date=1; Code: 452, e.displayText() = DB::Exception: Setting max_memory_usage should not be greater than 20000000000. Code: 452, e.displayText() = DB::Exception: Setting max_memory_usage should not be less than 5000000000. Code: 452, e.displayText() = DB::Exception: Setting force_index_by_date should not be changed. Note: the default profile has special handling: all the constraints defined for the default profile become the default constraints, so they restrict all the users until they’re overridden explicitly for these users. Original article","keywords":""},{"title":"LDAP","type":0,"sectionRef":"#","url":"docs/en/operations/external-authenticators/ldap","content":"","keywords":""},{"title":"LDAP Server Definition​","type":1,"pageTitle":"LDAP","url":"docs/en/operations/external-authenticators/ldap#ldap-server-definition","content":"To define LDAP server you must add ldap_servers section to the config.xml. Example &lt;clickhouse&gt; &lt;!- ... --&gt; &lt;ldap_servers&gt; &lt;!- Typical LDAP server. --&gt; &lt;my_ldap_server&gt; &lt;host&gt;localhost&lt;/host&gt; &lt;port&gt;636&lt;/port&gt; &lt;bind_dn&gt;uid={user_name},ou=users,dc=example,dc=com&lt;/bind_dn&gt; &lt;verification_cooldown&gt;300&lt;/verification_cooldown&gt; &lt;enable_tls&gt;yes&lt;/enable_tls&gt; &lt;tls_minimum_protocol_version&gt;tls1.2&lt;/tls_minimum_protocol_version&gt; &lt;tls_require_cert&gt;demand&lt;/tls_require_cert&gt; &lt;tls_cert_file&gt;/path/to/tls_cert_file&lt;/tls_cert_file&gt; &lt;tls_key_file&gt;/path/to/tls_key_file&lt;/tls_key_file&gt; &lt;tls_ca_cert_file&gt;/path/to/tls_ca_cert_file&lt;/tls_ca_cert_file&gt; &lt;tls_ca_cert_dir&gt;/path/to/tls_ca_cert_dir&lt;/tls_ca_cert_dir&gt; &lt;tls_cipher_suite&gt;ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:AES256-GCM-SHA384&lt;/tls_cipher_suite&gt; &lt;/my_ldap_server&gt; &lt;!- Typical Active Directory with configured user DN detection for further role mapping. --&gt; &lt;my_ad_server&gt; &lt;host&gt;localhost&lt;/host&gt; &lt;port&gt;389&lt;/port&gt; &lt;bind_dn&gt;EXAMPLE\\{user_name}&lt;/bind_dn&gt; &lt;user_dn_detection&gt; &lt;base_dn&gt;CN=Users,DC=example,DC=com&lt;/base_dn&gt; &lt;search_filter&gt;(&amp;amp;(objectClass=user)(sAMAccountName={user_name}))&lt;/search_filter&gt; &lt;/user_dn_detection&gt; &lt;enable_tls&gt;no&lt;/enable_tls&gt; &lt;/my_ad_server&gt; &lt;/ldap_servers&gt; &lt;/clickhouse&gt;  Note, that you can define multiple LDAP servers inside the ldap_servers section using distinct names. Parameters host — LDAP server hostname or IP, this parameter is mandatory and cannot be empty.port — LDAP server port, default is 636 if enable_tls is set to true, 389 otherwise.bind_dn — Template used to construct the DN to bind to. The resulting DN will be constructed by replacing all {user_name} substrings of the template with the actual user name during each authentication attempt. user_dn_detection — Section with LDAP search parameters for detecting the actual user DN of the bound user. This is mainly used in search filters for further role mapping when the server is Active Directory. The resulting user DN will be used when replacing {user_dn} substrings wherever they are allowed. By default, user DN is set equal to bind DN, but once search is performed, it will be updated with to the actual detected user DN value. base_dn — Template used to construct the base DN for the LDAP search. The resulting DN will be constructed by replacing all {user_name} and {bind_dn} substrings of the template with the actual user name and bind DN during the LDAP search. scope — Scope of the LDAP search. Accepted values are: base, one_level, children, subtree (the default). search_filter — Template used to construct the search filter for the LDAP search. The resulting filter will be constructed by replacing all {user_name}, {bind_dn}, and {base_dn} substrings of the template with the actual user name, bind DN, and base DN during the LDAP search.Note, that the special characters must be escaped properly in XML. verification_cooldown — A period of time, in seconds, after a successful bind attempt, during which the user will be assumed to be successfully authenticated for all consecutive requests without contacting the LDAP server. Specify 0 (the default) to disable caching and force contacting the LDAP server for each authentication request. enable_tls — A flag to trigger the use of the secure connection to the LDAP server. Specify no for plain text ldap:// protocol (not recommended).Specify yes for LDAP over SSL/TLS ldaps:// protocol (recommended, the default).Specify starttls for legacy StartTLS protocol (plain text ldap:// protocol, upgraded to TLS). tls_minimum_protocol_version — The minimum protocol version of SSL/TLS. Accepted values are: ssl2, ssl3, tls1.0, tls1.1, tls1.2 (the default). tls_require_cert — SSL/TLS peer certificate verification behavior. Accepted values are: never, allow, try, demand (the default). tls_cert_file — Path to certificate file.tls_key_file — Path to certificate key file.tls_ca_cert_file — Path to CA certificate file.tls_ca_cert_dir — Path to the directory containing CA certificates.tls_cipher_suite — Allowed cipher suite (in OpenSSL notation). "},{"title":"LDAP External Authenticator​","type":1,"pageTitle":"LDAP","url":"docs/en/operations/external-authenticators/ldap#ldap-external-authenticator","content":"A remote LDAP server can be used as a method for verifying passwords for locally defined users (users defined in users.xml or in local access control paths). To achieve this, specify previously defined LDAP server name instead of password or similar sections in the user definition. At each login attempt, ClickHouse tries to &quot;bind&quot; to the specified DN defined by the bind_dn parameter in the LDAP server definition using the provided credentials, and if successful, the user is considered authenticated. This is often called a &quot;simple bind&quot; method. Example &lt;clickhouse&gt; &lt;!- ... --&gt; &lt;users&gt; &lt;!- ... --&gt; &lt;my_user&gt; &lt;!- ... --&gt; &lt;ldap&gt; &lt;server&gt;my_ldap_server&lt;/server&gt; &lt;/ldap&gt; &lt;/my_user&gt; &lt;/users&gt; &lt;/clickhouse&gt;  Note, that user my_user refers to my_ldap_server. This LDAP server must be configured in the main config.xml file as described previously. When SQL-driven Access Control and Account Management is enabled, users that are authenticated by LDAP servers can also be created using the CREATE USER statement. Query: CREATE USER my_user IDENTIFIED WITH ldap SERVER 'my_ldap_server';  "},{"title":"LDAP Exernal User Directory​","type":1,"pageTitle":"LDAP","url":"docs/en/operations/external-authenticators/ldap#ldap-external-user-directory","content":"In addition to the locally defined users, a remote LDAP server can be used as a source of user definitions. To achieve this, specify previously defined LDAP server name (see LDAP Server Definition) in the ldap section inside the users_directories section of the config.xml file. At each login attempt, ClickHouse tries to find the user definition locally and authenticate it as usual. If the user is not defined, ClickHouse will assume the definition exists in the external LDAP directory and will try to &quot;bind&quot; to the specified DN at the LDAP server using the provided credentials. If successful, the user will be considered existing and authenticated. The user will be assigned roles from the list specified in the roles section. Additionally, LDAP &quot;search&quot; can be performed and results can be transformed and treated as role names and then be assigned to the user if the role_mapping section is also configured. All this implies that the SQL-driven Access Control and Account Management is enabled and roles are created using the CREATE ROLE statement. Example Goes into config.xml. &lt;clickhouse&gt; &lt;!- ... --&gt; &lt;user_directories&gt; &lt;!- Typical LDAP server. --&gt; &lt;ldap&gt; &lt;server&gt;my_ldap_server&lt;/server&gt; &lt;roles&gt; &lt;my_local_role1 /&gt; &lt;my_local_role2 /&gt; &lt;/roles&gt; &lt;role_mapping&gt; &lt;base_dn&gt;ou=groups,dc=example,dc=com&lt;/base_dn&gt; &lt;scope&gt;subtree&lt;/scope&gt; &lt;search_filter&gt;(&amp;amp;(objectClass=groupOfNames)(member={bind_dn}))&lt;/search_filter&gt; &lt;attribute&gt;cn&lt;/attribute&gt; &lt;prefix&gt;clickhouse_&lt;/prefix&gt; &lt;/role_mapping&gt; &lt;/ldap&gt; &lt;!- Typical Active Directory with role mapping that relies on the detected user DN. --&gt; &lt;ldap&gt; &lt;server&gt;my_ad_server&lt;/server&gt; &lt;role_mapping&gt; &lt;base_dn&gt;CN=Users,DC=example,DC=com&lt;/base_dn&gt; &lt;attribute&gt;CN&lt;/attribute&gt; &lt;scope&gt;subtree&lt;/scope&gt; &lt;search_filter&gt;(&amp;amp;(objectClass=group)(member={user_dn}))&lt;/search_filter&gt; &lt;prefix&gt;clickhouse_&lt;/prefix&gt; &lt;/role_mapping&gt; &lt;/ldap&gt; &lt;/user_directories&gt; &lt;/clickhouse&gt;  Note that my_ldap_server referred in the ldap section inside the user_directories section must be a previously defined LDAP server that is configured in the config.xml (see LDAP Server Definition). Parameters server — One of LDAP server names defined in the ldap_servers config section above. This parameter is mandatory and cannot be empty.roles — Section with a list of locally defined roles that will be assigned to each user retrieved from the LDAP server. If no roles are specified here or assigned during role mapping (below), user will not be able to perform any actions after authentication. role_mapping — Section with LDAP search parameters and mapping rules. When a user authenticates, while still bound to LDAP, an LDAP search is performed using search_filter and the name of the logged-in user. For each entry found during that search, the value of the specified attribute is extracted. For each attribute value that has the specified prefix, the prefix is removed, and the rest of the value becomes the name of a local role defined in ClickHouse, which is expected to be created beforehand by the CREATE ROLE statement.There can be multiple role_mapping sections defined inside the same ldap section. All of them will be applied. base_dn — Template used to construct the base DN for the LDAP search. The resulting DN will be constructed by replacing all {user_name}, {bind_dn}, and {user_dn} substrings of the template with the actual user name, bind DN, and user DN during each LDAP search. scope — Scope of the LDAP search. Accepted values are: base, one_level, children, subtree (the default). search_filter — Template used to construct the search filter for the LDAP search. The resulting filter will be constructed by replacing all {user_name}, {bind_dn}, {user_dn}, and {base_dn} substrings of the template with the actual user name, bind DN, user DN, and base DN during each LDAP search.Note, that the special characters must be escaped properly in XML. attribute — Attribute name whose values will be returned by the LDAP search. cn, by default.prefix — Prefix, that will be expected to be in front of each string in the original list of strings returned by the LDAP search. The prefix will be removed from the original strings and the resulting strings will be treated as local role names. Empty by default. Original article "},{"title":"Settings Profiles","type":0,"sectionRef":"#","url":"docs/en/operations/settings/settings-profiles","content":"Settings Profiles A settings profile is a collection of settings grouped under the same name. note ClickHouse also supports SQL-driven workflow for managing settings profiles. We recommend using it. The profile can have any name. You can specify the same profile for different users. The most important thing you can write in the settings profile is readonly=1, which ensures read-only access. Settings profiles can inherit from each other. To use inheritance, indicate one or multiple profile settings before the other settings that are listed in the profile. In case when one setting is defined in different profiles, the latest defined is used. To apply all the settings in a profile, set the profile setting. Example: Install the web profile. SET profile = 'web' Settings profiles are declared in the user config file. This is usually users.xml. Example: &lt;!-- Settings profiles --&gt; &lt;profiles&gt; &lt;!-- Default settings --&gt; &lt;default&gt; &lt;!-- The maximum number of threads when running a single query. --&gt; &lt;max_threads&gt;8&lt;/max_threads&gt; &lt;/default&gt; &lt;!-- Settings for quries from the user interface --&gt; &lt;web&gt; &lt;max_rows_to_read&gt;1000000000&lt;/max_rows_to_read&gt; &lt;max_bytes_to_read&gt;100000000000&lt;/max_bytes_to_read&gt; &lt;max_rows_to_group_by&gt;1000000&lt;/max_rows_to_group_by&gt; &lt;group_by_overflow_mode&gt;any&lt;/group_by_overflow_mode&gt; &lt;max_rows_to_sort&gt;1000000&lt;/max_rows_to_sort&gt; &lt;max_bytes_to_sort&gt;1000000000&lt;/max_bytes_to_sort&gt; &lt;max_result_rows&gt;100000&lt;/max_result_rows&gt; &lt;max_result_bytes&gt;100000000&lt;/max_result_bytes&gt; &lt;result_overflow_mode&gt;break&lt;/result_overflow_mode&gt; &lt;max_execution_time&gt;600&lt;/max_execution_time&gt; &lt;min_execution_speed&gt;1000000&lt;/min_execution_speed&gt; &lt;timeout_before_checking_execution_speed&gt;15&lt;/timeout_before_checking_execution_speed&gt; &lt;max_columns_to_read&gt;25&lt;/max_columns_to_read&gt; &lt;max_temporary_columns&gt;100&lt;/max_temporary_columns&gt; &lt;max_temporary_non_const_columns&gt;50&lt;/max_temporary_non_const_columns&gt; &lt;max_subquery_depth&gt;2&lt;/max_subquery_depth&gt; &lt;max_pipeline_depth&gt;25&lt;/max_pipeline_depth&gt; &lt;max_ast_depth&gt;50&lt;/max_ast_depth&gt; &lt;max_ast_elements&gt;100&lt;/max_ast_elements&gt; &lt;readonly&gt;1&lt;/readonly&gt; &lt;/web&gt; &lt;/profiles&gt; The example specifies two profiles: default and web. The default profile has a special purpose: it must always be present and is applied when starting the server. In other words, the default profile contains default settings. The web profile is a regular profile that can be set using the SET query or using a URL parameter in an HTTP query. Original article","keywords":""},{"title":"User Settings","type":0,"sectionRef":"#","url":"docs/en/operations/settings/settings-users","content":"","keywords":""},{"title":"user_name/password​","type":1,"pageTitle":"User Settings","url":"docs/en/operations/settings/settings-users#user-namepassword","content":"Password can be specified in plaintext or in SHA256 (hex format). To assign a password in plaintext (not recommended), place it in a password element. For example, &lt;password&gt;qwerty&lt;/password&gt;. The password can be left blank.  To assign a password using its SHA256 hash, place it in a password_sha256_hex element. For example, &lt;password_sha256_hex&gt;65e84be33532fb784c48129675f9eff3a682b27168c0ea744b2cf58ee02337c5&lt;/password_sha256_hex&gt;. Example of how to generate a password from shell: PASSWORD=$(base64 &lt; /dev/urandom | head -c8); echo &quot;$PASSWORD&quot;; echo -n &quot;$PASSWORD&quot; | sha256sum | tr -d '-' The first line of the result is the password. The second line is the corresponding SHA256 hash.  For compatibility with MySQL clients, password can be specified in double SHA1 hash. Place it in password_double_sha1_hex element. For example, &lt;password_double_sha1_hex&gt;08b4a0f1de6ad37da17359e592c8d74788a83eb0&lt;/password_double_sha1_hex&gt;. Example of how to generate a password from shell: PASSWORD=$(base64 &lt; /dev/urandom | head -c8); echo &quot;$PASSWORD&quot;; echo -n &quot;$PASSWORD&quot; | sha1sum | tr -d '-' | xxd -r -p | sha1sum | tr -d '-' The first line of the result is the password. The second line is the corresponding double SHA1 hash. "},{"title":"access_management​","type":1,"pageTitle":"User Settings","url":"docs/en/operations/settings/settings-users#access_management-user-setting","content":"This setting enables or disables using of SQL-driven access control and account management for the user. Possible values: 0 — Disabled.1 — Enabled. Default value: 0. "},{"title":"user_name/networks​","type":1,"pageTitle":"User Settings","url":"docs/en/operations/settings/settings-users#user-namenetworks","content":"List of networks from which the user can connect to the ClickHouse server. Each element of the list can have one of the following forms: &lt;ip&gt; — IP address or network mask. Examples: 213.180.204.3, 10.0.0.1/8, 10.0.0.1/255.255.255.0, 2a02:6b8::3, 2a02:6b8::3/64, 2a02:6b8::3/ffff:ffff:ffff:ffff::. &lt;host&gt; — Hostname. Example: example01.host.ru. To check access, a DNS query is performed, and all returned IP addresses are compared to the peer address. &lt;host_regexp&gt; — Regular expression for hostnames. Example, ^example\\d\\d-\\d\\d-\\d\\.host\\.ru$ To check access, a DNS PTR query is performed for the peer address and then the specified regexp is applied. Then, another DNS query is performed for the results of the PTR query and all the received addresses are compared to the peer address. We strongly recommend that regexp ends with $. All results of DNS requests are cached until the server restarts. Examples To open access for user from any network, specify: &lt;ip&gt;::/0&lt;/ip&gt;  warning It’s insecure to open access from any network unless you have a firewall properly configured or the server is not directly connected to Internet. To open access only from localhost, specify: &lt;ip&gt;::1&lt;/ip&gt; &lt;ip&gt;127.0.0.1&lt;/ip&gt;  "},{"title":"user_name/profile​","type":1,"pageTitle":"User Settings","url":"docs/en/operations/settings/settings-users#user-nameprofile","content":"You can assign a settings profile for the user. Settings profiles are configured in a separate section of the users.xml file. For more information, see Profiles of Settings. "},{"title":"user_name/quota​","type":1,"pageTitle":"User Settings","url":"docs/en/operations/settings/settings-users#user-namequota","content":"Quotas allow you to track or limit resource usage over a period of time. Quotas are configured in the quotassection of the users.xml configuration file. You can assign a quotas set for the user. For a detailed description of quotas configuration, see Quotas. "},{"title":"user_name/databases​","type":1,"pageTitle":"User Settings","url":"docs/en/operations/settings/settings-users#user-namedatabases","content":"In this section, you can limit rows that are returned by ClickHouse for SELECT queries made by the current user, thus implementing basic row-level security. Example The following configuration forces that user user1 can only see the rows of table1 as the result of SELECT queries, where the value of the id field is 1000. &lt;user1&gt; &lt;databases&gt; &lt;database_name&gt; &lt;table1&gt; &lt;filter&gt;id = 1000&lt;/filter&gt; &lt;/table1&gt; &lt;/database_name&gt; &lt;/databases&gt; &lt;/user1&gt;  The filter can be any expression resulting in a UInt8-type value. It usually contains comparisons and logical operators. Rows from database_name.table1 where filter results to 0 are not returned for this user. The filtering is incompatible with PREWHERE operations and disables WHERE→PREWHERE optimization. Original article "},{"title":"MergeTree tables settings","type":0,"sectionRef":"#","url":"docs/en/operations/settings/merge-tree-settings","content":"","keywords":""},{"title":"parts_to_throw_insert​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#parts-to-throw-insert","content":"If the number of active parts in a single partition exceeds the parts_to_throw_insert value, INSERT is interrupted with the Too many parts (N). Merges are processing significantly slower than inserts exception. Possible values: Any positive integer. Default value: 300. To achieve maximum performance of SELECT queries, it is necessary to minimize the number of parts processed, see Merge Tree. You can set a larger value to 600 (1200), this will reduce the probability of the Too many parts error, but at the same time SELECT performance might degrade. Also in case of a merge issue (for example, due to insufficient disk space) you will notice it later than it could be with the original 300. "},{"title":"parts_to_delay_insert​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#parts-to-delay-insert","content":"If the number of active parts in a single partition exceeds the parts_to_delay_insert value, an INSERT artificially slows down. Possible values: Any positive integer. Default value: 150. ClickHouse artificially executes INSERT longer (adds ‘sleep’) so that the background merge process can merge parts faster than they are added. "},{"title":"inactive_parts_to_throw_insert​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#inactive-parts-to-throw-insert","content":"If the number of inactive parts in a single partition more than the inactive_parts_to_throw_insert value, INSERT is interrupted with the &quot;Too many inactive parts (N). Parts cleaning are processing significantly slower than inserts&quot; exception. Possible values: Any positive integer. Default value: 0 (unlimited). "},{"title":"inactive_parts_to_delay_insert​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#inactive-parts-to-delay-insert","content":"If the number of inactive parts in a single partition in the table at least that many the inactive_parts_to_delay_insert value, an INSERT artificially slows down. It is useful when a server fails to clean up parts quickly enough. Possible values: Any positive integer. Default value: 0 (unlimited). "},{"title":"max_delay_to_insert​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#max-delay-to-insert","content":"The value in seconds, which is used to calculate the INSERT delay, if the number of active parts in a single partition exceeds the parts_to_delay_insert value. Possible values: Any positive integer. Default value: 1. The delay (in milliseconds) for INSERT is calculated by the formula: max_k = parts_to_throw_insert - parts_to_delay_insert k = 1 + parts_count_in_partition - parts_to_delay_insert delay_milliseconds = pow(max_delay_to_insert * 1000, k / max_k)  For example if a partition has 299 active parts and parts_to_throw_insert = 300, parts_to_delay_insert = 150, max_delay_to_insert = 1, INSERT is delayed for pow( 1 * 1000, (1 + 299 - 150) / (300 - 150) ) = 1000 milliseconds. "},{"title":"max_parts_in_total​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#max-parts-in-total","content":"If the total number of active parts in all partitions of a table exceeds the max_parts_in_total value INSERT is interrupted with the Too many parts (N) exception. Possible values: Any positive integer. Default value: 100000. A large number of parts in a table reduces performance of ClickHouse queries and increases ClickHouse boot time. Most often this is a consequence of an incorrect design (mistakes when choosing a partitioning strategy - too small partitions). "},{"title":"replicated_deduplication_window​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#replicated-deduplication-window","content":"The number of most recently inserted blocks for which Zookeeper stores hash sums to check for duplicates. Possible values: Any positive integer.0 (disable deduplication) Default value: 100. The Insert command creates one or more blocks (parts). When inserting into Replicated tables, ClickHouse for insert deduplication writes the hash sums of the created parts into Zookeeper. Hash sums are stored only for the most recent replicated_deduplication_window blocks. The oldest hash sums are removed from Zookeeper. A large number of replicated_deduplication_window slows down Inserts because it needs to compare more entries. The hash sum is calculated from the composition of the field names and types and the data of the inserted part (stream of bytes). "},{"title":"non_replicated_deduplication_window​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#non-replicated-deduplication-window","content":"The number of the most recently inserted blocks in the non-replicated MergeTree table for which hash sums are stored to check for duplicates. Possible values: Any positive integer.0 (disable deduplication). Default value: 0. A deduplication mechanism is used, similar to replicated tables (see replicated_deduplication_window setting). The hash sums of the created parts are written to a local file on a disk. "},{"title":"replicated_deduplication_window_seconds​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#replicated-deduplication-window-seconds","content":"The number of seconds after which the hash sums of the inserted blocks are removed from Zookeeper. Possible values: Any positive integer. Default value: 604800 (1 week). Similar to replicated_deduplication_window, replicated_deduplication_window_seconds specifies how long to store hash sums of blocks for insert deduplication. Hash sums older than replicated_deduplication_window_seconds are removed from Zookeeper, even if they are less than replicated_deduplication_window. "},{"title":"replicated_fetches_http_connection_timeout​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#replicated_fetches_http_connection_timeout","content":"HTTP connection timeout (in seconds) for part fetch requests. Inherited from default profile http_connection_timeout if not set explicitly. Possible values: Any positive integer.0 - Use value of http_connection_timeout. Default value: 0. "},{"title":"replicated_fetches_http_send_timeout​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#replicated_fetches_http_send_timeout","content":"HTTP send timeout (in seconds) for part fetch requests. Inherited from default profile http_send_timeout if not set explicitly. Possible values: Any positive integer.0 - Use value of http_send_timeout. Default value: 0. "},{"title":"replicated_fetches_http_receive_timeout​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#replicated_fetches_http_receive_timeout","content":"HTTP receive timeout (in seconds) for fetch part requests. Inherited from default profile http_receive_timeout if not set explicitly. Possible values: Any positive integer.0 - Use value of http_receive_timeout. Default value: 0. "},{"title":"max_replicated_fetches_network_bandwidth​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#max_replicated_fetches_network_bandwidth","content":"Limits the maximum speed of data exchange over the network in bytes per second for replicated fetches. This setting is applied to a particular table, unlike the max_replicated_fetches_network_bandwidth_for_server setting, which is applied to the server. You can limit both server network and network for a particular table, but for this the value of the table-level setting should be less than server-level one. Otherwise the server considers only the max_replicated_fetches_network_bandwidth_for_server setting. The setting isn't followed perfectly accurately. Possible values: Positive integer.0 — Unlimited. Default value: 0. Usage Could be used for throttling speed when replicating data to add or replace new nodes. "},{"title":"max_replicated_sends_network_bandwidth​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#max_replicated_sends_network_bandwidth","content":"Limits the maximum speed of data exchange over the network in bytes per second for replicated sends. This setting is applied to a particular table, unlike the max_replicated_sends_network_bandwidth_for_server setting, which is applied to the server. You can limit both server network and network for a particular table, but for this the value of the table-level setting should be less than server-level one. Otherwise the server considers only the max_replicated_sends_network_bandwidth_for_server setting. The setting isn't followed perfectly accurately. Possible values: Positive integer.0 — Unlimited. Default value: 0. Usage Could be used for throttling speed when replicating data to add or replace new nodes. "},{"title":"old_parts_lifetime​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#old-parts-lifetime","content":"The time (in seconds) of storing inactive parts to protect against data loss during spontaneous server reboots. Possible values: Any positive integer. Default value: 480. After merging several parts into a new part, ClickHouse marks the original parts as inactive and deletes them only after old_parts_lifetime seconds. Inactive parts are removed if they are not used by current queries, i.e. if the refcount of the part is zero. fsync is not called for new parts, so for some time new parts exist only in the server's RAM (OS cache). If the server is rebooted spontaneously, new parts can be lost or damaged. To protect data inactive parts are not deleted immediately. During startup ClickHouse checks the integrity of the parts. If the merged part is damaged ClickHouse returns the inactive parts to the active list, and later merges them again. Then the damaged part is renamed (the broken_ prefix is added) and moved to the detached folder. If the merged part is not damaged, then the original inactive parts are renamed (the ignored_ prefix is added) and moved to the detached folder. The default dirty_expire_centisecs value (a Linux kernel setting) is 30 seconds (the maximum time that written data is stored only in RAM), but under heavy loads on the disk system data can be written much later. Experimentally, a value of 480 seconds was chosen for old_parts_lifetime, during which a new part is guaranteed to be written to disk. "},{"title":"max_bytes_to_merge_at_max_space_in_pool​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#max-bytes-to-merge-at-max-space-in-pool","content":"The maximum total parts size (in bytes) to be merged into one part, if there are enough resources available.max_bytes_to_merge_at_max_space_in_pool -- roughly corresponds to the maximum possible part size created by an automatic background merge. Possible values: Any positive integer. Default value: 161061273600 (150 GB). The merge scheduler periodically analyzes the sizes and number of parts in partitions, and if there is enough free resources in the pool, it starts background merges. Merges occur until the total size of the source parts is larger than max_bytes_to_merge_at_max_space_in_pool. Merges initiated by OPTIMIZE FINAL ignore max_bytes_to_merge_at_max_space_in_pool and merge parts only taking into account available resources (free disk's space) until one part remains in the partition. "},{"title":"max_bytes_to_merge_at_min_space_in_pool​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#max-bytes-to-merge-at-min-space-in-pool","content":"The maximum total part size (in bytes) to be merged into one part, with the minimum available resources in the background pool. Possible values: Any positive integer. Default value: 1048576 (1 MB) max_bytes_to_merge_at_min_space_in_pool defines the maximum total size of parts which can be merged despite the lack of available disk space (in pool). This is necessary to reduce the number of small parts and the chance of Too many parts errors. Merges book disk space by doubling the total merged parts sizes. Thus, with a small amount of free disk space, a situation may happen that there is free space, but this space is already booked by ongoing large merges, so other merges unable to start, and the number of small parts grows with every insert. "},{"title":"merge_max_block_size​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#merge-max-block-size","content":"The number of rows that are read from the merged parts into memory. Possible values: Any positive integer. Default value: 8192 Merge reads rows from parts in blocks of merge_max_block_size rows, then merges and writes the result into a new part. The read block is placed in RAM, so merge_max_block_size affects the size of the RAM required for the merge. Thus, merges can consume a large amount of RAM for tables with very wide rows (if the average row size is 100kb, then when merging 10 parts, (100kb 10 8192) = ~ 8GB of RAM). By decreasing merge_max_block_size, you can reduce the amount of RAM required for a merge but slow down a merge. "},{"title":"max_part_loading_threads​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#max-part-loading-threads","content":"The maximum number of threads that read parts when ClickHouse starts. Possible values: Any positive integer. Default value: auto (number of CPU cores). During startup ClickHouse reads all parts of all tables (reads files with metadata of parts) to build a list of all parts in memory. In some systems with a large number of parts this process can take a long time, and this time might be shortened by increasing max_part_loading_threads (if this process is not CPU and disk I/O bound). "},{"title":"max_partitions_to_read​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#max-partitions-to-read","content":"Limits the maximum number of partitions that can be accessed in one query. The setting value specified when the table is created can be overridden via query-level setting. Possible values: Any positive integer. Default value: -1 (unlimited). "},{"title":"allow_floating_point_partition_key​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#allow_floating_point_partition_key","content":"Enables to allow floating-point number as a partition key. Possible values: 0 — Floating-point partition key not allowed.1 — Floating-point partition key allowed. Default value: 0. "},{"title":"check_sample_column_is_correct​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#check_sample_column_is_correct","content":"Enables the check at table creation, that the data type of a column for sampling or sampling expression is correct. The data type must be one of unsigned integer types: UInt8, UInt16, UInt32, UInt64. Possible values: true — The check is enabled.false — The check is disabled at table creation. Default value: true. By default, the ClickHouse server checks at table creation the data type of a column for sampling or sampling expression. If you already have tables with incorrect sampling expression and do not want the server to raise an exception during startup, set check_sample_column_is_correct to false. "},{"title":"min_bytes_to_rebalance_partition_over_jbod​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#min-bytes-to-rebalance-partition-over-jbod","content":"Sets minimal amount of bytes to enable balancing when distributing new big parts over volume disks JBOD. Possible values: Positive integer.0 — Balancing is disabled. Default value: 0. Usage The value of the min_bytes_to_rebalance_partition_over_jbod setting should not be less than the value of the max_bytes_to_merge_at_max_space_in_pool / 1024. Otherwise, ClickHouse throws an exception. "},{"title":"detach_not_byte_identical_parts​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#detach_not_byte_identical_parts","content":"Enables or disables detaching a data part on a replica after a merge or a mutation, if it is not byte-identical to data parts on other replicas. If disabled, the data part is removed. Activate this setting if you want to analyze such parts later. The setting is applicable to MergeTree tables with enabled data replication. Possible values: 0 — Parts are removed.1 — Parts are detached. Default value: 0. "},{"title":"merge_tree_clear_old_temporary_directories_interval_seconds​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#setting-merge-tree-clear-old-temporary-directories-interval-seconds","content":"Sets the interval in seconds for ClickHouse to execute the cleanup of old temporary directories. Possible values: Any positive integer. Default value: 60 seconds. "},{"title":"merge_tree_clear_old_parts_interval_seconds​","type":1,"pageTitle":"MergeTree tables settings","url":"docs/en/operations/settings/merge-tree-settings#setting-merge-tree-clear-old-parts-interval-seconds","content":"Sets the interval in seconds for ClickHouse to execute the cleanup of old parts, WALs, and mutations. Possible values: Any positive integer. Default value: 1 second. "},{"title":"Optional secured communication between ClickHouse and Zookeeper","type":0,"sectionRef":"#","url":"docs/en/operations/ssl-zookeeper","content":"Optional secured communication between ClickHouse and Zookeeper You should specify ssl.keyStore.location, ssl.keyStore.password and ssl.trustStore.location, ssl.trustStore.password for communication with ClickHouse client over SSL. These options are available from Zookeeper version 3.5.2. You can add zookeeper.crt to trusted certificates. sudo cp zookeeper.crt /usr/local/share/ca-certificates/zookeeper.crt sudo update-ca-certificates Client section in config.xml will look like: &lt;client&gt; &lt;certificateFile&gt;/etc/clickhouse-server/client.crt&lt;/certificateFile&gt; &lt;privateKeyFile&gt;/etc/clickhouse-server/client.key&lt;/privateKeyFile&gt; &lt;loadDefaultCAFile&gt;true&lt;/loadDefaultCAFile&gt; &lt;cacheSessions&gt;true&lt;/cacheSessions&gt; &lt;disableProtocols&gt;sslv2,sslv3&lt;/disableProtocols&gt; &lt;preferServerCiphers&gt;true&lt;/preferServerCiphers&gt; &lt;invalidCertificateHandler&gt; &lt;name&gt;RejectCertificateHandler&lt;/name&gt; &lt;/invalidCertificateHandler&gt; &lt;/client&gt; Add Zookeeper to ClickHouse config with some cluster and macros: &lt;clickhouse&gt; &lt;zookeeper&gt; &lt;node&gt; &lt;host&gt;localhost&lt;/host&gt; &lt;port&gt;2281&lt;/port&gt; &lt;secure&gt;1&lt;/secure&gt; &lt;/node&gt; &lt;/zookeeper&gt; &lt;/clickhouse&gt; Start clickhouse-server. In logs you should see: &lt;Trace&gt; ZooKeeper: initialized, hosts: secure://localhost:2281 Prefix secure:// indicates that connection is secured by SSL. To ensure traffic is encrypted run tcpdump on secured port: tcpdump -i any dst port 2281 -nnXS And query in clickhouse-client: SELECT * FROM system.zookeeper WHERE path = '/'; On unencrypted connection you will see in tcpdump output something like this: ..../zookeeper/quota. On encrypted connection you should not see this.","keywords":""},{"title":"System Tables","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/","content":"","keywords":""},{"title":"Introduction​","type":1,"pageTitle":"System Tables","url":"docs/en/operations/system-tables/#system-tables-introduction","content":"System tables provide information about: Server states, processes, and environment.Server’s internal processes. System tables: Located in the system database.Available only for reading data.Can’t be dropped or altered, but can be detached. Most of system tables store their data in RAM. A ClickHouse server creates such system tables at the start. Unlike other system tables, the system log tables metric_log, query_log, query_thread_log, trace_log, part_log, crash_log and text_log are served by MergeTree table engine and store their data in a filesystem by default. If you remove a table from a filesystem, the ClickHouse server creates the empty one again at the time of the next data writing. If system table schema changed in a new release, then ClickHouse renames the current table and creates a new one. System log tables can be customized by creating a config file with the same name as the table under /etc/clickhouse-server/config.d/, or setting corresponding elements in /etc/clickhouse-server/config.xml. Elements can be customized are: database: database the system log table belongs to. This option is deprecated now. All system log tables are under database system.table: table to insert data.partition_by: specify PARTITION BY expression.ttl: specify table TTL expression.flush_interval_milliseconds: interval of flushing data to disk.engine: provide full engine expression (starting with ENGINE = ) with parameters. This option is contradict with partition_by and ttl. If set together, the server would raise an exception and exit. An example: &lt;clickhouse&gt; &lt;query_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;query_log&lt;/table&gt; &lt;partition_by&gt;toYYYYMM(event_date)&lt;/partition_by&gt; &lt;ttl&gt;event_date + INTERVAL 30 DAY DELETE&lt;/ttl&gt; &lt;!-- &lt;engine&gt;ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_date, event_time) SETTINGS index_granularity = 1024&lt;/engine&gt; --&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;/query_log&gt; &lt;/clickhouse&gt;  By default, table growth is unlimited. To control a size of a table, you can use TTL settings for removing outdated log records. Also you can use the partitioning feature of MergeTree-engine tables. "},{"title":"Sources of System Metrics​","type":1,"pageTitle":"System Tables","url":"docs/en/operations/system-tables/#system-tables-sources-of-system-metrics","content":"For collecting system metrics ClickHouse server uses: CAP_NET_ADMIN capability.procfs (only in Linux). procfs If ClickHouse server does not have CAP_NET_ADMIN capability, it tries to fall back to ProcfsMetricsProvider. ProcfsMetricsProvider allows collecting per-query system metrics (for CPU and I/O). If procfs is supported and enabled on the system, ClickHouse server collects these metrics: OSCPUVirtualTimeMicrosecondsOSCPUWaitMicrosecondsOSIOWaitMicrosecondsOSReadCharsOSWriteCharsOSReadBytesOSWriteBytes Original article "},{"title":"External Disks for Storing Data","type":0,"sectionRef":"#","url":"docs/en/operations/storing-data","content":"","keywords":""},{"title":"Zero-copy Replication​","type":1,"pageTitle":"External Disks for Storing Data","url":"docs/en/operations/storing-data#zero-copy","content":"ClickHouse supports zero-copy replication for S3 and HDFS disks, which means that if the data is stored remotely on several machines and needs to be synchronized, then only the metadata is replicated (paths to the data parts), but not the data itself. "},{"title":"Configuring HDFS​","type":1,"pageTitle":"External Disks for Storing Data","url":"docs/en/operations/storing-data#configuring-hdfs","content":"MergeTree and Log family table engines can store data to HDFS using a disk with type HDFS. Configuration markup: &lt;clickhouse&gt; &lt;storage_configuration&gt; &lt;disks&gt; &lt;hdfs&gt; &lt;type&gt;hdfs&lt;/type&gt; &lt;endpoint&gt;hdfs://hdfs1:9000/clickhouse/&lt;/endpoint&gt; &lt;/hdfs&gt; &lt;/disks&gt; &lt;policies&gt; &lt;hdfs&gt; &lt;volumes&gt; &lt;main&gt; &lt;disk&gt;hdfs&lt;/disk&gt; &lt;/main&gt; &lt;/volumes&gt; &lt;/hdfs&gt; &lt;/policies&gt; &lt;/storage_configuration&gt; &lt;merge_tree&gt; &lt;min_bytes_for_wide_part&gt;0&lt;/min_bytes_for_wide_part&gt; &lt;/merge_tree&gt; &lt;/clickhouse&gt;  Required parameters: endpoint — HDFS endpoint URL in path format. Endpoint URL should contain a root path to store data. Optional parameters: min_bytes_for_seek — The minimal number of bytes to use seek operation instead of sequential read. Default value: 1 Mb. "},{"title":"Using Virtual File System for Data Encryption​","type":1,"pageTitle":"External Disks for Storing Data","url":"docs/en/operations/storing-data#encrypted-virtual-file-system","content":"You can encrypt the data stored on S3, or HDFS external disks, or on a local disk. To turn on the encryption mode, in the configuration file you must define a disk with the type encrypted and choose a disk on which the data will be saved. An encrypted disk ciphers all written files on the fly, and when you read files from an encrypted disk it deciphers them automatically. So you can work with an encrypted disk like with a normal one. Example of disk configuration: &lt;disks&gt; &lt;disk1&gt; &lt;type&gt;local&lt;/type&gt; &lt;path&gt;/path1/&lt;/path&gt; &lt;/disk1&gt; &lt;disk2&gt; &lt;type&gt;encrypted&lt;/type&gt; &lt;disk&gt;disk1&lt;/disk&gt; &lt;path&gt;path2/&lt;/path&gt; &lt;key&gt;_16_ascii_chars_&lt;/key&gt; &lt;/disk2&gt; &lt;/disks&gt;  For example, when ClickHouse writes data from some table to a file store/all_1_1_0/data.bin to disk1, then in fact this file will be written to the physical disk along the path /path1/store/all_1_1_0/data.bin. When writing the same file to disk2, it will actually be written to the physical disk at the path /path1/path2/store/all_1_1_0/data.bin in encrypted mode. Required parameters: type — encrypted. Otherwise the encrypted disk is not created.disk — Type of disk for data storage.key — The key for encryption and decryption. Type: Uint64. You can use key_hex parameter to encrypt in hexadecimal form. You can specify multiple keys using the id attribute (see example above). Optional parameters: path — Path to the location on the disk where the data will be saved. If not specified, the data will be saved in the root directory.current_key_id — The key used for encryption. All the specified keys can be used for decryption, and you can always switch to another key while maintaining access to previously encrypted data.algorithm — Algorithm for encryption. Possible values: AES_128_CTR, AES_192_CTR or AES_256_CTR. Default value: AES_128_CTR. The key length depends on the algorithm: AES_128_CTR — 16 bytes, AES_192_CTR — 24 bytes, AES_256_CTR — 32 bytes. Example of disk configuration: &lt;clickhouse&gt; &lt;storage_configuration&gt; &lt;disks&gt; &lt;disk_s3&gt; &lt;type&gt;s3&lt;/type&gt; &lt;endpoint&gt;... &lt;/disk_s3&gt; &lt;disk_s3_encrypted&gt; &lt;type&gt;encrypted&lt;/type&gt; &lt;disk&gt;disk_s3&lt;/disk&gt; &lt;algorithm&gt;AES_128_CTR&lt;/algorithm&gt; &lt;key_hex id=&quot;0&quot;&gt;00112233445566778899aabbccddeeff&lt;/key_hex&gt; &lt;key_hex id=&quot;1&quot;&gt;ffeeddccbbaa99887766554433221100&lt;/key_hex&gt; &lt;current_key_id&gt;1&lt;/current_key_id&gt; &lt;/disk_s3_encrypted&gt; &lt;/disks&gt; &lt;/storage_configuration&gt; &lt;/clickhouse&gt;  "},{"title":"Storing Data on Web Server​","type":1,"pageTitle":"External Disks for Storing Data","url":"docs/en/operations/storing-data#storing-data-on-webserver","content":"There is a tool clickhouse-static-files-uploader, which prepares a data directory for a given table (SELECT data_paths FROM system.tables WHERE name = 'table_name'). For each table you need, you get a directory of files. These files can be uploaded to, for example, a web server with static files. After this preparation, you can load this table into any ClickHouse server via DiskWeb. This is a read-only disk. Its data is only read and never modified. A new table is loaded to this disk via ATTACH TABLE query (see example below). Local disk is not actually used, each SELECT query will result in a http request to fetch required data. All modification of the table data will result in an exception, i.e. the following types of queries are not allowed: CREATE TABLE, ALTER TABLE, RENAME TABLE, DETACH TABLE and TRUNCATE TABLE. Web server storage is supported only for the MergeTree and Log engine families. To access the data stored on a web disk, use the storage_policy setting when executing the query. For example, ATTACH TABLE table_web UUID '{}' (id Int32) ENGINE = MergeTree() ORDER BY id SETTINGS storage_policy = 'web'. A ready test case. You need to add this configuration to config: &lt;clickhouse&gt; &lt;storage_configuration&gt; &lt;disks&gt; &lt;web&gt; &lt;type&gt;web&lt;/type&gt; &lt;endpoint&gt;https://clickhouse-datasets.s3.yandex.net/disk-with-static-files-tests/test-hits/&lt;/endpoint&gt; &lt;/web&gt; &lt;/disks&gt; &lt;policies&gt; &lt;web&gt; &lt;volumes&gt; &lt;main&gt; &lt;disk&gt;web&lt;/disk&gt; &lt;/main&gt; &lt;/volumes&gt; &lt;/web&gt; &lt;/policies&gt; &lt;/storage_configuration&gt; &lt;/clickhouse&gt;  And then execute this query: ATTACH TABLE test_hits UUID '1ae36516-d62d-4218-9ae3-6516d62da218' ( WatchID UInt64, JavaEnable UInt8, Title String, GoodEvent Int16, EventTime DateTime, EventDate Date, CounterID UInt32, ClientIP UInt32, ClientIP6 FixedString(16), RegionID UInt32, UserID UInt64, CounterClass Int8, OS UInt8, UserAgent UInt8, URL String, Referer String, URLDomain String, RefererDomain String, Refresh UInt8, IsRobot UInt8, RefererCategories Array(UInt16), URLCategories Array(UInt16), URLRegions Array(UInt32), RefererRegions Array(UInt32), ResolutionWidth UInt16, ResolutionHeight UInt16, ResolutionDepth UInt8, FlashMajor UInt8, FlashMinor UInt8, FlashMinor2 String, NetMajor UInt8, NetMinor UInt8, UserAgentMajor UInt16, UserAgentMinor FixedString(2), CookieEnable UInt8, JavascriptEnable UInt8, IsMobile UInt8, MobilePhone UInt8, MobilePhoneModel String, Params String, IPNetworkID UInt32, TraficSourceID Int8, SearchEngineID UInt16, SearchPhrase String, AdvEngineID UInt8, IsArtifical UInt8, WindowClientWidth UInt16, WindowClientHeight UInt16, ClientTimeZone Int16, ClientEventTime DateTime, SilverlightVersion1 UInt8, SilverlightVersion2 UInt8, SilverlightVersion3 UInt32, SilverlightVersion4 UInt16, PageCharset String, CodeVersion UInt32, IsLink UInt8, IsDownload UInt8, IsNotBounce UInt8, FUniqID UInt64, HID UInt32, IsOldCounter UInt8, IsEvent UInt8, IsParameter UInt8, DontCountHits UInt8, WithHash UInt8, HitColor FixedString(1), UTCEventTime DateTime, Age UInt8, Sex UInt8, Income UInt8, Interests UInt16, Robotness UInt8, GeneralInterests Array(UInt16), RemoteIP UInt32, RemoteIP6 FixedString(16), WindowName Int32, OpenerName Int32, HistoryLength Int16, BrowserLanguage FixedString(2), BrowserCountry FixedString(2), SocialNetwork String, SocialAction String, HTTPError UInt16, SendTiming Int32, DNSTiming Int32, ConnectTiming Int32, ResponseStartTiming Int32, ResponseEndTiming Int32, FetchTiming Int32, RedirectTiming Int32, DOMInteractiveTiming Int32, DOMContentLoadedTiming Int32, DOMCompleteTiming Int32, LoadEventStartTiming Int32, LoadEventEndTiming Int32, NSToDOMContentLoadedTiming Int32, FirstPaintTiming Int32, RedirectCount Int8, SocialSourceNetworkID UInt8, SocialSourcePage String, ParamPrice Int64, ParamOrderID String, ParamCurrency FixedString(3), ParamCurrencyID UInt16, GoalsReached Array(UInt32), OpenstatServiceName String, OpenstatCampaignID String, OpenstatAdID String, OpenstatSourceID String, UTMSource String, UTMMedium String, UTMCampaign String, UTMContent String, UTMTerm String, FromTag String, HasGCLID UInt8, RefererHash UInt64, URLHash UInt64, CLID UInt32, YCLID UInt64, ShareService String, ShareURL String, ShareTitle String, ParsedParams Nested( Key1 String, Key2 String, Key3 String, Key4 String, Key5 String, ValueDouble Float64), IslandID FixedString(16), RequestNum UInt32, RequestTry UInt8 ) ENGINE = MergeTree() PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate, intHash32(UserID)) SAMPLE BY intHash32(UserID) SETTINGS storage_policy='web';  Required parameters: type — web. Otherwise the disk is not created.endpoint — The endpoint URL in path format. Endpoint URL must contain a root path to store data, where they were uploaded. Optional parameters: min_bytes_for_seek — The minimal number of bytes to use seek operation instead of sequential read. Default value: 1 Mb.remote_fs_read_backoff_threashold — The maximum wait time when trying to read data for remote disk. Default value: 10000 seconds.remote_fs_read_backoff_max_tries — The maximum number of attempts to read with backoff. Default value: 5. If a query fails with an exception DB:Exception Unreachable URL, then you can try to adjust the settings: http_connection_timeout, http_receive_timeout, keep_alive_timeout. To get files for upload run:clickhouse static-files-disk-uploader --metadata-path &lt;path&gt; --output-dir &lt;dir&gt; (--metadata-path can be found in query SELECT data_paths FROM system.tables WHERE name = 'table_name'). When loading files by endpoint, they must be loaded into &lt;endpoint&gt;/store/ path, but config must contain only endpoint. If URL is not reachable on disk load when the server is starting up tables, then all errors are caught. If in this case there were errors, tables can be reloaded (become visible) via DETACH TABLE table_name -&gt; ATTACH TABLE table_name. If metadata was successfully loaded at server startup, then tables are available straight away. Use http_max_single_read_retries setting to limit the maximum number of retries during a single HTTP read. "},{"title":"Restrictions on Query Complexity","type":0,"sectionRef":"#","url":"docs/en/operations/settings/query-complexity","content":"","keywords":""},{"title":"max_memory_usage​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#settings_max_memory_usage","content":"The maximum amount of RAM to use for running a query on a single server. In the default configuration file, the maximum is 10 GB. The setting does not consider the volume of available memory or the total volume of memory on the machine. The restriction applies to a single query within a single server. You can use SHOW PROCESSLIST to see the current memory consumption for each query. Besides, the peak memory consumption is tracked for each query and written to the log. Memory usage is not monitored for the states of certain aggregate functions. Memory usage is not fully tracked for states of the aggregate functions min, max, any, anyLast, argMin, argMax from String and Array arguments. Memory consumption is also restricted by the parameters max_memory_usage_for_user and max_server_memory_usage. "},{"title":"max_memory_usage_for_user​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-memory-usage-for-user","content":"The maximum amount of RAM to use for running a user’s queries on a single server. Default values are defined in Settings.h. By default, the amount is not restricted (max_memory_usage_for_user = 0). See also the description of max_memory_usage. "},{"title":"max_rows_to_read​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-rows-to-read","content":"The following restrictions can be checked on each block (instead of on each row). That is, the restrictions can be broken a little. A maximum number of rows that can be read from a table when running a query. "},{"title":"max_bytes_to_read​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-bytes-to-read","content":"A maximum number of bytes (uncompressed data) that can be read from a table when running a query. "},{"title":"read_overflow_mode​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#read-overflow-mode","content":"What to do when the volume of data read exceeds one of the limits: ‘throw’ or ‘break’. By default, throw. "},{"title":"max_rows_to_read_leaf​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-rows-to-read-leaf","content":"The following restrictions can be checked on each block (instead of on each row). That is, the restrictions can be broken a little. A maximum number of rows that can be read from a local table on a leaf node when running a distributed query. While distributed queries can issue a multiple sub-queries to each shard (leaf) - this limit will be checked only on the read stage on the leaf nodes and ignored on results merging stage on the root node. For example, cluster consists of 2 shards and each shard contains a table with 100 rows. Then distributed query which suppose to read all the data from both tables with setting max_rows_to_read=150 will fail as in total it will be 200 rows. While query with max_rows_to_read_leaf=150 will succeed since leaf nodes will read 100 rows at max. "},{"title":"max_bytes_to_read_leaf​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-bytes-to-read-leaf","content":"A maximum number of bytes (uncompressed data) that can be read from a local table on a leaf node when running a distributed query. While distributed queries can issue a multiple sub-queries to each shard (leaf) - this limit will be checked only on the read stage on the leaf nodes and ignored on results merging stage on the root node. For example, cluster consists of 2 shards and each shard contains a table with 100 bytes of data. Then distributed query which suppose to read all the data from both tables with setting max_bytes_to_read=150 will fail as in total it will be 200 bytes. While query with max_bytes_to_read_leaf=150 will succeed since leaf nodes will read 100 bytes at max. "},{"title":"read_overflow_mode_leaf​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#read-overflow-mode-leaf","content":"What to do when the volume of data read exceeds one of the leaf limits: ‘throw’ or ‘break’. By default, throw. "},{"title":"max_rows_to_group_by​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#settings-max-rows-to-group-by","content":"A maximum number of unique keys received from aggregation. This setting lets you limit memory consumption when aggregating. "},{"title":"group_by_overflow_mode​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#group-by-overflow-mode","content":"What to do when the number of unique keys for aggregation exceeds the limit: ‘throw’, ‘break’, or ‘any’. By default, throw. Using the ‘any’ value lets you run an approximation of GROUP BY. The quality of this approximation depends on the statistical nature of the data. "},{"title":"max_bytes_before_external_group_by​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#settings-max_bytes_before_external_group_by","content":"Enables or disables execution of GROUP BY clauses in external memory. See GROUP BY in external memory. Possible values: Maximum volume of RAM (in bytes) that can be used by the single GROUP BY operation.0 — GROUP BY in external memory disabled. Default value: 0. "},{"title":"max_rows_to_sort​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-rows-to-sort","content":"A maximum number of rows before sorting. This allows you to limit memory consumption when sorting. "},{"title":"max_bytes_to_sort​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-bytes-to-sort","content":"A maximum number of bytes before sorting. "},{"title":"sort_overflow_mode​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#sort-overflow-mode","content":"What to do if the number of rows received before sorting exceeds one of the limits: ‘throw’ or ‘break’. By default, throw. "},{"title":"max_result_rows​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#setting-max_result_rows","content":"Limit on the number of rows in the result. Also checked for subqueries, and on remote servers when running parts of a distributed query. "},{"title":"max_result_bytes​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-result-bytes","content":"Limit on the number of bytes in the result. The same as the previous setting. "},{"title":"result_overflow_mode​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#result-overflow-mode","content":"What to do if the volume of the result exceeds one of the limits: ‘throw’ or ‘break’. By default, throw. Using ‘break’ is similar to using LIMIT. Break interrupts execution only at the block level. This means that amount of returned rows is greater than max_result_rows, multiple of max_block_size and depends on max_threads. Example: SET max_threads = 3, max_block_size = 3333; SET max_result_rows = 3334, result_overflow_mode = 'break'; SELECT * FROM numbers_mt(100000) FORMAT Null;  Result: 6666 rows in set. ...  "},{"title":"max_execution_time​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-execution-time","content":"Maximum query execution time in seconds. At this time, it is not checked for one of the sorting stages, or when merging and finalizing aggregate functions. "},{"title":"timeout_overflow_mode​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#timeout-overflow-mode","content":"What to do if the query is run longer than ‘max_execution_time’: ‘throw’ or ‘break’. By default, throw. "},{"title":"min_execution_speed​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#min-execution-speed","content":"Minimal execution speed in rows per second. Checked on every data block when ‘timeout_before_checking_execution_speed’ expires. If the execution speed is lower, an exception is thrown. "},{"title":"min_execution_speed_bytes​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#min-execution-speed-bytes","content":"A minimum number of execution bytes per second. Checked on every data block when ‘timeout_before_checking_execution_speed’ expires. If the execution speed is lower, an exception is thrown. "},{"title":"max_execution_speed​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-execution-speed","content":"A maximum number of execution rows per second. Checked on every data block when ‘timeout_before_checking_execution_speed’ expires. If the execution speed is high, the execution speed will be reduced. "},{"title":"max_execution_speed_bytes​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-execution-speed-bytes","content":"A maximum number of execution bytes per second. Checked on every data block when ‘timeout_before_checking_execution_speed’ expires. If the execution speed is high, the execution speed will be reduced. "},{"title":"timeout_before_checking_execution_speed​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#timeout-before-checking-execution-speed","content":"Checks that execution speed is not too slow (no less than ‘min_execution_speed’), after the specified time in seconds has expired. "},{"title":"max_columns_to_read​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-columns-to-read","content":"A maximum number of columns that can be read from a table in a single query. If a query requires reading a greater number of columns, it throws an exception. "},{"title":"max_temporary_columns​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-temporary-columns","content":"A maximum number of temporary columns that must be kept in RAM at the same time when running a query, including constant columns. If there are more temporary columns than this, it throws an exception. "},{"title":"max_temporary_non_const_columns​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-temporary-non-const-columns","content":"The same thing as ‘max_temporary_columns’, but without counting constant columns. Note that constant columns are formed fairly often when running a query, but they require approximately zero computing resources. "},{"title":"max_subquery_depth​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-subquery-depth","content":"Maximum nesting depth of subqueries. If subqueries are deeper, an exception is thrown. By default, 100. "},{"title":"max_pipeline_depth​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-pipeline-depth","content":"Maximum pipeline depth. Corresponds to the number of transformations that each data block goes through during query processing. Counted within the limits of a single server. If the pipeline depth is greater, an exception is thrown. By default, 1000. "},{"title":"max_ast_depth​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-ast-depth","content":"Maximum nesting depth of a query syntactic tree. If exceeded, an exception is thrown. At this time, it isn’t checked during parsing, but only after parsing the query. That is, a syntactic tree that is too deep can be created during parsing, but the query will fail. By default, 1000. "},{"title":"max_ast_elements​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-ast-elements","content":"A maximum number of elements in a query syntactic tree. If exceeded, an exception is thrown. In the same way as the previous setting, it is checked only after parsing the query. By default, 50,000. "},{"title":"max_rows_in_set​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-rows-in-set","content":"A maximum number of rows for a data set in the IN clause created from a subquery. "},{"title":"max_bytes_in_set​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-bytes-in-set","content":"A maximum number of bytes (uncompressed data) used by a set in the IN clause created from a subquery. "},{"title":"set_overflow_mode​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#set-overflow-mode","content":"What to do when the amount of data exceeds one of the limits: ‘throw’ or ‘break’. By default, throw. "},{"title":"max_rows_in_distinct​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-rows-in-distinct","content":"A maximum number of different rows when using DISTINCT. "},{"title":"max_bytes_in_distinct​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-bytes-in-distinct","content":"A maximum number of bytes used by a hash table when using DISTINCT. "},{"title":"distinct_overflow_mode​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#distinct-overflow-mode","content":"What to do when the amount of data exceeds one of the limits: ‘throw’ or ‘break’. By default, throw. "},{"title":"max_rows_to_transfer​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-rows-to-transfer","content":"A maximum number of rows that can be passed to a remote server or saved in a temporary table when using GLOBAL IN. "},{"title":"max_bytes_to_transfer​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-bytes-to-transfer","content":"A maximum number of bytes (uncompressed data) that can be passed to a remote server or saved in a temporary table when using GLOBAL IN. "},{"title":"transfer_overflow_mode​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#transfer-overflow-mode","content":"What to do when the amount of data exceeds one of the limits: ‘throw’ or ‘break’. By default, throw. "},{"title":"max_rows_in_join​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#settings-max_rows_in_join","content":"Limits the number of rows in the hash table that is used when joining tables. This settings applies to SELECT … JOIN operations and the Join table engine. If a query contains multiple joins, ClickHouse checks this setting for every intermediate result. ClickHouse can proceed with different actions when the limit is reached. Use the join_overflow_mode setting to choose the action. Possible values: Positive integer.0 — Unlimited number of rows. Default value: 0. "},{"title":"max_bytes_in_join​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#settings-max_bytes_in_join","content":"Limits the size in bytes of the hash table used when joining tables. This settings applies to SELECT … JOIN operations and Join table engine. If the query contains joins, ClickHouse checks this setting for every intermediate result. ClickHouse can proceed with different actions when the limit is reached. Use join_overflow_mode settings to choose the action. Possible values: Positive integer.0 — Memory control is disabled. Default value: 0. "},{"title":"join_overflow_mode​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#settings-join_overflow_mode","content":"Defines what action ClickHouse performs when any of the following join limits is reached: max_bytes_in_joinmax_rows_in_join Possible values: THROW — ClickHouse throws an exception and breaks operation.BREAK — ClickHouse breaks operation and does not throw an exception. Default value: THROW. See Also JOIN clauseJoin table engine "},{"title":"max_partitions_per_insert_block​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"docs/en/operations/settings/query-complexity#max-partitions-per-insert-block","content":"Limits the maximum number of partitions in a single inserted block. Positive integer.0 — Unlimited number of partitions. Default value: 100. Details When inserting data, ClickHouse calculates the number of partitions in the inserted block. If the number of partitions is more than max_partitions_per_insert_block, ClickHouse throws an exception with the following text: “Too many partitions for single INSERT block (more than” + toString(max_parts) + “). The limit is controlled by ‘max_partitions_per_insert_block’ setting. A large number of partitions is a common misconception. It will lead to severe negative performance impact, including slow server startup, slow INSERT queries and slow SELECT queries. Recommended total number of partitions for a table is under 1000..10000. Please note, that partitioning is not intended to speed up SELECT queries (ORDER BY key is sufficient to make range queries fast). Partitions are intended for data manipulation (DROP PARTITION, etc).” Original article "},{"title":"Permissions for Queries","type":0,"sectionRef":"#","url":"docs/en/operations/settings/permissions-for-queries","content":"","keywords":""},{"title":"readonly​","type":1,"pageTitle":"Permissions for Queries","url":"docs/en/operations/settings/permissions-for-queries#settings_readonly","content":"Restricts permissions for reading data, write data and change settings queries. See how the queries are divided into types above. Possible values: 0 — All queries are allowed.1 — Only read data queries are allowed.2 — Read data and change settings queries are allowed. After setting readonly = 1, the user can’t change readonly and allow_ddl settings in the current session. When using the GET method in the HTTP interface, readonly = 1 is set automatically. To modify data, use the POST method. Setting readonly = 1 prohibit the user from changing all the settings. There is a way to prohibit the user from changing only specific settings, for details see constraints on settings. Default value: 0 "},{"title":"allow_ddl​","type":1,"pageTitle":"Permissions for Queries","url":"docs/en/operations/settings/permissions-for-queries#settings_allow_ddl","content":"Allows or denies DDL queries. See how the queries are divided into types above. Possible values: 0 — DDL queries are not allowed.1 — DDL queries are allowed. You can’t execute SET allow_ddl = 1 if allow_ddl = 0 for the current session. Default value: 1 Original article "},{"title":"asynchronous_metrics","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/asynchronous_metrics","content":"asynchronous_metrics Contains metrics that are calculated periodically in the background. For example, the amount of RAM in use. Columns: metric (String) — Metric name.value (Float64) — Metric value. Example SELECT * FROM system.asynchronous_metrics LIMIT 10 ┌─metric──────────────────────────────────┬──────value─┐ │ jemalloc.background_thread.run_interval │ 0 │ │ jemalloc.background_thread.num_runs │ 0 │ │ jemalloc.background_thread.num_threads │ 0 │ │ jemalloc.retained │ 422551552 │ │ jemalloc.mapped │ 1682989056 │ │ jemalloc.resident │ 1656446976 │ │ jemalloc.metadata_thp │ 0 │ │ jemalloc.metadata │ 10226856 │ │ UncompressedCacheCells │ 0 │ │ MarkCacheFiles │ 0 │ └─────────────────────────────────────────┴────────────┘ See Also Monitoring — Base concepts of ClickHouse monitoring. system.metrics — Contains instantly calculated metrics. system.events — Contains a number of events that have occurred. system.metric_log — Contains a history of metrics values from tables system.metrics and system.events. Original article","keywords":""},{"title":"asynchronous_metric_log","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/asynchronous_metric_log","content":"asynchronous_metric_log Contains the historical values for system.asynchronous_metrics, which are saved once per minute. Enabled by default. Columns: event_date (Date) — Event date.event_time (DateTime) — Event time.event_time_microseconds (DateTime64) — Event time with microseconds resolution.name (String) — Metric name.value (Float64) — Metric value. Example SELECT * FROM system.asynchronous_metric_log LIMIT 10 ┌─event_date─┬──────────event_time─┬────event_time_microseconds─┬─name─────────────────────────────────────┬─────value─┐ │ 2020-09-05 │ 2020-09-05 15:56:30 │ 2020-09-05 15:56:30.025227 │ CPUFrequencyMHz_0 │ 2120.9 │ │ 2020-09-05 │ 2020-09-05 15:56:30 │ 2020-09-05 15:56:30.025227 │ jemalloc.arenas.all.pmuzzy │ 743 │ │ 2020-09-05 │ 2020-09-05 15:56:30 │ 2020-09-05 15:56:30.025227 │ jemalloc.arenas.all.pdirty │ 26288 │ │ 2020-09-05 │ 2020-09-05 15:56:30 │ 2020-09-05 15:56:30.025227 │ jemalloc.background_thread.run_intervals │ 0 │ │ 2020-09-05 │ 2020-09-05 15:56:30 │ 2020-09-05 15:56:30.025227 │ jemalloc.background_thread.num_runs │ 0 │ │ 2020-09-05 │ 2020-09-05 15:56:30 │ 2020-09-05 15:56:30.025227 │ jemalloc.retained │ 60694528 │ │ 2020-09-05 │ 2020-09-05 15:56:30 │ 2020-09-05 15:56:30.025227 │ jemalloc.mapped │ 303161344 │ │ 2020-09-05 │ 2020-09-05 15:56:30 │ 2020-09-05 15:56:30.025227 │ jemalloc.resident │ 260931584 │ │ 2020-09-05 │ 2020-09-05 15:56:30 │ 2020-09-05 15:56:30.025227 │ jemalloc.metadata │ 12079488 │ │ 2020-09-05 │ 2020-09-05 15:56:30 │ 2020-09-05 15:56:30.025227 │ jemalloc.allocated │ 133756128 │ └────────────┴─────────────────────┴────────────────────────────┴──────────────────────────────────────────┴───────────┘ See Also system.asynchronous_metrics — Contains metrics, calculated periodically in the background.system.metric_log — Contains history of metrics values from tables system.metrics and system.events, periodically flushed to disk. Original article","keywords":""},{"title":"data_skipping_indices","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/data_skipping_indices","content":"data_skipping_indices Contains information about existing data skipping indices in all the tables. Columns: database (String) — Database name.table (String) — Table name.name (String) — Index name.type (String) — Index type.expr (String) — Expression for the index calculation.granularity (UInt64) — The number of granules in the block.data_compressed_bytes (UInt64) — The size of compressed data, in bytes.data_uncompressed_bytes (UInt64) — The size of decompressed data, in bytes.marks_bytes (UInt64) — The size of marks, in bytes. Example SELECT * FROM system.data_skipping_indices LIMIT 2 FORMAT Vertical; Row 1: ────── database: default table: user_actions name: clicks_idx type: minmax expr: clicks granularity: 1 data_compressed_bytes: 58 data_uncompressed_bytes: 6 marks: 48 Row 2: ────── database: default table: users name: contacts_null_idx type: minmax expr: assumeNotNull(contacts_null) granularity: 1 data_compressed_bytes: 58 data_uncompressed_bytes: 6 marks: 48 ","keywords":""},{"title":"clusters","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/clusters","content":"clusters Contains information about clusters available in the config file and the servers in them. Columns: cluster (String) — The cluster name.shard_num (UInt32) — The shard number in the cluster, starting from 1.shard_weight (UInt32) — The relative weight of the shard when writing data.replica_num (UInt32) — The replica number in the shard, starting from 1.host_name (String) — The host name, as specified in the config.host_address (String) — The host IP address obtained from DNS.port (UInt16) — The port to use for connecting to the server.is_local (UInt8) — Flag that indicates whether the host is local.user (String) — The name of the user for connecting to the server.default_database (String) — The default database name.errors_count (UInt32) — The number of times this host failed to reach replica.slowdowns_count (UInt32) — The number of slowdowns that led to changing replica when establishing a connection with hedged requests.estimated_recovery_time (UInt32) — Seconds remaining until the replica error count is zeroed and it is considered to be back to normal. Example Query: SELECT * FROM system.clusters LIMIT 2 FORMAT Vertical; Result: Row 1: ────── cluster: test_cluster_two_shards shard_num: 1 shard_weight: 1 replica_num: 1 host_name: 127.0.0.1 host_address: 127.0.0.1 port: 9000 is_local: 1 user: default default_database: errors_count: 0 slowdowns_count: 0 estimated_recovery_time: 0 Row 2: ────── cluster: test_cluster_two_shards shard_num: 2 shard_weight: 1 replica_num: 1 host_name: 127.0.0.2 host_address: 127.0.0.2 port: 9000 is_local: 0 user: default default_database: errors_count: 0 slowdowns_count: 0 estimated_recovery_time: 0 See Also Table engine Distributeddistributed_replica_error_cap settingdistributed_replica_error_half_life setting Original article","keywords":""},{"title":"current_roles","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/current-roles","content":"current_roles Contains active roles of a current user. SET ROLE changes the contents of this table. Columns: role_name (String)) — Role name. with_admin_option (UInt8) — Flag that shows whether current_role is a role with ADMIN OPTION privilege. is_default (UInt8) — Flag that shows whether current_role is a default role. Original article","keywords":""},{"title":"contributors","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/contributors","content":"contributors Contains information about contributors. The order is random at query execution time. Columns: name (String) — Contributor (author) name from git log. Example SELECT * FROM system.contributors LIMIT 10 ┌─name─────────────┐ │ Olga Khvostikova │ │ Max Vetrov │ │ LiuYangkuan │ │ svladykin │ │ zamulla │ │ Šimon Podlipský │ │ BayoNet │ │ Ilya Khomutov │ │ Amy Krishnevsky │ │ Loud_Scream │ └──────────────────┘ To find out yourself in the table, use a query: SELECT * FROM system.contributors WHERE name = 'Olga Khvostikova' ┌─name─────────────┐ │ Olga Khvostikova │ └──────────────────┘ Original article","keywords":""},{"title":"columns","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/columns","content":"columns Contains information about columns in all the tables. You can use this table to get information similar to the DESCRIBE TABLE query, but for multiple tables at once. Columns from temporary tables are visible in the system.columns only in those session where they have been created. They are shown with the empty database field. The system.columns table contains the following columns (the column type is shown in brackets): database (String) — Database name.table (String) — Table name.name (String) — Column name.type (String) — Column type.position (UInt64) — Ordinal position of a column in a table starting with 1.default_kind (String) — Expression type (DEFAULT, MATERIALIZED, ALIAS) for the default value, or an empty string if it is not defined.default_expression (String) — Expression for the default value, or an empty string if it is not defined.data_compressed_bytes (UInt64) — The size of compressed data, in bytes.data_uncompressed_bytes (UInt64) — The size of decompressed data, in bytes.marks_bytes (UInt64) — The size of marks, in bytes.comment (String) — Comment on the column, or an empty string if it is not defined.is_in_partition_key (UInt8) — Flag that indicates whether the column is in the partition expression.is_in_sorting_key (UInt8) — Flag that indicates whether the column is in the sorting key expression.is_in_primary_key (UInt8) — Flag that indicates whether the column is in the primary key expression.is_in_sampling_key (UInt8) — Flag that indicates whether the column is in the sampling key expression.compression_codec (String) — Compression codec name.character_octet_length (Nullable(UInt64)) — Maximum length in bytes for binary data, character data, or text data and images. In ClickHouse makes sense only for FixedString data type. Otherwise, the NULL value is returned.numeric_precision (Nullable(UInt64)) — Accuracy of approximate numeric data, exact numeric data, integer data, or monetary data. In ClickHouse it is bitness for integer types and decimal precision for Decimal types. Otherwise, the NULL value is returned.numeric_precision_radix (Nullable(UInt64)) — The base of the number system is the accuracy of approximate numeric data, exact numeric data, integer data or monetary data. In ClickHouse it's 2 for integer types and 10 for Decimal types. Otherwise, the NULL value is returned.numeric_scale (Nullable(UInt64)) — The scale of approximate numeric data, exact numeric data, integer data, or monetary data. In ClickHouse makes sense only for Decimal types. Otherwise, the NULL value is returned.datetime_precision (Nullable(UInt64)) — Decimal precision of DateTime64 data type. For other data types, the NULL value is returned. Example SELECT * FROM system.columns LIMIT 2 FORMAT Vertical; Row 1: ────── database: INFORMATION_SCHEMA table: COLUMNS name: table_catalog type: String position: 1 default_kind: default_expression: data_compressed_bytes: 0 data_uncompressed_bytes: 0 marks_bytes: 0 comment: is_in_partition_key: 0 is_in_sorting_key: 0 is_in_primary_key: 0 is_in_sampling_key: 0 compression_codec: character_octet_length: ᴺᵁᴸᴸ numeric_precision: ᴺᵁᴸᴸ numeric_precision_radix: ᴺᵁᴸᴸ numeric_scale: ᴺᵁᴸᴸ datetime_precision: ᴺᵁᴸᴸ Row 2: ────── database: INFORMATION_SCHEMA table: COLUMNS name: table_schema type: String position: 2 default_kind: default_expression: data_compressed_bytes: 0 data_uncompressed_bytes: 0 marks_bytes: 0 comment: is_in_partition_key: 0 is_in_sorting_key: 0 is_in_primary_key: 0 is_in_sampling_key: 0 compression_codec: character_octet_length: ᴺᵁᴸᴸ numeric_precision: ᴺᵁᴸᴸ numeric_precision_radix: ᴺᵁᴸᴸ numeric_scale: ᴺᵁᴸᴸ datetime_precision: ᴺᵁᴸᴸ Original article","keywords":""},{"title":"crash_log","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/crash-log","content":"crash_log Contains information about stack traces for fatal errors. The table does not exist in the database by default, it is created only when fatal errors occur. Columns: event_date (Datetime) — Date of the event.event_time (Datetime) — Time of the event.timestamp_ns (UInt64) — Timestamp of the event with nanoseconds.signal (Int32) — Signal number.thread_id (UInt64) — Thread ID.query_id (String) — Query ID.trace (Array(UInt64)) — Stack trace at the moment of crash. Each element is a virtual memory address inside ClickHouse server process.trace_full (Array(String)) — Stack trace at the moment of crash. Each element contains a called method inside ClickHouse server process.version (String) — ClickHouse server version.revision (UInt32) — ClickHouse server revision.build_id (String) — BuildID that is generated by compiler. Example Query: SELECT * FROM system.crash_log ORDER BY event_time DESC LIMIT 1; Result (not full): Row 1: ────── event_date: 2020-10-14 event_time: 2020-10-14 15:47:40 timestamp_ns: 1602679660271312710 signal: 11 thread_id: 23624 query_id: 428aab7c-8f5c-44e9-9607-d16b44467e69 trace: [188531193,...] trace_full: ['3. DB::(anonymous namespace)::FunctionFormatReadableTimeDelta::executeImpl(std::__1::vector&lt;DB::ColumnWithTypeAndName, std::__1::allocator&lt;DB::ColumnWithTypeAndName&gt; &gt;&amp;, std::__1::vector&lt;unsigned long, std::__1::allocator&lt;unsigned long&gt; &gt; const&amp;, unsigned long, unsigned long) const @ 0xb3cc1f9 in /home/username/work/ClickHouse/build/programs/clickhouse',...] version: ClickHouse 20.11.1.1 revision: 54442 build_id: See also trace_log system table Original article","keywords":""},{"title":"data_type_families","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/data_type_families","content":"data_type_families Contains information about supported data types. Columns: name (String) — Data type name.case_insensitive (UInt8) — Property that shows whether you can use a data type name in a query in case insensitive manner or not. For example, Date and date are both valid.alias_to (String) — Data type name for which name is an alias. Example SELECT * FROM system.data_type_families WHERE alias_to = 'String' ┌─name───────┬─case_insensitive─┬─alias_to─┐ │ LONGBLOB │ 1 │ String │ │ LONGTEXT │ 1 │ String │ │ TINYTEXT │ 1 │ String │ │ TEXT │ 1 │ String │ │ VARCHAR │ 1 │ String │ │ MEDIUMBLOB │ 1 │ String │ │ BLOB │ 1 │ String │ │ TINYBLOB │ 1 │ String │ │ CHAR │ 1 │ String │ │ MEDIUMTEXT │ 1 │ String │ └────────────┴──────────────────┴──────────┘ See Also Syntax — Information about supported syntax. Original article","keywords":""},{"title":"databases","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/databases","content":"databases Contains information about the databases that are available to the current user. Columns: name (String) — Database name.engine (String) — Database engine.data_path (String) — Data path.metadata_path (String) — Metadata path.uuid (UUID) — Database UUID.comment (String) — Database comment. The name column from this system table is used for implementing the SHOW DATABASES query. Example Create a database. CREATE DATABASE test; Check all of the available databases to the user. SELECT * FROM system.databases; ┌─name───────────────┬─engine─┬─data_path──────────────────┬─metadata_path───────────────────────────────────────────────────────┬─uuid─────────────────────────────────┬─comment─┐ │ INFORMATION_SCHEMA │ Memory │ /var/lib/clickhouse/ │ │ 00000000-0000-0000-0000-000000000000 │ │ │ default │ Atomic │ /var/lib/clickhouse/store/ │ /var/lib/clickhouse/store/d31/d317b4bd-3595-4386-81ee-c2334694128a/ │ 24363899-31d7-42a0-a436-389931d752a0 │ │ │ information_schema │ Memory │ /var/lib/clickhouse/ │ │ 00000000-0000-0000-0000-000000000000 │ │ │ system │ Atomic │ /var/lib/clickhouse/store/ │ /var/lib/clickhouse/store/1d1/1d1c869d-e465-4b1b-a51f-be033436ebf9/ │ 03e9f3d1-cc88-4a49-83e9-f3d1cc881a49 │ │ └────────────────────┴────────┴────────────────────────────┴─────────────────────────────────────────────────────────────────────┴──────────────────────────────────────┴─────────┘ ","keywords":""},{"title":"disks","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/disks","content":"disks Contains information about disks defined in the server configuration. Columns: name (String) — Name of a disk in the server configuration.path (String) — Path to the mount point in the file system.free_space (UInt64) — Free space on disk in bytes.total_space (UInt64) — Disk volume in bytes.keep_free_space (UInt64) — Amount of disk space that should stay free on disk in bytes. Defined in the keep_free_space_bytes parameter of disk configuration. Example :) SELECT * FROM system.disks; ┌─name────┬─path─────────────────┬───free_space─┬──total_space─┬─keep_free_space─┐ │ default │ /var/lib/clickhouse/ │ 276392587264 │ 490652508160 │ 0 │ └─────────┴──────────────────────┴──────────────┴──────────────┴─────────────────┘ 1 rows in set. Elapsed: 0.001 sec. Original article","keywords":""},{"title":"dictionaries","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/dictionaries","content":"dictionaries Contains information about external dictionaries. Columns: database (String) — Name of the database containing the dictionary created by DDL query. Empty string for other dictionaries.name (String) — Dictionary name.uuid (UUID) — Dictionary UUID.status (Enum8) — Dictionary status. Possible values: NOT_LOADED — Dictionary was not loaded because it was not used.LOADED — Dictionary loaded successfully.FAILED — Unable to load the dictionary as a result of an error.LOADING — Dictionary is loading now.LOADED_AND_RELOADING — Dictionary is loaded successfully, and is being reloaded right now (frequent reasons: SYSTEM RELOAD DICTIONARY query, timeout, dictionary config has changed).FAILED_AND_RELOADING — Could not load the dictionary as a result of an error and is loading now. origin (String) — Path to the configuration file that describes the dictionary.type (String) — Type of a dictionary allocation. Storing Dictionaries in Memory.key.names (Array(String)) — Array of key names provided by the dictionary.key.types (Array(String)) — Corresponding array of key types provided by the dictionary.attribute.names (Array(String)) — Array of attribute names provided by the dictionary.attribute.types (Array(String)) — Corresponding array of attribute types provided by the dictionary.bytes_allocated (UInt64) — Amount of RAM allocated for the dictionary.query_count (UInt64) — Number of queries since the dictionary was loaded or since the last successful reboot.hit_rate (Float64) — For cache dictionaries, the percentage of uses for which the value was in the cache.found_rate (Float64) — The percentage of uses for which the value was found.element_count (UInt64) — Number of items stored in the dictionary.load_factor (Float64) — Percentage filled in the dictionary (for a hashed dictionary, the percentage filled in the hash table).source (String) — Text describing the data source for the dictionary.lifetime_min (UInt64) — Minimum lifetime of the dictionary in memory, after which ClickHouse tries to reload the dictionary (if invalidate_query is set, then only if it has changed). Set in seconds.lifetime_max (UInt64) — Maximum lifetime of the dictionary in memory, after which ClickHouse tries to reload the dictionary (if invalidate_query is set, then only if it has changed). Set in seconds.loading_start_time (DateTime) — Start time for loading the dictionary.last_successful_update_time (DateTime) — End time for loading or updating the dictionary. Helps to monitor some troubles with external sources and investigate causes.loading_duration (Float32) — Duration of a dictionary loading.last_exception (String) — Text of the error that occurs when creating or reloading the dictionary if the dictionary couldn’t be created.comment (String) — Text of the comment to dictionary. Example Configure the dictionary: CREATE DICTIONARY dictionary_with_comment ( id UInt64, value String ) PRIMARY KEY id SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() TABLE 'source_table')) LAYOUT(FLAT()) LIFETIME(MIN 0 MAX 1000) COMMENT 'The temporary dictionary'; Make sure that the dictionary is loaded. SELECT * FROM system.dictionaries LIMIT 1 FORMAT Vertical; Row 1: ────── database: default name: dictionary_with_comment uuid: 4654d460-0d03-433a-8654-d4600d03d33a status: NOT_LOADED origin: 4654d460-0d03-433a-8654-d4600d03d33a type: key.names: ['id'] key.types: ['UInt64'] attribute.names: ['value'] attribute.types: ['String'] bytes_allocated: 0 query_count: 0 hit_rate: 0 found_rate: 0 element_count: 0 load_factor: 0 source: lifetime_min: 0 lifetime_max: 0 loading_start_time: 1970-01-01 00:00:00 last_successful_update_time: 1970-01-01 00:00:00 loading_duration: 0 last_exception: comment: The temporary dictionary ","keywords":""},{"title":"errors","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/errors","content":"errors Contains error codes with the number of times they have been triggered. Columns: name (String) — name of the error (errorCodeToName).code (Int32) — code number of the error.value (UInt64) — the number of times this error has been happened.last_error_time (DateTime) — time when the last error happened.last_error_message (String) — message for the last error.last_error_trace (Array(UInt64)) — A stack trace which represents a list of physical addresses where the called methods are stored.remote (UInt8) — remote exception (i.e. received during one of the distributed query). Example SELECT name, code, value FROM system.errors WHERE value &gt; 0 ORDER BY code ASC LIMIT 1 ┌─name─────────────┬─code─┬─value─┐ │ CANNOT_OPEN_FILE │ 76 │ 1 │ └──────────────────┴──────┴───────┘ WITH arrayMap(x -&gt; demangle(addressToSymbol(x)), last_error_trace) AS all SELECT name, arrayStringConcat(all, '\\n') AS res FROM system.errors LIMIT 1 SETTINGS allow_introspection_functions=1\\G ","keywords":""},{"title":"distribution_queue","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/distribution_queue","content":"distribution_queue Contains information about local files that are in the queue to be sent to the shards. These local files contain new parts that are created by inserting new data into the Distributed table in asynchronous mode. Columns: database (String) — Name of the database. table (String) — Name of the table. data_path (String) — Path to the folder with local files. is_blocked (UInt8) — Flag indicates whether sending local files to the server is blocked. error_count (UInt64) — Number of errors. data_files (UInt64) — Number of local files in a folder. data_compressed_bytes (UInt64) — Size of compressed data in local files, in bytes. broken_data_files (UInt64) — Number of files that has been marked as broken (due to an error). broken_data_compressed_bytes (UInt64) — Size of compressed data in broken files, in bytes. last_exception (String) — Text message about the last error that occurred (if any). Example SELECT * FROM system.distribution_queue LIMIT 1 FORMAT Vertical; Row 1: ────── database: default table: dist data_path: ./store/268/268bc070-3aad-4b1a-9cf2-4987580161af/default@127%2E0%2E0%2E2:9000/ is_blocked: 1 error_count: 0 data_files: 1 data_compressed_bytes: 499 last_exception: See Also Distributed table engine Original article","keywords":""},{"title":"functions","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/functions","content":"functions Contains information about normal and aggregate functions. Columns: name(String) – The name of the function.is_aggregate(UInt8) — Whether the function is aggregate. Example SELECT * FROM system.functions LIMIT 10; ┌─name──────────────────┬─is_aggregate─┬─case_insensitive─┬─alias_to─┬─create_query─┬─origin─┐ │ logTrace │ 0 │ 0 │ │ │ System │ │ aes_decrypt_mysql │ 0 │ 0 │ │ │ System │ │ aes_encrypt_mysql │ 0 │ 0 │ │ │ System │ │ decrypt │ 0 │ 0 │ │ │ System │ │ encrypt │ 0 │ 0 │ │ │ System │ │ toBool │ 0 │ 0 │ │ │ System │ │ windowID │ 0 │ 0 │ │ │ System │ │ hopStart │ 0 │ 0 │ │ │ System │ │ hop │ 0 │ 0 │ │ │ System │ │ snowflakeToDateTime64 │ 0 │ 0 │ │ │ System │ └───────────────────────┴──────────────┴──────────────────┴──────────┴──────────────┴────────┘ 10 rows in set. Elapsed: 0.002 sec. Original article","keywords":""},{"title":"Server Settings","type":0,"sectionRef":"#","url":"docs/en/operations/server-configuration-parameters/settings","content":"","keywords":""},{"title":"builtin_dictionaries_reload_interval​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#builtin-dictionaries-reload-interval","content":"The interval in seconds before reloading built-in dictionaries. ClickHouse reloads built-in dictionaries every x seconds. This makes it possible to edit dictionaries “on the fly” without restarting the server. Default value: 3600. Example &lt;builtin_dictionaries_reload_interval&gt;3600&lt;/builtin_dictionaries_reload_interval&gt;  "},{"title":"compression​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server-settings-compression","content":"Data compression settings for MergeTree-engine tables. warning Don’t use it if you have just started using ClickHouse. Configuration template: &lt;compression&gt; &lt;case&gt; &lt;min_part_size&gt;...&lt;/min_part_size&gt; &lt;min_part_size_ratio&gt;...&lt;/min_part_size_ratio&gt; &lt;method&gt;...&lt;/method&gt; &lt;level&gt;...&lt;/level&gt; &lt;/case&gt; ... &lt;/compression&gt;  &lt;case&gt; fields: min_part_size – The minimum size of a data part.min_part_size_ratio – The ratio of the data part size to the table size.method – Compression method. Acceptable values: lz4, lz4hc, zstd.level – Compression level. See Codecs. You can configure multiple &lt;case&gt; sections. Actions when conditions are met: If a data part matches a condition set, ClickHouse uses the specified compression method.If a data part matches multiple condition sets, ClickHouse uses the first matched condition set. If no conditions met for a data part, ClickHouse uses the lz4 compression. Example &lt;compression incl=&quot;clickhouse_compression&quot;&gt; &lt;case&gt; &lt;min_part_size&gt;10000000000&lt;/min_part_size&gt; &lt;min_part_size_ratio&gt;0.01&lt;/min_part_size_ratio&gt; &lt;method&gt;zstd&lt;/method&gt; &lt;level&gt;1&lt;/level&gt; &lt;/case&gt; &lt;/compression&gt;  "},{"title":"encryption​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server-settings-encryption","content":"Configures a command to obtain a key to be used by encryption codecs. Key (or keys) should be written in environment variables or set in the configuration file. Keys can be hex or string with a length equal to 16 bytes. Example Loading from config: &lt;encryption_codecs&gt; &lt;aes_128_gcm_siv&gt; &lt;key&gt;1234567812345678&lt;/key&gt; &lt;/aes_128_gcm_siv&gt; &lt;/encryption_codecs&gt;  note Storing keys in the configuration file is not recommended. It isn't secure. You can move the keys into a separate config file on a secure disk and put a symlink to that config file to config.d/ folder. Loading from config, when the key is in hex: &lt;encryption_codecs&gt; &lt;aes_128_gcm_siv&gt; &lt;key_hex&gt;00112233445566778899aabbccddeeff&lt;/key_hex&gt; &lt;/aes_128_gcm_siv&gt; &lt;/encryption_codecs&gt;  Loading key from the environment variable: &lt;encryption_codecs&gt; &lt;aes_128_gcm_siv&gt; &lt;key_hex from_env=&quot;ENVVAR&quot;&gt;&lt;/key_hex&gt; &lt;/aes_128_gcm_siv&gt; &lt;/encryption_codecs&gt;  Here current_key_id sets the current key for encryption, and all specified keys can be used for decryption. Each of these methods can be applied for multiple keys: &lt;encryption_codecs&gt; &lt;aes_128_gcm_siv&gt; &lt;key_hex id=&quot;0&quot;&gt;00112233445566778899aabbccddeeff&lt;/key_hex&gt; &lt;key_hex id=&quot;1&quot; from_env=&quot;ENVVAR&quot;&gt;&lt;/key_hex&gt; &lt;current_key_id&gt;1&lt;/current_key_id&gt; &lt;/aes_128_gcm_siv&gt; &lt;/encryption_codecs&gt;  Here current_key_id shows current key for encryption. Also, users can add nonce that must be 12 bytes long (by default encryption and decryption processes use nonce that consists of zero bytes): &lt;encryption_codecs&gt; &lt;aes_128_gcm_siv&gt; &lt;nonce&gt;012345678910&lt;/nonce&gt; &lt;/aes_128_gcm_siv&gt; &lt;/encryption_codecs&gt;  Or it can be set in hex: &lt;encryption_codecs&gt; &lt;aes_128_gcm_siv&gt; &lt;nonce_hex&gt;abcdefabcdef&lt;/nonce_hex&gt; &lt;/aes_128_gcm_siv&gt; &lt;/encryption_codecs&gt;  Everything mentioned above can be applied for aes_256_gcm_siv (but the key must be 32 bytes long). "},{"title":"custom_settings_prefixes​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#custom_settings_prefixes","content":"List of prefixes for custom settings. The prefixes must be separated with commas. Example &lt;custom_settings_prefixes&gt;custom_&lt;/custom_settings_prefixes&gt;  See Also Custom settings "},{"title":"core_dump​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-core_dump","content":"Configures soft limit for core dump file size. Possible values: Positive integer. Default value: 1073741824 (1 GB). note Hard limit is configured via system tools Example &lt;core_dump&gt; &lt;size_limit&gt;1073741824&lt;/size_limit&gt; &lt;/core_dump&gt;  "},{"title":"database_atomic_delay_before_drop_table_sec​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#database_atomic_delay_before_drop_table_sec","content":"Sets the delay before remove table data in seconds. If the query has SYNC modifier, this setting is ignored. Default value: 480 (8 minute). "},{"title":"default_database​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#default-database","content":"The default database. To get a list of databases, use the SHOW DATABASES query. Example &lt;default_database&gt;default&lt;/default_database&gt;  "},{"title":"default_profile​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#default-profile","content":"Default settings profile. Settings profiles are located in the file specified in the parameter user_config. Example &lt;default_profile&gt;default&lt;/default_profile&gt;  "},{"title":"default_replica_path​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#default_replica_path","content":"The path to the table in ZooKeeper. Example &lt;default_replica_path&gt;/clickhouse/tables/{uuid}/{shard}&lt;/default_replica_path&gt;  "},{"title":"default_replica_name​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#default_replica_name","content":"The replica name in ZooKeeper. Example &lt;default_replica_name&gt;{replica}&lt;/default_replica_name&gt;  "},{"title":"dictionaries_config​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-dictionaries_config","content":"The path to the config file for external dictionaries. Path: Specify the absolute path or the path relative to the server config file.The path can contain wildcards * and ?. See also “External dictionaries”. Example &lt;dictionaries_config&gt;*_dictionary.xml&lt;/dictionaries_config&gt;  "},{"title":"dictionaries_lazy_load​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-dictionaries_lazy_load","content":"Lazy loading of dictionaries. If true, then each dictionary is created on first use. If dictionary creation failed, the function that was using the dictionary throws an exception. If false, all dictionaries are created when the server starts, if the dictionary or dictionaries are created too long or are created with errors, then the server boots without of these dictionaries and continues to try to create these dictionaries. The default is true. Example &lt;dictionaries_lazy_load&gt;true&lt;/dictionaries_lazy_load&gt;  "},{"title":"format_schema_path​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-format_schema_path","content":"The path to the directory with the schemes for the input data, such as schemas for the CapnProto format. Example  &lt;!-- Directory containing schema files for various input formats. --&gt; &lt;format_schema_path&gt;format_schemas/&lt;/format_schema_path&gt;  "},{"title":"graphite​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-graphite","content":"Sending data to Graphite. Settings: host – The Graphite server.port – The port on the Graphite server.interval – The interval for sending, in seconds.timeout – The timeout for sending data, in seconds.root_path – Prefix for keys.metrics – Sending data from the system.metrics table.events – Sending deltas data accumulated for the time period from the system.events table.events_cumulative – Sending cumulative data from the system.events table.asynchronous_metrics – Sending data from the system.asynchronous_metrics table. You can configure multiple &lt;graphite&gt; clauses. For instance, you can use this for sending different data at different intervals. Example &lt;graphite&gt; &lt;host&gt;localhost&lt;/host&gt; &lt;port&gt;42000&lt;/port&gt; &lt;timeout&gt;0.1&lt;/timeout&gt; &lt;interval&gt;60&lt;/interval&gt; &lt;root_path&gt;one_min&lt;/root_path&gt; &lt;metrics&gt;true&lt;/metrics&gt; &lt;events&gt;true&lt;/events&gt; &lt;events_cumulative&gt;false&lt;/events_cumulative&gt; &lt;asynchronous_metrics&gt;true&lt;/asynchronous_metrics&gt; &lt;/graphite&gt;  "},{"title":"graphite_rollup​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-graphite-rollup","content":"Settings for thinning data for Graphite. For more details, see GraphiteMergeTree. Example &lt;graphite_rollup_example&gt; &lt;default&gt; &lt;function&gt;max&lt;/function&gt; &lt;retention&gt; &lt;age&gt;0&lt;/age&gt; &lt;precision&gt;60&lt;/precision&gt; &lt;/retention&gt; &lt;retention&gt; &lt;age&gt;3600&lt;/age&gt; &lt;precision&gt;300&lt;/precision&gt; &lt;/retention&gt; &lt;retention&gt; &lt;age&gt;86400&lt;/age&gt; &lt;precision&gt;3600&lt;/precision&gt; &lt;/retention&gt; &lt;/default&gt; &lt;/graphite_rollup_example&gt;  "},{"title":"http_port/https_port​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#http-porthttps-port","content":"The port for connecting to the server over HTTP(s). If https_port is specified, openSSL must be configured. If http_port is specified, the OpenSSL configuration is ignored even if it is set. Example &lt;https_port&gt;9999&lt;/https_port&gt;  "},{"title":"http_server_default_response​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-http_server_default_response","content":"The page that is shown by default when you access the ClickHouse HTTP(s) server. The default value is “Ok.” (with a line feed at the end) Example Opens https://tabix.io/ when accessing http://localhost: http_port. &lt;http_server_default_response&gt; &lt;![CDATA[&lt;html ng-app=&quot;SMI2&quot;&gt;&lt;head&gt;&lt;base href=&quot;http://ui.tabix.io/&quot;&gt;&lt;/head&gt;&lt;body&gt;&lt;div ui-view=&quot;&quot; class=&quot;content-ui&quot;&gt;&lt;/div&gt;&lt;script src=&quot;http://loader.tabix.io/master.js&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;]]&gt; &lt;/http_server_default_response&gt;  "},{"title":"hsts_max_age​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#hsts-max-age","content":"Expired time for HSTS in seconds. The default value is 0 means clickhouse disabled HSTS. If you set a positive number, the HSTS will be enabled and the max-age is the number you set. Example &lt;hsts_max_age&gt;600000&lt;/hsts_max_age&gt;  "},{"title":"include_from​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-include_from","content":"The path to the file with substitutions. For more information, see the section “Configuration files”. Example &lt;include_from&gt;/etc/metrica.xml&lt;/include_from&gt;  "},{"title":"interserver_http_port​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#interserver-http-port","content":"Port for exchanging data between ClickHouse servers. Example &lt;interserver_http_port&gt;9009&lt;/interserver_http_port&gt;  "},{"title":"interserver_http_host​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#interserver-http-host","content":"The hostname that can be used by other servers to access this server. If omitted, it is defined in the same way as the hostname-f command. Useful for breaking away from a specific network interface. Example &lt;interserver_http_host&gt;example.clickhouse.com&lt;/interserver_http_host&gt;  "},{"title":"interserver_https_port​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#interserver-https-port","content":"Port for exchanging data between ClickHouse servers over HTTPS. Example &lt;interserver_https_port&gt;9010&lt;/interserver_https_port&gt;  "},{"title":"interserver_https_host​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#interserver-https-host","content":"Similar to interserver_http_host, except that this hostname can be used by other servers to access this server over HTTPS. Example &lt;interserver_https_host&gt;example.clickhouse.com&lt;/interserver_https_host&gt;  "},{"title":"interserver_http_credentials​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server-settings-interserver-http-credentials","content":"A username and a password used to connect to other servers during replication. Also the server authenticates other replicas using these credentials. So, interserver_http_credentials must be the same for all replicas in a cluster. By default, if interserver_http_credentials section is omitted, authentication is not used during replication. note interserver_http_credentials settings do not relate to a ClickHouse client credentials configuration. note These credentials are common for replication via HTTP and HTTPS. The section contains the following parameters: user — Username.password — Password.allow_empty — If true, then other replicas are allowed to connect without authentication even if credentials are set. If false, then connections without authentication are refused. Default value: false.old — Contains old user and password used during credential rotation. Several old sections can be specified. Credentials Rotation ClickHouse supports dynamic interserver credentials rotation without stopping all replicas at the same time to update their configuration. Credentials can be changed in several steps. To enable authentication, set interserver_http_credentials.allow_empty to true and add credentials. This allows connections with authentication and without it. &lt;interserver_http_credentials&gt; &lt;user&gt;admin&lt;/user&gt; &lt;password&gt;111&lt;/password&gt; &lt;allow_empty&gt;true&lt;/allow_empty&gt; &lt;/interserver_http_credentials&gt;  After configuring all replicas set allow_empty to false or remove this setting. It makes authentication with new credentials mandatory. To change existing credentials, move the username and the password to interserver_http_credentials.old section and update user and password with new values. At this point the server uses new credentials to connect to other replicas and accepts connections with either new or old credentials. &lt;interserver_http_credentials&gt; &lt;user&gt;admin&lt;/user&gt; &lt;password&gt;222&lt;/password&gt; &lt;old&gt; &lt;user&gt;admin&lt;/user&gt; &lt;password&gt;111&lt;/password&gt; &lt;/old&gt; &lt;old&gt; &lt;user&gt;temp&lt;/user&gt; &lt;password&gt;000&lt;/password&gt; &lt;/old&gt; &lt;/interserver_http_credentials&gt;  When new credentials are applied to all replicas, old credentials may be removed. "},{"title":"keep_alive_timeout​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#keep-alive-timeout","content":"The number of seconds that ClickHouse waits for incoming requests before closing the connection. Defaults to 10 seconds. Example &lt;keep_alive_timeout&gt;10&lt;/keep_alive_timeout&gt;  "},{"title":"listen_host​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-listen_host","content":"Restriction on hosts that requests can come from. If you want the server to answer all of them, specify ::. Examples: &lt;listen_host&gt;::1&lt;/listen_host&gt; &lt;listen_host&gt;127.0.0.1&lt;/listen_host&gt;  "},{"title":"listen_backlog​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-listen_backlog","content":"Backlog (queue size of pending connections) of the listen socket. Default value: 4096 (as in linux 5.4+). Usually this value does not need to be changed, since: default value is large enough,and for accepting client's connections server has separate thread. So even if you have TcpExtListenOverflows (from nstat) non zero and this counter grows for ClickHouse server it does not mean that this value need to be increased, since: usually if 4096 is not enough it shows some internal ClickHouse scaling issue, so it is better to report an issue.and it does not mean that the server can handle more connections later (and even if it could, by that moment clients may be gone or disconnected). Examples: &lt;listen_backlog&gt;4096&lt;/listen_backlog&gt;  "},{"title":"logger​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-logger","content":"Logging settings. Keys: level – Logging level. Acceptable values: trace, debug, information, warning, error.log – The log file. Contains all the entries according to level.errorlog – Error log file.size – Size of the file. Applies to log and errorlog. Once the file reaches size, ClickHouse archives and renames it, and creates a new log file in its place.count – The number of archived log files that ClickHouse stores. Example &lt;logger&gt; &lt;level&gt;trace&lt;/level&gt; &lt;log&gt;/var/log/clickhouse-server/clickhouse-server.log&lt;/log&gt; &lt;errorlog&gt;/var/log/clickhouse-server/clickhouse-server.err.log&lt;/errorlog&gt; &lt;size&gt;1000M&lt;/size&gt; &lt;count&gt;10&lt;/count&gt; &lt;/logger&gt;  Writing to the syslog is also supported. Config example: &lt;logger&gt; &lt;use_syslog&gt;1&lt;/use_syslog&gt; &lt;syslog&gt; &lt;address&gt;syslog.remote:10514&lt;/address&gt; &lt;hostname&gt;myhost.local&lt;/hostname&gt; &lt;facility&gt;LOG_LOCAL6&lt;/facility&gt; &lt;format&gt;syslog&lt;/format&gt; &lt;/syslog&gt; &lt;/logger&gt;  Keys for syslog: use_syslog — Required setting if you want to write to the syslog.address — The host[:port] of syslogd. If omitted, the local daemon is used.hostname — Optional. The name of the host that logs are sent from.facility — The syslog facility keyword in uppercase letters with the “LOG_” prefix: (LOG_USER, LOG_DAEMON, LOG_LOCAL3, and so on). Default value: LOG_USER if address is specified, LOG_DAEMON otherwise.format – Message format. Possible values: bsd and syslog. "},{"title":"send_crash_reports​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-send_crash_reports","content":"Settings for opt-in sending crash reports to the ClickHouse core developers team via Sentry. Enabling it, especially in pre-production environments, is highly appreciated. The server will need access to the public Internet via IPv4 (at the time of writing IPv6 is not supported by Sentry) for this feature to be functioning properly. Keys: enabled – Boolean flag to enable the feature, false by default. Set to true to allow sending crash reports.endpoint – You can override the Sentry endpoint URL for sending crash reports. It can be either a separate Sentry account or your self-hosted Sentry instance. Use the Sentry DSN syntax.anonymize - Avoid attaching the server hostname to the crash report.http_proxy - Configure HTTP proxy for sending crash reports.debug - Sets the Sentry client into debug mode.tmp_path - Filesystem path for temporary crash report state. Recommended way to use &lt;send_crash_reports&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/send_crash_reports&gt;  "},{"title":"macros​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#macros","content":"Parameter substitutions for replicated tables. Can be omitted if replicated tables are not used. For more information, see the section Creating replicated tables. Example &lt;macros incl=&quot;macros&quot; optional=&quot;true&quot; /&gt;  "},{"title":"mark_cache_size​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server-mark-cache-size","content":"Approximate size (in bytes) of the cache of marks used by table engines of the MergeTree family. The cache is shared for the server and memory is allocated as needed. Example &lt;mark_cache_size&gt;5368709120&lt;/mark_cache_size&gt;  "},{"title":"max_server_memory_usage​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#max_server_memory_usage","content":"Limits total RAM usage by the ClickHouse server. Possible values: Positive integer.0 — Auto. Default value: 0. Additional Info The default max_server_memory_usage value is calculated as memory_amount * max_server_memory_usage_to_ram_ratio. See also max_memory_usagemax_server_memory_usage_to_ram_ratio "},{"title":"max_server_memory_usage_to_ram_ratio​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#max_server_memory_usage_to_ram_ratio","content":"Defines the fraction of total physical RAM amount, available to the ClickHouse server. If the server tries to utilize more, the memory is cut down to the appropriate amount. Possible values: Positive double.0 — The ClickHouse server can use all available RAM. Default value: 0.9. Usage On hosts with low RAM and swap, you possibly need setting max_server_memory_usage_to_ram_ratio larger than 1. Example &lt;max_server_memory_usage_to_ram_ratio&gt;0.9&lt;/max_server_memory_usage_to_ram_ratio&gt;  See Also max_server_memory_usage "},{"title":"max_concurrent_queries​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#max-concurrent-queries","content":"The maximum number of simultaneously processed queries related to MergeTree table. Queries may be limited by other settings: max_concurrent_insert_queries, max_concurrent_select_queries, max_concurrent_queries_for_user, max_concurrent_queries_for_all_users, min_marks_to_honor_max_concurrent_queries. note These settings can be modified at runtime and will take effect immediately. Queries that are already running will remain unchanged. Possible values: Positive integer.0 — No limit. Default value: 100. Example &lt;max_concurrent_queries&gt;100&lt;/max_concurrent_queries&gt;  "},{"title":"max_concurrent_insert_queries​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#max-concurrent-insert-queries","content":"The maximum number of simultaneously processed INSERT queries. note These settings can be modified at runtime and will take effect immediately. Queries that are already running will remain unchanged. Possible values: Positive integer.0 — No limit. Default value: 0. Example &lt;max_concurrent_insert_queries&gt;100&lt;/max_concurrent_insert_queries&gt;  "},{"title":"max_concurrent_select_queries​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#max-concurrent-select-queries","content":"The maximum number of simultaneously processed SELECT queries. note These settings can be modified at runtime and will take effect immediately. Queries that are already running will remain unchanged. Possible values: Positive integer.0 — No limit. Default value: 0. Example &lt;max_concurrent_select_queries&gt;100&lt;/max_concurrent_select_queries&gt;  "},{"title":"max_concurrent_queries_for_user​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#max-concurrent-queries-for-user","content":"The maximum number of simultaneously processed queries related to MergeTree table per user. Possible values: Positive integer.0 — No limit. Default value: 0. Example &lt;max_concurrent_queries_for_user&gt;5&lt;/max_concurrent_queries_for_user&gt;  "},{"title":"max_concurrent_queries_for_all_users​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#max-concurrent-queries-for-all-users","content":"Throw exception if the value of this setting is less or equal than the current number of simultaneously processed queries. Example: max_concurrent_queries_for_all_users can be set to 99 for all users and database administrator can set it to 100 for itself to run queries for investigation even when the server is overloaded. Modifying the setting for one query or user does not affect other queries. Possible values: Positive integer.0 — No limit. Default value: 0. Example &lt;max_concurrent_queries_for_all_users&gt;99&lt;/max_concurrent_queries_for_all_users&gt;  See Also max_concurrent_queries "},{"title":"min_marks_to_honor_max_concurrent_queries​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#min-marks-to-honor-max-concurrent-queries","content":"The minimal number of marks read by the query for applying the max_concurrent_queries setting. Possible values: Positive integer.0 — Disabled. Example &lt;min_marks_to_honor_max_concurrent_queries&gt;10&lt;/min_marks_to_honor_max_concurrent_queries&gt;  "},{"title":"max_connections​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#max-connections","content":"The maximum number of inbound connections. Example &lt;max_connections&gt;4096&lt;/max_connections&gt;  "},{"title":"max_open_files​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#max-open-files","content":"The maximum number of open files. By default: maximum. We recommend using this option in Mac OS X since the getrlimit() function returns an incorrect value. Example &lt;max_open_files&gt;262144&lt;/max_open_files&gt;  "},{"title":"max_table_size_to_drop​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#max-table-size-to-drop","content":"Restriction on deleting tables. If the size of a MergeTree table exceeds max_table_size_to_drop (in bytes), you can’t delete it using a DROP query. If you still need to delete the table without restarting the ClickHouse server, create the &lt;clickhouse-path&gt;/flags/force_drop_table file and run the DROP query. Default value: 50 GB. The value 0 means that you can delete all tables without any restrictions. Example &lt;max_table_size_to_drop&gt;0&lt;/max_table_size_to_drop&gt;  "},{"title":"max_thread_pool_size​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#max-thread-pool-size","content":"ClickHouse uses threads from the Global Thread pool to process queries. If there is no idle thread to process a query, then a new thread is created in the pool. max_thread_pool_size limits the maximum number of threads in the pool. Possible values: Positive integer. Default value: 10000. Example &lt;max_thread_pool_size&gt;12000&lt;/max_thread_pool_size&gt;  "},{"title":"max_thread_pool_free_size​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#max-thread-pool-free-size","content":"If the number of idle threads in the Global Thread pool is greater than max_thread_pool_free_size, then ClickHouse releases resources occupied by some threads and the pool size is decreased. Threads can be created again if necessary. Possible values: Positive integer. Default value: 1000. Example &lt;max_thread_pool_free_size&gt;1200&lt;/max_thread_pool_free_size&gt;  "},{"title":"thread_pool_queue_size​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#thread-pool-queue-size","content":"The maximum number of jobs that can be scheduled on the Global Thread pool. Increasing queue size leads to larger memory usage. It is recommended to keep this value equal to max_thread_pool_size. Possible values: Positive integer. Default value: 10000. Example &lt;thread_pool_queue_size&gt;12000&lt;/thread_pool_queue_size&gt;  "},{"title":"merge_tree​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-merge_tree","content":"Fine tuning for tables in the MergeTree. For more information, see the MergeTreeSettings.h header file. Example &lt;merge_tree&gt; &lt;max_suspicious_broken_parts&gt;5&lt;/max_suspicious_broken_parts&gt; &lt;/merge_tree&gt;  "},{"title":"metric_log​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#metric_log","content":"It is enabled by default. If it`s not, you can do this manually. Enabling To manually turn on metrics history collection system.metric_log, create /etc/clickhouse-server/config.d/metric_log.xml with the following content: &lt;clickhouse&gt; &lt;metric_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;metric_log&lt;/table&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;collect_interval_milliseconds&gt;1000&lt;/collect_interval_milliseconds&gt; &lt;/metric_log&gt; &lt;/clickhouse&gt;  Disabling To disable metric_log setting, you should create the following file /etc/clickhouse-server/config.d/disable_metric_log.xml with the following content: &lt;clickhouse&gt; &lt;metric_log remove=&quot;1&quot; /&gt; &lt;/clickhouse&gt;  "},{"title":"replicated_merge_tree​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-replicated_merge_tree","content":"Fine tuning for tables in the ReplicatedMergeTree. This setting has a higher priority. For more information, see the MergeTreeSettings.h header file. Example &lt;replicated_merge_tree&gt; &lt;max_suspicious_broken_parts&gt;5&lt;/max_suspicious_broken_parts&gt; &lt;/replicated_merge_tree&gt;  "},{"title":"openSSL​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-openssl","content":"SSL client/server configuration. Support for SSL is provided by the libpoco library. The interface is described in the file SSLManager.h Keys for server/client settings: privateKeyFile – The path to the file with the secret key of the PEM certificate. The file may contain a key and certificate at the same time.certificateFile – The path to the client/server certificate file in PEM format. You can omit it if privateKeyFile contains the certificate.caConfig – The path to the file or directory that contains trusted root certificates.verificationMode – The method for checking the node’s certificates. Details are in the description of the Context class. Possible values: none, relaxed, strict, once.verificationDepth – The maximum length of the verification chain. Verification will fail if the certificate chain length exceeds the set value.loadDefaultCAFile – Indicates that built-in CA certificates for OpenSSL will be used. Acceptable values: true, false. |cipherList – Supported OpenSSL encryptions. For example: ALL:!ADH:!LOW:!EXP:!MD5:@STRENGTH.cacheSessions – Enables or disables caching sessions. Must be used in combination with sessionIdContext. Acceptable values: true, false.sessionIdContext – A unique set of random characters that the server appends to each generated identifier. The length of the string must not exceed SSL_MAX_SSL_SESSION_ID_LENGTH. This parameter is always recommended since it helps avoid problems both if the server caches the session and if the client requested caching. Default value: ${application.name}.sessionCacheSize – The maximum number of sessions that the server caches. Default value: 1024*20. 0 – Unlimited sessions.sessionTimeout – Time for caching the session on the server.extendedVerification – Automatically extended verification of certificates after the session ends. Acceptable values: true, false.requireTLSv1 – Require a TLSv1 connection. Acceptable values: true, false.requireTLSv1_1 – Require a TLSv1.1 connection. Acceptable values: true, false.requireTLSv1_2 – Require a TLSv1.2 connection. Acceptable values: true, false.fips – Activates OpenSSL FIPS mode. Supported if the library’s OpenSSL version supports FIPS.privateKeyPassphraseHandler – Class (PrivateKeyPassphraseHandler subclass) that requests the passphrase for accessing the private key. For example: &lt;privateKeyPassphraseHandler&gt;, &lt;name&gt;KeyFileHandler&lt;/name&gt;, &lt;options&gt;&lt;password&gt;test&lt;/password&gt;&lt;/options&gt;, &lt;/privateKeyPassphraseHandler&gt;.invalidCertificateHandler – Class (a subclass of CertificateHandler) for verifying invalid certificates. For example: &lt;invalidCertificateHandler&gt; &lt;name&gt;ConsoleCertificateHandler&lt;/name&gt; &lt;/invalidCertificateHandler&gt; .disableProtocols – Protocols that are not allowed to use.preferServerCiphers – Preferred server ciphers on the client. Example of settings: &lt;openSSL&gt; &lt;server&gt; &lt;!-- openssl req -subj &quot;/CN=localhost&quot; -new -newkey rsa:2048 -days 365 -nodes -x509 -keyout /etc/clickhouse-server/server.key -out /etc/clickhouse-server/server.crt --&gt; &lt;certificateFile&gt;/etc/clickhouse-server/server.crt&lt;/certificateFile&gt; &lt;privateKeyFile&gt;/etc/clickhouse-server/server.key&lt;/privateKeyFile&gt; &lt;!-- openssl dhparam -out /etc/clickhouse-server/dhparam.pem 4096 --&gt; &lt;dhParamsFile&gt;/etc/clickhouse-server/dhparam.pem&lt;/dhParamsFile&gt; &lt;verificationMode&gt;none&lt;/verificationMode&gt; &lt;loadDefaultCAFile&gt;true&lt;/loadDefaultCAFile&gt; &lt;cacheSessions&gt;true&lt;/cacheSessions&gt; &lt;disableProtocols&gt;sslv2,sslv3&lt;/disableProtocols&gt; &lt;preferServerCiphers&gt;true&lt;/preferServerCiphers&gt; &lt;/server&gt; &lt;client&gt; &lt;loadDefaultCAFile&gt;true&lt;/loadDefaultCAFile&gt; &lt;cacheSessions&gt;true&lt;/cacheSessions&gt; &lt;disableProtocols&gt;sslv2,sslv3&lt;/disableProtocols&gt; &lt;preferServerCiphers&gt;true&lt;/preferServerCiphers&gt; &lt;!-- Use for self-signed: &lt;verificationMode&gt;none&lt;/verificationMode&gt; --&gt; &lt;invalidCertificateHandler&gt; &lt;!-- Use for self-signed: &lt;name&gt;AcceptCertificateHandler&lt;/name&gt; --&gt; &lt;name&gt;RejectCertificateHandler&lt;/name&gt; &lt;/invalidCertificateHandler&gt; &lt;/client&gt; &lt;/openSSL&gt;  "},{"title":"part_log​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-part-log","content":"Logging events that are associated with MergeTree. For instance, adding or merging data. You can use the log to simulate merge algorithms and compare their characteristics. You can visualize the merge process. Queries are logged in the system.part_log table, not in a separate file. You can configure the name of this table in the table parameter (see below). Use the following parameters to configure logging: database – Name of the database.table – Name of the system table.partition_by — Custom partitioning key for a system table. Can't be used if engine defined.engine - MergeTree Engine Definition for a system table. Can't be used if partition_by defined.flush_interval_milliseconds – Interval for flushing data from the buffer in memory to the table. Example &lt;part_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;part_log&lt;/table&gt; &lt;partition_by&gt;toMonday(event_date)&lt;/partition_by&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;/part_log&gt;  "},{"title":"path​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-path","content":"The path to the directory containing data. note The trailing slash is mandatory. Example &lt;path&gt;/var/lib/clickhouse/&lt;/path&gt;  "},{"title":"prometheus​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-prometheus","content":"Exposing metrics data for scraping from Prometheus. Settings: endpoint – HTTP endpoint for scraping metrics by prometheus server. Start from ‘/’.port – Port for endpoint.metrics – Flag that sets to expose metrics from the system.metrics table.events – Flag that sets to expose metrics from the system.events table.asynchronous_metrics – Flag that sets to expose current metrics values from the system.asynchronous_metrics table. Example  &lt;prometheus&gt; &lt;endpoint&gt;/metrics&lt;/endpoint&gt; &lt;port&gt;8001&lt;/port&gt; &lt;metrics&gt;true&lt;/metrics&gt; &lt;events&gt;true&lt;/events&gt; &lt;asynchronous_metrics&gt;true&lt;/asynchronous_metrics&gt; &lt;/prometheus&gt;  "},{"title":"query_log​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-query-log","content":"Setting for logging queries received with the log_queries=1 setting. Queries are logged in the system.query_log table, not in a separate file. You can change the name of the table in the table parameter (see below). Use the following parameters to configure logging: database – Name of the database.table – Name of the system table the queries will be logged in.partition_by — Custom partitioning key for a system table. Can't be used if engine defined.engine - MergeTree Engine Definition for a system table. Can't be used if partition_by defined.flush_interval_milliseconds – Interval for flushing data from the buffer in memory to the table. If the table does not exist, ClickHouse will create it. If the structure of the query log changed when the ClickHouse server was updated, the table with the old structure is renamed, and a new table is created automatically. Example &lt;query_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;query_log&lt;/table&gt; &lt;engine&gt;Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + INTERVAL 30 day&lt;/engine&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;/query_log&gt;  "},{"title":"query_thread_log​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-query_thread_log","content":"Setting for logging threads of queries received with the log_query_threads=1 setting. Queries are logged in the system.query_thread_log table, not in a separate file. You can change the name of the table in the table parameter (see below). Use the following parameters to configure logging: database – Name of the database.table – Name of the system table the queries will be logged in.partition_by — Custom partitioning key for a system table. Can't be used if engine defined.engine - MergeTree Engine Definition for a system table. Can't be used if partition_by defined.flush_interval_milliseconds – Interval for flushing data from the buffer in memory to the table. If the table does not exist, ClickHouse will create it. If the structure of the query thread log changed when the ClickHouse server was updated, the table with the old structure is renamed, and a new table is created automatically. Example &lt;query_thread_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;query_thread_log&lt;/table&gt; &lt;partition_by&gt;toMonday(event_date)&lt;/partition_by&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;/query_thread_log&gt;  "},{"title":"query_views_log​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-query_views_log","content":"Setting for logging views (live, materialized etc) dependant of queries received with the log_query_views=1 setting. Queries are logged in the system.query_views_log table, not in a separate file. You can change the name of the table in the table parameter (see below). Use the following parameters to configure logging: database – Name of the database.table – Name of the system table the queries will be logged in.partition_by — Custom partitioning key for a system table. Can't be used if engine defined.engine - MergeTree Engine Definition for a system table. Can't be used if partition_by defined.flush_interval_milliseconds – Interval for flushing data from the buffer in memory to the table. If the table does not exist, ClickHouse will create it. If the structure of the query views log changed when the ClickHouse server was updated, the table with the old structure is renamed, and a new table is created automatically. Example &lt;query_views_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;query_views_log&lt;/table&gt; &lt;partition_by&gt;toYYYYMM(event_date)&lt;/partition_by&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;/query_views_log&gt;  "},{"title":"text_log​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-text_log","content":"Settings for the text_log system table for logging text messages. Parameters: level — Maximum Message Level (by default Trace) which will be stored in a table.database — Database name.table — Table name.partition_by — Custom partitioning key for a system table. Can't be used if engine defined.engine - MergeTree Engine Definition for a system table. Can't be used if partition_by defined.flush_interval_milliseconds — Interval for flushing data from the buffer in memory to the table. Example &lt;clickhouse&gt; &lt;text_log&gt; &lt;level&gt;notice&lt;/level&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;text_log&lt;/table&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;!-- &lt;partition_by&gt;event_date&lt;/partition_by&gt; --&gt; &lt;engine&gt;Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + INTERVAL 30 day&lt;/engine&gt; &lt;/text_log&gt; &lt;/clickhouse&gt;  "},{"title":"trace_log​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-trace_log","content":"Settings for the trace_log system table operation. Parameters: database — Database for storing a table.table — Table name.partition_by — Custom partitioning key for a system table. Can't be used if engine defined.engine - MergeTree Engine Definition for a system table. Can't be used if partition_by defined.flush_interval_milliseconds — Interval for flushing data from the buffer in memory to the table. The default server configuration file config.xml contains the following settings section: &lt;trace_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;trace_log&lt;/table&gt; &lt;partition_by&gt;toYYYYMM(event_date)&lt;/partition_by&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;/trace_log&gt;  "},{"title":"query_masking_rules​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#query-masking-rules","content":"Regexp-based rules, which will be applied to queries as well as all log messages before storing them in server logs,system.query_log, system.text_log, system.processes tables, and in logs sent to the client. That allows preventing sensitive data leakage from SQL queries (like names, emails, personal identifiers or credit card numbers) to logs. Example &lt;query_masking_rules&gt; &lt;rule&gt; &lt;name&gt;hide SSN&lt;/name&gt; &lt;regexp&gt;(^|\\D)\\d{3}-\\d{2}-\\d{4}($|\\D)&lt;/regexp&gt; &lt;replace&gt;000-00-0000&lt;/replace&gt; &lt;/rule&gt; &lt;/query_masking_rules&gt;  Config fields: name - name for the rule (optional)regexp - RE2 compatible regular expression (mandatory)replace - substitution string for sensitive data (optional, by default - six asterisks) The masking rules are applied to the whole query (to prevent leaks of sensitive data from malformed / non-parsable queries). system.events table have counter QueryMaskingRulesMatch which have an overall number of query masking rules matches. For distributed queries each server have to be configured separately, otherwise, subqueries passed to other nodes will be stored without masking. "},{"title":"remote_servers​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server-settings-remote-servers","content":"Configuration of clusters used by the Distributed table engine and by the cluster table function. Example &lt;remote_servers incl=&quot;clickhouse_remote_servers&quot; /&gt;  For the value of the incl attribute, see the section “Configuration files”. See Also skip_unavailable_shards "},{"title":"timezone​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-timezone","content":"The server’s time zone. Specified as an IANA identifier for the UTC timezone or geographic location (for example, Africa/Abidjan). The time zone is necessary for conversions between String and DateTime formats when DateTime fields are output to text format (printed on the screen or in a file), and when getting DateTime from a string. Besides, the time zone is used in functions that work with the time and date if they didn’t receive the time zone in the input parameters. Example &lt;timezone&gt;Asia/Istanbul&lt;/timezone&gt;  "},{"title":"tcp_port​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-tcp_port","content":"Port for communicating with clients over the TCP protocol. Example &lt;tcp_port&gt;9000&lt;/tcp_port&gt;  "},{"title":"tcp_port_secure​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-tcp_port_secure","content":"TCP port for secure communication with clients. Use it with OpenSSL settings. Possible values Positive integer. Default value &lt;tcp_port_secure&gt;9440&lt;/tcp_port_secure&gt;  "},{"title":"mysql_port​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-mysql_port","content":"Port for communicating with clients over MySQL protocol. Possible values Positive integer. Example &lt;mysql_port&gt;9004&lt;/mysql_port&gt;  "},{"title":"postgresql_port​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-postgresql_port","content":"Port for communicating with clients over PostgreSQL protocol. Possible values Positive integer. Example &lt;postgresql_port&gt;9005&lt;/postgresql_port&gt;  "},{"title":"tmp_path​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#tmp-path","content":"Path to temporary data for processing large queries. note The trailing slash is mandatory. Example &lt;tmp_path&gt;/var/lib/clickhouse/tmp/&lt;/tmp_path&gt;  "},{"title":"tmp_policy​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#tmp-policy","content":"Policy from storage_configuration to store temporary files. If not set, tmp_path is used, otherwise it is ignored. note move_factor is ignored.keep_free_space_bytes is ignored.max_data_part_size_bytes is ignored.Уou must have exactly one volume in that policy. "},{"title":"uncompressed_cache_size​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server-settings-uncompressed_cache_size","content":"Cache size (in bytes) for uncompressed data used by table engines from the MergeTree. There is one shared cache for the server. Memory is allocated on demand. The cache is used if the option use_uncompressed_cache is enabled. The uncompressed cache is advantageous for very short queries in individual cases. Example &lt;uncompressed_cache_size&gt;8589934592&lt;/uncompressed_cache_size&gt;  "},{"title":"user_files_path​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server_configuration_parameters-user_files_path","content":"The directory with user files. Used in the table function file(). Example &lt;user_files_path&gt;/var/lib/clickhouse/user_files/&lt;/user_files_path&gt;  "},{"title":"users_config​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#users-config","content":"Path to the file that contains: User configurations.Access rights.Settings profiles.Quota settings. Example &lt;users_config&gt;users.xml&lt;/users_config&gt;  "},{"title":"zookeeper​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server-settings_zookeeper","content":"Contains settings that allow ClickHouse to interact with a ZooKeeper cluster. ClickHouse uses ZooKeeper for storing metadata of replicas when using replicated tables. If replicated tables are not used, this section of parameters can be omitted. This section contains the following parameters: node — ZooKeeper endpoint. You can set multiple endpoints. For example:  &lt;node index=&quot;1&quot;&gt; &lt;host&gt;example_host&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt;   The `index` attribute specifies the node order when trying to connect to the ZooKeeper cluster.  session_timeout_ms — Maximum timeout for the client session in milliseconds.operation_timeout_ms — Maximum timeout for one operation in milliseconds.root — The znode that is used as the root for znodes used by the ClickHouse server. Optional.identity — User and password, that can be required by ZooKeeper to give access to requested znodes. Optional. Example configuration &lt;zookeeper&gt; &lt;node&gt; &lt;host&gt;example1&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node&gt; &lt;host&gt;example2&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;session_timeout_ms&gt;30000&lt;/session_timeout_ms&gt; &lt;operation_timeout_ms&gt;10000&lt;/operation_timeout_ms&gt; &lt;!-- Optional. Chroot suffix. Should exist. --&gt; &lt;root&gt;/path/to/zookeeper/node&lt;/root&gt; &lt;!-- Optional. Zookeeper digest ACL string. --&gt; &lt;identity&gt;user:password&lt;/identity&gt; &lt;/zookeeper&gt;  See Also ReplicationZooKeeper Programmer’s GuideOptional secured communication between ClickHouse and Zookeeper "},{"title":"use_minimalistic_part_header_in_zookeeper​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server-settings-use_minimalistic_part_header_in_zookeeper","content":"Storage method for data part headers in ZooKeeper. This setting only applies to the MergeTree family. It can be specified: Globally in the merge_tree section of the config.xml file. ClickHouse uses the setting for all the tables on the server. You can change the setting at any time. Existing tables change their behaviour when the setting changes. For each table. When creating a table, specify the corresponding engine setting. The behaviour of an existing table with this setting does not change, even if the global setting changes. Possible values 0 — Functionality is turned off.1 — Functionality is turned on. If use_minimalistic_part_header_in_zookeeper = 1, then replicated tables store the headers of the data parts compactly using a single znode. If the table contains many columns, this storage method significantly reduces the volume of the data stored in Zookeeper. note After applying use_minimalistic_part_header_in_zookeeper = 1, you can’t downgrade the ClickHouse server to a version that does not support this setting. Be careful when upgrading ClickHouse on servers in a cluster. Don’t upgrade all the servers at once. It is safer to test new versions of ClickHouse in a test environment, or on just a few servers of a cluster. Data part headers already stored with this setting can't be restored to their previous (non-compact) representation. Default value: 0. "},{"title":"disable_internal_dns_cache​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server-settings-disable-internal-dns-cache","content":"Disables the internal DNS cache. Recommended for operating ClickHouse in systems with frequently changing infrastructure such as Kubernetes. Default value: 0. "},{"title":"dns_cache_update_period​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server-settings-dns-cache-update-period","content":"The period of updating IP addresses stored in the ClickHouse internal DNS cache (in seconds). The update is performed asynchronously, in a separate system thread. Default value: 15. See also background_schedule_pool_size "},{"title":"distributed_ddl​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#server-settings-distributed_ddl","content":"Manage executing distributed ddl queries (CREATE, DROP, ALTER, RENAME) on cluster. Works only if ZooKeeper is enabled. Example &lt;distributed_ddl&gt; &lt;!-- Path in ZooKeeper to queue with DDL queries --&gt; &lt;path&gt;/clickhouse/task_queue/ddl&lt;/path&gt; &lt;!-- Settings from this profile will be used to execute DDL queries --&gt; &lt;profile&gt;default&lt;/profile&gt; &lt;!-- Controls how much ON CLUSTER queries can be run simultaneously. --&gt; &lt;pool_size&gt;1&lt;/pool_size&gt; &lt;!-- Cleanup settings (active tasks will not be removed) --&gt; &lt;!-- Controls task TTL (default 1 week) --&gt; &lt;task_max_lifetime&gt;604800&lt;/task_max_lifetime&gt; &lt;!-- Controls how often cleanup should be performed (in seconds) --&gt; &lt;cleanup_delay_period&gt;60&lt;/cleanup_delay_period&gt; &lt;!-- Controls how many tasks could be in the queue --&gt; &lt;max_tasks_in_queue&gt;1000&lt;/max_tasks_in_queue&gt; &lt;/distributed_ddl&gt;  "},{"title":"access_control_path​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#access_control_path","content":"Path to a folder where a ClickHouse server stores user and role configurations created by SQL commands. Default value: /var/lib/clickhouse/access/. See also Access Control and Account Management "},{"title":"user_directories​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#user_directories","content":"Section of the configuration file that contains settings: Path to configuration file with predefined users.Path to folder where users created by SQL commands are stored.ZooKeeper node path where users created by SQL commands are stored and replicated (experimental). If this section is specified, the path from users_config and access_control_path won't be used. The user_directories section can contain any number of items, the order of the items means their precedence (the higher the item the higher the precedence). Examples &lt;user_directories&gt; &lt;users_xml&gt; &lt;path&gt;/etc/clickhouse-server/users.xml&lt;/path&gt; &lt;/users_xml&gt; &lt;local_directory&gt; &lt;path&gt;/var/lib/clickhouse/access/&lt;/path&gt; &lt;/local_directory&gt; &lt;/user_directories&gt;  Users, roles, row policies, quotas, and profiles can be also stored in ZooKeeper: &lt;user_directories&gt; &lt;users_xml&gt; &lt;path&gt;/etc/clickhouse-server/users.xml&lt;/path&gt; &lt;/users_xml&gt; &lt;replicated&gt; &lt;zookeeper_path&gt;/clickhouse/access/&lt;/zookeeper_path&gt; &lt;/replicated&gt; &lt;/user_directories&gt;  You can also define sections memory — means storing information only in memory, without writing to disk, and ldap — means storing information on an LDAP server. To add an LDAP server as a remote user directory of users that are not defined locally, define a single ldap section with a following parameters: server — one of LDAP server names defined in ldap_servers config section. This parameter is mandatory and cannot be empty.roles — section with a list of locally defined roles that will be assigned to each user retrieved from the LDAP server. If no roles are specified, user will not be able to perform any actions after authentication. If any of the listed roles is not defined locally at the time of authentication, the authentication attempt will fail as if the provided password was incorrect. Example &lt;ldap&gt; &lt;server&gt;my_ldap_server&lt;/server&gt; &lt;roles&gt; &lt;my_local_role1 /&gt; &lt;my_local_role2 /&gt; &lt;/roles&gt; &lt;/ldap&gt;  "},{"title":"total_memory_profiler_step​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#total-memory-profiler-step","content":"Sets the memory size (in bytes) for a stack trace at every peak allocation step. The data is stored in the system.trace_log system table with query_id equal to an empty string. Possible values: Positive integer. Default value: 4194304. "},{"title":"total_memory_tracker_sample_probability​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#total-memory-tracker-sample-probability","content":"Allows to collect random allocations and deallocations and writes them in the system.trace_log system table with trace_type equal to a MemorySample with the specified probability. The probability is for every allocation or deallocations, regardless of the size of the allocation. Note that sampling happens only when the amount of untracked memory exceeds the untracked memory limit (default value is 4 MiB). It can be lowered if total_memory_profiler_step is lowered. You can set total_memory_profiler_step equal to 1 for extra fine-grained sampling. Possible values: Positive integer.0 — Writing of random allocations and deallocations in the system.trace_log system table is disabled. Default value: 0. "},{"title":"mmap_cache_size​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#mmap-cache-size","content":"Sets the cache size (in bytes) for mapped files. This setting allows to avoid frequent open/mmap/munmap/close calls (which are very expensive due to consequent page faults) and to reuse mappings from several threads and queries. The setting value is the number of mapped regions (usually equal to the number of mapped files). The amount of data in mapped files can be monitored in system.metrics, system.metric_log system tables by the MMappedFiles and MMappedFileBytes metrics, in system.asynchronous_metrics, system.asynchronous_metrics_log by the MMapCacheCells metric, and also in system.events, system.processes, system.query_log, system.query_thread_log, system.query_views_log by the CreatedReadBufferMMap, CreatedReadBufferMMapFailed, MMappedFileCacheHits, MMappedFileCacheMisses events. Note that the amount of data in mapped files does not consume memory directly and is not accounted in query or server memory usage — because this memory can be discarded similar to OS page cache. The cache is dropped (the files are closed) automatically on the removal of old parts in tables of the MergeTree family, also it can be dropped manually by the SYSTEM DROP MMAP CACHE query. Possible values: Positive integer. Default value: 1000. "},{"title":"compiled_expression_cache_size​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#compiled-expression-cache-size","content":"Sets the cache size (in bytes) for compiled expressions. Possible values: Positive integer. Default value: 134217728. "},{"title":"compiled_expression_cache_elements_size​","type":1,"pageTitle":"Server Settings","url":"docs/en/operations/server-configuration-parameters/settings#compiled_expression_cache_elements_size","content":"Sets the cache size (in elements) for compiled expressions. Possible values: Positive integer. Default value: 10000. "},{"title":"events","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/events","content":"events Contains information about the number of events that have occurred in the system. For example, in the table, you can find how many SELECT queries were processed since the ClickHouse server started. Columns: event (String) — Event name.value (UInt64) — Number of events occurred.description (String) — Event description. Example SELECT * FROM system.events LIMIT 5 ┌─event─────────────────────────────────┬─value─┬─description────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ Query │ 12 │ Number of queries to be interpreted and potentially executed. Does not include queries that failed to parse or were rejected due to AST size limits, quota limits or limits on the number of simultaneously running queries. May include internal queries initiated by ClickHouse itself. Does not count subqueries. │ │ SelectQuery │ 8 │ Same as Query, but only for SELECT queries. │ │ FileOpen │ 73 │ Number of files opened. │ │ ReadBufferFromFileDescriptorRead │ 155 │ Number of reads (read/pread) from a file descriptor. Does not include sockets. │ │ ReadBufferFromFileDescriptorReadBytes │ 9931 │ Number of bytes read from file descriptors. If the file is compressed, this will show the compressed data size. │ └───────────────────────────────────────┴───────┴────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ See Also system.asynchronous_metrics — Contains periodically calculated metrics.system.metrics — Contains instantly calculated metrics.system.metric_log — Contains a history of metrics values from tables system.metrics и system.events.Monitoring — Base concepts of ClickHouse monitoring. Original article","keywords":""},{"title":"detached_parts","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/detached_parts","content":"detached_parts Contains information about detached parts of MergeTree tables. The reason column specifies why the part was detached. For user-detached parts, the reason is empty. Such parts can be attached with ALTER TABLE ATTACH PARTITION|PART command. For the description of other columns, see system.parts. If part name is invalid, values of some columns may be NULL. Such parts can be deleted with ALTER TABLE DROP DETACHED PART. Original article","keywords":""},{"title":"enabled_roles","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/enabled-roles","content":"enabled_roles Contains all active roles at the moment, including current role of the current user and granted roles for current role. Columns: role_name (String)) — Role name.with_admin_option (UInt8) — Flag that shows whether enabled_role is a role with ADMIN OPTION privilege.is_current (UInt8) — Flag that shows whether enabled_role is a current role of a current user.is_default (UInt8) — Flag that shows whether enabled_role is a default role. Original article","keywords":""},{"title":"distributed_ddl_queue","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/distributed_ddl_queue","content":"distributed_ddl_queue Contains information about distributed ddl queries (ON CLUSTER clause) that were executed on a cluster. Columns: entry (String) — Query id.host_name (String) — Hostname.host_address (String) — IP address that the Hostname resolves to.port (UInt16) — Host Port.status (Enum8) — Status of the query.cluster (String) — Cluster name.query (String) — Query executed.initiator (String) — Node that executed the query.query_start_time (DateTime) — Query start time.query_finish_time (DateTime) — Query finish time.query_duration_ms (UInt64) — Duration of query execution (in milliseconds).exception_code (Enum8) — Exception code from ZooKeeper. Example SELECT * FROM system.distributed_ddl_queue WHERE cluster = 'test_cluster' LIMIT 2 FORMAT Vertical Query id: f544e72a-6641-43f1-836b-24baa1c9632a Row 1: ────── entry: query-0000000000 host_name: clickhouse01 host_address: 172.23.0.11 port: 9000 status: Finished cluster: test_cluster query: CREATE DATABASE test_db UUID '4a82697e-c85e-4e5b-a01e-a36f2a758456' ON CLUSTER test_cluster initiator: clickhouse01:9000 query_start_time: 2020-12-30 13:07:51 query_finish_time: 2020-12-30 13:07:51 query_duration_ms: 6 exception_code: ZOK Row 2: ────── entry: query-0000000000 host_name: clickhouse02 host_address: 172.23.0.12 port: 9000 status: Finished cluster: test_cluster query: CREATE DATABASE test_db UUID '4a82697e-c85e-4e5b-a01e-a36f2a758456' ON CLUSTER test_cluster initiator: clickhouse01:9000 query_start_time: 2020-12-30 13:07:51 query_finish_time: 2020-12-30 13:07:51 query_duration_ms: 6 exception_code: ZOK 2 rows in set. Elapsed: 0.025 sec. Original article","keywords":""},{"title":"grants","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/grants","content":"grants Privileges granted to ClickHouse user accounts. Columns: user_name (Nullable(String)) — User name. role_name (Nullable(String)) — Role assigned to user account. access_type (Enum8) — Access parameters for ClickHouse user account. database (Nullable(String)) — Name of a database. table (Nullable(String)) — Name of a table. column (Nullable(String)) — Name of a column to which access is granted. is_partial_revoke (UInt8) — Logical value. It shows whether some privileges have been revoked. Possible values: 0 — The row describes a partial revoke. 1 — The row describes a grant. grant_option (UInt8) — Permission is granted WITH GRANT OPTION, see GRANT. Original article","keywords":""},{"title":"graphite_retentions","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/graphite_retentions","content":"graphite_retentions Contains information about parameters graphite_rollup which are used in tables with *GraphiteMergeTree engines. Columns: config_name (String) - graphite_rollup parameter name.regexp (String) - A pattern for the metric name.function (String) - The name of the aggregating function.age (UInt64) - The minimum age of the data in seconds.precision (UInt64) - How precisely to define the age of the data in seconds.priority (UInt16) - Pattern priority.is_default (UInt8) - Whether the pattern is the default.Tables.database (Array(String)) - Array of names of database tables that use the config_name parameter.Tables.table (Array(String)) - Array of table names that use the config_name parameter. Original article","keywords":""},{"title":"licenses","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/licenses","content":"licenses Сontains licenses of third-party libraries that are located in the contrib directory of ClickHouse sources. Columns: library_name (String) — Name of the library, which is license connected with.license_type (String) — License type — e.g. Apache, MIT.license_path (String) — Path to the file with the license text.license_text (String) — License text. Example SELECT library_name, license_type, license_path FROM system.licenses LIMIT 15 ┌─library_name───────┬─license_type─┬─license_path────────────────────────┐ │ FastMemcpy │ MIT │ /contrib/FastMemcpy/LICENSE │ │ arrow │ Apache │ /contrib/arrow/LICENSE.txt │ │ avro │ Apache │ /contrib/avro/LICENSE.txt │ │ aws-c-common │ Apache │ /contrib/aws-c-common/LICENSE │ │ aws-c-event-stream │ Apache │ /contrib/aws-c-event-stream/LICENSE │ │ aws-checksums │ Apache │ /contrib/aws-checksums/LICENSE │ │ aws │ Apache │ /contrib/aws/LICENSE.txt │ │ base64 │ BSD 2-clause │ /contrib/base64/LICENSE │ │ boost │ Boost │ /contrib/boost/LICENSE_1_0.txt │ │ brotli │ MIT │ /contrib/brotli/LICENSE │ │ capnproto │ MIT │ /contrib/capnproto/LICENSE │ │ cassandra │ Apache │ /contrib/cassandra/LICENSE.txt │ │ cctz │ Apache │ /contrib/cctz/LICENSE.txt │ │ cityhash102 │ MIT │ /contrib/cityhash102/COPYING │ │ cppkafka │ BSD 2-clause │ /contrib/cppkafka/LICENSE │ └────────────────────┴──────────────┴─────────────────────────────────────┘ Original article","keywords":""},{"title":"merge_tree_settings","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/merge_tree_settings","content":"merge_tree_settings Contains information about settings for MergeTree tables. Columns: name (String) — Setting name.value (String) — Setting value.description (String) — Setting description.type (String) — Setting type (implementation specific string value).changed (UInt8) — Whether the setting was explicitly defined in the config or explicitly changed. Example :) SELECT * FROM system.merge_tree_settings LIMIT 4 FORMAT Vertical; Row 1: ────── name: index_granularity value: 8192 changed: 0 description: How many rows correspond to one primary key value. type: SettingUInt64 Row 2: ────── name: min_bytes_for_wide_part value: 0 changed: 0 description: Minimal uncompressed size in bytes to create part in wide format instead of compact type: SettingUInt64 Row 3: ────── name: min_rows_for_wide_part value: 0 changed: 0 description: Minimal number of rows to create part in wide format instead of compact type: SettingUInt64 Row 4: ────── name: merge_max_block_size value: 8192 changed: 0 description: How many rows in blocks should be formed for merge operations. type: SettingUInt64 4 rows in set. Elapsed: 0.001 sec. Original article","keywords":""},{"title":"merges","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/merges","content":"merges Contains information about merges and part mutations currently in process for tables in the MergeTree family. Columns: database (String) — The name of the database the table is in.table (String) — Table name.elapsed (Float64) — The time elapsed (in seconds) since the merge started.progress (Float64) — The percentage of completed work from 0 to 1.num_parts (UInt64) — The number of pieces to be merged.result_part_name (String) — The name of the part that will be formed as the result of merging.is_mutation (UInt8) — 1 if this process is a part mutation.total_size_bytes_compressed (UInt64) — The total size of the compressed data in the merged chunks.total_size_marks (UInt64) — The total number of marks in the merged parts.bytes_read_uncompressed (UInt64) — Number of bytes read, uncompressed.rows_read (UInt64) — Number of rows read.bytes_written_uncompressed (UInt64) — Number of bytes written, uncompressed.rows_written (UInt64) — Number of rows written.memory_usage (UInt64) — Memory consumption of the merge process.thread_id (UInt64) — Thread ID of the merge process.merge_type — The type of current merge. Empty if it's an mutation.merge_algorithm — The algorithm used in current merge. Empty if it's an mutation. Original article","keywords":""},{"title":"metrics","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/metrics","content":"metrics Contains metrics which can be calculated instantly, or have a current value. For example, the number of simultaneously processed queries or the current replica delay. This table is always up to date. Columns: metric (String) — Metric name.value (Int64) — Metric value.description (String) — Metric description. The list of supported metrics you can find in the src/Common/CurrentMetrics.cpp source file of ClickHouse. Example SELECT * FROM system.metrics LIMIT 10 ┌─metric─────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ Query │ 1 │ Number of executing queries │ │ Merge │ 0 │ Number of executing background merges │ │ PartMutation │ 0 │ Number of mutations (ALTER DELETE/UPDATE) │ │ ReplicatedFetch │ 0 │ Number of data parts being fetched from replicas │ │ ReplicatedSend │ 0 │ Number of data parts being sent to replicas │ │ ReplicatedChecks │ 0 │ Number of data parts checking for consistency │ │ BackgroundPoolTask │ 0 │ Number of active tasks in BackgroundProcessingPool (merges, mutations, fetches, or replication queue bookkeeping) │ │ BackgroundSchedulePoolTask │ 0 │ Number of active tasks in BackgroundSchedulePool. This pool is used for periodic ReplicatedMergeTree tasks, like cleaning old data parts, altering data parts, replica re-initialization, etc. │ │ DiskSpaceReservedForMerge │ 0 │ Disk space reserved for currently running background merges. It is slightly more than the total size of currently merging parts. │ │ DistributedSend │ 0 │ Number of connections to remote servers sending data that was INSERTed into Distributed tables. Both synchronous and asynchronous mode. │ └────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ See Also system.asynchronous_metrics — Contains periodically calculated metrics.system.events — Contains a number of events that occurred.system.metric_log — Contains a history of metrics values from tables system.metrics and system.events.Monitoring — Base concepts of ClickHouse monitoring. Original article","keywords":""},{"title":"mutations","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/mutations","content":"mutations The table contains information about mutations of MergeTree tables and their progress. Each mutation command is represented by a single row. Columns: database (String) — The name of the database to which the mutation was applied. table (String) — The name of the table to which the mutation was applied. mutation_id (String) — The ID of the mutation. For replicated tables these IDs correspond to znode names in the &lt;table_path_in_zookeeper&gt;/mutations/ directory in ZooKeeper. For non-replicated tables the IDs correspond to file names in the data directory of the table. command (String) — The mutation command string (the part of the query after ALTER TABLE [db.]table). create_time (Datetime) — Date and time when the mutation command was submitted for execution. block_numbers.partition_id (Array(String)) — For mutations of replicated tables, the array contains the partitions' IDs (one record for each partition). For mutations of non-replicated tables the array is empty. block_numbers.number (Array(Int64)) — For mutations of replicated tables, the array contains one record for each partition, with the block number that was acquired by the mutation. Only parts that contain blocks with numbers less than this number will be mutated in the partition. In non-replicated tables, block numbers in all partitions form a single sequence. This means that for mutations of non-replicated tables, the column will contain one record with a single block number acquired by the mutation. parts_to_do_names (Array(String)) — An array of names of data parts that need to be mutated for the mutation to complete. parts_to_do (Int64) — The number of data parts that need to be mutated for the mutation to complete. is_done (UInt8) — The flag whether the mutation is done or not. Possible values: 1 if the mutation is completed,0 if the mutation is still in process. note Even if parts_to_do = 0 it is possible that a mutation of a replicated table is not completed yet because of a long-running INSERT query, that will create a new data part needed to be mutated. If there were problems with mutating some data parts, the following columns contain additional information: latest_failed_part (String) — The name of the most recent part that could not be mutated. latest_fail_time (Datetime) — The date and time of the most recent part mutation failure. latest_fail_reason (String) — The exception message that caused the most recent part mutation failure. See Also MutationsMergeTree table engineReplicatedMergeTree family Original article","keywords":""},{"title":"one","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/one","content":"one This table contains a single row with a single dummy UInt8 column containing the value 0. This table is used if a SELECT query does not specify the FROM clause. This is similar to the DUAL table found in other DBMSs. Example :) SELECT * FROM system.one LIMIT 10; ┌─dummy─┐ │ 0 │ └───────┘ 1 rows in set. Elapsed: 0.001 sec. Original article","keywords":""},{"title":"metric_log","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/metric_log","content":"metric_log Contains history of metrics values from tables system.metrics and system.events, periodically flushed to disk. Columns: event_date (Date) — Event date.event_time (DateTime) — Event time.event_time_microseconds (DateTime64) — Event time with microseconds resolution. Example SELECT * FROM system.metric_log LIMIT 1 FORMAT Vertical; Row 1: ────── event_date: 2020-09-05 event_time: 2020-09-05 16:22:33 event_time_microseconds: 2020-09-05 16:22:33.196807 milliseconds: 196 ProfileEvent_Query: 0 ProfileEvent_SelectQuery: 0 ProfileEvent_InsertQuery: 0 ProfileEvent_FailedQuery: 0 ProfileEvent_FailedSelectQuery: 0 ... ... CurrentMetric_Revision: 54439 CurrentMetric_VersionInteger: 20009001 CurrentMetric_RWLockWaitingReaders: 0 CurrentMetric_RWLockWaitingWriters: 0 CurrentMetric_RWLockActiveReaders: 0 CurrentMetric_RWLockActiveWriters: 0 CurrentMetric_GlobalThread: 74 CurrentMetric_GlobalThreadActive: 26 CurrentMetric_LocalThread: 0 CurrentMetric_LocalThreadActive: 0 CurrentMetric_DistributedFilesToInsert: 0 See also metric_log setting — Enabling and disabling the setting.system.asynchronous_metrics — Contains periodically calculated metrics.system.events — Contains a number of events that occurred.system.metrics — Contains instantly calculated metrics.Monitoring — Base concepts of ClickHouse monitoring. Original article","keywords":""},{"title":"parts_columns","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/parts_columns","content":"parts_columns Contains information about parts and columns of MergeTree tables. Each row describes one data part. Columns: partition (String) — The partition name. To learn what a partition is, see the description of the ALTER query. Formats: YYYYMM for automatic partitioning by month.any_string when partitioning manually. name (String) — Name of the data part. part_type (String) — The data part storing format. Possible values: Wide — Each column is stored in a separate file in a filesystem.Compact — All columns are stored in one file in a filesystem. Data storing format is controlled by the min_bytes_for_wide_part and min_rows_for_wide_part settings of the MergeTree table. active (UInt8) — Flag that indicates whether the data part is active. If a data part is active, it’s used in a table. Otherwise, it’s deleted. Inactive data parts remain after merging. marks (UInt64) — The number of marks. To get the approximate number of rows in a data part, multiply marks by the index granularity (usually 8192) (this hint does not work for adaptive granularity). rows (UInt64) — The number of rows. bytes_on_disk (UInt64) — Total size of all the data part files in bytes. data_compressed_bytes (UInt64) — Total size of compressed data in the data part. All the auxiliary files (for example, files with marks) are not included. data_uncompressed_bytes (UInt64) — Total size of uncompressed data in the data part. All the auxiliary files (for example, files with marks) are not included. marks_bytes (UInt64) — The size of the file with marks. modification_time (DateTime) — The time the directory with the data part was modified. This usually corresponds to the time of data part creation. remove_time (DateTime) — The time when the data part became inactive. refcount (UInt32) — The number of places where the data part is used. A value greater than 2 indicates that the data part is used in queries or merges. min_date (Date) — The minimum value of the date key in the data part. max_date (Date) — The maximum value of the date key in the data part. partition_id (String) — ID of the partition. min_block_number (UInt64) — The minimum number of data parts that make up the current part after merging. max_block_number (UInt64) — The maximum number of data parts that make up the current part after merging. level (UInt32) — Depth of the merge tree. Zero means that the current part was created by insert rather than by merging other parts. data_version (UInt64) — Number that is used to determine which mutations should be applied to the data part (mutations with a version higher than data_version). primary_key_bytes_in_memory (UInt64) — The amount of memory (in bytes) used by primary key values. primary_key_bytes_in_memory_allocated (UInt64) — The amount of memory (in bytes) reserved for primary key values. database (String) — Name of the database. table (String) — Name of the table. engine (String) — Name of the table engine without parameters. disk_name (String) — Name of a disk that stores the data part. path (String) — Absolute path to the folder with data part files. column (String) — Name of the column. type (String) — Column type. column_position (UInt64) — Ordinal position of a column in a table starting with 1. default_kind (String) — Expression type (DEFAULT, MATERIALIZED, ALIAS) for the default value, or an empty string if it is not defined. default_expression (String) — Expression for the default value, or an empty string if it is not defined. column_bytes_on_disk (UInt64) — Total size of the column in bytes. column_data_compressed_bytes (UInt64) — Total size of compressed data in the column, in bytes. column_data_uncompressed_bytes (UInt64) — Total size of the decompressed data in the column, in bytes. column_marks_bytes (UInt64) — The size of the column with marks, in bytes. bytes (UInt64) — Alias for bytes_on_disk. marks_size (UInt64) — Alias for marks_bytes. Example SELECT * FROM system.parts_columns LIMIT 1 FORMAT Vertical; Row 1: ────── partition: tuple() name: all_1_2_1 part_type: Wide active: 1 marks: 2 rows: 2 bytes_on_disk: 155 data_compressed_bytes: 56 data_uncompressed_bytes: 4 marks_bytes: 96 modification_time: 2020-09-23 10:13:36 remove_time: 2106-02-07 06:28:15 refcount: 1 min_date: 1970-01-01 max_date: 1970-01-01 partition_id: all min_block_number: 1 max_block_number: 2 level: 1 data_version: 1 primary_key_bytes_in_memory: 2 primary_key_bytes_in_memory_allocated: 64 database: default table: 53r93yleapyears engine: MergeTree disk_name: default path: /var/lib/clickhouse/data/default/53r93yleapyears/all_1_2_1/ column: id type: Int8 column_position: 1 default_kind: default_expression: column_bytes_on_disk: 76 column_data_compressed_bytes: 28 column_data_uncompressed_bytes: 2 column_marks_bytes: 48 See Also MergeTree family Original article","keywords":""},{"title":"numbers","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/numbers","content":"numbers This table contains a single UInt64 column named number that contains almost all the natural numbers starting from zero. You can use this table for tests, or if you need to do a brute force search. Reads from this table are not parallelized. Example :) SELECT * FROM system.numbers LIMIT 10; ┌─number─┐ │ 0 │ │ 1 │ │ 2 │ │ 3 │ │ 4 │ │ 5 │ │ 6 │ │ 7 │ │ 8 │ │ 9 │ └────────┘ 10 rows in set. Elapsed: 0.001 sec. Original article","keywords":""},{"title":"numbers_mt","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/numbers_mt","content":"numbers_mt The same as system.numbers but reads are parallelized. The numbers can be returned in any order. Used for tests. Example :) SELECT * FROM system.numbers_mt LIMIT 10; ┌─number─┐ │ 0 │ │ 1 │ │ 2 │ │ 3 │ │ 4 │ │ 5 │ │ 6 │ │ 7 │ │ 8 │ │ 9 │ └────────┘ 10 rows in set. Elapsed: 0.001 sec. Original article","keywords":""},{"title":"opentelemetry_span_log","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/opentelemetry_span_log","content":"opentelemetry_span_log Contains information about trace spans for executed queries. Columns: trace_id (UUID) — ID of the trace for executed query. span_id (UInt64) — ID of the trace span. parent_span_id (UInt64) — ID of the parent trace span. operation_name (String) — The name of the operation. start_time_us (UInt64) — The start time of the trace span (in microseconds). finish_time_us (UInt64) — The finish time of the trace span (in microseconds). finish_date (Date) — The finish date of the trace span. attribute.names (Array(String)) — Attribute names depending on the trace span. They are filled in according to the recommendations in the OpenTelemetry standard. attribute.values (Array(String)) — Attribute values depending on the trace span. They are filled in according to the recommendations in the OpenTelemetry standard. Example Query: SELECT * FROM system.opentelemetry_span_log LIMIT 1 FORMAT Vertical; Result: Row 1: ────── trace_id: cdab0847-0d62-61d5-4d38-dd65b19a1914 span_id: 701487461015578150 parent_span_id: 2991972114672045096 operation_name: DB::Block DB::InterpreterSelectQuery::getSampleBlockImpl() start_time_us: 1612374594529090 finish_time_us: 1612374594529108 finish_date: 2021-02-03 attribute.names: [] attribute.values: [] See Also OpenTelemetry Original article","keywords":""},{"title":"parts","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/parts","content":"parts Contains information about parts of MergeTree tables. Each row describes one data part. Columns: partition (String) – The partition name. To learn what a partition is, see the description of the ALTER query. Formats: YYYYMM for automatic partitioning by month.any_string when partitioning manually. name (String) – Name of the data part. part_type (String) — The data part storing format. Possible Values: Wide — Each column is stored in a separate file in a filesystem.Compact — All columns are stored in one file in a filesystem. Data storing format is controlled by the min_bytes_for_wide_part and min_rows_for_wide_part settings of the MergeTree table. active (UInt8) – Flag that indicates whether the data part is active. If a data part is active, it’s used in a table. Otherwise, it’s deleted. Inactive data parts remain after merging. marks (UInt64) – The number of marks. To get the approximate number of rows in a data part, multiply marks by the index granularity (usually 8192) (this hint does not work for adaptive granularity). rows (UInt64) – The number of rows. bytes_on_disk (UInt64) – Total size of all the data part files in bytes. data_compressed_bytes (UInt64) – Total size of compressed data in the data part. All the auxiliary files (for example, files with marks) are not included. data_uncompressed_bytes (UInt64) – Total size of uncompressed data in the data part. All the auxiliary files (for example, files with marks) are not included. marks_bytes (UInt64) – The size of the file with marks. secondary_indices_compressed_bytes (UInt64) – Total size of compressed data for secondary indices in the data part. All the auxiliary files (for example, files with marks) are not included. secondary_indices_uncompressed_bytes (UInt64) – Total size of uncompressed data for secondary indices in the data part. All the auxiliary files (for example, files with marks) are not included. secondary_indices_marks_bytes (UInt64) – The size of the file with marks for secondary indices. modification_time (DateTime) – The time the directory with the data part was modified. This usually corresponds to the time of data part creation. remove_time (DateTime) – The time when the data part became inactive. refcount (UInt32) – The number of places where the data part is used. A value greater than 2 indicates that the data part is used in queries or merges. min_date (Date) – The minimum value of the date key in the data part. max_date (Date) – The maximum value of the date key in the data part. min_time (DateTime) – The minimum value of the date and time key in the data part. max_time(DateTime) – The maximum value of the date and time key in the data part. partition_id (String) – ID of the partition. min_block_number (UInt64) – The minimum number of data parts that make up the current part after merging. max_block_number (UInt64) – The maximum number of data parts that make up the current part after merging. level (UInt32) – Depth of the merge tree. Zero means that the current part was created by insert rather than by merging other parts. data_version (UInt64) – Number that is used to determine which mutations should be applied to the data part (mutations with a version higher than data_version). primary_key_bytes_in_memory (UInt64) – The amount of memory (in bytes) used by primary key values. primary_key_bytes_in_memory_allocated (UInt64) – The amount of memory (in bytes) reserved for primary key values. is_frozen (UInt8) – Flag that shows that a partition data backup exists. 1, the backup exists. 0, the backup does not exist. For more details, see FREEZE PARTITION database (String) – Name of the database. table (String) – Name of the table. engine (String) – Name of the table engine without parameters. path (String) – Absolute path to the folder with data part files. disk_name (String) – Name of a disk that stores the data part. hash_of_all_files (String) – sipHash128 of compressed files. hash_of_uncompressed_files (String) – sipHash128 of uncompressed files (files with marks, index file etc.). uncompressed_hash_of_compressed_files (String) – sipHash128 of data in the compressed files as if they were uncompressed. delete_ttl_info_min (DateTime) — The minimum value of the date and time key for TTL DELETE rule. delete_ttl_info_max (DateTime) — The maximum value of the date and time key for TTL DELETE rule. move_ttl_info.expression (Array(String)) — Array of expressions. Each expression defines a TTL MOVE rule. warning The move_ttl_info.expression array is kept mostly for backward compatibility, now the simpliest way to check TTL MOVE rule is to use the move_ttl_info.min and move_ttl_info.max fields. move_ttl_info.min (Array(DateTime)) — Array of date and time values. Each element describes the minimum key value for a TTL MOVE rule. move_ttl_info.max (Array(DateTime)) — Array of date and time values. Each element describes the maximum key value for a TTL MOVE rule. bytes (UInt64) – Alias for bytes_on_disk. marks_size (UInt64) – Alias for marks_bytes. Example SELECT * FROM system.parts LIMIT 1 FORMAT Vertical; Row 1: ────── partition: tuple() name: all_1_4_1_6 part_type: Wide active: 1 marks: 2 rows: 6 bytes_on_disk: 310 data_compressed_bytes: 157 data_uncompressed_bytes: 91 secondary_indices_compressed_bytes: 58 secondary_indices_uncompressed_bytes: 6 secondary_indices_marks_bytes: 48 marks_bytes: 144 modification_time: 2020-06-18 13:01:49 remove_time: 1970-01-01 00:00:00 refcount: 1 min_date: 1970-01-01 max_date: 1970-01-01 min_time: 1970-01-01 00:00:00 max_time: 1970-01-01 00:00:00 partition_id: all min_block_number: 1 max_block_number: 4 level: 1 data_version: 6 primary_key_bytes_in_memory: 8 primary_key_bytes_in_memory_allocated: 64 is_frozen: 0 database: default table: months engine: MergeTree disk_name: default path: /var/lib/clickhouse/data/default/months/all_1_4_1_6/ hash_of_all_files: 2d0657a16d9430824d35e327fcbd87bf hash_of_uncompressed_files: 84950cc30ba867c77a408ae21332ba29 uncompressed_hash_of_compressed_files: 1ad78f1c6843bbfb99a2c931abe7df7d delete_ttl_info_min: 1970-01-01 00:00:00 delete_ttl_info_max: 1970-01-01 00:00:00 move_ttl_info.expression: [] move_ttl_info.min: [] move_ttl_info.max: [] See Also MergeTree familyTTL for Columns and Tables Original article","keywords":""},{"title":"quota_limits","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/quota_limits","content":"quota_limits Contains information about maximums for all intervals of all quotas. Any number of rows or zero can correspond to one quota. Columns: quota_name (String) — Quota name.duration (UInt32) — Length of the time interval for calculating resource consumption, in seconds.is_randomized_interval (UInt8) — Logical value. It shows whether the interval is randomized. Interval always starts at the same time if it is not randomized. For example, an interval of 1 minute always starts at an integer number of minutes (i.e. it can start at 11:20:00, but it never starts at 11:20:01), an interval of one day always starts at midnight UTC. If interval is randomized, the very first interval starts at random time, and subsequent intervals starts one by one. Values:0 — Interval is not randomized.1 — Interval is randomized.max_queries (Nullable(UInt64)) — Maximum number of queries.max_query_selects (Nullable(UInt64)) — Maximum number of select queries.max_query_inserts (Nullable(UInt64)) — Maximum number of insert queries.max_errors (Nullable(UInt64)) — Maximum number of errors.max_result_rows (Nullable(UInt64)) — Maximum number of result rows.max_result_bytes (Nullable(UInt64)) — Maximum number of RAM volume in bytes used to store a queries result.max_read_rows (Nullable(UInt64)) — Maximum number of rows read from all tables and table functions participated in queries.max_read_bytes (Nullable(UInt64)) — Maximum number of bytes read from all tables and table functions participated in queries.max_execution_time (Nullable(Float64)) — Maximum of the query execution time, in seconds. Original article","keywords":""},{"title":"INFORMATION_SCHEMA","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/information_schema","content":"","keywords":""},{"title":"COLUMNS​","type":1,"pageTitle":"INFORMATION_SCHEMA","url":"docs/en/operations/system-tables/information_schema#columns","content":"Contains columns read from the system.columns system table and columns that are not supported in ClickHouse or do not make sense (always NULL), but must be by the standard. Columns: table_catalog (String) — The name of the database in which the table is located.table_schema (String) — The name of the database in which the table is located.table_name (String) — Table name.column_name (String) — Column name.ordinal_position (UInt64) — Ordinal position of a column in a table starting with 1.column_default (String) — Expression for the default value, or an empty string if it is not defined.is_nullable (UInt8) — Flag that indicates whether the column type is Nullable.data_type (String) — Column type.character_maximum_length (Nullable(UInt64)) — Maximum length in bytes for binary data, character data, or text data and images. In ClickHouse makes sense only for FixedString data type. Otherwise, the NULL value is returned.character_octet_length (Nullable(UInt64)) — Maximum length in bytes for binary data, character data, or text data and images. In ClickHouse makes sense only for FixedString data type. Otherwise, the NULL value is returned.numeric_precision (Nullable(UInt64)) — Accuracy of approximate numeric data, exact numeric data, integer data, or monetary data. In ClickHouse it is bitness for integer types and decimal precision for Decimal types. Otherwise, the NULL value is returned.numeric_precision_radix (Nullable(UInt64)) — The base of the number system is the accuracy of approximate numeric data, exact numeric data, integer data or monetary data. In ClickHouse it's 2 for integer types and 10 for Decimal types. Otherwise, the NULL value is returned.numeric_scale (Nullable(UInt64)) — The scale of approximate numeric data, exact numeric data, integer data, or monetary data. In ClickHouse makes sense only for Decimal types. Otherwise, the NULL value is returned.datetime_precision (Nullable(UInt64)) — Decimal precision of DateTime64 data type. For other data types, the NULL value is returned.character_set_catalog (Nullable(String)) — NULL, not supported.character_set_schema (Nullable(String)) — NULL, not supported.character_set_name (Nullable(String)) — NULL, not supported.collation_catalog (Nullable(String)) — NULL, not supported.collation_schema (Nullable(String)) — NULL, not supported.collation_name (Nullable(String)) — NULL, not supported.domain_catalog (Nullable(String)) — NULL, not supported.domain_schema (Nullable(String)) — NULL, not supported.domain_name (Nullable(String)) — NULL, not supported. Example Query: SELECT * FROM INFORMATION_SCHEMA.COLUMNS WHERE (table_schema=currentDatabase() OR table_schema='') AND table_name NOT LIKE '%inner%' LIMIT 1 FORMAT Vertical;  Result: Row 1: ────── table_catalog: default table_schema: default table_name: describe_example column_name: id ordinal_position: 1 column_default: is_nullable: 0 data_type: UInt64 character_maximum_length: ᴺᵁᴸᴸ character_octet_length: ᴺᵁᴸᴸ numeric_precision: 64 numeric_precision_radix: 2 numeric_scale: 0 datetime_precision: ᴺᵁᴸᴸ character_set_catalog: ᴺᵁᴸᴸ character_set_schema: ᴺᵁᴸᴸ character_set_name: ᴺᵁᴸᴸ collation_catalog: ᴺᵁᴸᴸ collation_schema: ᴺᵁᴸᴸ collation_name: ᴺᵁᴸᴸ domain_catalog: ᴺᵁᴸᴸ domain_schema: ᴺᵁᴸᴸ domain_name: ᴺᵁᴸᴸ  "},{"title":"SCHEMATA​","type":1,"pageTitle":"INFORMATION_SCHEMA","url":"docs/en/operations/system-tables/information_schema#schemata","content":"Contains columns read from the system.databases system table and columns that are not supported in ClickHouse or do not make sense (always NULL), but must be by the standard. Columns: catalog_name (String) — The name of the database.schema_name (String) — The name of the database.schema_owner (String) — Schema owner name, always 'default'.default_character_set_catalog (Nullable(String)) — NULL, not supported.default_character_set_schema (Nullable(String)) — NULL, not supported.default_character_set_name (Nullable(String)) — NULL, not supported.sql_path (Nullable(String)) — NULL, not supported. Example Query: SELECT * FROM information_schema.schemata WHERE schema_name ILIKE 'information_schema' LIMIT 1 FORMAT Vertical;  Result: Row 1: ────── catalog_name: INFORMATION_SCHEMA schema_name: INFORMATION_SCHEMA schema_owner: default default_character_set_catalog: ᴺᵁᴸᴸ default_character_set_schema: ᴺᵁᴸᴸ default_character_set_name: ᴺᵁᴸᴸ sql_path: ᴺᵁᴸᴸ  "},{"title":"TABLES​","type":1,"pageTitle":"INFORMATION_SCHEMA","url":"docs/en/operations/system-tables/information_schema#tables","content":"Contains columns read from the system.tables system table. Columns: table_catalog (String) — The name of the database in which the table is located.table_schema (String) — The name of the database in which the table is located.table_name (String) — Table name.table_type (Enum8) — Table type. Possible values: BASE TABLEVIEWFOREIGN TABLELOCAL TEMPORARYSYSTEM VIEW Example Query: SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE (table_schema = currentDatabase() OR table_schema = '') AND table_name NOT LIKE '%inner%' LIMIT 1 FORMAT Vertical;  Result: Row 1: ────── table_catalog: default table_schema: default table_name: describe_example table_type: BASE TABLE  "},{"title":"VIEWS​","type":1,"pageTitle":"INFORMATION_SCHEMA","url":"docs/en/operations/system-tables/information_schema#views","content":"Contains columns read from the system.tables system table, when the table engine View is used. Columns: table_catalog (String) — The name of the database in which the table is located.table_schema (String) — The name of the database in which the table is located.table_name (String) — Table name.view_definition (String) — SELECT query for view.check_option (String) — NONE, no checking.is_updatable (Enum8) — NO, the view is not updated.is_insertable_into (Enum8) — Shows whether the created view is materialized. Possible values: NO — The created view is not materialized.YES — The created view is materialized. is_trigger_updatable (Enum8) — NO, the trigger is not updated.is_trigger_deletable (Enum8) — NO, the trigger is not deleted.is_trigger_insertable_into (Enum8) — NO, no data is inserted into the trigger. Example Query: CREATE VIEW v (n Nullable(Int32), f Float64) AS SELECT n, f FROM t; CREATE MATERIALIZED VIEW mv ENGINE = Null AS SELECT * FROM system.one; SELECT * FROM information_schema.views WHERE table_schema = currentDatabase() LIMIT 1 FORMAT Vertical;  Result: Row 1: ────── table_catalog: default table_schema: default table_name: mv view_definition: SELECT * FROM system.one check_option: NONE is_updatable: NO is_insertable_into: YES is_trigger_updatable: NO is_trigger_deletable: NO is_trigger_insertable_into: NO  "},{"title":"quota_usage","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/quota_usage","content":"","keywords":""},{"title":"See Also​","type":1,"pageTitle":"quota_usage","url":"docs/en/operations/system-tables/quota_usage#see-also","content":"SHOW QUOTA Original article "},{"title":"part_log","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/part_log","content":"part_log The system.part_log table is created only if the part_log server setting is specified. This table contains information about events that occurred with data parts in the MergeTree family tables, such as adding or merging data. The system.part_log table contains the following columns: query_id (String) — Identifier of the INSERT query that created this data part.event_type (Enum8) — Type of the event that occurred with the data part. Can have one of the following values: NEW_PART — Inserting of a new data part.MERGE_PARTS — Merging of data parts.DOWNLOAD_PART — Downloading a data part.REMOVE_PART — Removing or detaching a data part using DETACH PARTITION.MUTATE_PART — Mutating of a data part.MOVE_PART — Moving the data part from the one disk to another one. event_date (Date) — Event date.event_time (DateTime) — Event time.event_time_microseconds (DateTime64) — Event time with microseconds precision.duration_ms (UInt64) — Duration.database (String) — Name of the database the data part is in.table (String) — Name of the table the data part is in.part_name (String) — Name of the data part.partition_id (String) — ID of the partition that the data part was inserted to. The column takes the all value if the partitioning is by tuple().path_on_disk (String) — Absolute path to the folder with data part files.rows (UInt64) — The number of rows in the data part.size_in_bytes (UInt64) — Size of the data part in bytes.merged_from (Array(String)) — An array of names of the parts which the current part was made up from (after the merge).bytes_uncompressed (UInt64) — Size of uncompressed bytes.read_rows (UInt64) — The number of rows was read during the merge.read_bytes (UInt64) — The number of bytes was read during the merge.peak_memory_usage (Int64) — The maximum difference between the amount of allocated and freed memory in context of this thread.error (UInt16) — The code number of the occurred error.exception (String) — Text message of the occurred error. The system.part_log table is created after the first inserting data to the MergeTree table. Example SELECT * FROM system.part_log LIMIT 1 FORMAT Vertical; Row 1: ────── query_id: 983ad9c7-28d5-4ae1-844e-603116b7de31 event_type: NewPart event_date: 2021-02-02 event_time: 2021-02-02 11:14:28 event_time_microseconds: 2021-02-02 11:14:28.861919 duration_ms: 35 database: default table: log_mt_2 part_name: all_1_1_0 partition_id: all path_on_disk: db/data/default/log_mt_2/all_1_1_0/ rows: 115418 size_in_bytes: 1074311 merged_from: [] bytes_uncompressed: 0 read_rows: 0 read_bytes: 0 peak_memory_usage: 0 error: 0 exception: Original article","keywords":""},{"title":"quotas_usage","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/quotas_usage","content":"","keywords":""},{"title":"See Also​","type":1,"pageTitle":"quotas_usage","url":"docs/en/operations/system-tables/quotas_usage#see-also","content":"SHOW QUOTA Original article "},{"title":"processes","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/processes","content":"processes This system table is used for implementing the SHOW PROCESSLIST query. Columns: user (String) – The user who made the query. Keep in mind that for distributed processing, queries are sent to remote servers under the default user. The field contains the username for a specific query, not for a query that this query initiated.address (String) – The IP address the request was made from. The same for distributed processing. To track where a distributed query was originally made from, look at system.processes on the query requestor server.elapsed (Float64) – The time in seconds since request execution started.rows_read (UInt64) – The number of rows read from the table. For distributed processing, on the requestor server, this is the total for all remote servers.bytes_read (UInt64) – The number of uncompressed bytes read from the table. For distributed processing, on the requestor server, this is the total for all remote servers.total_rows_approx (UInt64) – The approximation of the total number of rows that should be read. For distributed processing, on the requestor server, this is the total for all remote servers. It can be updated during request processing, when new sources to process become known.memory_usage (UInt64) – Amount of RAM the request uses. It might not include some types of dedicated memory. See the max_memory_usage setting.query (String) – The query text. For INSERT, it does not include the data to insert.query_id (String) – Query ID, if defined. :) SELECT * FROM system.processes LIMIT 10 FORMAT Vertical; Row 1: ────── is_initial_query: 1 user: default query_id: 35a360fa-3743-441d-8e1f-228c938268da address: ::ffff:172.23.0.1 port: 47588 initial_user: default initial_query_id: 35a360fa-3743-441d-8e1f-228c938268da initial_address: ::ffff:172.23.0.1 initial_port: 47588 interface: 1 os_user: bharatnc client_hostname: tower client_name: ClickHouse client_revision: 54437 client_version_major: 20 client_version_minor: 7 client_version_patch: 2 http_method: 0 http_user_agent: quota_key: elapsed: 0.000582537 is_cancelled: 0 read_rows: 0 read_bytes: 0 total_rows_approx: 0 written_rows: 0 written_bytes: 0 memory_usage: 0 peak_memory_usage: 0 query: SELECT * from system.processes LIMIT 10 FORMAT Vertical; thread_ids: [67] ProfileEvents: {'Query':1,'SelectQuery':1,'ReadCompressedBytes':36,'CompressedReadBufferBlocks':1,'CompressedReadBufferBytes':10,'IOBufferAllocs':1,'IOBufferAllocBytes':89,'ContextLock':15,'RWLockAcquiredReadLocks':1} Settings: {'background_pool_size':'32','load_balancing':'random','allow_suspicious_low_cardinality_types':'1','distributed_aggregation_memory_efficient':'1','skip_unavailable_shards':'1','log_queries':'1','max_bytes_before_external_group_by':'20000000000','max_bytes_before_external_sort':'20000000000','allow_introspection_functions':'1'} 1 rows in set. Elapsed: 0.002 sec. Original article","keywords":""},{"title":"query_log","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/query_log","content":"query_log Contains information about executed queries, for example, start time, duration of processing, error messages. note This table does not contain the ingested data for INSERT queries. You can change settings of queries logging in the query_log section of the server configuration. You can disable queries logging by setting log_queries = 0. We do not recommend to turn off logging because information in this table is important for solving issues. The flushing period of data is set in flush_interval_milliseconds parameter of the query_log server settings section. To force flushing, use the SYSTEM FLUSH LOGS query. ClickHouse does not delete data from the table automatically. See Introduction for more details. The system.query_log table registers two kinds of queries: Initial queries that were run directly by the client.Child queries that were initiated by other queries (for distributed query execution). For these types of queries, information about the parent queries is shown in the initial_* columns. Each query creates one or two rows in the query_log table, depending on the status (see the type column) of the query: If the query execution was successful, two rows with the QueryStart and QueryFinish types are created.If an error occurred during query processing, two events with the QueryStart and ExceptionWhileProcessing types are created.If an error occurred before launching the query, a single event with the ExceptionBeforeStart type is created. You can use the log_queries_probability setting to reduce the number of queries, registered in the query_log table. You can use the log_formatted_queries setting to log formatted queries to the formatted_query column. Columns: type (Enum8) — Type of an event that occurred when executing the query. Values: 'QueryStart' = 1 — Successful start of query execution.'QueryFinish' = 2 — Successful end of query execution.'ExceptionBeforeStart' = 3 — Exception before the start of query execution.'ExceptionWhileProcessing' = 4 — Exception during the query execution. event_date (Date) — Query starting date.event_time (DateTime) — Query starting time.event_time_microseconds (DateTime) — Query starting time with microseconds precision.query_start_time (DateTime) — Start time of query execution.query_start_time_microseconds (DateTime64) — Start time of query execution with microsecond precision.query_duration_ms (UInt64) — Duration of query execution in milliseconds.read_rows (UInt64) — Total number of rows read from all tables and table functions participated in query. It includes usual subqueries, subqueries for IN and JOIN. For distributed queries read_rows includes the total number of rows read at all replicas. Each replica sends it’s read_rows value, and the server-initiator of the query summarizes all received and local values. The cache volumes do not affect this value.read_bytes (UInt64) — Total number of bytes read from all tables and table functions participated in query. It includes usual subqueries, subqueries for IN and JOIN. For distributed queries read_bytes includes the total number of rows read at all replicas. Each replica sends it’s read_bytes value, and the server-initiator of the query summarizes all received and local values. The cache volumes do not affect this value.written_rows (UInt64) — For INSERT queries, the number of written rows. For other queries, the column value is 0.written_bytes (UInt64) — For INSERT queries, the number of written bytes. For other queries, the column value is 0.result_rows (UInt64) — Number of rows in a result of the SELECT query, or a number of rows in the INSERT query.result_bytes (UInt64) — RAM volume in bytes used to store a query result.memory_usage (UInt64) — Memory consumption by the query.current_database (String) — Name of the current database.query (String) — Query string.formatted_query (String) — Formatted query string.normalized_query_hash (UInt64) — Identical hash value without the values of literals for similar queries.query_kind (LowCardinality(String)) — Type of the query.databases (Array(LowCardinality(String))) — Names of the databases present in the query.tables (Array(LowCardinality(String))) — Names of the tables present in the query.views (Array(LowCardinality(String))) — Names of the (materialized or live) views present in the query.columns (Array(LowCardinality(String))) — Names of the columns present in the query.projections (String) — Names of the projections used during the query execution.exception_code (Int32) — Code of an exception.exception (String) — Exception message.stack_trace (String) — Stack trace. An empty string, if the query was completed successfully.is_initial_query (UInt8) — Query type. Possible values: 1 — Query was initiated by the client.0 — Query was initiated by another query as part of distributed query execution. user (String) — Name of the user who initiated the current query.query_id (String) — ID of the query.address (IPv6) — IP address that was used to make the query.port (UInt16) — The client port that was used to make the query.initial_user (String) — Name of the user who ran the initial query (for distributed query execution).initial_query_id (String) — ID of the initial query (for distributed query execution).initial_address (IPv6) — IP address that the parent query was launched from.initial_port (UInt16) — The client port that was used to make the parent query.initial_query_start_time (DateTime) — Initial query starting time (for distributed query execution).initial_query_start_time_microseconds (DateTime64) — Initial query starting time with microseconds precision (for distributed query execution).interface (UInt8) — Interface that the query was initiated from. Possible values: 1 — TCP.2 — HTTP. os_user (String) — Operating system username who runs clickhouse-client.client_hostname (String) — Hostname of the client machine where the clickhouse-client or another TCP client is run.client_name (String) — The clickhouse-client or another TCP client name.client_revision (UInt32) — Revision of the clickhouse-client or another TCP client.client_version_major (UInt32) — Major version of the clickhouse-client or another TCP client.client_version_minor (UInt32) — Minor version of the clickhouse-client or another TCP client.client_version_patch (UInt32) — Patch component of the clickhouse-client or another TCP client version.http_method (UInt8) — HTTP method that initiated the query. Possible values: 0 — The query was launched from the TCP interface.1 — GET method was used.2 — POST method was used. http_user_agent (String) — HTTP header UserAgent passed in the HTTP query.http_referer (String) — HTTP header Referer passed in the HTTP query (contains an absolute or partial address of the page making the query).forwarded_for (String) — HTTP header X-Forwarded-For passed in the HTTP query.quota_key (String) — The quota key specified in the quotas setting (see keyed).revision (UInt32) — ClickHouse revision.ProfileEvents (Map(String, UInt64)) — ProfileEvents that measure different metrics. The description of them could be found in the table system.eventsSettings (Map(String, String)) — Settings that were changed when the client ran the query. To enable logging changes to settings, set the log_query_settings parameter to 1.log_comment (String) — Log comment. It can be set to arbitrary string no longer than max_query_size. An empty string if it is not defined.thread_ids (Array(UInt64)) — Thread ids that are participating in query execution.used_aggregate_functions (Array(String)) — Canonical names of aggregate functions, which were used during query execution.used_aggregate_function_combinators (Array(String)) — Canonical names of aggregate functions combinators, which were used during query execution.used_database_engines (Array(String)) — Canonical names of database engines, which were used during query execution.used_data_type_families (Array(String)) — Canonical names of data type families, which were used during query execution.used_dictionaries (Array(String)) — Canonical names of dictionaries, which were used during query execution.used_formats (Array(String)) — Canonical names of formats, which were used during query execution.used_functions (Array(String)) — Canonical names of functions, which were used during query execution.used_storages (Array(String)) — Canonical names of storages, which were used during query execution.used_table_functions (Array(String)) — Canonical names of table functions, which were used during query execution. Example SELECT * FROM system.query_log WHERE type = 'QueryFinish' ORDER BY query_start_time DESC LIMIT 1 FORMAT Vertical; Row 1: ────── type: QueryFinish event_date: 2021-11-03 event_time: 2021-11-03 16:13:54 event_time_microseconds: 2021-11-03 16:13:54.953024 query_start_time: 2021-11-03 16:13:54 query_start_time_microseconds: 2021-11-03 16:13:54.952325 query_duration_ms: 0 read_rows: 69 read_bytes: 6187 written_rows: 0 written_bytes: 0 result_rows: 69 result_bytes: 48256 memory_usage: 0 current_database: default query: DESCRIBE TABLE system.query_log formatted_query: normalized_query_hash: 8274064835331539124 query_kind: databases: [] tables: [] columns: [] projections: [] views: [] exception_code: 0 exception: stack_trace: is_initial_query: 1 user: default query_id: 7c28bbbb-753b-4eba-98b1-efcbe2b9bdf6 address: ::ffff:127.0.0.1 port: 40452 initial_user: default initial_query_id: 7c28bbbb-753b-4eba-98b1-efcbe2b9bdf6 initial_address: ::ffff:127.0.0.1 initial_port: 40452 initial_query_start_time: 2021-11-03 16:13:54 initial_query_start_time_microseconds: 2021-11-03 16:13:54.952325 interface: 1 os_user: sevirov client_hostname: clickhouse.ru-central1.internal client_name: ClickHouse client_revision: 54449 client_version_major: 21 client_version_minor: 10 client_version_patch: 1 http_method: 0 http_user_agent: http_referer: forwarded_for: quota_key: revision: 54456 log_comment: thread_ids: [30776,31174] ProfileEvents: {'Query':1,'NetworkSendElapsedMicroseconds':59,'NetworkSendBytes':2643,'SelectedRows':69,'SelectedBytes':6187,'ContextLock':9,'RWLockAcquiredReadLocks':1,'RealTimeMicroseconds':817,'UserTimeMicroseconds':427,'SystemTimeMicroseconds':212,'OSCPUVirtualTimeMicroseconds':639,'OSReadChars':894,'OSWriteChars':319} Settings: {'load_balancing':'random','max_memory_usage':'10000000000'} used_aggregate_functions: [] used_aggregate_function_combinators: [] used_database_engines: [] used_data_type_families: [] used_dictionaries: [] used_formats: [] used_functions: [] used_storages: [] used_table_functions: [] See Also system.query_thread_log — This table contains information about each query execution thread.","keywords":""},{"title":"quotas","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/quotas","content":"","keywords":""},{"title":"See Also​","type":1,"pageTitle":"quotas","url":"docs/en/operations/system-tables/quotas#see-also","content":"SHOW QUOTAS Original article "},{"title":"query_views_log","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/query_views_log","content":"query_views_log Contains information about the dependent views executed when running a query, for example, the view type or the execution time. To start logging: Configure parameters in the query_views_log section.Set log_query_views to 1. The flushing period of data is set in flush_interval_milliseconds parameter of the query_views_log server settings section. To force flushing, use the SYSTEM FLUSH LOGS query. ClickHouse does not delete data from the table automatically. See Introduction for more details. You can use the log_queries_probability setting to reduce the number of queries, registered in the query_views_log table. Columns: event_date (Date) — The date when the last event of the view happened.event_time (DateTime) — The date and time when the view finished execution.event_time_microseconds (DateTime) — The date and time when the view finished execution with microseconds precision.view_duration_ms (UInt64) — Duration of view execution (sum of its stages) in milliseconds.initial_query_id (String) — ID of the initial query (for distributed query execution).view_name (String) — Name of the view.view_uuid (UUID) — UUID of the view.view_type (Enum8) — Type of the view. Values: 'Default' = 1 — Default views. Should not appear in this log.'Materialized' = 2 — Materialized views.'Live' = 3 — Live views. view_query (String) — The query executed by the view.view_target (String) — The name of the view target table.read_rows (UInt64) — Number of read rows.read_bytes (UInt64) — Number of read bytes.written_rows (UInt64) — Number of written rows.written_bytes (UInt64) — Number of written bytes.peak_memory_usage (Int64) — The maximum difference between the amount of allocated and freed memory in context of this view.ProfileEvents (Map(String, UInt64)) — ProfileEvents that measure different metrics. The description of them could be found in the table system.events.status (Enum8) — Status of the view. Values: 'QueryStart' = 1 — Successful start the view execution. Should not appear.'QueryFinish' = 2 — Successful end of the view execution.'ExceptionBeforeStart' = 3 — Exception before the start of the view execution.'ExceptionWhileProcessing' = 4 — Exception during the view execution. exception_code (Int32) — Code of an exception.exception (String) — Exception message.stack_trace (String) — Stack trace. An empty string, if the query was completed successfully. Example Query: SELECT * FROM system.query_views_log LIMIT 1 \\G; Result: Row 1: ────── event_date: 2021-06-22 event_time: 2021-06-22 13:23:07 event_time_microseconds: 2021-06-22 13:23:07.738221 view_duration_ms: 0 initial_query_id: c3a1ac02-9cad-479b-af54-9e9c0a7afd70 view_name: default.matview_inner view_uuid: 00000000-0000-0000-0000-000000000000 view_type: Materialized view_query: SELECT * FROM default.table_b view_target: default.`.inner.matview_inner` read_rows: 4 read_bytes: 64 written_rows: 2 written_bytes: 32 peak_memory_usage: 4196188 ProfileEvents: {'FileOpen':2,'WriteBufferFromFileDescriptorWrite':2,'WriteBufferFromFileDescriptorWriteBytes':187,'IOBufferAllocs':3,'IOBufferAllocBytes':3145773,'FunctionExecute':3,'DiskWriteElapsedMicroseconds':13,'InsertedRows':2,'InsertedBytes':16,'SelectedRows':4,'SelectedBytes':48,'ContextLock':16,'RWLockAcquiredReadLocks':1,'RealTimeMicroseconds':698,'SoftPageFaults':4,'OSReadChars':463} status: QueryFinish exception_code: 0 exception: stack_trace: See Also system.query_log — Description of the query_log system table which contains common information about queries execution.system.query_thread_log — This table contains information about each query execution thread. Original article","keywords":""},{"title":"query_thread_log","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/query_thread_log","content":"query_thread_log Contains information about threads that execute queries, for example, thread name, thread start time, duration of query processing. To start logging: Configure parameters in the query_thread_log section.Set log_query_threads to 1. The flushing period of data is set in flush_interval_milliseconds parameter of the query_thread_log server settings section. To force flushing, use the SYSTEM FLUSH LOGS query. ClickHouse does not delete data from the table automatically. See Introduction for more details. You can use the log_queries_probability setting to reduce the number of queries, registered in the query_thread_log table. Columns: event_date (Date) — The date when the thread has finished execution of the query.event_time (DateTime) — The date and time when the thread has finished execution of the query.event_time_microsecinds (DateTime) — The date and time when the thread has finished execution of the query with microseconds precision.query_start_time (DateTime) — Start time of query execution.query_start_time_microseconds (DateTime64) — Start time of query execution with microsecond precision.query_duration_ms (UInt64) — Duration of query execution.read_rows (UInt64) — Number of read rows.read_bytes (UInt64) — Number of read bytes.written_rows (UInt64) — For INSERT queries, the number of written rows. For other queries, the column value is 0.written_bytes (UInt64) — For INSERT queries, the number of written bytes. For other queries, the column value is 0.memory_usage (Int64) — The difference between the amount of allocated and freed memory in context of this thread.peak_memory_usage (Int64) — The maximum difference between the amount of allocated and freed memory in context of this thread.thread_name (String) — Name of the thread.thread_number (UInt32) — Internal thread ID.thread_id (Int32) — thread ID.master_thread_id (UInt64) — OS initial ID of initial thread.query (String) — Query string.is_initial_query (UInt8) — Query type. Possible values: 1 — Query was initiated by the client.0 — Query was initiated by another query for distributed query execution. user (String) — Name of the user who initiated the current query.query_id (String) — ID of the query.address (IPv6) — IP address that was used to make the query.port (UInt16) — The client port that was used to make the query.initial_user (String) — Name of the user who ran the initial query (for distributed query execution).initial_query_id (String) — ID of the initial query (for distributed query execution).initial_address (IPv6) — IP address that the parent query was launched from.initial_port (UInt16) — The client port that was used to make the parent query.interface (UInt8) — Interface that the query was initiated from. Possible values: 1 — TCP.2 — HTTP. os_user (String) — OS’s username who runs clickhouse-client.client_hostname (String) — Hostname of the client machine where the clickhouse-client or another TCP client is run.client_name (String) — The clickhouse-client or another TCP client name.client_revision (UInt32) — Revision of the clickhouse-client or another TCP client.client_version_major (UInt32) — Major version of the clickhouse-client or another TCP client.client_version_minor (UInt32) — Minor version of the clickhouse-client or another TCP client.client_version_patch (UInt32) — Patch component of the clickhouse-client or another TCP client version.http_method (UInt8) — HTTP method that initiated the query. Possible values: 0 — The query was launched from the TCP interface.1 — GET method was used.2 — POST method was used. http_user_agent (String) — The UserAgent header passed in the HTTP request.quota_key (String) — The “quota key” specified in the quotas setting (see keyed).revision (UInt32) — ClickHouse revision.ProfileEvents (Map(String, UInt64)) — ProfileEvents that measure different metrics for this thread. The description of them could be found in the table system.events. Example SELECT * FROM system.query_thread_log LIMIT 1 \\G Row 1: ────── event_date: 2020-09-11 event_time: 2020-09-11 10:08:17 event_time_microseconds: 2020-09-11 10:08:17.134042 query_start_time: 2020-09-11 10:08:17 query_start_time_microseconds: 2020-09-11 10:08:17.063150 query_duration_ms: 70 read_rows: 0 read_bytes: 0 written_rows: 1 written_bytes: 12 memory_usage: 4300844 peak_memory_usage: 4300844 thread_name: TCPHandler thread_id: 638133 master_thread_id: 638133 query: INSERT INTO test1 VALUES is_initial_query: 1 user: default query_id: 50a320fd-85a8-49b8-8761-98a86bcbacef address: ::ffff:127.0.0.1 port: 33452 initial_user: default initial_query_id: 50a320fd-85a8-49b8-8761-98a86bcbacef initial_address: ::ffff:127.0.0.1 initial_port: 33452 interface: 1 os_user: bharatnc client_hostname: tower client_name: ClickHouse client_revision: 54437 client_version_major: 20 client_version_minor: 7 client_version_patch: 2 http_method: 0 http_user_agent: quota_key: revision: 54440 ProfileEvents: {'Query':1,'SelectQuery':1,'ReadCompressedBytes':36,'CompressedReadBufferBlocks':1,'CompressedReadBufferBytes':10,'IOBufferAllocs':1,'IOBufferAllocBytes':89,'ContextLock':15,'RWLockAcquiredReadLocks':1} See Also system.query_log — Description of the query_log system table which contains common information about queries execution.system.query_views_log — This table contains information about each view executed during a query. Original article","keywords":""},{"title":"Settings","type":0,"sectionRef":"#","url":"docs/en/operations/settings/","content":"","keywords":""},{"title":"distributed_product_mode​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#distributed-product-mode","content":"Changes the behaviour of distributed subqueries. ClickHouse applies this setting when the query contains the product of distributed tables, i.e. when the query for a distributed table contains a non-GLOBAL subquery for the distributed table. Restrictions: Only applied for IN and JOIN subqueries.Only if the FROM section uses a distributed table containing more than one shard.If the subquery concerns a distributed table containing more than one shard.Not used for a table-valued remote function. Possible values: deny — Default value. Prohibits using these types of subqueries (returns the “Double-distributed in/JOIN subqueries is denied” exception).local — Replaces the database and table in the subquery with local ones for the destination server (shard), leaving the normal IN/JOIN.global — Replaces the IN/JOIN query with GLOBAL IN/GLOBAL JOIN.allow — Allows the use of these types of subqueries. "},{"title":"prefer_global_in_and_join​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#prefer-global-in-and-join","content":"Enables the replacement of IN/JOIN operators with GLOBAL IN/GLOBAL JOIN. Possible values: 0 — Disabled. IN/JOIN operators are not replaced with GLOBAL IN/GLOBAL JOIN.1 — Enabled. IN/JOIN operators are replaced with GLOBAL IN/GLOBAL JOIN. Default value: 0. Usage Although SET distributed_product_mode=global can change the queries behavior for the distributed tables, it's not suitable for local tables or tables from external resources. Here is when the prefer_global_in_and_join setting comes into play. For example, we have query serving nodes that contain local tables, which are not suitable for distribution. We need to scatter their data on the fly during distributed processing with the GLOBAL keyword — GLOBAL IN/GLOBAL JOIN. Another use case of prefer_global_in_and_join is accessing tables created by external engines. This setting helps to reduce the number of calls to external sources while joining such tables: only one call per query. See also: Distributed subqueries for more information on how to use GLOBAL IN/GLOBAL JOIN "},{"title":"enable_optimize_predicate_expression​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#enable-optimize-predicate-expression","content":"Turns on predicate pushdown in SELECT queries. Predicate pushdown may significantly reduce network traffic for distributed queries. Possible values: 0 — Disabled.1 — Enabled. Default value: 1. Usage Consider the following queries: SELECT count() FROM test_table WHERE date = '2018-10-10'SELECT count() FROM (SELECT * FROM test_table) WHERE date = '2018-10-10' If enable_optimize_predicate_expression = 1, then the execution time of these queries is equal because ClickHouse applies WHERE to the subquery when processing it. If enable_optimize_predicate_expression = 0, then the execution time of the second query is much longer because the WHERE clause applies to all the data after the subquery finishes. "},{"title":"fallback_to_stale_replicas_for_distributed_queries​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-fallback_to_stale_replicas_for_distributed_queries","content":"Forces a query to an out-of-date replica if updated data is not available. See Replication. ClickHouse selects the most relevant from the outdated replicas of the table. Used when performing SELECT from a distributed table that points to replicated tables. By default, 1 (enabled). "},{"title":"force_index_by_date​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-force_index_by_date","content":"Disables query execution if the index can’t be used by date. Works with tables in the MergeTree family. If force_index_by_date=1, ClickHouse checks whether the query has a date key condition that can be used for restricting data ranges. If there is no suitable condition, it throws an exception. However, it does not check whether the condition reduces the amount of data to read. For example, the condition Date != ' 2000-01-01 ' is acceptable even when it matches all the data in the table (i.e., running the query requires a full scan). For more information about ranges of data in MergeTree tables, see MergeTree. "},{"title":"force_primary_key​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#force-primary-key","content":"Disables query execution if indexing by the primary key is not possible. Works with tables in the MergeTree family. If force_primary_key=1, ClickHouse checks to see if the query has a primary key condition that can be used for restricting data ranges. If there is no suitable condition, it throws an exception. However, it does not check whether the condition reduces the amount of data to read. For more information about data ranges in MergeTree tables, see MergeTree. "},{"title":"use_skip_indexes​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-use_skip_indexes","content":"Use data skipping indexes during query execution. Possible values: 0 — Disabled.1 — Enabled. Default value: 1. "},{"title":"force_data_skipping_indices​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-force_data_skipping_indices","content":"Disables query execution if passed data skipping indices wasn't used. Consider the following example: CREATE TABLE data ( key Int, d1 Int, d1_null Nullable(Int), INDEX d1_idx d1 TYPE minmax GRANULARITY 1, INDEX d1_null_idx assumeNotNull(d1_null) TYPE minmax GRANULARITY 1 ) Engine=MergeTree() ORDER BY key; SELECT * FROM data_01515; SELECT * FROM data_01515 SETTINGS force_data_skipping_indices=''; -- query will produce CANNOT_PARSE_TEXT error. SELECT * FROM data_01515 SETTINGS force_data_skipping_indices='d1_idx'; -- query will produce INDEX_NOT_USED error. SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='d1_idx'; -- Ok. SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='`d1_idx`'; -- Ok (example of full featured parser). SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='`d1_idx`, d1_null_idx'; -- query will produce INDEX_NOT_USED error, since d1_null_idx is not used. SELECT * FROM data_01515 WHERE d1 = 0 AND assumeNotNull(d1_null) = 0 SETTINGS force_data_skipping_indices='`d1_idx`, d1_null_idx'; -- Ok.  Works with tables in the MergeTree family. "},{"title":"format_schema​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#format-schema","content":"This parameter is useful when you are using formats that require a schema definition, such as Cap’n Proto or Protobuf. The value depends on the format. "},{"title":"fsync_metadata​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#fsync-metadata","content":"Enables or disables fsync when writing .sql files. Enabled by default. It makes sense to disable it if the server has millions of tiny tables that are constantly being created and destroyed. "},{"title":"function_range_max_elements_in_block​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-function_range_max_elements_in_block","content":"Sets the safety threshold for data volume generated by function range. Defines the maximum number of values generated by function per block of data (sum of array sizes for every row in a block). Possible values: Positive integer. Default value: 500,000,000. See Also max_block_sizemin_insert_block_size_rows "},{"title":"enable_http_compression​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-enable_http_compression","content":"Enables or disables data compression in the response to an HTTP request. For more information, read the HTTP interface description. Possible values: 0 — Disabled.1 — Enabled. Default value: 0. "},{"title":"http_zlib_compression_level​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-http_zlib_compression_level","content":"Sets the level of data compression in the response to an HTTP request if enable_http_compression = 1. Possible values: Numbers from 1 to 9. Default value: 3. "},{"title":"http_native_compression_disable_checksumming_on_decompress​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-http_native_compression_disable_checksumming_on_decompress","content":"Enables or disables checksum verification when decompressing the HTTP POST data from the client. Used only for ClickHouse native compression format (not used with gzip or deflate). For more information, read the HTTP interface description. Possible values: 0 — Disabled.1 — Enabled. Default value: 0. "},{"title":"http_max_uri_size​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#http-max-uri-size","content":"Sets the maximum URI length of an HTTP request. Possible values: Positive integer. Default value: 1048576. "},{"title":"table_function_remote_max_addresses​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#table_function_remote_max_addresses","content":"Sets the maximum number of addresses generated from patterns for the remote function. Possible values: Positive integer. Default value: 1000. "},{"title":"glob_expansion_max_elements {#glob_expansion_max_elements }​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#glob_expansion_max_elements--glob_expansion_max_elements-","content":"Sets the maximum number of addresses generated from patterns for external storages and table functions (like url) except the remote function. Possible values: Positive integer. Default value: 1000. "},{"title":"send_progress_in_http_headers​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-send_progress_in_http_headers","content":"Enables or disables X-ClickHouse-Progress HTTP response headers in clickhouse-server responses. For more information, read the HTTP interface description. Possible values: 0 — Disabled.1 — Enabled. Default value: 0. "},{"title":"max_http_get_redirects​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#setting-max_http_get_redirects","content":"Limits the maximum number of HTTP GET redirect hops for URL-engine tables. The setting applies to both types of tables: those created by the CREATE TABLE query and by the url table function. Possible values: Any positive integer number of hops.0 — No hops allowed. Default value: 0. "},{"title":"input_format_allow_errors_num​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-input_format_allow_errors_num","content":"Sets the maximum number of acceptable errors when reading from text formats (CSV, TSV, etc.). The default value is 0. Always pair it with input_format_allow_errors_ratio. If an error occurred while reading rows but the error counter is still less than input_format_allow_errors_num, ClickHouse ignores the row and moves on to the next one. If both input_format_allow_errors_num and input_format_allow_errors_ratio are exceeded, ClickHouse throws an exception. "},{"title":"input_format_allow_errors_ratio​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-input_format_allow_errors_ratio","content":"Sets the maximum percentage of errors allowed when reading from text formats (CSV, TSV, etc.). The percentage of errors is set as a floating-point number between 0 and 1. The default value is 0. Always pair it with input_format_allow_errors_num. If an error occurred while reading rows but the error counter is still less than input_format_allow_errors_ratio, ClickHouse ignores the row and moves on to the next one. If both input_format_allow_errors_num and input_format_allow_errors_ratio are exceeded, ClickHouse throws an exception. "},{"title":"input_format_parquet_import_nested​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#input_format_parquet_import_nested","content":"Enables or disables the ability to insert the data into Nested columns as an array of structs in Parquet input format. Possible values: 0 — Data can not be inserted into Nested columns as an array of structs.1 — Data can be inserted into Nested columns as an array of structs. Default value: 0. "},{"title":"input_format_arrow_import_nested​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#input_format_arrow_import_nested","content":"Enables or disables the ability to insert the data into Nested columns as an array of structs in Arrow input format. Possible values: 0 — Data can not be inserted into Nested columns as an array of structs.1 — Data can be inserted into Nested columns as an array of structs. Default value: 0. "},{"title":"input_format_orc_import_nested​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#input_format_orc_import_nested","content":"Enables or disables the ability to insert the data into Nested columns as an array of structs in ORC input format. Possible values: 0 — Data can not be inserted into Nested columns as an array of structs.1 — Data can be inserted into Nested columns as an array of structs. Default value: 0. "},{"title":"input_format_values_interpret_expressions​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-input_format_values_interpret_expressions","content":"Enables or disables the full SQL parser if the fast stream parser can’t parse the data. This setting is used only for the Values format at the data insertion. For more information about syntax parsing, see the Syntax section. Possible values: 0 — Disabled. In this case, you must provide formatted data. See the Formats section. 1 — Enabled. In this case, you can use an SQL expression as a value, but data insertion is much slower this way. If you insert only formatted data, then ClickHouse behaves as if the setting value is 0. Default value: 1. Example of Use Insert the DateTime type value with the different settings. SET input_format_values_interpret_expressions = 0; INSERT INTO datetime_t VALUES (now())  Exception on client: Code: 27. DB::Exception: Cannot parse input: expected ) before: now()): (at row 1)  SET input_format_values_interpret_expressions = 1; INSERT INTO datetime_t VALUES (now())  Ok.  The last query is equivalent to the following: SET input_format_values_interpret_expressions = 0; INSERT INTO datetime_t SELECT now()  Ok.  "},{"title":"input_format_values_deduce_templates_of_expressions​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-input_format_values_deduce_templates_of_expressions","content":"Enables or disables template deduction for SQL expressions in Values format. It allows parsing and interpreting expressions in Values much faster if expressions in consecutive rows have the same structure. ClickHouse tries to deduce the template of an expression, parse the following rows using this template and evaluate the expression on a batch of successfully parsed rows. Possible values: 0 — Disabled.1 — Enabled. Default value: 1. For the following query: INSERT INTO test VALUES (lower('Hello')), (lower('world')), (lower('INSERT')), (upper('Values')), ...  If input_format_values_interpret_expressions=1 and format_values_deduce_templates_of_expressions=0, expressions are interpreted separately for each row (this is very slow for large number of rows).If input_format_values_interpret_expressions=0 and format_values_deduce_templates_of_expressions=1, expressions in the first, second and third rows are parsed using template lower(String) and interpreted together, expression in the forth row is parsed with another template (upper(String)).If input_format_values_interpret_expressions=1 and format_values_deduce_templates_of_expressions=1, the same as in previous case, but also allows fallback to interpreting expressions separately if it’s not possible to deduce template. "},{"title":"input_format_values_accurate_types_of_literals​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-input-format-values-accurate-types-of-literals","content":"This setting is used only when input_format_values_deduce_templates_of_expressions = 1. Expressions for some column may have the same structure, but contain numeric literals of different types, e.g. (..., abs(0), ...), -- UInt64 literal (..., abs(3.141592654), ...), -- Float64 literal (..., abs(-1), ...), -- Int64 literal  Possible values: 0 — Disabled. In this case, ClickHouse may use a more general type for some literals (e.g., Float64 or Int64 instead of UInt64 for 42), but it may cause overflow and precision issues. 1 — Enabled. In this case, ClickHouse checks the actual type of literal and uses an expression template of the corresponding type. In some cases, it may significantly slow down expression evaluation in Values. Default value: 1. "},{"title":"input_format_defaults_for_omitted_fields​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#session_settings-input_format_defaults_for_omitted_fields","content":"When performing INSERT queries, replace omitted input column values with default values of the respective columns. This option only applies to JSONEachRow, CSV, TabSeparated formats and formats with WithNames/WithNamesAndTypes suffixes. note When this option is enabled, extended table metadata are sent from server to client. It consumes additional computing resources on the server and can reduce performance. Possible values: 0 — Disabled.1 — Enabled. Default value: 1. "},{"title":"input_format_tsv_empty_as_default​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-input-format-tsv-empty-as-default","content":"When enabled, replace empty input fields in TSV with default values. For complex default expressions input_format_defaults_for_omitted_fields must be enabled too. Disabled by default. "},{"title":"input_format_csv_empty_as_default​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-input-format-csv-empty-as-default","content":"When enabled, replace empty input fields in CSV with default values. For complex default expressions input_format_defaults_for_omitted_fields must be enabled too. Enabled by default. "},{"title":"input_format_tsv_enum_as_number​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-input_format_tsv_enum_as_number","content":"When enabled, always treat enum values as enum ids for TSV input format. It's recommended to enable this setting if data contains only enum ids to optimize enum parsing. Possible values: 0 — Enum values are parsed as values or as enum IDs.1 — Enum values are parsed only as enum IDs. Default value: 0. Example Consider the table: CREATE TABLE table_with_enum_column_for_tsv_insert (Id Int32,Value Enum('first' = 1, 'second' = 2)) ENGINE=Memory();  When the input_format_tsv_enum_as_number setting is enabled: Query: SET input_format_tsv_enum_as_number = 1; INSERT INTO table_with_enum_column_for_tsv_insert FORMAT TSV 102 2; SELECT * FROM table_with_enum_column_for_tsv_insert;  Result: ┌──Id─┬─Value──┐ │ 102 │ second │ └─────┴────────┘  Query: SET input_format_tsv_enum_as_number = 1; INSERT INTO table_with_enum_column_for_tsv_insert FORMAT TSV 103 'first';  throws an exception. When the input_format_tsv_enum_as_number setting is disabled: Query: SET input_format_tsv_enum_as_number = 0; INSERT INTO table_with_enum_column_for_tsv_insert FORMAT TSV 102 2; INSERT INTO table_with_enum_column_for_tsv_insert FORMAT TSV 103 'first'; SELECT * FROM table_with_enum_column_for_tsv_insert;  Result: ┌──Id─┬─Value──┐ │ 102 │ second │ └─────┴────────┘ ┌──Id─┬─Value──┐ │ 103 │ first │ └─────┴────────┘  "},{"title":"input_format_null_as_default​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-input-format-null-as-default","content":"Enables or disables the initialization of NULL fields with default values, if data type of these fields is not nullable. If column type is not nullable and this setting is disabled, then inserting NULL causes an exception. If column type is nullable, then NULL values are inserted as is, regardless of this setting. This setting is applicable to INSERT ... VALUES queries for text input formats. Possible values: 0 — Inserting NULL into a not nullable column causes an exception.1 — NULL fields are initialized with default column values. Default value: 1. "},{"title":"insert_null_as_default​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#insert_null_as_default","content":"Enables or disables the insertion of default values instead of NULL into columns with not nullable data type. If column type is not nullable and this setting is disabled, then inserting NULL causes an exception. If column type is nullable, then NULL values are inserted as is, regardless of this setting. This setting is applicable to INSERT ... SELECT queries. Note that SELECT subqueries may be concatenated with UNION ALL clause. Possible values: 0 — Inserting NULL into a not nullable column causes an exception.1 — Default column value is inserted instead of NULL. Default value: 1. "},{"title":"input_format_skip_unknown_fields​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-input-format-skip-unknown-fields","content":"Enables or disables skipping insertion of extra data. When writing data, ClickHouse throws an exception if input data contain columns that do not exist in the target table. If skipping is enabled, ClickHouse does not insert extra data and does not throw an exception. Supported formats: JSONEachRowCSVWithNamesTabSeparatedWithNamesTSKV Possible values: 0 — Disabled.1 — Enabled. Default value: 0. "},{"title":"input_format_import_nested_json​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-input_format_import_nested_json","content":"Enables or disables the insertion of JSON data with nested objects. Supported formats: JSONEachRow Possible values: 0 — Disabled.1 — Enabled. Default value: 0. See also: Usage of Nested Structures with the JSONEachRow format. "},{"title":"input_format_with_names_use_header​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-input-format-with-names-use-header","content":"Enables or disables checking the column order when inserting data. To improve insert performance, we recommend disabling this check if you are sure that the column order of the input data is the same as in the target table. Supported formats: CSVWithNamesCSVWithNamesTabSeparatedWithNamesTabSeparatedWithNamesAndTypesJSONCompactEachRowWithNamesJSONCompactEachRowWithNamesAndTypesJSONCompactStringsEachRowWithNamesJSONCompactStringsEachRowWithNamesAndTypesRowBinaryWithNamesRowBinaryWithNamesAndTypes Possible values: 0 — Disabled.1 — Enabled. Default value: 1. "},{"title":"input_format_with_types_use_header​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-input-format-with-types-use-header","content":"Controls whether format parser should check if data types from the input data match data types from the target table. Supported formats: CSVWithNamesCSVWithNamesTabSeparatedWithNamesTabSeparatedWithNamesAndTypesJSONCompactEachRowWithNamesJSONCompactEachRowWithNamesAndTypesJSONCompactStringsEachRowWithNamesJSONCompactStringsEachRowWithNamesAndTypesRowBinaryWithNamesRowBinaryWithNamesAndTypes Possible values: 0 — Disabled.1 — Enabled. Default value: 1. "},{"title":"date_time_input_format​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-date_time_input_format","content":"Allows choosing a parser of the text representation of date and time. The setting does not apply to date and time functions. Possible values: 'best_effort' — Enables extended parsing. ClickHouse can parse the basic YYYY-MM-DD HH:MM:SS format and all ISO 8601 date and time formats. For example, '2018-06-08T01:02:03.000Z'. 'basic' — Use basic parser. ClickHouse can parse only the basic YYYY-MM-DD HH:MM:SS or YYYY-MM-DD format. For example, 2019-08-20 10:18:56 or 2019-08-20. Default value: 'basic'. See also: DateTime data type.Functions for working with dates and times. "},{"title":"date_time_output_format​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-date_time_output_format","content":"Allows choosing different output formats of the text representation of date and time. Possible values: simple - Simple output format. ClickHouse output date and time YYYY-MM-DD hh:mm:ss format. For example, 2019-08-20 10:18:56. The calculation is performed according to the data type's time zone (if present) or server time zone. iso - ISO output format. ClickHouse output date and time in ISO 8601 YYYY-MM-DDThh:mm:ssZ format. For example, 2019-08-20T10:18:56Z. Note that output is in UTC (Z means UTC). unix_timestamp - Unix timestamp output format. ClickHouse output date and time in Unix timestamp format. For example 1566285536. Default value: simple. See also: DateTime data type.Functions for working with dates and times. "},{"title":"join_default_strictness​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-join_default_strictness","content":"Sets default strictness for JOIN clauses. Possible values: ALL — If the right table has several matching rows, ClickHouse creates a Cartesian product from matching rows. This is the normal JOIN behaviour from standard SQL.ANY — If the right table has several matching rows, only the first one found is joined. If the right table has only one matching row, the results of ANY and ALL are the same.ASOF — For joining sequences with an uncertain match.Empty string — If ALL or ANY is not specified in the query, ClickHouse throws an exception. Default value: ALL. "},{"title":"join_algorithm​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-join_algorithm","content":"Specifies JOIN algorithm. Possible values: hash — Hash join algorithm is used.partial_merge — Sort-merge algorithm is used.prefer_partial_merge — ClickHouse always tries to use merge join if possible.auto — ClickHouse tries to change hash join to merge join on the fly to avoid out of memory. Default value: hash. When using hash algorithm the right part of JOIN is uploaded into RAM. When using partial_merge algorithm ClickHouse sorts the data and dumps it to the disk. The merge algorithm in ClickHouse differs a bit from the classic realization. First ClickHouse sorts the right table by join key in blocks and creates min-max index for sorted blocks. Then it sorts parts of left table by join key and joins them over right table. The min-max index is also used to skip unneeded right table blocks. "},{"title":"join_any_take_last_row​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-join_any_take_last_row","content":"Changes behaviour of join operations with ANY strictness. warning This setting applies only for JOIN operations with Join engine tables. Possible values: 0 — If the right table has more than one matching row, only the first one found is joined.1 — If the right table has more than one matching row, only the last one found is joined. Default value: 0. See also: JOIN clauseJoin table enginejoin_default_strictness "},{"title":"join_use_nulls​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#join_use_nulls","content":"Sets the type of JOIN behaviour. When merging tables, empty cells may appear. ClickHouse fills them differently based on this setting. Possible values: 0 — The empty cells are filled with the default value of the corresponding field type.1 — JOIN behaves the same way as in standard SQL. The type of the corresponding field is converted to Nullable, and empty cells are filled with NULL. Default value: 0. "},{"title":"partial_merge_join_optimizations​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#partial_merge_join_optimizations","content":"Disables optimizations in partial merge join algorithm for JOIN queries. By default, this setting enables improvements that could lead to wrong results. If you see suspicious results in your queries, disable optimizations by this setting. Optimizations can be different in different versions of the ClickHouse server. Possible values: 0 — Optimizations disabled.1 — Optimizations enabled. Default value: 1. "},{"title":"partial_merge_join_rows_in_right_blocks​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#partial_merge_join_rows_in_right_blocks","content":"Limits sizes of right-hand join data blocks in partial merge join algorithm for JOIN queries. ClickHouse server: Splits right-hand join data into blocks with up to the specified number of rows.Indexes each block with its minimum and maximum values.Unloads prepared blocks to disk if it is possible. Possible values: Any positive integer. Recommended range of values: [1000, 100000]. Default value: 65536. "},{"title":"join_on_disk_max_files_to_merge​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#join_on_disk_max_files_to_merge","content":"Limits the number of files allowed for parallel sorting in MergeJoin operations when they are executed on disk. The bigger the value of the setting, the more RAM used and the less disk I/O needed. Possible values: Any positive integer, starting from 2. Default value: 64. "},{"title":"any_join_distinct_right_table_keys​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#any_join_distinct_right_table_keys","content":"Enables legacy ClickHouse server behaviour in ANY INNER|LEFT JOIN operations. warning Use this setting only for backward compatibility if your use cases depend on legacy JOIN behaviour. When the legacy behaviour enabled: Results of t1 ANY LEFT JOIN t2 and t2 ANY RIGHT JOIN t1 operations are not equal because ClickHouse uses the logic with many-to-one left-to-right table keys mapping.Results of ANY INNER JOIN operations contain all rows from the left table like the SEMI LEFT JOIN operations do. When the legacy behaviour disabled: Results of t1 ANY LEFT JOIN t2 and t2 ANY RIGHT JOIN t1 operations are equal because ClickHouse uses the logic which provides one-to-many keys mapping in ANY RIGHT JOIN operations.Results of ANY INNER JOIN operations contain one row per key from both the left and right tables. Possible values: 0 — Legacy behaviour is disabled.1 — Legacy behaviour is enabled. Default value: 0. See also: JOIN strictness "},{"title":"temporary_files_codec​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#temporary_files_codec","content":"Sets compression codec for temporary files used in sorting and joining operations on disk. Possible values: LZ4 — LZ4 compression is applied.NONE — No compression is applied. Default value: LZ4. "},{"title":"max_block_size​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#setting-max_block_size","content":"In ClickHouse, data is processed by blocks (sets of column parts). The internal processing cycles for a single block are efficient enough, but there are noticeable expenditures on each block. The max_block_size setting is a recommendation for what size of the block (in a count of rows) to load from tables. The block size shouldn’t be too small, so that the expenditures on each block are still noticeable, but not too large so that the query with LIMIT that is completed after the first block is processed quickly. The goal is to avoid consuming too much memory when extracting a large number of columns in multiple threads and to preserve at least some cache locality. Default value: 65,536. Blocks the size of max_block_size are not always loaded from the table. If it is obvious that less data needs to be retrieved, a smaller block is processed. "},{"title":"preferred_block_size_bytes​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#preferred-block-size-bytes","content":"Used for the same purpose as max_block_size, but it sets the recommended block size in bytes by adapting it to the number of rows in the block. However, the block size cannot be more than max_block_size rows. By default: 1,000,000. It only works when reading from MergeTree engines. "},{"title":"merge_tree_min_rows_for_concurrent_read​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#setting-merge-tree-min-rows-for-concurrent-read","content":"If the number of rows to be read from a file of a MergeTree table exceeds merge_tree_min_rows_for_concurrent_read then ClickHouse tries to perform a concurrent reading from this file on several threads. Possible values: Positive integer. Default value: 163840. "},{"title":"merge_tree_min_rows_for_concurrent_read_for_remote_filesystem​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#merge-tree-min-rows-for-concurrent-read-for-remote-filesystem","content":"The minimum number of lines to read from one file before MergeTree engine can parallelize reading, when reading from remote filesystem. Possible values: Positive integer. Default value: 163840. "},{"title":"merge_tree_min_bytes_for_concurrent_read​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#setting-merge-tree-min-bytes-for-concurrent-read","content":"If the number of bytes to read from one file of a MergeTree-engine table exceeds merge_tree_min_bytes_for_concurrent_read, then ClickHouse tries to concurrently read from this file in several threads. Possible value: Positive integer. Default value: 251658240. "},{"title":"merge_tree_min_bytes_for_concurrent_read_for_remote_filesystem​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#merge-tree-min-bytes-for-concurrent-read-for-remote-filesystem","content":"The minimum number of bytes to read from one file before MergeTree engine can parallelize reading, when reading from remote filesystem. Possible values: Positive integer. Default value: 251658240. "},{"title":"merge_tree_min_rows_for_seek​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#setting-merge-tree-min-rows-for-seek","content":"If the distance between two data blocks to be read in one file is less than merge_tree_min_rows_for_seek rows, then ClickHouse does not seek through the file but reads the data sequentially. Possible values: Any positive integer. Default value: 0. "},{"title":"merge_tree_min_bytes_for_seek​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#setting-merge-tree-min-bytes-for-seek","content":"If the distance between two data blocks to be read in one file is less than merge_tree_min_bytes_for_seek bytes, then ClickHouse sequentially reads a range of file that contains both blocks, thus avoiding extra seek. Possible values: Any positive integer. Default value: 0. "},{"title":"merge_tree_coarse_index_granularity​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#setting-merge-tree-coarse-index-granularity","content":"When searching for data, ClickHouse checks the data marks in the index file. If ClickHouse finds that required keys are in some range, it divides this range into merge_tree_coarse_index_granularity subranges and searches the required keys there recursively. Possible values: Any positive even integer. Default value: 8. "},{"title":"merge_tree_max_rows_to_use_cache​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#setting-merge-tree-max-rows-to-use-cache","content":"If ClickHouse should read more than merge_tree_max_rows_to_use_cache rows in one query, it does not use the cache of uncompressed blocks. The cache of uncompressed blocks stores data extracted for queries. ClickHouse uses this cache to speed up responses to repeated small queries. This setting protects the cache from trashing by queries that read a large amount of data. The uncompressed_cache_size server setting defines the size of the cache of uncompressed blocks. Possible values: Any positive integer. Default value: 128 ✕ 8192. "},{"title":"merge_tree_max_bytes_to_use_cache​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#setting-merge-tree-max-bytes-to-use-cache","content":"If ClickHouse should read more than merge_tree_max_bytes_to_use_cache bytes in one query, it does not use the cache of uncompressed blocks. The cache of uncompressed blocks stores data extracted for queries. ClickHouse uses this cache to speed up responses to repeated small queries. This setting protects the cache from trashing by queries that read a large amount of data. The uncompressed_cache_size server setting defines the size of the cache of uncompressed blocks. Possible values: Any positive integer. Default value: 2013265920. "},{"title":"min_bytes_to_use_direct_io​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-min-bytes-to-use-direct-io","content":"The minimum data volume required for using direct I/O access to the storage disk. ClickHouse uses this setting when reading data from tables. If the total storage volume of all the data to be read exceeds min_bytes_to_use_direct_io bytes, then ClickHouse reads the data from the storage disk with the O_DIRECT option. Possible values: 0 — Direct I/O is disabled.Positive integer. Default value: 0. "},{"title":"network_compression_method​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#network_compression_method","content":"Sets the method of data compression that is used for communication between servers and between server and clickhouse-client. Possible values: LZ4 — sets LZ4 compression method.ZSTD — sets ZSTD compression method. Default value: LZ4. See Also network_zstd_compression_level "},{"title":"network_zstd_compression_level​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#network_zstd_compression_level","content":"Adjusts the level of ZSTD compression. Used only when network_compression_method is set to ZSTD. Possible values: Positive integer from 1 to 15. Default value: 1. "},{"title":"log_queries​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-log-queries","content":"Setting up query logging. Queries sent to ClickHouse with this setup are logged according to the rules in the query_log server configuration parameter. Example: log_queries=1  "},{"title":"log_queries_min_query_duration_ms​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-log-queries-min-query-duration-ms","content":"If enabled (non-zero), queries faster then the value of this setting will not be logged (you can think about this as a long_query_time for MySQL Slow Query Log), and this basically means that you will not find them in the following tables: system.query_logsystem.query_thread_log Only the queries with the following type will get to the log: QUERY_FINISH EXCEPTION_WHILE_PROCESSING Type: milliseconds Default value: 0 (any query) "},{"title":"log_queries_min_type​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-log-queries-min-type","content":"query_log minimal type to log. Possible values: QUERY_START (=1)QUERY_FINISH (=2)EXCEPTION_BEFORE_START (=3)EXCEPTION_WHILE_PROCESSING (=4) Default value: QUERY_START. Can be used to limit which entities will go to query_log, say you are interested only in errors, then you can use EXCEPTION_WHILE_PROCESSING: log_queries_min_type='EXCEPTION_WHILE_PROCESSING'  "},{"title":"log_query_threads​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-log-query-threads","content":"Setting up query threads logging. Query threads log into system.query_thread_log table. This setting have effect only when log_queries is true. Queries’ threads run by ClickHouse with this setup are logged according to the rules in the query_thread_log server configuration parameter. Possible values: 0 — Disabled.1 — Enabled. Default value: 1. Example log_query_threads=1  "},{"title":"log_query_views​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-log-query-views","content":"Setting up query views logging. When a query run by ClickHouse with this setup on has associated views (materialized or live views), they are logged in the query_views_log server configuration parameter. Example: log_query_views=1  "},{"title":"log_formatted_queries​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-log-formatted-queries","content":"Allows to log formatted queries to the system.query_log system table. Possible values: 0 — Formatted queries are not logged in the system table.1 — Formatted queries are logged in the system table. Default value: 0. "},{"title":"log_comment​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-log-comment","content":"Specifies the value for the log_comment field of the system.query_log table and comment text for the server log. It can be used to improve the readability of server logs. Additionally, it helps to select queries related to the test from the system.query_log after running clickhouse-test. Possible values: Any string no longer than max_query_size. If length is exceeded, the server throws an exception. Default value: empty string. Example Query: SET log_comment = 'log_comment test', log_queries = 1; SELECT 1; SYSTEM FLUSH LOGS; SELECT type, query FROM system.query_log WHERE log_comment = 'log_comment test' AND event_date &gt;= yesterday() ORDER BY event_time DESC LIMIT 2;  Result: ┌─type────────┬─query─────┐ │ QueryStart │ SELECT 1; │ │ QueryFinish │ SELECT 1; │ └─────────────┴───────────┘  "},{"title":"max_insert_block_size​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-max_insert_block_size","content":"The size of blocks (in a count of rows) to form for insertion into a table. This setting only applies in cases when the server forms the blocks. For example, for an INSERT via the HTTP interface, the server parses the data format and forms blocks of the specified size. But when using clickhouse-client, the client parses the data itself, and the ‘max_insert_block_size’ setting on the server does not affect the size of the inserted blocks. The setting also does not have a purpose when using INSERT SELECT, since data is inserted using the same blocks that are formed after SELECT. Default value: 1,048,576. The default is slightly more than max_block_size. The reason for this is because certain table engines (*MergeTree) form a data part on the disk for each inserted block, which is a fairly large entity. Similarly, *MergeTree tables sort data during insertion, and a large enough block size allow sorting more data in RAM. "},{"title":"min_insert_block_size_rows​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#min-insert-block-size-rows","content":"Sets the minimum number of rows in the block which can be inserted into a table by an INSERT query. Smaller-sized blocks are squashed into bigger ones. Possible values: Positive integer.0 — Squashing disabled. Default value: 1048576. "},{"title":"min_insert_block_size_bytes​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#min-insert-block-size-bytes","content":"Sets the minimum number of bytes in the block which can be inserted into a table by an INSERT query. Smaller-sized blocks are squashed into bigger ones. Possible values: Positive integer.0 — Squashing disabled. Default value: 268435456. "},{"title":"max_replica_delay_for_distributed_queries​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-max_replica_delay_for_distributed_queries","content":"Disables lagging replicas for distributed queries. See Replication. Sets the time in seconds. If a replica lags more than the set value, this replica is not used. Default value: 300. Used when performing SELECT from a distributed table that points to replicated tables. "},{"title":"max_threads​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-max_threads","content":"The maximum number of query processing threads, excluding threads for retrieving data from remote servers (see the ‘max_distributed_connections’ parameter). This parameter applies to threads that perform the same stages of the query processing pipeline in parallel. For example, when reading from a table, if it is possible to evaluate expressions with functions, filter with WHERE and pre-aggregate for GROUP BY in parallel using at least ‘max_threads’ number of threads, then ‘max_threads’ are used. Default value: the number of physical CPU cores. For queries that are completed quickly because of a LIMIT, you can set a lower ‘max_threads’. For example, if the necessary number of entries are located in every block and max_threads = 8, then 8 blocks are retrieved, although it would have been enough to read just one. The smaller the max_threads value, the less memory is consumed. "},{"title":"max_insert_threads​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-max-insert-threads","content":"The maximum number of threads to execute the INSERT SELECT query. Possible values: 0 (or 1) — INSERT SELECT no parallel execution.Positive integer. Bigger than 1. Default value: 0. Parallel INSERT SELECT has effect only if the SELECT part is executed in parallel, see max_threads setting. Higher values will lead to higher memory usage. "},{"title":"max_compress_block_size​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#max-compress-block-size","content":"The maximum size of blocks of uncompressed data before compressing for writing to a table. By default, 1,048,576 (1 MiB). Specifying smaller block size generally leads to slightly reduced compression ratio, the compression and decompression speed increases slightly due to cache locality, and memory consumption is reduced. warning This is an expert-level setting, and you shouldn't change it if you're just getting started with ClickHouse. Don’t confuse blocks for compression (a chunk of memory consisting of bytes) with blocks for query processing (a set of rows from a table). "},{"title":"min_compress_block_size​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#min-compress-block-size","content":"For MergeTree tables. In order to reduce latency when processing queries, a block is compressed when writing the next mark if its size is at least min_compress_block_size. By default, 65,536. The actual size of the block, if the uncompressed data is less than max_compress_block_size, is no less than this value and no less than the volume of data for one mark. Let’s look at an example. Assume that index_granularity was set to 8192 during table creation. We are writing a UInt32-type column (4 bytes per value). When writing 8192 rows, the total will be 32 KB of data. Since min_compress_block_size = 65,536, a compressed block will be formed for every two marks. We are writing a URL column with the String type (average size of 60 bytes per value). When writing 8192 rows, the average will be slightly less than 500 KB of data. Since this is more than 65,536, a compressed block will be formed for each mark. In this case, when reading data from the disk in the range of a single mark, extra data won’t be decompressed. warning This is an expert-level setting, and you shouldn't change it if you're just getting started with ClickHouse. "},{"title":"max_query_size​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-max_query_size","content":"The maximum part of a query that can be taken to RAM for parsing with the SQL parser. The INSERT query also contains data for INSERT that is processed by a separate stream parser (that consumes O(1) RAM), which is not included in this restriction. Default value: 256 KiB. "},{"title":"max_parser_depth​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#max_parser_depth","content":"Limits maximum recursion depth in the recursive descent parser. Allows controlling the stack size. Possible values: Positive integer.0 — Recursion depth is unlimited. Default value: 1000. "},{"title":"interactive_delay​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#interactive-delay","content":"The interval in microseconds for checking whether request execution has been cancelled and sending the progress. Default value: 100,000 (checks for cancelling and sends the progress ten times per second). "},{"title":"connect_timeout, receive_timeout, send_timeout​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#connect-timeout-receive-timeout-send-timeout","content":"Timeouts in seconds on the socket used for communicating with the client. Default value: 10, 300, 300. "},{"title":"cancel_http_readonly_queries_on_client_close​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#cancel-http-readonly-queries-on-client-close","content":"Cancels HTTP read-only queries (e.g. SELECT) when a client closes the connection without waiting for the response. Default value: 0 "},{"title":"poll_interval​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#poll-interval","content":"Lock in a wait loop for the specified number of seconds. Default value: 10. "},{"title":"max_distributed_connections​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#max-distributed-connections","content":"The maximum number of simultaneous connections with remote servers for distributed processing of a single query to a single Distributed table. We recommend setting a value no less than the number of servers in the cluster. Default value: 1024. The following parameters are only used when creating Distributed tables (and when launching a server), so there is no reason to change them at runtime. "},{"title":"distributed_connections_pool_size​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#distributed-connections-pool-size","content":"The maximum number of simultaneous connections with remote servers for distributed processing of all queries to a single Distributed table. We recommend setting a value no less than the number of servers in the cluster. Default value: 1024. "},{"title":"max_distributed_depth​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#max-distributed-depth","content":"Limits the maximum depth of recursive queries for Distributed tables. If the value is exceeded, the server throws an exception. Possible values: Positive integer.0 — Unlimited depth. Default value: 5. "},{"title":"max_replicated_fetches_network_bandwidth_for_server​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#max_replicated_fetches_network_bandwidth_for_server","content":"Limits the maximum speed of data exchange over the network in bytes per second for replicated fetches for the server. Only has meaning at server startup. You can also limit the speed for a particular table with max_replicated_fetches_network_bandwidth setting. The setting isn't followed perfectly accurately. Possible values: Positive integer.0 — Unlimited. Default value: 0. Usage Could be used for throttling speed when replicating the data to add or replace new nodes. note 60000000 bytes/s approximatly corresponds to 457 Mbps (60000000 / 1024 / 1024 * 8). "},{"title":"max_replicated_sends_network_bandwidth_for_server​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#max_replicated_sends_network_bandwidth_for_server","content":"Limits the maximum speed of data exchange over the network in bytes per second for replicated sends for the server. Only has meaning at server startup. You can also limit the speed for a particular table with max_replicated_sends_network_bandwidth setting. The setting isn't followed perfectly accurately. Possible values: Positive integer.0 — Unlimited. Default value: 0. Usage Could be used for throttling speed when replicating the data to add or replace new nodes. note 60000000 bytes/s approximatly corresponds to 457 Mbps (60000000 / 1024 / 1024 * 8). "},{"title":"connect_timeout_with_failover_ms​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#connect-timeout-with-failover-ms","content":"The timeout in milliseconds for connecting to a remote server for a Distributed table engine, if the ‘shard’ and ‘replica’ sections are used in the cluster definition. If unsuccessful, several attempts are made to connect to various replicas. Default value: 50. "},{"title":"connection_pool_max_wait_ms​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#connection-pool-max-wait-ms","content":"The wait time in milliseconds for a connection when the connection pool is full. Possible values: Positive integer.0 — Infinite timeout. Default value: 0. "},{"title":"connections_with_failover_max_tries​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#connections-with-failover-max-tries","content":"The maximum number of connection attempts with each replica for the Distributed table engine. Default value: 3. "},{"title":"extremes​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#extremes","content":"Whether to count extreme values (the minimums and maximums in columns of a query result). Accepts 0 or 1. By default, 0 (disabled). For more information, see the section “Extreme values”. "},{"title":"kafka_max_wait_ms​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#kafka-max-wait-ms","content":"The wait time in milliseconds for reading messages from Kafka before retry. Possible values: Positive integer.0 — Infinite timeout. Default value: 5000. See also: Apache Kafka "},{"title":"use_uncompressed_cache​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#setting-use_uncompressed_cache","content":"Whether to use a cache of uncompressed blocks. Accepts 0 or 1. By default, 0 (disabled). Using the uncompressed cache (only for tables in the MergeTree family) can significantly reduce latency and increase throughput when working with a large number of short queries. Enable this setting for users who send frequent short requests. Also pay attention to the uncompressed_cache_size configuration parameter (only set in the config file) – the size of uncompressed cache blocks. By default, it is 8 GiB. The uncompressed cache is filled in as needed and the least-used data is automatically deleted. For queries that read at least a somewhat large volume of data (one million rows or more), the uncompressed cache is disabled automatically to save space for truly small queries. This means that you can keep the ‘use_uncompressed_cache’ setting always set to 1. "},{"title":"replace_running_query​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#replace-running-query","content":"When using the HTTP interface, the ‘query_id’ parameter can be passed. This is any string that serves as the query identifier. If a query from the same user with the same ‘query_id’ already exists at this time, the behaviour depends on the ‘replace_running_query’ parameter. 0 (default) – Throw an exception (do not allow the query to run if a query with the same ‘query_id’ is already running). 1 – Cancel the old query and start running the new one. Set this parameter to 1 for implementing suggestions for segmentation conditions. After entering the next character, if the old query hasn’t finished yet, it should be cancelled. "},{"title":"replace_running_query_max_wait_ms​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#replace-running-query-max-wait-ms","content":"The wait time for running the query with the same query_id to finish, when the replace_running_query setting is active. Possible values: Positive integer.0 — Throwing an exception that does not allow to run a new query if the server already executes a query with the same query_id. Default value: 5000. "},{"title":"stream_flush_interval_ms​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#stream-flush-interval-ms","content":"Works for tables with streaming in the case of a timeout, or when a thread generates max_insert_block_size rows. The default value is 7500. The smaller the value, the more often data is flushed into the table. Setting the value too low leads to poor performance. "},{"title":"load_balancing​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-load_balancing","content":"Specifies the algorithm of replicas selection that is used for distributed query processing. ClickHouse supports the following algorithms of choosing replicas: Random (by default)Nearest hostnameIn orderFirst or randomRound robin See also: distributed_replica_max_ignored_errors "},{"title":"Random (by Default)​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#load_balancing-random","content":"load_balancing = random  The number of errors is counted for each replica. The query is sent to the replica with the fewest errors, and if there are several of these, to anyone of them. Disadvantages: Server proximity is not accounted for; if the replicas have different data, you will also get different data. "},{"title":"Nearest Hostname​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#load_balancing-nearest_hostname","content":"load_balancing = nearest_hostname  The number of errors is counted for each replica. Every 5 minutes, the number of errors is integrally divided by 2. Thus, the number of errors is calculated for a recent time with exponential smoothing. If there is one replica with a minimal number of errors (i.e. errors occurred recently on the other replicas), the query is sent to it. If there are multiple replicas with the same minimal number of errors, the query is sent to the replica with a hostname that is most similar to the server’s hostname in the config file (for the number of different characters in identical positions, up to the minimum length of both hostnames). For instance, example01-01-1 and example01-01-2 are different in one position, while example01-01-1 and example01-02-2 differ in two places. This method might seem primitive, but it does not require external data about network topology, and it does not compare IP addresses, which would be complicated for our IPv6 addresses. Thus, if there are equivalent replicas, the closest one by name is preferred. We can also assume that when sending a query to the same server, in the absence of failures, a distributed query will also go to the same servers. So even if different data is placed on the replicas, the query will return mostly the same results. "},{"title":"In Order​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#load_balancing-in_order","content":"load_balancing = in_order  Replicas with the same number of errors are accessed in the same order as they are specified in the configuration. This method is appropriate when you know exactly which replica is preferable. "},{"title":"First or Random​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#load_balancing-first_or_random","content":"load_balancing = first_or_random  This algorithm chooses the first replica in the set or a random replica if the first is unavailable. It’s effective in cross-replication topology setups, but useless in other configurations. The first_or_random algorithm solves the problem of the in_order algorithm. With in_order, if one replica goes down, the next one gets a double load while the remaining replicas handle the usual amount of traffic. When using the first_or_random algorithm, the load is evenly distributed among replicas that are still available. It's possible to explicitly define what the first replica is by using the setting load_balancing_first_offset. This gives more control to rebalance query workloads among replicas. "},{"title":"Round Robin​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#load_balancing-round_robin","content":"load_balancing = round_robin  This algorithm uses a round-robin policy across replicas with the same number of errors (only the queries with round_robin policy is accounted). "},{"title":"prefer_localhost_replica​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-prefer-localhost-replica","content":"Enables/disables preferable using the localhost replica when processing distributed queries. Possible values: 1 — ClickHouse always sends a query to the localhost replica if it exists.0 — ClickHouse uses the balancing strategy specified by the load_balancing setting. Default value: 1. warning Disable this setting if you use max_parallel_replicas. "},{"title":"totals_mode​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#totals-mode","content":"How to calculate TOTALS when HAVING is present, as well as when max_rows_to_group_by and group_by_overflow_mode = ‘any’ are present. See the section “WITH TOTALS modifier”. "},{"title":"totals_auto_threshold​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#totals-auto-threshold","content":"The threshold for totals_mode = 'auto'. See the section “WITH TOTALS modifier”. "},{"title":"max_parallel_replicas​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-max_parallel_replicas","content":"The maximum number of replicas for each shard when executing a query. Possible values: Positive integer. Default value: 1. Additional Info This setting is useful for replicated tables with a sampling key. A query may be processed faster if it is executed on several servers in parallel. But the query performance may degrade in the following cases: The position of the sampling key in the partitioning key does not allow efficient range scans.Adding a sampling key to the table makes filtering by other columns less efficient.The sampling key is an expression that is expensive to calculate.The cluster latency distribution has a long tail, so that querying more servers increases the query overall latency. warning This setting will produce incorrect results when joins or subqueries are involved, and all tables don't meet certain requirements. See Distributed Subqueries and max_parallel_replicas for more details. "},{"title":"compile_expressions​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#compile-expressions","content":"Enables or disables compilation of frequently used simple functions and operators to native code with LLVM at runtime. Possible values: 0 — Disabled.1 — Enabled. Default value: 1. "},{"title":"min_count_to_compile_expression​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#min-count-to-compile-expression","content":"Minimum count of executing same expression before it is get compiled. Default value: 3. "},{"title":"compile_aggregate_expressions​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#compile_aggregate_expressions","content":"Enables or disables JIT-compilation of aggregate functions to native code. Enabling this setting can improve the performance. Possible values: 0 — Aggregation is done without JIT compilation.1 — Aggregation is done using JIT compilation. Default value: 1. See Also min_count_to_compile_aggregate_expression "},{"title":"min_count_to_compile_aggregate_expression​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#min_count_to_compile_aggregate_expression","content":"The minimum number of identical aggregate expressions to start JIT-compilation. Works only if the compile_aggregate_expressions setting is enabled. Possible values: Positive integer.0 — Identical aggregate expressions are always JIT-compiled. Default value: 3. "},{"title":"output_format_json_quote_64bit_integers​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#session_settings-output_format_json_quote_64bit_integers","content":"Controls quoting of 64-bit or bigger integers (like UInt64 or Int128) when they are output in a JSON format. Such integers are enclosed in quotes by default. This behavior is compatible with most JavaScript implementations. Possible values: 0 — Integers are output without quotes.1 — Integers are enclosed in quotes. Default value: 1. "},{"title":"output_format_json_quote_denormals​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-output_format_json_quote_denormals","content":"Enables +nan, -nan, +inf, -inf outputs in JSON output format. Possible values: 0 — Disabled.1 — Enabled. Default value: 0. Example Consider the following table account_orders: ┌─id─┬─name───┬─duration─┬─period─┬─area─┐ │ 1 │ Andrew │ 20 │ 0 │ 400 │ │ 2 │ John │ 40 │ 0 │ 0 │ │ 3 │ Bob │ 15 │ 0 │ -100 │ └────┴────────┴──────────┴────────┴──────┘  When output_format_json_quote_denormals = 0, the query returns null values in output: SELECT area/period FROM account_orders FORMAT JSON;  { &quot;meta&quot;: [ { &quot;name&quot;: &quot;divide(area, period)&quot;, &quot;type&quot;: &quot;Float64&quot; } ], &quot;data&quot;: [ { &quot;divide(area, period)&quot;: null }, { &quot;divide(area, period)&quot;: null }, { &quot;divide(area, period)&quot;: null } ], &quot;rows&quot;: 3, &quot;statistics&quot;: { &quot;elapsed&quot;: 0.003648093, &quot;rows_read&quot;: 3, &quot;bytes_read&quot;: 24 } }  When output_format_json_quote_denormals = 1, the query returns: { &quot;meta&quot;: [ { &quot;name&quot;: &quot;divide(area, period)&quot;, &quot;type&quot;: &quot;Float64&quot; } ], &quot;data&quot;: [ { &quot;divide(area, period)&quot;: &quot;inf&quot; }, { &quot;divide(area, period)&quot;: &quot;-nan&quot; }, { &quot;divide(area, period)&quot;: &quot;-inf&quot; } ], &quot;rows&quot;: 3, &quot;statistics&quot;: { &quot;elapsed&quot;: 0.000070241, &quot;rows_read&quot;: 3, &quot;bytes_read&quot;: 24 } }  "},{"title":"format_csv_delimiter​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-format_csv_delimiter","content":"The character is interpreted as a delimiter in the CSV data. By default, the delimiter is ,. "},{"title":"input_format_csv_enum_as_number​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-input_format_csv_enum_as_number","content":"When enabled, always treat enum values as enum ids for CSV input format. It's recommended to enable this setting if data contains only enum ids to optimize enum parsing. Possible values: 0 — Enum values are parsed as values or as enum IDs.1 — Enum values are parsed only as enum IDs. Default value: 0. Examples Consider the table: CREATE TABLE table_with_enum_column_for_csv_insert (Id Int32,Value Enum('first' = 1, 'second' = 2)) ENGINE=Memory();  When the input_format_csv_enum_as_number setting is enabled: Query: SET input_format_csv_enum_as_number = 1; INSERT INTO table_with_enum_column_for_csv_insert FORMAT CSV 102,2  Result: ┌──Id─┬─Value──┐ │ 102 │ second │ └─────┴────────┘  Query: SET input_format_csv_enum_as_number = 1; INSERT INTO table_with_enum_column_for_csv_insert FORMAT CSV 103,'first'  throws an exception. When the input_format_csv_enum_as_number setting is disabled: Query: SET input_format_csv_enum_as_number = 0; INSERT INTO table_with_enum_column_for_csv_insert FORMAT CSV 102,2 INSERT INTO table_with_enum_column_for_csv_insert FORMAT CSV 103,'first' SELECT * FROM table_with_enum_column_for_csv_insert;  Result: ┌──Id─┬─Value──┐ │ 102 │ second │ └─────┴────────┘ ┌──Id─┬─Value─┐ │ 103 │ first │ └─────┴───────┘  "},{"title":"output_format_csv_crlf_end_of_line​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-output-format-csv-crlf-end-of-line","content":"Use DOS/Windows-style line separator (CRLF) in CSV instead of Unix style (LF). "},{"title":"output_format_tsv_crlf_end_of_line​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-output-format-tsv-crlf-end-of-line","content":"Use DOC/Windows-style line separator (CRLF) in TSV instead of Unix style (LF). "},{"title":"insert_quorum​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-insert_quorum","content":"Enables the quorum writes. If insert_quorum &lt; 2, the quorum writes are disabled.If insert_quorum &gt;= 2, the quorum writes are enabled. Default value: 0. Quorum writes INSERT succeeds only when ClickHouse manages to correctly write data to the insert_quorum of replicas during the insert_quorum_timeout. If for any reason the number of replicas with successful writes does not reach the insert_quorum, the write is considered failed and ClickHouse will delete the inserted block from all the replicas where data has already been written. When insert_quorum_parallel is disabled, all replicas in the quorum are consistent, i.e. they contain data from all previous INSERT queries (the INSERT sequence is linearized). When reading data written using insert_quorum and insert_quorum_parallel is disabled, you can turn on sequential consistency for SELECT queries using select_sequential_consistency. ClickHouse generates an exception: If the number of available replicas at the time of the query is less than the insert_quorum.When insert_quorum_parallel is disabled and an attempt to write data is made when the previous block has not yet been inserted in insert_quorum of replicas. This situation may occur if the user tries to perform another INSERT query to the same table before the previous one with insert_quorum is completed. See also: insert_quorum_timeoutinsert_quorum_parallelselect_sequential_consistency "},{"title":"insert_quorum_timeout​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-insert_quorum_timeout","content":"Write to a quorum timeout in milliseconds. If the timeout has passed and no write has taken place yet, ClickHouse will generate an exception and the client must repeat the query to write the same block to the same or any other replica. Default value: 600 000 milliseconds (ten minutes). See also: insert_quoruminsert_quorum_parallelselect_sequential_consistency "},{"title":"insert_quorum_parallel​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-insert_quorum_parallel","content":"Enables or disables parallelism for quorum INSERT queries. If enabled, additional INSERT queries can be sent while previous queries have not yet finished. If disabled, additional writes to the same table will be rejected. Possible values: 0 — Disabled.1 — Enabled. Default value: 1. See also: insert_quoruminsert_quorum_timeoutselect_sequential_consistency "},{"title":"select_sequential_consistency​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-select_sequential_consistency","content":"Enables or disables sequential consistency for SELECT queries. Requires insert_quorum_parallel to be disabled (enabled by default). Possible values: 0 — Disabled.1 — Enabled. Default value: 0. Usage When sequential consistency is enabled, ClickHouse allows the client to execute the SELECT query only for those replicas that contain data from all previous INSERT queries executed with insert_quorum. If the client refers to a partial replica, ClickHouse will generate an exception. The SELECT query will not include data that has not yet been written to the quorum of replicas. When insert_quorum_parallel is enabled (the default), then select_sequential_consistency does not work. This is because parallel INSERT queries can be written to different sets of quorum replicas so there is no guarantee a single replica will have received all writes. See also: insert_quoruminsert_quorum_timeoutinsert_quorum_parallel "},{"title":"insert_deduplicate​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-insert-deduplicate","content":"Enables or disables block deduplication of INSERT (for Replicated* tables). Possible values: 0 — Disabled.1 — Enabled. Default value: 1. By default, blocks inserted into replicated tables by the INSERT statement are deduplicated (see Data Replication). "},{"title":"deduplicate_blocks_in_dependent_materialized_views​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-deduplicate-blocks-in-dependent-materialized-views","content":"Enables or disables the deduplication check for materialized views that receive data from Replicated* tables. Possible values:  0 — Disabled. 1 — Enabled.  Default value: 0. Usage By default, deduplication is not performed for materialized views but is done upstream, in the source table. If an INSERTed block is skipped due to deduplication in the source table, there will be no insertion into attached materialized views. This behaviour exists to enable the insertion of highly aggregated data into materialized views, for cases where inserted blocks are the same after materialized view aggregation but derived from different INSERTs into the source table. At the same time, this behaviour “breaks” INSERT idempotency. If an INSERT into the main table was successful and INSERT into a materialized view failed (e.g. because of communication failure with Zookeeper) a client will get an error and can retry the operation. However, the materialized view won’t receive the second insert because it will be discarded by deduplication in the main (source) table. The setting deduplicate_blocks_in_dependent_materialized_views allows for changing this behaviour. On retry, a materialized view will receive the repeat insert and will perform a deduplication check by itself, ignoring check result for the source table, and will insert rows lost because of the first failure. "},{"title":"insert_deduplication_token​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#insert_deduplication_token","content":"The setting allows a user to provide own deduplication semantic in MergeTree/ReplicatedMergeTree For example, by providing a unique value for the setting in each INSERT statement, user can avoid the same inserted data being deduplicated. Possilbe values: Any string Default value: empty string (disabled) insert_deduplication_token is used for deduplication only when not empty. Example: CREATE TABLE test_table ( A Int64 ) ENGINE = MergeTree ORDER BY A SETTINGS non_replicated_deduplication_window = 100; INSERT INTO test_table Values SETTINGS insert_deduplication_token = 'test' (1); -- the next insert won't be deduplicated because insert_deduplication_token is different INSERT INTO test_table Values SETTINGS insert_deduplication_token = 'test1' (1); -- the next insert will be deduplicated because insert_deduplication_token -- is the same as one of the previous INSERT INTO test_table Values SETTINGS insert_deduplication_token = 'test' (2); SELECT * FROM test_table ┌─A─┐ │ 1 │ └───┘ ┌─A─┐ │ 1 │ └───┘  "},{"title":"max_network_bytes​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-max-network-bytes","content":"Limits the data volume (in bytes) that is received or transmitted over the network when executing a query. This setting applies to every individual query. Possible values: Positive integer.0 — Data volume control is disabled. Default value: 0. "},{"title":"max_network_bandwidth​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-max-network-bandwidth","content":"Limits the speed of the data exchange over the network in bytes per second. This setting applies to every query. Possible values: Positive integer.0 — Bandwidth control is disabled. Default value: 0. "},{"title":"max_network_bandwidth_for_user​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-max-network-bandwidth-for-user","content":"Limits the speed of the data exchange over the network in bytes per second. This setting applies to all concurrently running queries performed by a single user. Possible values: Positive integer.0 — Control of the data speed is disabled. Default value: 0. "},{"title":"max_network_bandwidth_for_all_users​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-max-network-bandwidth-for-all-users","content":"Limits the speed that data is exchanged at over the network in bytes per second. This setting applies to all concurrently running queries on the server. Possible values: Positive integer.0 — Control of the data speed is disabled. Default value: 0. "},{"title":"count_distinct_implementation​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-count_distinct_implementation","content":"Specifies which of the uniq* functions should be used to perform the COUNT(DISTINCT …) construction. Possible values: uniquniqCombineduniqCombined64uniqHLL12uniqExact Default value: uniqExact. "},{"title":"skip_unavailable_shards​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-skip_unavailable_shards","content":"Enables or disables silently skipping of unavailable shards. Shard is considered unavailable if all its replicas are unavailable. A replica is unavailable in the following cases: ClickHouse can’t connect to replica for any reason. When connecting to a replica, ClickHouse performs several attempts. If all these attempts fail, the replica is considered unavailable. Replica can’t be resolved through DNS. If replica’s hostname can’t be resolved through DNS, it can indicate the following situations: Replica’s host has no DNS record. It can occur in systems with dynamic DNS, for example, Kubernetes, where nodes can be unresolvable during downtime, and this is not an error. Configuration error. ClickHouse configuration file contains a wrong hostname. Possible values: 1 — skipping enabled. If a shard is unavailable, ClickHouse returns a result based on partial data and does not report node availability issues. 0 — skipping disabled. If a shard is unavailable, ClickHouse throws an exception. Default value: 0. "},{"title":"distributed_group_by_no_merge​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#distributed-group-by-no-merge","content":"Do not merge aggregation states from different servers for distributed query processing, you can use this in case it is for certain that there are different keys on different shards Possible values: 0 — Disabled (final query processing is done on the initiator node).1 - Do not merge aggregation states from different servers for distributed query processing (query completelly processed on the shard, initiator only proxy the data), can be used in case it is for certain that there are different keys on different shards.2 - Same as 1 but applies ORDER BY and LIMIT (it is not possible when the query processed completelly on the remote node, like for distributed_group_by_no_merge=1) on the initiator (can be used for queries with ORDER BY and/or LIMIT). Default value: 0 Example SELECT * FROM remote('127.0.0.{2,3}', system.one) GROUP BY dummy LIMIT 1 SETTINGS distributed_group_by_no_merge = 1 FORMAT PrettyCompactMonoBlock ┌─dummy─┐ │ 0 │ │ 0 │ └───────┘  SELECT * FROM remote('127.0.0.{2,3}', system.one) GROUP BY dummy LIMIT 1 SETTINGS distributed_group_by_no_merge = 2 FORMAT PrettyCompactMonoBlock ┌─dummy─┐ │ 0 │ └───────┘  "},{"title":"distributed_push_down_limit​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#distributed-push-down-limit","content":"Enables or disables LIMIT applying on each shard separatelly. This will allow to avoid: Sending extra rows over network;Processing rows behind the limit on the initiator. Starting from 21.9 version you cannot get inaccurate results anymore, since distributed_push_down_limit changes query execution only if at least one of the conditions met: distributed_group_by_no_merge &gt; 0.Query does not have GROUP BY/DISTINCT/LIMIT BY, but it has ORDER BY/LIMIT.Query has GROUP BY/DISTINCT/LIMIT BY with ORDER BY/LIMIT and: optimize_skip_unused_shards is enabled.optimize_distributed_group_by_sharding_key is enabled. Possible values: 0 — Disabled.1 — Enabled. Default value: 1. See also: distributed_group_by_no_mergeoptimize_skip_unused_shardsoptimize_distributed_group_by_sharding_key "},{"title":"optimize_skip_unused_shards_limit​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#optimize-skip-unused-shards-limit","content":"Limit for number of sharding key values, turns off optimize_skip_unused_shards if the limit is reached. Too many values may require significant amount for processing, while the benefit is doubtful, since if you have huge number of values in IN (...), then most likely the query will be sent to all shards anyway. Default value: 1000 "},{"title":"optimize_skip_unused_shards​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#optimize-skip-unused-shards","content":"Enables or disables skipping of unused shards for SELECT queries that have sharding key condition in WHERE/PREWHERE (assuming that the data is distributed by sharding key, otherwise a query yields incorrect result). Possible values: 0 — Disabled.1 — Enabled. Default value: 0 "},{"title":"optimize_skip_unused_shards_rewrite_in​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#optimize-skip-unused-shards-rewrite-in","content":"Rewrite IN in query for remote shards to exclude values that does not belong to the shard (requires optimize_skip_unused_shards). Possible values: 0 — Disabled.1 — Enabled. Default value: 1 (since it requires optimize_skip_unused_shards anyway, which 0 by default) "},{"title":"allow_nondeterministic_optimize_skip_unused_shards​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#allow-nondeterministic-optimize-skip-unused-shards","content":"Allow nondeterministic (like rand or dictGet, since later has some caveats with updates) functions in sharding key. Possible values: 0 — Disallowed.1 — Allowed. Default value: 0 "},{"title":"optimize_skip_unused_shards_nesting​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#optimize-skip-unused-shards-nesting","content":"Controls optimize_skip_unused_shards (hence still requires optimize_skip_unused_shards) depends on the nesting level of the distributed query (case when you have Distributed table that look into another Distributed table). Possible values: 0 — Disabled, optimize_skip_unused_shards works always.1 — Enables optimize_skip_unused_shards only for the first level.2 — Enables optimize_skip_unused_shards up to the second level. Default value: 0 "},{"title":"force_optimize_skip_unused_shards​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#force-optimize-skip-unused-shards","content":"Enables or disables query execution if optimize_skip_unused_shards is enabled and skipping of unused shards is not possible. If the skipping is not possible and the setting is enabled, an exception will be thrown. Possible values: 0 — Disabled. ClickHouse does not throw an exception.1 — Enabled. Query execution is disabled only if the table has a sharding key.2 — Enabled. Query execution is disabled regardless of whether a sharding key is defined for the table. Default value: 0 "},{"title":"force_optimize_skip_unused_shards_nesting​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-force_optimize_skip_unused_shards_nesting","content":"Controls force_optimize_skip_unused_shards (hence still requires force_optimize_skip_unused_shards) depends on the nesting level of the distributed query (case when you have Distributed table that look into another Distributed table). Possible values: 0 - Disabled, force_optimize_skip_unused_shards works always.1 — Enables force_optimize_skip_unused_shards only for the first level.2 — Enables force_optimize_skip_unused_shards up to the second level. Default value: 0 "},{"title":"optimize_distributed_group_by_sharding_key​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#optimize-distributed-group-by-sharding-key","content":"Optimize GROUP BY sharding_key queries, by avoiding costly aggregation on the initiator server (which will reduce memory usage for the query on the initiator server). The following types of queries are supported (and all combinations of them): SELECT DISTINCT [..., ]sharding_key[, ...] FROM distSELECT ... FROM dist GROUP BY sharding_key[, ...]SELECT ... FROM dist GROUP BY sharding_key[, ...] ORDER BY xSELECT ... FROM dist GROUP BY sharding_key[, ...] LIMIT 1SELECT ... FROM dist GROUP BY sharding_key[, ...] LIMIT 1 BY x The following types of queries are not supported (support for some of them may be added later): SELECT ... GROUP BY sharding_key[, ...] WITH TOTALSSELECT ... GROUP BY sharding_key[, ...] WITH ROLLUPSELECT ... GROUP BY sharding_key[, ...] WITH CUBESELECT ... GROUP BY sharding_key[, ...] SETTINGS extremes=1 Possible values: 0 — Disabled.1 — Enabled. Default value: 0 See also: distributed_group_by_no_mergedistributed_push_down_limitoptimize_skip_unused_shards note Right now it requires optimize_skip_unused_shards (the reason behind this is that one day it may be enabled by default, and it will work correctly only if data was inserted via Distributed table, i.e. data is distributed according to sharding_key). "},{"title":"optimize_throw_if_noop​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#setting-optimize_throw_if_noop","content":"Enables or disables throwing an exception if an OPTIMIZE query didn’t perform a merge. By default, OPTIMIZE returns successfully even if it didn’t do anything. This setting lets you differentiate these situations and get the reason in an exception message. Possible values: 1 — Throwing an exception is enabled.0 — Throwing an exception is disabled. Default value: 0. "},{"title":"optimize_functions_to_subcolumns​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#optimize-functions-to-subcolumns","content":"Enables or disables optimization by transforming some functions to reading subcolumns. This reduces the amount of data to read. These functions can be transformed: length to read the size0 subcolumn.empty to read the size0 subcolumn.notEmpty to read the size0 subcolumn.isNull to read the null subcolumn.isNotNull to read the null subcolumn.count to read the null subcolumn.mapKeys to read the keys subcolumn.mapValues to read the values subcolumn. Possible values: 0 — Optimization disabled.1 — Optimization enabled. Default value: 0. "},{"title":"optimize_trivial_count_query​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#optimize-trivial-count-query","content":"Enables or disables the optimization to trivial query SELECT count() FROM table using metadata from MergeTree. If you need to use row-level security, disable this setting. Possible values: 0 — Optimization disabled.1 — Optimization enabled. Default value: 1. See also: optimize_functions_to_subcolumns "},{"title":"distributed_replica_error_half_life​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-distributed_replica_error_half_life","content":"Type: secondsDefault value: 60 seconds Controls how fast errors in distributed tables are zeroed. If a replica is unavailable for some time, accumulates 5 errors, and distributed_replica_error_half_life is set to 1 second, then the replica is considered normal 3 seconds after the last error. See also: load_balancingTable engine Distributeddistributed_replica_error_capdistributed_replica_max_ignored_errors "},{"title":"distributed_replica_error_cap​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-distributed_replica_error_cap","content":"Type: unsigned intDefault value: 1000 The error count of each replica is capped at this value, preventing a single replica from accumulating too many errors. See also: load_balancingTable engine Distributeddistributed_replica_error_half_lifedistributed_replica_max_ignored_errors "},{"title":"distributed_replica_max_ignored_errors​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-distributed_replica_max_ignored_errors","content":"Type: unsigned intDefault value: 0 The number of errors that will be ignored while choosing replicas (according to load_balancing algorithm). See also: load_balancingTable engine Distributeddistributed_replica_error_capdistributed_replica_error_half_life "},{"title":"distributed_directory_monitor_sleep_time_ms​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#distributed_directory_monitor_sleep_time_ms","content":"Base interval for the Distributed table engine to send data. The actual interval grows exponentially in the event of errors. Possible values: A positive integer number of milliseconds. Default value: 100 milliseconds. "},{"title":"distributed_directory_monitor_max_sleep_time_ms​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#distributed_directory_monitor_max_sleep_time_ms","content":"Maximum interval for the Distributed table engine to send data. Limits exponential growth of the interval set in the distributed_directory_monitor_sleep_time_ms setting. Possible values: A positive integer number of milliseconds. Default value: 30000 milliseconds (30 seconds). "},{"title":"distributed_directory_monitor_batch_inserts​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#distributed_directory_monitor_batch_inserts","content":"Enables/disables inserted data sending in batches. When batch sending is enabled, the Distributed table engine tries to send multiple files of inserted data in one operation instead of sending them separately. Batch sending improves cluster performance by better-utilizing server and network resources. Possible values: 1 — Enabled.0 — Disabled. Default value: 0. "},{"title":"distributed_directory_monitor_split_batch_on_failure​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#distributed_directory_monitor_split_batch_on_failure","content":"Enables/disables splitting batches on failures. Sometimes sending particular batch to the remote shard may fail, because of some complex pipeline after (i.e. MATERIALIZED VIEW with GROUP BY) due to Memory limit exceeded or similar errors. In this case, retrying will not help (and this will stuck distributed sends for the table) but sending files from that batch one by one may succeed INSERT. So installing this setting to 1 will disable batching for such batches (i.e. temporary disables distributed_directory_monitor_batch_inserts for failed batches). Possible values: 1 — Enabled.0 — Disabled. Default value: 0. note This setting also affects broken batches (that may appears because of abnormal server (machine) termination and no fsync_after_insert/fsync_directories for Distributed table engine). warning You should not rely on automatic batch splitting, since this may hurt performance. "},{"title":"os_thread_priority​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#setting-os-thread-priority","content":"Sets the priority (nice) for threads that execute queries. The OS scheduler considers this priority when choosing the next thread to run on each available CPU core. warning To use this setting, you need to set the CAP_SYS_NICE capability. The clickhouse-server package sets it up during installation. Some virtual environments do not allow you to set the CAP_SYS_NICE capability. In this case, clickhouse-server shows a message about it at the start. Possible values: You can set values in the range [-20, 19]. Lower values mean higher priority. Threads with low nice priority values are executed more frequently than threads with high values. High values are preferable for long-running non-interactive queries because it allows them to quickly give up resources in favour of short interactive queries when they arrive. Default value: 0. "},{"title":"query_profiler_real_time_period_ns​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#query_profiler_real_time_period_ns","content":"Sets the period for a real clock timer of the query profiler. Real clock timer counts wall-clock time. Possible values: Positive integer number, in nanoseconds. Recommended values: - 10000000 (100 times a second) nanoseconds and less for single queries. - 1000000000 (once a second) for cluster-wide profiling. 0 for turning off the timer. Type: UInt64. Default value: 1000000000 nanoseconds (once a second). See also: System table trace_log "},{"title":"query_profiler_cpu_time_period_ns​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#query_profiler_cpu_time_period_ns","content":"Sets the period for a CPU clock timer of the query profiler. This timer counts only CPU time. Possible values: A positive integer number of nanoseconds. Recommended values: - 10000000 (100 times a second) nanoseconds and more for single queries. - 1000000000 (once a second) for cluster-wide profiling. 0 for turning off the timer. Type: UInt64. Default value: 1000000000 nanoseconds. See also: System table trace_log "},{"title":"allow_introspection_functions​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-allow_introspection_functions","content":"Enables or disables introspections functions for query profiling. Possible values: 1 — Introspection functions enabled.0 — Introspection functions disabled. Default value: 0. See Also Sampling Query ProfilerSystem table trace_log "},{"title":"input_format_parallel_parsing​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#input-format-parallel-parsing","content":"Enables or disables order-preserving parallel parsing of data formats. Supported only for TSV, TKSV, CSV and JSONEachRow formats. Possible values: 1 — Enabled.0 — Disabled. Default value: 1. "},{"title":"output_format_parallel_formatting​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#output-format-parallel-formatting","content":"Enables or disables parallel formatting of data formats. Supported only for TSV, TKSV, CSV and JSONEachRow formats. Possible values: 1 — Enabled.0 — Disabled. Default value: 1. "},{"title":"min_chunk_bytes_for_parallel_parsing​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#min-chunk-bytes-for-parallel-parsing","content":"Type: unsigned intDefault value: 1 MiB The minimum chunk size in bytes, which each thread will parse in parallel. "},{"title":"output_format_avro_codec​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-output_format_avro_codec","content":"Sets the compression codec used for output Avro file. Type: string Possible values: null — No compressiondeflate — Compress with Deflate (zlib)snappy — Compress with Snappy Default value: snappy (if available) or deflate. "},{"title":"output_format_avro_sync_interval​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#settings-output_format_avro_sync_interval","content":"Sets minimum data size (in bytes) between synchronization markers for output Avro file. Type: unsigned int Possible values: 32 (32 bytes) - 1073741824 (1 GiB) Default value: 32768 (32 KiB) "},{"title":"output_format_avro_string_column_pattern​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#output_format_avro_string_column_pattern","content":"Regexp of column names of type String to output as Avro string (default is bytes). RE2 syntax is supported. Type: string "},{"title":"format_avro_schema_registry_url​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#format_avro_schema_registry_url","content":"Sets Confluent Schema Registry URL to use with AvroConfluent format. Default value: Empty. "},{"title":"input_format_avro_allow_missing_fields​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#input_format_avro_allow_missing_fields","content":"Enables using fields that are not specified in Avro or AvroConfluent format schema. When a field is not found in the schema, ClickHouse uses the default value instead of throwing an exception. Possible values: 0 — Disabled.1 — Enabled. Default value: 0. "},{"title":"background_pool_size​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#background_pool_size","content":"Sets the number of threads performing background operations in table engines (for example, merges in MergeTree engine tables). This setting is applied from the default profile at the ClickHouse server start and can’t be changed in a user session. By adjusting this setting, you manage CPU and disk load. Smaller pool size utilizes less CPU and disk resources, but background processes advance slower which might eventually impact query performance. Before changing it, please also take a look at related MergeTree settings, such as number_of_free_entries_in_pool_to_lower_max_size_of_merge and number_of_free_entries_in_pool_to_execute_mutation. Possible values: Any positive integer. Default value: 16. "},{"title":"merge_selecting_sleep_ms​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#merge_selecting_sleep_ms","content":"Sleep time for merge selecting when no part is selected. A lower setting triggers selecting tasks in background_schedule_pool frequently, which results in a large number of requests to Zookeeper in large-scale clusters. Possible values: Any positive integer. Default value: 5000. "},{"title":"parallel_distributed_insert_select​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#parallel_distributed_insert_select","content":"Enables parallel distributed INSERT ... SELECT query. If we execute INSERT INTO distributed_table_a SELECT ... FROM distributed_table_b queries and both tables use the same cluster, and both tables are either replicated or non-replicated, then this query is processed locally on every shard. Possible values: 0 — Disabled.1 — SELECT will be executed on each shard from the underlying table of the distributed engine.2 — SELECT and INSERT will be executed on each shard from/to the underlying table of the distributed engine. Default value: 0. "},{"title":"insert_distributed_sync​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#insert_distributed_sync","content":"Enables or disables synchronous data insertion into a Distributed table. By default, when inserting data into a Distributed table, the ClickHouse server sends data to cluster nodes in asynchronous mode. When insert_distributed_sync=1, the data is processed synchronously, and the INSERT operation succeeds only after all the data is saved on all shards (at least one replica for each shard if internal_replication is true). Possible values: 0 — Data is inserted in asynchronous mode.1 — Data is inserted in synchronous mode. Default value: 0. See Also Distributed Table EngineManaging Distributed Tables "},{"title":"insert_distributed_one_random_shard​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#insert_distributed_one_random_shard","content":"Enables or disables random shard insertion into a Distributed table when there is no distributed key. By default, when inserting data into a Distributed table with more than one shard, the ClickHouse server will reject any insertion request if there is no distributed key. When insert_distributed_one_random_shard = 1, insertions are allowed and data is forwarded randomly among all shards. Possible values: 0 — Insertion is rejected if there are multiple shards and no distributed key is given.1 — Insertion is done randomly among all available shards when no distributed key is given. Default value: 0. "},{"title":"insert_shard_id​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#insert_shard_id","content":"If not 0, specifies the shard of Distributed table into which the data will be inserted synchronously. If insert_shard_id value is incorrect, the server will throw an exception. To get the number of shards on requested_cluster, you can check server config or use this query: SELECT uniq(shard_num) FROM system.clusters WHERE cluster = 'requested_cluster';  Possible values: 0 — Disabled.Any number from 1 to shards_num of corresponding Distributed table. Default value: 0. Example Query: CREATE TABLE x AS system.numbers ENGINE = MergeTree ORDER BY number; CREATE TABLE x_dist AS x ENGINE = Distributed('test_cluster_two_shards_localhost', currentDatabase(), x); INSERT INTO x_dist SELECT * FROM numbers(5) SETTINGS insert_shard_id = 1; SELECT * FROM x_dist ORDER BY number ASC;  Result: ┌─number─┐ │ 0 │ │ 0 │ │ 1 │ │ 1 │ │ 2 │ │ 2 │ │ 3 │ │ 3 │ │ 4 │ │ 4 │ └────────┘  "},{"title":"use_compact_format_in_distributed_parts_names​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#use_compact_format_in_distributed_parts_names","content":"Uses compact format for storing blocks for async (insert_distributed_sync) INSERT into tables with Distributed engine. Possible values: 0 — Uses user[:password]@host:port#default_database directory format.1 — Uses [shard{shard_index}[_replica{replica_index}]] directory format. Default value: 1. note with use_compact_format_in_distributed_parts_names=0 changes from cluster definition will not be applied for async INSERT.with use_compact_format_in_distributed_parts_names=1 changing the order of the nodes in the cluster definition, will change the shard_index/replica_index so be aware. "},{"title":"background_buffer_flush_schedule_pool_size​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#background_buffer_flush_schedule_pool_size","content":"Sets the number of threads performing background flush in Buffer-engine tables. This setting is applied at the ClickHouse server start and can’t be changed in a user session. Possible values: Any positive integer. Default value: 16. "},{"title":"background_move_pool_size​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#background_move_pool_size","content":"Sets the number of threads performing background moves of data parts for MergeTree-engine tables. This setting is applied at the ClickHouse server start and can’t be changed in a user session. Possible values: Any positive integer. Default value: 8. "},{"title":"background_schedule_pool_size​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#background_schedule_pool_size","content":"Sets the number of threads performing background tasks for replicated tables, Kafka streaming, DNS cache updates. This setting is applied at ClickHouse server start and can’t be changed in a user session. Possible values: Any positive integer. Default value: 128. "},{"title":"background_fetches_pool_size​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#background_fetches_pool_size","content":"Sets the number of threads performing background fetches for replicated tables. This setting is applied at the ClickHouse server start and can’t be changed in a user session. For production usage with frequent small insertions or slow ZooKeeper cluster is recommended to use default value. Possible values: Any positive integer. Default value: 8. "},{"title":"always_fetch_merged_part​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#always_fetch_merged_part","content":"Prohibits data parts merging in Replicated*MergeTree-engine tables. When merging is prohibited, the replica never merges parts and always downloads merged parts from other replicas. If there is no required data yet, the replica waits for it. CPU and disk load on the replica server decreases, but the network load on the cluster increases. This setting can be useful on servers with relatively weak CPUs or slow disks, such as servers for backups storage. Possible values: 0 — Replicated*MergeTree-engine tables merge data parts at the replica.1 — Replicated*MergeTree-engine tables do not merge data parts at the replica. The tables download merged data parts from other replicas. Default value: 0. See Also Data Replication "},{"title":"background_distributed_schedule_pool_size​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#background_distributed_schedule_pool_size","content":"Sets the number of threads performing background tasks for distributed sends. This setting is applied at the ClickHouse server start and can’t be changed in a user session. Possible values: Any positive integer. Default value: 16. "},{"title":"background_message_broker_schedule_pool_size​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#background_message_broker_schedule_pool_size","content":"Sets the number of threads performing background tasks for message streaming. This setting is applied at the ClickHouse server start and can’t be changed in a user session. Possible values: Any positive integer. Default value: 16. See Also Kafka engine.RabbitMQ engine. "},{"title":"validate_polygons​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#validate_polygons","content":"Enables or disables throwing an exception in the pointInPolygon function, if the polygon is self-intersecting or self-tangent. Possible values: 0 — Throwing an exception is disabled. pointInPolygon accepts invalid polygons and returns possibly incorrect results for them.1 — Throwing an exception is enabled. Default value: 1. "},{"title":"transform_null_in​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#transform_null_in","content":"Enables equality of NULL values for IN operator. By default, NULL values can’t be compared because NULL means undefined value. Thus, comparison expr = NULL must always return false. With this setting NULL = NULL returns true for IN operator. Possible values: 0 — Comparison of NULL values in IN operator returns false.1 — Comparison of NULL values in IN operator returns true. Default value: 0. Example Consider the null_in table: ┌──idx─┬─────i─┐ │ 1 │ 1 │ │ 2 │ NULL │ │ 3 │ 3 │ └──────┴───────┘  Query: SELECT idx, i FROM null_in WHERE i IN (1, NULL) SETTINGS transform_null_in = 0;  Result: ┌──idx─┬────i─┐ │ 1 │ 1 │ └──────┴──────┘  Query: SELECT idx, i FROM null_in WHERE i IN (1, NULL) SETTINGS transform_null_in = 1;  Result: ┌──idx─┬─────i─┐ │ 1 │ 1 │ │ 2 │ NULL │ └──────┴───────┘  See Also NULL Processing in IN Operators "},{"title":"low_cardinality_max_dictionary_size​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#low_cardinality_max_dictionary_size","content":"Sets a maximum size in rows of a shared global dictionary for the LowCardinality data type that can be written to a storage file system. This setting prevents issues with RAM in case of unlimited dictionary growth. All the data that can’t be encoded due to maximum dictionary size limitation ClickHouse writes in an ordinary method. Possible values: Any positive integer. Default value: 8192. "},{"title":"low_cardinality_use_single_dictionary_for_part​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#low_cardinality_use_single_dictionary_for_part","content":"Turns on or turns off using of single dictionary for the data part. By default, the ClickHouse server monitors the size of dictionaries and if a dictionary overflows then the server starts to write the next one. To prohibit creating several dictionaries set low_cardinality_use_single_dictionary_for_part = 1. Possible values: 1 — Creating several dictionaries for the data part is prohibited.0 — Creating several dictionaries for the data part is not prohibited. Default value: 0. "},{"title":"low_cardinality_allow_in_native_format​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#low_cardinality_allow_in_native_format","content":"Allows or restricts using the LowCardinality data type with the Native format. If usage of LowCardinality is restricted, ClickHouse server converts LowCardinality-columns to ordinary ones for SELECT queries, and convert ordinary columns to LowCardinality-columns for INSERT queries. This setting is required mainly for third-party clients which do not support LowCardinality data type. Possible values: 1 — Usage of LowCardinality is not restricted.0 — Usage of LowCardinality is restricted. Default value: 1. "},{"title":"allow_suspicious_low_cardinality_types​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#allow_suspicious_low_cardinality_types","content":"Allows or restricts using LowCardinality with data types with fixed size of 8 bytes or less: numeric data types and FixedString(8_bytes_or_less). For small fixed values using of LowCardinality is usually inefficient, because ClickHouse stores a numeric index for each row. As a result: Disk space usage can rise.RAM consumption can be higher, depending on a dictionary size.Some functions can work slower due to extra coding/encoding operations. Merge times in MergeTree-engine tables can grow due to all the reasons described above. Possible values: 1 — Usage of LowCardinality is not restricted.0 — Usage of LowCardinality is restricted. Default value: 0. "},{"title":"min_insert_block_size_rows_for_materialized_views​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#min-insert-block-size-rows-for-materialized-views","content":"Sets the minimum number of rows in the block which can be inserted into a table by an INSERT query. Smaller-sized blocks are squashed into bigger ones. This setting is applied only for blocks inserted into materialized view. By adjusting this setting, you control blocks squashing while pushing to materialized view and avoid excessive memory usage. Possible values: Any positive integer.0 — Squashing disabled. Default value: 1048576. See Also min_insert_block_size_rows "},{"title":"min_insert_block_size_bytes_for_materialized_views​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#min-insert-block-size-bytes-for-materialized-views","content":"Sets the minimum number of bytes in the block which can be inserted into a table by an INSERT query. Smaller-sized blocks are squashed into bigger ones. This setting is applied only for blocks inserted into materialized view. By adjusting this setting, you control blocks squashing while pushing to materialized view and avoid excessive memory usage. Possible values: Any positive integer.0 — Squashing disabled. Default value: 268435456. See also min_insert_block_size_bytes "},{"title":"output_format_pretty_grid_charset​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#output-format-pretty-grid-charset","content":"Allows changing a charset which is used for printing grids borders. Available charsets are UTF-8, ASCII. Example SET output_format_pretty_grid_charset = 'UTF-8'; SELECT * FROM a; ┌─a─┐ │ 1 │ └───┘ SET output_format_pretty_grid_charset = 'ASCII'; SELECT * FROM a; +-a-+ | 1 | +---+  "},{"title":"optimize_read_in_order​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#optimize_read_in_order","content":"Enables ORDER BY optimization in SELECT queries for reading data from MergeTree tables. Possible values: 0 — ORDER BY optimization is disabled.1 — ORDER BY optimization is enabled. Default value: 1. See Also ORDER BY Clause "},{"title":"optimize_aggregation_in_order​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#optimize_aggregation_in_order","content":"Enables GROUP BY optimization in SELECT queries for aggregating data in corresponding order in MergeTree tables. Possible values: 0 — GROUP BY optimization is disabled.1 — GROUP BY optimization is enabled. Default value: 0. See Also GROUP BY optimization "},{"title":"mutations_sync​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#mutations_sync","content":"Allows to execute ALTER TABLE ... UPDATE|DELETE queries (mutations) synchronously. Possible values: 0 - Mutations execute asynchronously.1 - The query waits for all mutations to complete on the current server.2 - The query waits for all mutations to complete on all replicas (if they exist). Default value: 0. See Also Synchronicity of ALTER QueriesMutations "},{"title":"ttl_only_drop_parts​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#ttl_only_drop_parts","content":"Enables or disables complete dropping of data parts where all rows are expired in MergeTree tables. When ttl_only_drop_parts is disabled (by default), the ClickHouse server only deletes expired rows according to their TTL. When ttl_only_drop_parts is enabled, the ClickHouse server drops a whole part when all rows in it are expired. Dropping whole parts instead of partial cleaning TTL-d rows allows having shorter merge_with_ttl_timeout times and lower impact on system performance. Possible values: 0 — The complete dropping of data parts is disabled.1 — The complete dropping of data parts is enabled. Default value: 0. See Also CREATE TABLE query clauses and settings (merge_with_ttl_timeout setting)Table TTL "},{"title":"lock_acquire_timeout​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#lock_acquire_timeout","content":"Defines how many seconds a locking request waits before failing. Locking timeout is used to protect from deadlocks while executing read/write operations with tables. When the timeout expires and the locking request fails, the ClickHouse server throws an exception &quot;Locking attempt timed out! Possible deadlock avoided. Client should retry.&quot; with error code DEADLOCK_AVOIDED. Possible values: Positive integer (in seconds).0 — No locking timeout. Default value: 120 seconds. "},{"title":"cast_keep_nullable​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#cast_keep_nullable","content":"Enables or disables keeping of the Nullable data type in CAST operations. When the setting is enabled and the argument of CAST function is Nullable, the result is also transformed to Nullable type. When the setting is disabled, the result always has the destination type exactly. Possible values: 0 — The CAST result has exactly the destination type specified.1 — If the argument type is Nullable, the CAST result is transformed to Nullable(DestinationDataType). Default value: 0. Examples The following query results in the destination data type exactly: SET cast_keep_nullable = 0; SELECT CAST(toNullable(toInt32(0)) AS Int32) as x, toTypeName(x);  Result: ┌─x─┬─toTypeName(CAST(toNullable(toInt32(0)), 'Int32'))─┐ │ 0 │ Int32 │ └───┴───────────────────────────────────────────────────┘  The following query results in the Nullable modification on the destination data type: SET cast_keep_nullable = 1; SELECT CAST(toNullable(toInt32(0)) AS Int32) as x, toTypeName(x);  Result: ┌─x─┬─toTypeName(CAST(toNullable(toInt32(0)), 'Int32'))─┐ │ 0 │ Nullable(Int32) │ └───┴───────────────────────────────────────────────────┘  See Also CAST function "},{"title":"output_format_pretty_max_value_width​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#output_format_pretty_max_value_width","content":"Limits the width of value displayed in Pretty formats. If the value width exceeds the limit, the value is cut. Possible values: Positive integer.0 — The value is cut completely. Default value: 10000 symbols. Examples Query: SET output_format_pretty_max_value_width = 10; SELECT range(number) FROM system.numbers LIMIT 10 FORMAT PrettyCompactNoEscapes;  Result: ┌─range(number)─┐ │ [] │ │ [0] │ │ [0,1] │ │ [0,1,2] │ │ [0,1,2,3] │ │ [0,1,2,3,4⋯ │ │ [0,1,2,3,4⋯ │ │ [0,1,2,3,4⋯ │ │ [0,1,2,3,4⋯ │ │ [0,1,2,3,4⋯ │ └───────────────┘  Query with zero width: SET output_format_pretty_max_value_width = 0; SELECT range(number) FROM system.numbers LIMIT 5 FORMAT PrettyCompactNoEscapes;  Result: ┌─range(number)─┐ │ ⋯ │ │ ⋯ │ │ ⋯ │ │ ⋯ │ │ ⋯ │ └───────────────┘  "},{"title":"output_format_pretty_row_numbers​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#output_format_pretty_row_numbers","content":"Adds row numbers to output in the Pretty format. Possible values: 0 — Output without row numbers.1 — Output with row numbers. Default value: 0. Example Query: SET output_format_pretty_row_numbers = 1; SELECT TOP 3 name, value FROM system.settings;  Result:  ┌─name────────────────────┬─value───┐ 1. │ min_compress_block_size │ 65536 │ 2. │ max_compress_block_size │ 1048576 │ 3. │ max_block_size │ 65505 │ └─────────────────────────┴─────────┘  "},{"title":"system_events_show_zero_values​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#system_events_show_zero_values","content":"Allows to select zero-valued events from system.events. Some monitoring systems require passing all the metrics values to them for each checkpoint, even if the metric value is zero. Possible values: 0 — Disabled.1 — Enabled. Default value: 0. Examples Query SELECT * FROM system.events WHERE event='QueryMemoryLimitExceeded';  Result Ok.  Query SET system_events_show_zero_values = 1; SELECT * FROM system.events WHERE event='QueryMemoryLimitExceeded';  Result ┌─event────────────────────┬─value─┬─description───────────────────────────────────────────┐ │ QueryMemoryLimitExceeded │ 0 │ Number of times when memory limit exceeded for query. │ └──────────────────────────┴───────┴───────────────────────────────────────────────────────┘  "},{"title":"persistent​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#persistent","content":"Disables persistency for the Set and Join table engines. Reduces the I/O overhead. Suitable for scenarios that pursue performance and do not require persistence. Possible values: 1 — Enabled.0 — Disabled. Default value: 1. "},{"title":"format_csv_null_representation​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#format_csv_null_representation","content":"Defines the representation of NULL for CSV output and input formats. User can set any string as a value, for example, My NULL. Default value: \\N. Examples Query SELECT * from csv_custom_null FORMAT CSV;  Result 788 \\N \\N  Query SET format_csv_null_representation = 'My NULL'; SELECT * FROM csv_custom_null FORMAT CSV;  Result 788 My NULL My NULL  "},{"title":"format_tsv_null_representation​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#format_tsv_null_representation","content":"Defines the representation of NULL for TSV output and input formats. User can set any string as a value, for example, My NULL. Default value: \\N. Examples Query SELECT * FROM tsv_custom_null FORMAT TSV;  Result 788 \\N \\N  Query SET format_tsv_null_representation = 'My NULL'; SELECT * FROM tsv_custom_null FORMAT TSV;  Result 788 My NULL My NULL  "},{"title":"output_format_json_array_of_rows​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#output-format-json-array-of-rows","content":"Enables the ability to output all rows as a JSON array in the JSONEachRow format. Possible values: 1 — ClickHouse outputs all rows as an array, each row in the JSONEachRow format.0 — ClickHouse outputs each row separately in the JSONEachRow format. Default value: 0. Example of a query with the enabled setting Query: SET output_format_json_array_of_rows = 1; SELECT number FROM numbers(3) FORMAT JSONEachRow;  Result: [ {&quot;number&quot;:&quot;0&quot;}, {&quot;number&quot;:&quot;1&quot;}, {&quot;number&quot;:&quot;2&quot;} ]  Example of a query with the disabled setting Query: SET output_format_json_array_of_rows = 0; SELECT number FROM numbers(3) FORMAT JSONEachRow;  Result: {&quot;number&quot;:&quot;0&quot;} {&quot;number&quot;:&quot;1&quot;} {&quot;number&quot;:&quot;2&quot;}  "},{"title":"allow_nullable_key​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#allow-nullable-key","content":"Allows using of the Nullable-typed values in a sorting and a primary key for MergeTree tables. Possible values: 1 — Nullable-type expressions are allowed in keys.0 — Nullable-type expressions are not allowed in keys. Default value: 0. warning Nullable primary key usually indicates bad design. It is forbidden in almost all main stream DBMS. The feature is mainly for AggregatingMergeTree and is not heavily tested. Use with care. warning Do not enable this feature in version &lt;= 21.8. It's not properly implemented and may lead to server crash. "},{"title":"aggregate_functions_null_for_empty​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#aggregate_functions_null_for_empty","content":"Enables or disables rewriting all aggregate functions in a query, adding -OrNull suffix to them. Enable it for SQL standard compatibility. It is implemented via query rewrite (similar to count_distinct_implementation setting) to get consistent results for distributed queries. Possible values: 0 — Disabled.1 — Enabled. Default value: 0. Example Consider the following query with aggregate functions: SELECT SUM(-1), MAX(0) FROM system.one WHERE 0;  With aggregate_functions_null_for_empty = 0 it would produce: ┌─SUM(-1)─┬─MAX(0)─┐ │ 0 │ 0 │ └─────────┴────────┘  With aggregate_functions_null_for_empty = 1 the result would be: ┌─SUMOrNull(-1)─┬─MAXOrNull(0)─┐ │ NULL │ NULL │ └───────────────┴──────────────┘  "},{"title":"union_default_mode​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#union-default-mode","content":"Sets a mode for combining SELECT query results. The setting is only used when shared with UNION without explicitly specifying the UNION ALL or UNION DISTINCT. Possible values: 'DISTINCT' — ClickHouse outputs rows as a result of combining queries removing duplicate rows.'ALL' — ClickHouse outputs all rows as a result of combining queries including duplicate rows.'' — ClickHouse generates an exception when used with UNION. Default value: ''. See examples in UNION. "},{"title":"data_type_default_nullable​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#data_type_default_nullable","content":"Allows data types without explicit modifiers NULL or NOT NULL in column definition will be Nullable. Possible values: 1 — The data types in column definitions are set to Nullable by default.0 — The data types in column definitions are set to not Nullable by default. Default value: 0. "},{"title":"execute_merges_on_single_replica_time_threshold​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#execute-merges-on-single-replica-time-threshold","content":"Enables special logic to perform merges on replicas. Possible values: Positive integer (in seconds).0 — Special merges logic is not used. Merges happen in the usual way on all the replicas. Default value: 0. Usage Selects one replica to perform the merge on. Sets the time threshold from the start of the merge. Other replicas wait for the merge to finish, then download the result. If the time threshold passes and the selected replica does not perform the merge, then the merge is performed on other replicas as usual. High values for that threshold may lead to replication delays. It can be useful when merges are CPU bounded not IO bounded (performing heavy data compression, calculating aggregate functions or default expressions that require a large amount of calculations, or just very high number of tiny merges). "},{"title":"max_final_threads​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#max-final-threads","content":"Sets the maximum number of parallel threads for the SELECT query data read phase with the FINAL modifier. Possible values: Positive integer.0 or 1 — Disabled. SELECT queries are executed in a single thread. Default value: 16. "},{"title":"opentelemetry_start_trace_probability​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#opentelemetry-start-trace-probability","content":"Sets the probability that the ClickHouse can start a trace for executed queries (if no parent trace context is supplied). Possible values: 0 — The trace for all executed queries is disabled (if no parent trace context is supplied).Positive floating-point number in the range [0..1]. For example, if the setting value is 0,5, ClickHouse can start a trace on average for half of the queries.1 — The trace for all executed queries is enabled. Default value: 0. "},{"title":"optimize_on_insert​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#optimize-on-insert","content":"Enables or disables data transformation before the insertion, as if merge was done on this block (according to table engine). Possible values: 0 — Disabled.1 — Enabled. Default value: 1. Example The difference between enabled and disabled: Query: SET optimize_on_insert = 1; CREATE TABLE test1 (`FirstTable` UInt32) ENGINE = ReplacingMergeTree ORDER BY FirstTable; INSERT INTO test1 SELECT number % 2 FROM numbers(5); SELECT * FROM test1; SET optimize_on_insert = 0; CREATE TABLE test2 (`SecondTable` UInt32) ENGINE = ReplacingMergeTree ORDER BY SecondTable; INSERT INTO test2 SELECT number % 2 FROM numbers(5); SELECT * FROM test2;  Result: ┌─FirstTable─┐ │ 0 │ │ 1 │ └────────────┘ ┌─SecondTable─┐ │ 0 │ │ 0 │ │ 0 │ │ 1 │ │ 1 │ └─────────────┘  Note that this setting influences Materialized view and MaterializedMySQL behaviour. "},{"title":"engine_file_empty_if_not_exists​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#engine-file-empty_if-not-exists","content":"Allows to select data from a file engine table without file. Possible values: 0 — SELECT throws exception.1 — SELECT returns empty result. Default value: 0. "},{"title":"engine_file_truncate_on_insert​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#engine-file-truncate-on-insert","content":"Enables or disables truncate before insert in File engine tables. Possible values: 0 — INSERT query appends new data to the end of the file.1 — INSERT replaces existing content of the file with the new data. Default value: 0. "},{"title":"allow_experimental_geo_types​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#allow-experimental-geo-types","content":"Allows working with experimental geo data types. Possible values: 0 — Working with geo data types is disabled.1 — Working with geo data types is enabled. Default value: 0. "},{"title":"database_atomic_wait_for_drop_and_detach_synchronously​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#database_atomic_wait_for_drop_and_detach_synchronously","content":"Adds a modifier SYNC to all DROP and DETACH queries. Possible values: 0 — Queries will be executed with delay.1 — Queries will be executed without delay. Default value: 0. "},{"title":"show_table_uuid_in_table_create_query_if_not_nil​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#show_table_uuid_in_table_create_query_if_not_nil","content":"Sets the SHOW TABLE query display. Possible values: 0 — The query will be displayed without table UUID.1 — The query will be displayed with table UUID. Default value: 0. "},{"title":"allow_experimental_live_view​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#allow-experimental-live-view","content":"Allows creation of experimental live views. Possible values: 0 — Working with live views is disabled.1 — Working with live views is enabled. Default value: 0. "},{"title":"live_view_heartbeat_interval​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#live-view-heartbeat-interval","content":"Sets the heartbeat interval in seconds to indicate live view is alive . Default value: 15. "},{"title":"max_live_view_insert_blocks_before_refresh​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#max-live-view-insert-blocks-before-refresh","content":"Sets the maximum number of inserted blocks after which mergeable blocks are dropped and query for live view is re-executed. Default value: 64. "},{"title":"temporary_live_view_timeout​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#temporary-live-view-timeout","content":"Sets the interval in seconds after which live view with timeout is deleted. Default value: 5. "},{"title":"periodic_live_view_refresh​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#periodic-live-view-refresh","content":"Sets the interval in seconds after which periodically refreshed live view is forced to refresh. Default value: 60. "},{"title":"http_connection_timeout​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#http_connection_timeout","content":"HTTP connection timeout (in seconds). Possible values: Any positive integer.0 - Disabled (infinite timeout). Default value: 1. "},{"title":"http_send_timeout​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#http_send_timeout","content":"HTTP send timeout (in seconds). Possible values: Any positive integer.0 - Disabled (infinite timeout). Default value: 1800. "},{"title":"http_receive_timeout​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#http_receive_timeout","content":"HTTP receive timeout (in seconds). Possible values: Any positive integer.0 - Disabled (infinite timeout). Default value: 1800. "},{"title":"check_query_single_value_result​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#check_query_single_value_result","content":"Defines the level of detail for the CHECK TABLE query result for MergeTree family engines . Possible values: 0 — the query shows a check status for every individual data part of a table.1 — the query shows the general table check status. Default value: 0. "},{"title":"prefer_column_name_to_alias​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#prefer-column-name-to-alias","content":"Enables or disables using the original column names instead of aliases in query expressions and clauses. It especially matters when alias is the same as the column name, see Expression Aliases. Enable this setting to make aliases syntax rules in ClickHouse more compatible with most other database engines. Possible values: 0 — The column name is substituted with the alias.1 — The column name is not substituted with the alias. Default value: 0. Example The difference between enabled and disabled: Query: SET prefer_column_name_to_alias = 0; SELECT avg(number) AS number, max(number) FROM numbers(10);  Result: Received exception from server (version 21.5.1): Code: 184. DB::Exception: Received from localhost:9000. DB::Exception: Aggregate function avg(number) is found inside another aggregate function in query: While processing avg(number) AS number.  Query: SET prefer_column_name_to_alias = 1; SELECT avg(number) AS number, max(number) FROM numbers(10);  Result: ┌─number─┬─max(number)─┐ │ 4.5 │ 9 │ └────────┴─────────────┘  "},{"title":"limit​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#limit","content":"Sets the maximum number of rows to get from the query result. It adjusts the value set by the LIMIT clause, so that the limit, specified in the query, cannot exceed the limit, set by this setting. Possible values: 0 — The number of rows is not limited.Positive integer. Default value: 0. "},{"title":"offset​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#offset","content":"Sets the number of rows to skip before starting to return rows from the query. It adjusts the offset set by the OFFSET clause, so that these two values are summarized. Possible values: 0 — No rows are skipped .Positive integer. Default value: 0. Example Input table: CREATE TABLE test (i UInt64) ENGINE = MergeTree() ORDER BY i; INSERT INTO test SELECT number FROM numbers(500);  Query: SET limit = 5; SET offset = 7; SELECT * FROM test LIMIT 10 OFFSET 100;  Result: ┌───i─┐ │ 107 │ │ 108 │ │ 109 │ └─────┘  "},{"title":"optimize_syntax_fuse_functions​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#optimize_syntax_fuse_functions","content":"Enables to fuse aggregate functions with identical argument. It rewrites query contains at least two aggregate functions from sum, count or avg with identical argument to sumCount. Possible values: 0 — Functions with identical argument are not fused.1 — Functions with identical argument are fused. Default value: 0. Example Query: CREATE TABLE fuse_tbl(a Int8, b Int8) Engine = Log; SET optimize_syntax_fuse_functions = 1; EXPLAIN SYNTAX SELECT sum(a), sum(b), count(b), avg(b) from fuse_tbl FORMAT TSV;  Result: SELECT sum(a), sumCount(b).1, sumCount(b).2, (sumCount(b).1) / (sumCount(b).2) FROM fuse_tbl  "},{"title":"allow_experimental_database_replicated​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#allow_experimental_database_replicated","content":"Enables to create databases with Replicated engine. Possible values: 0 — Disabled.1 — Enabled. Default value: 0. "},{"title":"database_replicated_initial_query_timeout_sec​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#database_replicated_initial_query_timeout_sec","content":"Sets how long initial DDL query should wait for Replicated database to precess previous DDL queue entries in seconds. Possible values: Positive integer.0 — Unlimited. Default value: 300. "},{"title":"distributed_ddl_task_timeout​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#distributed_ddl_task_timeout","content":"Sets timeout for DDL query responses from all hosts in cluster. If a DDL request has not been performed on all hosts, a response will contain a timeout error and a request will be executed in an async mode. Negative value means infinite. Possible values: Positive integer.0 — Async mode.Negative integer — infinite timeout. Default value: 180. "},{"title":"distributed_ddl_output_mode​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#distributed_ddl_output_mode","content":"Sets format of distributed DDL query result. Possible values: throw — Returns result set with query execution status for all hosts where query is finished. If query has failed on some hosts, then it will rethrow the first exception. If query is not finished yet on some hosts and distributed_ddl_task_timeout exceeded, then it throws TIMEOUT_EXCEEDED exception.none — Is similar to throw, but distributed DDL query returns no result set.null_status_on_timeout — Returns NULL as execution status in some rows of result set instead of throwing TIMEOUT_EXCEEDED if query is not finished on the corresponding hosts.never_throw — Do not throw TIMEOUT_EXCEEDED and do not rethrow exceptions if query has failed on some hosts. Default value: throw. "},{"title":"flatten_nested​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#flatten-nested","content":"Sets the data format of a nested columns. Possible values: 1 — Nested column is flattened to separate arrays.0 — Nested column stays a single array of tuples. Default value: 1. Usage If the setting is set to 0, it is possible to use an arbitrary level of nesting. Examples Query: SET flatten_nested = 1; CREATE TABLE t_nest (`n` Nested(a UInt32, b UInt32)) ENGINE = MergeTree ORDER BY tuple(); SHOW CREATE TABLE t_nest;  Result: ┌─statement───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE default.t_nest ( `n.a` Array(UInt32), `n.b` Array(UInt32) ) ENGINE = MergeTree ORDER BY tuple() SETTINGS index_granularity = 8192 │ └─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘  Query: SET flatten_nested = 0; CREATE TABLE t_nest (`n` Nested(a UInt32, b UInt32)) ENGINE = MergeTree ORDER BY tuple(); SHOW CREATE TABLE t_nest;  Result: ┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE default.t_nest ( `n` Nested(a UInt32, b UInt32) ) ENGINE = MergeTree ORDER BY tuple() SETTINGS index_granularity = 8192 │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘  "},{"title":"external_table_functions_use_nulls​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#external-table-functions-use-nulls","content":"Defines how mysql, postgresql and odbc] table functions use Nullable columns. Possible values: 0 — The table function explicitly uses Nullable columns.1 — The table function implicitly uses Nullable columns. Default value: 1. Usage If the setting is set to 0, the table function does not make Nullable columns and inserts default values instead of NULL. This is also applicable for NULL values inside arrays. "},{"title":"output_format_arrow_low_cardinality_as_dictionary​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#output-format-arrow-low-cardinality-as-dictionary","content":"Allows to convert the LowCardinality type to the DICTIONARY type of the Arrow format for SELECT queries. Possible values: 0 — The LowCardinality type is not converted to the DICTIONARY type.1 — The LowCardinality type is converted to the DICTIONARY type. Default value: 0. "},{"title":"allow_experimental_projection_optimization​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#allow-experimental-projection-optimization","content":"Enables or disables projection optimization when processing SELECT queries. Possible values: 0 — Projection optimization disabled.1 — Projection optimization enabled. Default value: 0. "},{"title":"force_optimize_projection​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#force-optimize-projection","content":"Enables or disables the obligatory use of projections in SELECT queries, when projection optimization is enabled (see allow_experimental_projection_optimization setting). Possible values: 0 — Projection optimization is not obligatory.1 — Projection optimization is obligatory. Default value: 0. "},{"title":"replication_alter_partitions_sync​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#replication-alter-partitions-sync","content":"Allows to set up waiting for actions to be executed on replicas by ALTER, OPTIMIZE or TRUNCATE queries. Possible values: 0 — Do not wait.1 — Wait for own execution.2 — Wait for everyone. Default value: 1. "},{"title":"replication_wait_for_inactive_replica_timeout​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#replication-wait-for-inactive-replica-timeout","content":"Specifies how long (in seconds) to wait for inactive replicas to execute ALTER, OPTIMIZE or TRUNCATE queries. Possible values: 0 — Do not wait.Negative integer — Wait for unlimited time.Positive integer — The number of seconds to wait. Default value: 120 seconds. "},{"title":"regexp_max_matches_per_row​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#regexp-max-matches-per-row","content":"Sets the maximum number of matches for a single regular expression per row. Use it to protect against memory overload when using greedy regular expression in the extractAllGroupsHorizontal function. Possible values: Positive integer. Default value: 1000. "},{"title":"http_max_single_read_retries​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#http-max-single-read-retries","content":"Sets the maximum number of retries during a single HTTP read. Possible values: Positive integer. Default value: 1024. "},{"title":"log_queries_probability​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#log-queries-probability","content":"Allows a user to write to query_log, query_thread_log, and query_views_log system tables only a sample of queries selected randomly with the specified probability. It helps to reduce the load with a large volume of queries in a second. Possible values: 0 — Queries are not logged in the system tables.Positive floating-point number in the range [0..1]. For example, if the setting value is 0.5, about half of the queries are logged in the system tables.1 — All queries are logged in the system tables. Default value: 1. "},{"title":"short_circuit_function_evaluation​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#short-circuit-function-evaluation","content":"Allows calculating the if, multiIf, and, and or functions according to a short scheme. This helps optimize the execution of complex expressions in these functions and prevent possible exceptions (such as division by zero when it is not expected). Possible values: enable — Enables short-circuit function evaluation for functions that are suitable for it (can throw an exception or computationally heavy).force_enable — Enables short-circuit function evaluation for all functions.disable — Disables short-circuit function evaluation. Default value: enable. "},{"title":"max_hyperscan_regexp_length​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#max-hyperscan-regexp-length","content":"Defines the maximum length for each regular expression in the hyperscan multi-match functions. Possible values: Positive integer.0 - The length is not limited. Default value: 0. Example Query: SELECT multiMatchAny('abcd', ['ab','bcd','c','d']) SETTINGS max_hyperscan_regexp_length = 3;  Result: ┌─multiMatchAny('abcd', ['ab', 'bcd', 'c', 'd'])─┐ │ 1 │ └────────────────────────────────────────────────┘  Query: SELECT multiMatchAny('abcd', ['ab','bcd','c','d']) SETTINGS max_hyperscan_regexp_length = 2;  Result: Exception: Regexp length too large.  See Also max_hyperscan_regexp_total_length "},{"title":"max_hyperscan_regexp_total_length​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#max-hyperscan-regexp-total-length","content":"Sets the maximum length total of all regular expressions in each hyperscan multi-match function. Possible values: Positive integer.0 - The length is not limited. Default value: 0. Example Query: SELECT multiMatchAny('abcd', ['a','b','c','d']) SETTINGS max_hyperscan_regexp_total_length = 5;  Result: ┌─multiMatchAny('abcd', ['a', 'b', 'c', 'd'])─┐ │ 1 │ └─────────────────────────────────────────────┘  Query: SELECT multiMatchAny('abcd', ['ab','bc','c','d']) SETTINGS max_hyperscan_regexp_total_length = 5;  Result: Exception: Total regexp lengths too large.  See Also max_hyperscan_regexp_length "},{"title":"enable_positional_arguments​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#enable-positional-arguments","content":"Enables or disables supporting positional arguments for GROUP BY, LIMIT BY, ORDER BY statements. When you want to use column numbers instead of column names in these clauses, set enable_positional_arguments = 1. Possible values: 0 — Positional arguments aren't supported.1 — Positional arguments are supported: column numbers can use instead of column names. Default value: 0. Example Query: CREATE TABLE positional_arguments(one Int, two Int, three Int) ENGINE=Memory(); INSERT INTO positional_arguments VALUES (10, 20, 30), (20, 20, 10), (30, 10, 20); SET enable_positional_arguments = 1; SELECT * FROM positional_arguments ORDER BY 2,3;  Result: ┌─one─┬─two─┬─three─┐ │ 30 │ 10 │ 20 │ │ 20 │ 20 │ 10 │ │ 10 │ 20 │ 30 │ └─────┴─────┴───────┘  "},{"title":"optimize_move_to_prewhere​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#optimize_move_to_prewhere","content":"Enables or disables automatic PREWHERE optimization in SELECT queries. Works only for *MergeTree tables. Possible values: 0 — Automatic PREWHERE optimization is disabled.1 — Automatic PREWHERE optimization is enabled. Default value: 1. "},{"title":"optimize_move_to_prewhere_if_final​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#optimize_move_to_prewhere_if_final","content":"Enables or disables automatic PREWHERE optimization in SELECT queries with FINAL modifier. Works only for *MergeTree tables. Possible values: 0 — Automatic PREWHERE optimization in SELECT queries with FINAL modifier is disabled.1 — Automatic PREWHERE optimization in SELECT queries with FINAL modifier is enabled. Default value: 0. See Also optimize_move_to_prewhere setting "},{"title":"describe_include_subcolumns​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#describe_include_subcolumns","content":"Enables describing subcolumns for a DESCRIBE query. For example, members of a Tuple or subcolumns of a Map, Nullable or an Array data type. Possible values: 0 — Subcolumns are not included in DESCRIBE queries.1 — Subcolumns are included in DESCRIBE queries. Default value: 0. Example See an example for the DESCRIBE statement. "},{"title":"async_insert​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#async-insert","content":"Enables or disables asynchronous inserts. This makes sense only for insertion over HTTP protocol. Note that deduplication isn't working for such inserts. If enabled, the data is combined into batches before the insertion into tables, so it is possible to do small and frequent insertions into ClickHouse (up to 15000 queries per second) without buffer tables. The data is inserted either after the async_insert_max_data_size is exceeded or after async_insert_busy_timeout_ms milliseconds since the first INSERT query. If the async_insert_stale_timeout_ms is set to a non-zero value, the data is inserted after async_insert_stale_timeout_ms milliseconds since the last query. If wait_for_async_insert is enabled, every client will wait for the data to be processed and flushed to the table. Otherwise, the query would be processed almost instantly, even if the data is not inserted. Possible values: 0 — Insertions are made synchronously, one after another.1 — Multiple asynchronous insertions enabled. Default value: 0. "},{"title":"async_insert_threads​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#async-insert-threads","content":"The maximum number of threads for background data parsing and insertion. Possible values: Positive integer.0 — Asynchronous insertions are disabled. Default value: 16. "},{"title":"wait_for_async_insert​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#wait-for-async-insert","content":"Enables or disables waiting for processing of asynchronous insertion. If enabled, server will return OK only after the data is inserted. Otherwise, it will return OK even if the data wasn't inserted. Possible values: 0 — Server returns OK even if the data is not yet inserted.1 — Server returns OK only after the data is inserted. Default value: 1. "},{"title":"wait_for_async_insert_timeout​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#wait-for-async-insert-timeout","content":"The timeout in seconds for waiting for processing of asynchronous insertion. Possible values: Positive integer.0 — Disabled. Default value: lock_acquire_timeout. "},{"title":"async_insert_max_data_size​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#async-insert-max-data-size","content":"The maximum size of the unparsed data in bytes collected per query before being inserted. Possible values: Positive integer.0 — Asynchronous insertions are disabled. Default value: 1000000. "},{"title":"async_insert_busy_timeout_ms​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#async-insert-busy-timeout-ms","content":"The maximum timeout in milliseconds since the first INSERT query before inserting collected data. Possible values: Positive integer.0 — Timeout disabled. Default value: 200. "},{"title":"async_insert_stale_timeout_ms​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#async-insert-stale-timeout-ms","content":"The maximum timeout in milliseconds since the last INSERT query before dumping collected data. If enabled, the settings prolongs the async_insert_busy_timeout_ms with every INSERT query as long as async_insert_max_data_size is not exceeded. Possible values: Positive integer.0 — Timeout disabled. Default value: 0. "},{"title":"alter_partition_verbose_result​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#alter-partition-verbose-result","content":"Enables or disables the display of information about the parts to which the manipulation operations with partitions and parts have been successfully applied. Applicable to ATTACH PARTITION|PART and to FREEZE PARTITION. Possible values: 0 — disable verbosity.1 — enable verbosity. Default value: 0. Example CREATE TABLE test(a Int64, d Date, s String) ENGINE = MergeTree PARTITION BY toYYYYMM(d) ORDER BY a; INSERT INTO test VALUES(1, '2021-01-01', ''); INSERT INTO test VALUES(1, '2021-01-01', ''); ALTER TABLE test DETACH PARTITION ID '202101'; ALTER TABLE test ATTACH PARTITION ID '202101' SETTINGS alter_partition_verbose_result = 1; ┌─command_type─────┬─partition_id─┬─part_name────┬─old_part_name─┐ │ ATTACH PARTITION │ 202101 │ 202101_7_7_0 │ 202101_5_5_0 │ │ ATTACH PARTITION │ 202101 │ 202101_8_8_0 │ 202101_6_6_0 │ └──────────────────┴──────────────┴──────────────┴───────────────┘ ALTER TABLE test FREEZE SETTINGS alter_partition_verbose_result = 1; ┌─command_type─┬─partition_id─┬─part_name────┬─backup_name─┬─backup_path───────────────────┬─part_backup_path────────────────────────────────────────────┐ │ FREEZE ALL │ 202101 │ 202101_7_7_0 │ 8 │ /var/lib/clickhouse/shadow/8/ │ /var/lib/clickhouse/shadow/8/data/default/test/202101_7_7_0 │ │ FREEZE ALL │ 202101 │ 202101_8_8_0 │ 8 │ /var/lib/clickhouse/shadow/8/ │ /var/lib/clickhouse/shadow/8/data/default/test/202101_8_8_0 │ └──────────────┴──────────────┴──────────────┴─────────────┴───────────────────────────────┴─────────────────────────────────────────────────────────────┘  "},{"title":"format_capn_proto_enum_comparising_mode​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#format-capn-proto-enum-comparising-mode","content":"Determines how to map ClickHouse Enum data type and CapnProto Enum data type from schema. Possible values: 'by_values' — Values in enums should be the same, names can be different.'by_names' — Names in enums should be the same, values can be different.'by_name_case_insensitive' — Names in enums should be the same case-insensitive, values can be different. Default value: 'by_values'. "},{"title":"min_bytes_to_use_mmap_io​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#min-bytes-to-use-mmap-io","content":"This is an experimental setting. Sets the minimum amount of memory for reading large files without copying data from the kernel to userspace. Recommended threshold is about 64 MB, because mmap/munmap is slow. It makes sense only for large files and helps only if data reside in the page cache. Possible values: Positive integer.0 — Big files read with only copying data from kernel to userspace. Default value: 0. "},{"title":"format_custom_escaping_rule​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#format-custom-escaping-rule","content":"Sets the field escaping rule for CustomSeparated data format. Possible values: 'Escaped' — Similarly to TSV.'Quoted' — Similarly to Values.'CSV' — Similarly to CSV.'JSON' — Similarly to JSONEachRow.'XML' — Similarly to XML.'Raw' — Extracts subpatterns as a whole, no escaping rules, similarly to TSVRaw. Default value: 'Escaped'. "},{"title":"format_custom_field_delimiter​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#format-custom-field-delimiter","content":"Sets the character that is interpreted as a delimiter between the fields for CustomSeparated data format. Default value: '\\t'. "},{"title":"format_custom_row_before_delimiter​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#format-custom-row-before-delimiter","content":"Sets the character that is interpreted as a delimiter before the field of the first column for CustomSeparated data format. Default value: ''. "},{"title":"format_custom_row_after_delimiter​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#format-custom-row-after-delimiter","content":"Sets the character that is interpreted as a delimiter after the field of the last column for CustomSeparated data format. Default value: '\\n'. "},{"title":"format_custom_row_between_delimiter​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#format-custom-row-between-delimiter","content":"Sets the character that is interpreted as a delimiter between the rows for CustomSeparated data format. Default value: ''. "},{"title":"format_custom_result_before_delimiter​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#format-custom-result-before-delimiter","content":"Sets the character that is interpreted as a prefix before the result set for CustomSeparated data format. Default value: ''. "},{"title":"format_custom_result_after_delimiter​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#format-custom-result-after-delimiter","content":"Sets the character that is interpreted as a suffix after the result set for CustomSeparated data format. Default value: ''. "},{"title":"shutdown_wait_unfinished_queries​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#shutdown_wait_unfinished_queries","content":"Enables or disables waiting unfinished queries when shutdown server. Possible values: 0 — Disabled.1 — Enabled. The wait time equal shutdown_wait_unfinished config. Default value: 0. "},{"title":"shutdown_wait_unfinished​","type":1,"pageTitle":"Settings","url":"docs/en/operations/settings/#shutdown_wait_unfinished","content":"The waiting time in seconds for currently handled connections when shutdown server. Default Value: 5. "},{"title":"session_log","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/session_log","content":"session_log Contains information about all successful and failed login and logout events. Columns: type (Enum8) — Login/logout result. Possible values: LoginFailure — Login error.LoginSuccess — Successful login.Logout — Logout from the system. auth_id (UUID) — Authentication ID, which is a UUID that is automatically generated each time user logins.session_id (String) — Session ID that is passed by client via HTTP interface.event_date (Date) — Login/logout date.event_time (DateTime) — Login/logout time.event_time_microseconds (DateTime64) — Login/logout starting time with microseconds precision.user (String) — User name.auth_type (Enum8) — The authentication type. Possible values: NO_PASSWORDPLAINTEXT_PASSWORDSHA256_PASSWORDDOUBLE_SHA1_PASSWORDLDAPKERBEROS profiles (Array(LowCardinality(String))) — The list of profiles set for all roles and/or users.roles (Array(LowCardinality(String))) — The list of roles to which the profile is applied.settings (Array(Tuple(LowCardinality(String), String))) — Settings that were changed when the client logged in/out.client_address (IPv6) — The IP address that was used to log in/out.client_port (UInt16) — The client port that was used to log in/out.interface (Enum8) — The interface from which the login was initiated. Possible values: TCPHTTPgRPCMySQLPostgreSQL client_hostname (String) — The hostname of the client machine where the clickhouse-client or another TCP client is run.client_name (String) — The clickhouse-client or another TCP client name.client_revision (UInt32) — Revision of the clickhouse-client or another TCP client.client_version_major (UInt32) — The major version of the clickhouse-client or another TCP client.client_version_minor (UInt32) — The minor version of the clickhouse-client or another TCP client.client_version_patch (UInt32) — Patch component of the clickhouse-client or another TCP client version.failure_reason (String) — The exception message containing the reason for the login/logout failure. Example Query: SELECT * FROM system.session_log LIMIT 1 FORMAT Vertical; Result: Row 1: ────── type: LoginSuccess auth_id: 45e6bd83-b4aa-4a23-85e6-bd83b4aa1a23 session_id: event_date: 2021-10-14 event_time: 2021-10-14 20:33:52 event_time_microseconds: 2021-10-14 20:33:52.104247 user: default auth_type: PLAINTEXT_PASSWORD profiles: ['default'] roles: [] settings: [('load_balancing','random'),('max_memory_usage','10000000000')] client_address: ::ffff:127.0.0.1 client_port: 38490 interface: TCP client_hostname: client_name: ClickHouse client client_revision: 54449 client_version_major: 21 client_version_minor: 10 client_version_patch: 0 failure_reason: ","keywords":""},{"title":"replicated_fetches","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/replicated_fetches","content":"replicated_fetches Contains information about currently running background fetches. Columns: database (String) — Name of the database. table (String) — Name of the table. elapsed (Float64) — The time elapsed (in seconds) since showing currently running background fetches started. progress (Float64) — The percentage of completed work from 0 to 1. result_part_name (String) — The name of the part that will be formed as the result of showing currently running background fetches. result_part_path (String) — Absolute path to the part that will be formed as the result of showing currently running background fetches. partition_id (String) — ID of the partition. total_size_bytes_compressed (UInt64) — The total size (in bytes) of the compressed data in the result part. bytes_read_compressed (UInt64) — The number of compressed bytes read from the result part. source_replica_path (String) — Absolute path to the source replica. source_replica_hostname (String) — Hostname of the source replica. source_replica_port (UInt16) — Port number of the source replica. interserver_scheme (String) — Name of the interserver scheme. URI (String) — Uniform resource identifier. to_detached (UInt8) — The flag indicates whether the currently running background fetch is being performed using the TO DETACHED expression. thread_id (UInt64) — Thread identifier. Example SELECT * FROM system.replicated_fetches LIMIT 1 FORMAT Vertical; Row 1: ────── database: default table: t elapsed: 7.243039876 progress: 0.41832135995612835 result_part_name: all_0_0_0 result_part_path: /var/lib/clickhouse/store/700/70080a04-b2de-4adf-9fa5-9ea210e81766/all_0_0_0/ partition_id: all total_size_bytes_compressed: 1052783726 bytes_read_compressed: 440401920 source_replica_path: /clickhouse/test/t/replicas/1 source_replica_hostname: node1 source_replica_port: 9009 interserver_scheme: http URI: http://node1:9009/?endpoint=DataPartsExchange%3A%2Fclickhouse%2Ftest%2Ft%2Freplicas%2F1&amp;part=all_0_0_0&amp;client_protocol_version=4&amp;compress=false to_detached: 0 thread_id: 54 See Also Managing ReplicatedMergeTree Tables Original article","keywords":""},{"title":"replication_queue","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/replication_queue","content":"replication_queue Contains information about tasks from replication queues stored in ZooKeeper for tables in the ReplicatedMergeTree family. Columns: database (String) — Name of the database. table (String) — Name of the table. replica_name (String) — Replica name in ZooKeeper. Different replicas of the same table have different names. position (UInt32) — Position of the task in the queue. node_name (String) — Node name in ZooKeeper. type (String) — Type of the task in the queue, one of: GET_PART — Get the part from another replica.ATTACH_PART — Attach the part, possibly from our own replica (if found in the detached folder). You may think of it as a GET_PART with some optimizations as they're nearly identical.MERGE_PARTS — Merge the parts.DROP_RANGE — Delete the parts in the specified partition in the specified number range.CLEAR_COLUMN — NOTE: Deprecated. Drop specific column from specified partition.CLEAR_INDEX — NOTE: Deprecated. Drop specific index from specified partition.REPLACE_RANGE — Drop a certain range of parts and replace them with new ones.MUTATE_PART — Apply one or several mutations to the part.ALTER_METADATA — Apply alter modification according to global /metadata and /columns paths. create_time (Datetime) — Date and time when the task was submitted for execution. required_quorum (UInt32) — The number of replicas waiting for the task to complete with confirmation of completion. This column is only relevant for the GET_PARTS task. source_replica (String) — Name of the source replica. new_part_name (String) — Name of the new part. parts_to_merge (Array (String)) — Names of parts to merge or update. is_detach (UInt8) — The flag indicates whether the DETACH_PARTS task is in the queue. is_currently_executing (UInt8) — The flag indicates whether a specific task is being performed right now. num_tries (UInt32) — The number of failed attempts to complete the task. last_exception (String) — Text message about the last error that occurred (if any). last_attempt_time (Datetime) — Date and time when the task was last attempted. num_postponed (UInt32) — The number of postponed tasks. postpone_reason (String) — The reason why the task was postponed. last_postpone_time (Datetime) — Date and time when the task was last postponed. merge_type (String) — Type of the current merge. Empty if it's a mutation. Example SELECT * FROM system.replication_queue LIMIT 1 FORMAT Vertical; Row 1: ────── database: merge table: visits_v2 replica_name: mtgiga001-1t position: 15 node_name: queue-0009325559 type: MERGE_PARTS create_time: 2020-12-07 14:04:21 required_quorum: 0 source_replica: mtgiga001-1t new_part_name: 20201130_121373_121384_2 parts_to_merge: ['20201130_121373_121378_1','20201130_121379_121379_0','20201130_121380_121380_0','20201130_121381_121381_0','20201130_121382_121382_0','20201130_121383_121383_0','20201130_121384_121384_0'] is_detach: 0 is_currently_executing: 0 num_tries: 36 last_exception: Code: 226, e.displayText() = DB::Exception: Marks file '/opt/clickhouse/data/merge/visits_v2/tmp_fetch_20201130_121373_121384_2/CounterID.mrk' does not exist (version 20.8.7.15 (official build)) last_attempt_time: 2020-12-08 17:35:54 num_postponed: 0 postpone_reason: last_postpone_time: 1970-01-01 03:00:00 See Also Managing ReplicatedMergeTree Tables Original article","keywords":""},{"title":"row_policies","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/row_policies","content":"","keywords":""},{"title":"See Also​","type":1,"pageTitle":"row_policies","url":"docs/en/operations/system-tables/row_policies#see-also","content":"SHOW POLICIES Original article "},{"title":"replicas","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/replicas","content":"replicas Contains information and status for replicated tables residing on the local server. This table can be used for monitoring. The table contains a row for every Replicated* table. Example: SELECT * FROM system.replicas WHERE table = 'test_table' FORMAT Vertical Query id: dc6dcbcb-dc28-4df9-ae27-4354f5b3b13e Row 1: ─────── database: db table: test_table engine: ReplicatedMergeTree is_leader: 1 can_become_leader: 1 is_readonly: 0 is_session_expired: 0 future_parts: 0 parts_to_check: 0 zookeeper_path: /test/test_table replica_name: r1 replica_path: /test/test_table/replicas/r1 columns_version: -1 queue_size: 27 inserts_in_queue: 27 merges_in_queue: 0 part_mutations_in_queue: 0 queue_oldest_time: 2021-10-12 14:48:48 inserts_oldest_time: 2021-10-12 14:48:48 merges_oldest_time: 1970-01-01 03:00:00 part_mutations_oldest_time: 1970-01-01 03:00:00 oldest_part_to_get: 1_17_17_0 oldest_part_to_merge_to: oldest_part_to_mutate_to: log_max_index: 206 log_pointer: 207 last_queue_update: 2021-10-12 14:50:08 absolute_delay: 99 total_replicas: 5 active_replicas: 5 last_queue_update_exception: zookeeper_exception: replica_is_active: {'r1':1,'r2':1} Columns: database (String) - Database nametable (String) - Table nameengine (String) - Table engine nameis_leader (UInt8) - Whether the replica is the leader. Multiple replicas can be leaders at the same time. A replica can be prevented from becoming a leader using the merge_tree setting replicated_can_become_leader. The leaders are responsible for scheduling background merges. Note that writes can be performed to any replica that is available and has a session in ZK, regardless of whether it is a leader.can_become_leader (UInt8) - Whether the replica can be a leader.is_readonly (UInt8) - Whether the replica is in read-only mode. This mode is turned on if the config does not have sections with ZooKeeper, if an unknown error occurred when reinitializing sessions in ZooKeeper, and during session reinitialization in ZooKeeper.is_session_expired (UInt8) - the session with ZooKeeper has expired. Basically the same as is_readonly.future_parts (UInt32) - The number of data parts that will appear as the result of INSERTs or merges that haven’t been done yet.parts_to_check (UInt32) - The number of data parts in the queue for verification. A part is put in the verification queue if there is suspicion that it might be damaged.zookeeper_path (String) - Path to table data in ZooKeeper.replica_name (String) - Replica name in ZooKeeper. Different replicas of the same table have different names.replica_path (String) - Path to replica data in ZooKeeper. The same as concatenating ‘zookeeper_path/replicas/replica_path’.columns_version (Int32) - Version number of the table structure. Indicates how many times ALTER was performed. If replicas have different versions, it means some replicas haven’t made all of the ALTERs yet.queue_size (UInt32) - Size of the queue for operations waiting to be performed. Operations include inserting blocks of data, merges, and certain other actions. It usually coincides with future_parts.inserts_in_queue (UInt32) - Number of inserts of blocks of data that need to be made. Insertions are usually replicated fairly quickly. If this number is large, it means something is wrong.merges_in_queue (UInt32) - The number of merges waiting to be made. Sometimes merges are lengthy, so this value may be greater than zero for a long time.part_mutations_in_queue (UInt32) - The number of mutations waiting to be made.queue_oldest_time (DateTime) - If queue_size greater than 0, shows when the oldest operation was added to the queue.inserts_oldest_time (DateTime) - See queue_oldest_timemerges_oldest_time (DateTime) - See queue_oldest_timepart_mutations_oldest_time (DateTime) - See queue_oldest_time The next 4 columns have a non-zero value only where there is an active session with ZK. log_max_index (UInt64) - Maximum entry number in the log of general activity.log_pointer (UInt64) - Maximum entry number in the log of general activity that the replica copied to its execution queue, plus one. If log_pointer is much smaller than log_max_index, something is wrong.last_queue_update (DateTime) - When the queue was updated last time.absolute_delay (UInt64) - How big lag in seconds the current replica has.total_replicas (UInt8) - The total number of known replicas of this table.active_replicas (UInt8) - The number of replicas of this table that have a session in ZooKeeper (i.e., the number of functioning replicas).last_queue_update_exception (String) - When the queue contains broken entries. Especially important when ClickHouse breaks backward compatibility between versions and log entries written by newer versions aren't parseable by old versions.zookeeper_exception (String) - The last exception message, got if the error happened when fetching the info from ZooKeeper. replica_is_active (Map(String, UInt8)) — Map between replica name and is replica active. If you request all the columns, the table may work a bit slowly, since several reads from ZooKeeper are made for each row. If you do not request the last 4 columns (log_max_index, log_pointer, total_replicas, active_replicas), the table works quickly. For example, you can check that everything is working correctly like this: SELECT database, table, is_leader, is_readonly, is_session_expired, future_parts, parts_to_check, columns_version, queue_size, inserts_in_queue, merges_in_queue, log_max_index, log_pointer, total_replicas, active_replicas FROM system.replicas WHERE is_readonly OR is_session_expired OR future_parts &gt; 20 OR parts_to_check &gt; 10 OR queue_size &gt; 20 OR inserts_in_queue &gt; 10 OR log_max_index - log_pointer &gt; 10 OR total_replicas &lt; 2 OR active_replicas &lt; total_replicas If this query does not return anything, it means that everything is fine. Original article","keywords":""},{"title":"roles","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/roles","content":"","keywords":""},{"title":"See Also​","type":1,"pageTitle":"roles","url":"docs/en/operations/system-tables/roles#see-also","content":"SHOW ROLES Original article "},{"title":"role_grants","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/role-grants","content":"role_grants Contains the role grants for users and roles. To add entries to this table, use GRANT role TO user. Columns: user_name (Nullable(String)) — User name. role_name (Nullable(String)) — Role name. granted_role_name (String) — Name of role granted to the role_name role. To grant one role to another one use GRANT role1 TO role2. granted_role_is_default (UInt8) — Flag that shows whether granted_role is a default role. Possible values: 1 — granted_role is a default role.0 — granted_role is not a default role. with_admin_option (UInt8) — Flag that shows whether granted_role is a role with ADMIN OPTION privilege. Possible values: 1 — The role has ADMIN OPTION privilege.0 — The role without ADMIN OPTION privilege. Original article","keywords":""},{"title":"text_log","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/text_log","content":"text_log Contains logging entries. The logging level which goes to this table can be limited to the text_log.level server setting. Columns: event_date (Date) — Date of the entry.event_time (DateTime) — Time of the entry.event_time_microseconds (DateTime) — Time of the entry with microseconds precision.microseconds (UInt32) — Microseconds of the entry.thread_name (String) — Name of the thread from which the logging was done.thread_id (UInt64) — OS thread ID.level (Enum8) — Entry level. Possible values: 1 or 'Fatal'.2 or 'Critical'.3 or 'Error'.4 or 'Warning'.5 or 'Notice'.6 or 'Information'.7 or 'Debug'.8 or 'Trace'. query_id (String) — ID of the query.logger_name (LowCardinality(String)) — Name of the logger (i.e. DDLWorker).message (String) — The message itself.revision (UInt32) — ClickHouse revision.source_file (LowCardinality(String)) — Source file from which the logging was done.source_line (UInt64) — Source line from which the logging was done. Example SELECT * FROM system.text_log LIMIT 1 \\G Row 1: ────── event_date: 2020-09-10 event_time: 2020-09-10 11:23:07 event_time_microseconds: 2020-09-10 11:23:07.871397 microseconds: 871397 thread_name: clickhouse-serv thread_id: 564917 level: Information query_id: logger_name: DNSCacheUpdater message: Update period 15 seconds revision: 54440 source_file: /ClickHouse/src/Interpreters/DNSCacheUpdater.cpp; void DB::DNSCacheUpdater::start() source_line: 45 Original article","keywords":""},{"title":"table_engines","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/table_engines","content":"table_engines Contains description of table engines supported by server and their feature support information. This table contains the following columns (the column type is shown in brackets): name (String) — The name of table engine.supports_settings (UInt8) — Flag that indicates if table engine supports SETTINGS clause.supports_skipping_indices (UInt8) — Flag that indicates if table engine supports skipping indices.supports_ttl (UInt8) — Flag that indicates if table engine supports TTL.supports_sort_order (UInt8) — Flag that indicates if table engine supports clauses PARTITION_BY, PRIMARY_KEY, ORDER_BY and SAMPLE_BY.supports_replication (UInt8) — Flag that indicates if table engine supports data replication.supports_duduplication (UInt8) — Flag that indicates if table engine supports data deduplication.supports_parallel_insert (UInt8) — Flag that indicates if table engine supports parallel insert (see max_insert_threads setting). Example: SELECT * FROM system.table_engines WHERE name in ('Kafka', 'MergeTree', 'ReplicatedCollapsingMergeTree') ┌─name──────────────────────────┬─supports_settings─┬─supports_skipping_indices─┬─supports_sort_order─┬─supports_ttl─┬─supports_replication─┬─supports_deduplication─┬─supports_parallel_insert─┐ │ MergeTree │ 1 │ 1 │ 1 │ 1 │ 0 │ 0 │ 1 │ │ Kafka │ 1 │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ │ ReplicatedCollapsingMergeTree │ 1 │ 1 │ 1 │ 1 │ 1 │ 1 │ 1 │ └───────────────────────────────┴───────────────────┴───────────────────────────┴─────────────────────┴──────────────┴──────────────────────┴────────────────────────┴──────────────────────────┘ See also MergeTree family query clausesKafka settingsJoin settings Original article","keywords":""},{"title":"settings_profiles","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/settings_profiles","content":"","keywords":""},{"title":"See Also​","type":1,"pageTitle":"settings_profiles","url":"docs/en/operations/system-tables/settings_profiles#see-also","content":"SHOW PROFILES Original article "},{"title":"trace_log","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/trace_log","content":"trace_log Contains stack traces collected by the sampling query profiler. ClickHouse creates this table when the trace_log server configuration section is set. Also the query_profiler_real_time_period_ns and query_profiler_cpu_time_period_ns settings should be set. To analyze logs, use the addressToLine, addressToLineWithInlines, addressToSymbol and demangle introspection functions. Columns: event_date (Date) — Date of sampling moment. event_time (DateTime) — Timestamp of the sampling moment. event_time_microseconds (DateTime64) — Timestamp of the sampling moment with microseconds precision. timestamp_ns (UInt64) — Timestamp of the sampling moment in nanoseconds. revision (UInt32) — ClickHouse server build revision. When connecting to the server by clickhouse-client, you see the string similar to Connected to ClickHouse server version 19.18.1 revision 54429.. This field contains the revision, but not the version of a server. trace_type (Enum8) — Trace type: Real represents collecting stack traces by wall-clock time.CPU represents collecting stack traces by CPU time.Memory represents collecting allocations and deallocations when memory allocation exceeds the subsequent watermark.MemorySample represents collecting random allocations and deallocations. thread_number (UInt32) — Thread identifier. query_id (String) — Query identifier that can be used to get details about a query that was running from the query_log system table. trace (Array(UInt64)) — Stack trace at the moment of sampling. Each element is a virtual memory address inside ClickHouse server process. Example SELECT * FROM system.trace_log LIMIT 1 \\G Row 1: ────── event_date: 2020-09-10 event_time: 2020-09-10 11:23:09 event_time_microseconds: 2020-09-10 11:23:09.872924 timestamp_ns: 1599762189872924510 revision: 54440 trace_type: Memory thread_id: 564963 query_id: trace: [371912858,371912789,371798468,371799717,371801313,371790250,624462773,566365041,566440261,566445834,566460071,566459914,566459842,566459580,566459469,566459389,566459341,566455774,371993941,371988245,372158848,372187428,372187309,372187093,372185478,140222123165193,140222122205443] size: 5244400 Original article","keywords":""},{"title":"users","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/users","content":"","keywords":""},{"title":"See Also​","type":1,"pageTitle":"users","url":"docs/en/operations/system-tables/users#see-also","content":"SHOW USERS Original article "},{"title":"storage_policies","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/storage_policies","content":"storage_policies Contains information about storage policies and volumes defined in the server configuration. Columns: policy_name (String) — Name of the storage policy.volume_name (String) — Volume name defined in the storage policy.volume_priority (UInt64) — Volume order number in the configuration, the data fills the volumes according this priority, i.e. data during inserts and merges is written to volumes with a lower priority (taking into account other rules: TTL, max_data_part_size, move_factor).disks (Array(String)) — Disk names, defined in the storage policy.max_data_part_size (UInt64) — Maximum size of a data part that can be stored on volume disks (0 — no limit).move_factor (Float64) — Ratio of free disk space. When the ratio exceeds the value of configuration parameter, ClickHouse start to move data to the next volume in order.prefer_not_to_merge (UInt8) — Value of the prefer_not_to_merge setting. When this setting is enabled, merging data on this volume is not allowed. This allows controlling how ClickHouse works with slow disks. If the storage policy contains more then one volume, then information for each volume is stored in the individual row of the table. Original article","keywords":""},{"title":"settings_profile_elements","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/settings_profile_elements","content":"settings_profile_elements Describes the content of the settings profile: Сonstraints.Roles and users that the setting applies to.Parent settings profiles. Columns: profile_name (Nullable(String)) — Setting profile name. user_name (Nullable(String)) — User name. role_name (Nullable(String)) — Role name. index (UInt64) — Sequential number of the settings profile element. setting_name (Nullable(String)) — Setting name. value (Nullable(String)) — Setting value. min (Nullable(String)) — The minimum value of the setting. NULL if not set. max (Nullable(String)) — The maximum value of the setting. NULL if not set. readonly (Nullable(UInt8)) — Profile that allows only read queries. inherit_profile (Nullable(String)) — A parent profile for this setting profile. NULL if not set. Setting profile will inherit all the settings' values and constraints (min, max, readonly) from its parent profiles. Original article","keywords":""},{"title":"settings","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/settings","content":"settings Contains information about session settings for current user. Columns: name (String) — Setting name.value (String) — Setting value.changed (UInt8) — Shows whether a setting is changed from its default value.description (String) — Short setting description.min (Nullable(String)) — Minimum value of the setting, if any is set via constraints. If the setting has no minimum value, contains NULL.max (Nullable(String)) — Maximum value of the setting, if any is set via constraints. If the setting has no maximum value, contains NULL.readonly (UInt8) — Shows whether the current user can change the setting: 0 — Current user can change the setting.1 — Current user can’t change the setting. Example The following example shows how to get information about settings which name contains min_i. SELECT * FROM system.settings WHERE name LIKE '%min_i%' ┌─name────────────────────────────────────────┬─value─────┬─changed─┬─description───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬─min──┬─max──┬─readonly─┐ │ min_insert_block_size_rows │ 1048576 │ 0 │ Squash blocks passed to INSERT query to specified size in rows, if blocks are not big enough. │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ 0 │ │ min_insert_block_size_bytes │ 268435456 │ 0 │ Squash blocks passed to INSERT query to specified size in bytes, if blocks are not big enough. │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ 0 │ │ read_backoff_min_interval_between_events_ms │ 1000 │ 0 │ Settings to reduce the number of threads in case of slow reads. Do not pay attention to the event, if the previous one has passed less than a certain amount of time. │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ 0 │ └─────────────────────────────────────────────┴───────────┴─────────┴───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴──────┴──────┴──────────┘ Using of WHERE changed can be useful, for example, when you want to check: Whether settings in configuration files are loaded correctly and are in use.Settings that changed in the current session. SELECT * FROM system.settings WHERE changed AND name='load_balancing' See also SettingsPermissions for QueriesConstraints on SettingsSHOW SETTINGS statement Original article","keywords":""},{"title":"time_zones","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/time_zones","content":"time_zones Contains a list of time zones that are supported by the ClickHouse server. This list of timezones might vary depending on the version of ClickHouse. Columns: time_zone (String) — List of supported time zones. Example SELECT * FROM system.time_zones LIMIT 10 ┌─time_zone──────────┐ │ Africa/Abidjan │ │ Africa/Accra │ │ Africa/Addis_Ababa │ │ Africa/Algiers │ │ Africa/Asmara │ │ Africa/Asmera │ │ Africa/Bamako │ │ Africa/Bangui │ │ Africa/Banjul │ │ Africa/Bissau │ └────────────────────┘ Original article","keywords":""},{"title":"ClickHouse Utility","type":0,"sectionRef":"#","url":"docs/en/operations/utilities/","content":"ClickHouse Utility clickhouse-local — Allows running SQL queries on data without starting the ClickHouse server, similar to how awk does this.clickhouse-copier — Copies (and reshards) data from one cluster to another cluster.clickhouse-benchmark — Loads server with the custom queries and settings.clickhouse-format — Enables formatting input queries.ClickHouse obfuscator — Obfuscates data.ClickHouse compressor — Compresses and decompresses data.clickhouse-odbc-bridge — A proxy server for ODBC driver.","keywords":""},{"title":"stack_trace","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/stack_trace","content":"stack_trace Contains stack traces of all server threads. Allows developers to introspect the server state. To analyze stack frames, use the addressToLine, addressToLineWithInlines, addressToSymbol and demangle introspection functions. Columns: thread_name (String) — Thread name.thread_id (UInt64) — Thread identifier.query_id (String) — Query identifier that can be used to get details about a query that was running from the query_log system table.trace (Array(UInt64)) — A stack trace which represents a list of physical addresses where the called methods are stored. Example Enabling introspection functions: SET allow_introspection_functions = 1; Getting symbols from ClickHouse object files: WITH arrayMap(x -&gt; demangle(addressToSymbol(x)), trace) AS all SELECT thread_name, thread_id, query_id, arrayStringConcat(all, '\\n') AS res FROM system.stack_trace LIMIT 1 \\G; Row 1: ────── thread_name: clickhouse-serv thread_id: 686 query_id: 1a11f70b-626d-47c1-b948-f9c7b206395d res: sigqueue DB::StorageSystemStackTrace::fillData(std::__1::vector&lt;COW&lt;DB::IColumn&gt;::mutable_ptr&lt;DB::IColumn&gt;, std::__1::allocator&lt;COW&lt;DB::IColumn&gt;::mutable_ptr&lt;DB::IColumn&gt; &gt; &gt;&amp;, DB::Context const&amp;, DB::SelectQueryInfo const&amp;) const DB::IStorageSystemOneBlock&lt;DB::StorageSystemStackTrace&gt;::read(std::__1::vector&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, std::__1::allocator&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;, DB::SelectQueryInfo const&amp;, DB::Context const&amp;, DB::QueryProcessingStage::Enum, unsigned long, unsigned int) DB::InterpreterSelectQuery::executeFetchColumns(DB::QueryProcessingStage::Enum, DB::QueryPipeline&amp;, std::__1::shared_ptr&lt;DB::PrewhereInfo&gt; const&amp;, std::__1::vector&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, std::__1::allocator&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;) DB::InterpreterSelectQuery::executeImpl(DB::QueryPipeline&amp;, std::__1::shared_ptr&lt;DB::IBlockInputStream&gt; const&amp;, std::__1::optional&lt;DB::Pipe&gt;) DB::InterpreterSelectQuery::execute() DB::InterpreterSelectWithUnionQuery::execute() DB::executeQueryImpl(char const*, char const*, DB::Context&amp;, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer*) DB::executeQuery(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, DB::Context&amp;, bool, DB::QueryProcessingStage::Enum, bool) DB::TCPHandler::runImpl() DB::TCPHandler::run() Poco::Net::TCPServerConnection::start() Poco::Net::TCPServerDispatcher::run() Poco::PooledThread::run() Poco::ThreadImpl::runnableEntry(void*) start_thread __clone Getting filenames and line numbers in ClickHouse source code: WITH arrayMap(x -&gt; addressToLine(x), trace) AS all, arrayFilter(x -&gt; x LIKE '%/dbms/%', all) AS dbms SELECT thread_name, thread_id, query_id, arrayStringConcat(notEmpty(dbms) ? dbms : all, '\\n') AS res FROM system.stack_trace LIMIT 1 \\G; Row 1: ────── thread_name: clickhouse-serv thread_id: 686 query_id: cad353e7-1c29-4b2e-949f-93e597ab7a54 res: /lib/x86_64-linux-gnu/libc-2.27.so /build/obj-x86_64-linux-gnu/../src/Storages/System/StorageSystemStackTrace.cpp:182 /build/obj-x86_64-linux-gnu/../contrib/libcxx/include/vector:656 /build/obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:1338 /build/obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:751 /build/obj-x86_64-linux-gnu/../contrib/libcxx/include/optional:224 /build/obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectWithUnionQuery.cpp:192 /build/obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:384 /build/obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:643 /build/obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:251 /build/obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:1197 /build/obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerConnection.cpp:57 /build/obj-x86_64-linux-gnu/../contrib/libcxx/include/atomic:856 /build/obj-x86_64-linux-gnu/../contrib/poco/Foundation/include/Poco/Mutex_POSIX.h:59 /build/obj-x86_64-linux-gnu/../contrib/poco/Foundation/include/Poco/AutoPtr.h:223 /lib/x86_64-linux-gnu/libpthread-2.27.so /lib/x86_64-linux-gnu/libc-2.27.so See Also Introspection Functions — Which introspection functions are available and how to use them.system.trace_log — Contains stack traces collected by the sampling query profiler.arrayMap — Description and usage example of the arrayMap function.arrayFilter — Description and usage example of the arrayFilter function.","keywords":""},{"title":"tables","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/tables","content":"tables Contains metadata of each table that the server knows about. Detached tables are not shown in system.tables. Temporary tables are visible in the system.tables only in those session where they have been created. They are shown with the empty database field and with the is_temporary flag switched on. Columns: database (String) — The name of the database the table is in. name (String) — Table name. engine (String) — Table engine name (without parameters). is_temporary (UInt8) - Flag that indicates whether the table is temporary. data_path (String) - Path to the table data in the file system. metadata_path (String) - Path to the table metadata in the file system. metadata_modification_time (DateTime) - Time of latest modification of the table metadata. dependencies_database (Array(String)) - Database dependencies. dependencies_table (Array(String)) - Table dependencies (MaterializedView tables based on the current table). create_table_query (String) - The query that was used to create the table. engine_full (String) - Parameters of the table engine. as_select (String) - SELECT query for view. partition_key (String) - The partition key expression specified in the table. sorting_key (String) - The sorting key expression specified in the table. primary_key (String) - The primary key expression specified in the table. sampling_key (String) - The sampling key expression specified in the table. storage_policy (String) - The storage policy: MergeTreeDistributed total_rows (Nullable(UInt64)) - Total number of rows, if it is possible to quickly determine exact number of rows in the table, otherwise NULL (including underying Buffer table). total_bytes (Nullable(UInt64)) - Total number of bytes, if it is possible to quickly determine exact number of bytes for the table on storage, otherwise NULL (does not includes any underlying storage). If the table stores data on disk, returns used space on disk (i.e. compressed).If the table stores data in memory, returns approximated number of used bytes in memory. lifetime_rows (Nullable(UInt64)) - Total number of rows INSERTed since server start (only for Buffer tables). lifetime_bytes (Nullable(UInt64)) - Total number of bytes INSERTed since server start (only for Buffer tables). comment (String) - The comment for the table. has_own_data (UInt8) — Flag that indicates whether the table itself stores some data on disk or only accesses some other source. The system.tables table is used in SHOW TABLES query implementation. Example SELECT * FROM system.tables LIMIT 2 FORMAT Vertical; Row 1: ────── database: base name: t1 uuid: 81b1c20a-b7c6-4116-a2ce-7583fb6b6736 engine: MergeTree is_temporary: 0 data_paths: ['/var/lib/clickhouse/store/81b/81b1c20a-b7c6-4116-a2ce-7583fb6b6736/'] metadata_path: /var/lib/clickhouse/store/461/461cf698-fd0b-406d-8c01-5d8fd5748a91/t1.sql metadata_modification_time: 2021-01-25 19:14:32 dependencies_database: [] dependencies_table: [] create_table_query: CREATE TABLE base.t1 (`n` UInt64) ENGINE = MergeTree ORDER BY n SETTINGS index_granularity = 8192 engine_full: MergeTree ORDER BY n SETTINGS index_granularity = 8192 as_select: SELECT database AS table_catalog partition_key: sorting_key: n primary_key: n sampling_key: storage_policy: default total_rows: 1 total_bytes: 99 lifetime_rows: ᴺᵁᴸᴸ lifetime_bytes: ᴺᵁᴸᴸ comment: has_own_data: 0 Row 2: ────── database: default name: 53r93yleapyears uuid: 00000000-0000-0000-0000-000000000000 engine: MergeTree is_temporary: 0 data_paths: ['/var/lib/clickhouse/data/default/53r93yleapyears/'] metadata_path: /var/lib/clickhouse/metadata/default/53r93yleapyears.sql metadata_modification_time: 2020-09-23 09:05:36 dependencies_database: [] dependencies_table: [] create_table_query: CREATE TABLE default.`53r93yleapyears` (`id` Int8, `febdays` Int8) ENGINE = MergeTree ORDER BY id SETTINGS index_granularity = 8192 engine_full: MergeTree ORDER BY id SETTINGS index_granularity = 8192 as_select: SELECT name AS catalog_name partition_key: sorting_key: id primary_key: id sampling_key: storage_policy: default total_rows: 2 total_bytes: 155 lifetime_rows: ᴺᵁᴸᴸ lifetime_bytes: ᴺᵁᴸᴸ comment: has_own_data: 0 ","keywords":""},{"title":"ClickHouse Upgrade","type":0,"sectionRef":"#","url":"docs/en/operations/update","content":"ClickHouse Upgrade If ClickHouse was installed from deb packages, execute the following commands on the server: $ sudo apt-get update $ sudo apt-get install clickhouse-client clickhouse-server $ sudo service clickhouse-server restart If you installed ClickHouse using something other than the recommended deb packages, use the appropriate update method. note You can update multiple servers at once as soon as there is no moment when all replicas of one shard are offline. The upgrade of older version of ClickHouse to specific version: As an example: xx.yy.a.b is a current stable version. The latest stable version could be found here $ sudo apt-get update $ sudo apt-get install clickhouse-server=xx.yy.a.b clickhouse-client=xx.yy.a.b clickhouse-common-static=xx.yy.a.b $ sudo service clickhouse-server restart ","keywords":""},{"title":"zookeeper_log","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/zookeeper_log","content":"zookeeper_log This table contains information about the parameters of the request to the ZooKeeper server and the response from it. For requests, only columns with request parameters are filled in, and the remaining columns are filled with default values (0 or NULL). When the response arrives, the data from the response is added to the other columns. Columns with request parameters: type (Enum) — Event type in the ZooKeeper client. Can have one of the following values: Request — The request has been sent.Response — The response was received.Finalize — The connection is lost, no response was received. event_date (Date) — The date when the event happened.event_time (DateTime64) — The date and time when the event happened.address (IPv6) — IP address of ZooKeeper server that was used to make the request.port (UInt16) — The port of ZooKeeper server that was used to make the request.session_id (Int64) — The session ID that the ZooKeeper server sets for each connection.xid (Int32) — The ID of the request within the session. This is usually a sequential request number. It is the same for the request row and the paired response/finalize row.has_watch (UInt8) — The request whether the watch has been set.op_num (Enum) — The type of request or response.path (String) — The path to the ZooKeeper node specified in the request, or an empty string if the request not requires specifying a path.data (String) — The data written to the ZooKeeper node (for the SET and CREATE requests — what the request wanted to write, for the response to the GET request — what was read) or an empty string.is_ephemeral (UInt8) — Is the ZooKeeper node being created as an ephemeral.is_sequential (UInt8) — Is the ZooKeeper node being created as an sequential.version (Nullable(Int32)) — The version of the ZooKeeper node that the request expects when executing. This is supported for CHECK, SET, REMOVE requests (is relevant -1 if the request does not check the version or NULL for other requests that do not support version checking).requests_size (UInt32) — The number of requests included in the multi request (this is a special request that consists of several consecutive ordinary requests and executes them atomically). All requests included in multi request will have the same xid.request_idx (UInt32) — The number of the request included in multi request (for multi request — 0, then in order from 1). Columns with request response parameters: zxid (Int64) — ZooKeeper transaction ID. The serial number issued by the ZooKeeper server in response to a successfully executed request (0 if the request was not executed/returned an error/the client does not know whether the request was executed).error (Nullable(Enum)) — Error code. Can have many values, here are just some of them: ZOK — The request was executed seccessfully.ZCONNECTIONLOSS — The connection was lost.ZOPERATIONTIMEOUT — The request execution timeout has expired.ZSESSIONEXPIRED — The session has expired.NULL — The request is completed. watch_type (Nullable(Enum)) — The type of the watch event (for responses with op_num = Watch), for the remaining responses: NULL.watch_state (Nullable(Enum)) — The status of the watch event (for responses with op_num = Watch), for the remaining responses: NULL.path_created (String) — The path to the created ZooKeeper node (for responses to the CREATE request), may differ from the path if the node is created as a sequential.stat_czxid (Int64) — The zxid of the change that caused this ZooKeeper node to be created.stat_mzxid (Int64) — The zxid of the change that last modified this ZooKeeper node.stat_pzxid (Int64) — The transaction ID of the change that last modified childern of this ZooKeeper node.stat_version (Int32) — The number of changes to the data of this ZooKeeper node.stat_cversion (Int32) — The number of changes to the children of this ZooKeeper node.stat_dataLength (Int32) — The length of the data field of this ZooKeeper node.stat_numChildren (Int32) — The number of children of this ZooKeeper node.children (Array(String)) — The list of child ZooKeeper nodes (for responses to LIST request). Example Query: SELECT * FROM system.zookeeper_log WHERE (session_id = '106662742089334927') AND (xid = '10858') FORMAT Vertical; Result: Row 1: ────── type: Request event_date: 2021-08-09 event_time: 2021-08-09 21:38:30.291792 address: :: port: 2181 session_id: 106662742089334927 xid: 10858 has_watch: 1 op_num: List path: /clickhouse/task_queue/ddl data: is_ephemeral: 0 is_sequential: 0 version: ᴺᵁᴸᴸ requests_size: 0 request_idx: 0 zxid: 0 error: ᴺᵁᴸᴸ watch_type: ᴺᵁᴸᴸ watch_state: ᴺᵁᴸᴸ path_created: stat_czxid: 0 stat_mzxid: 0 stat_pzxid: 0 stat_version: 0 stat_cversion: 0 stat_dataLength: 0 stat_numChildren: 0 children: [] Row 2: ────── type: Response event_date: 2021-08-09 event_time: 2021-08-09 21:38:30.292086 address: :: port: 2181 session_id: 106662742089334927 xid: 10858 has_watch: 1 op_num: List path: /clickhouse/task_queue/ddl data: is_ephemeral: 0 is_sequential: 0 version: ᴺᵁᴸᴸ requests_size: 0 request_idx: 0 zxid: 16926267 error: ZOK watch_type: ᴺᵁᴸᴸ watch_state: ᴺᵁᴸᴸ path_created: stat_czxid: 16925469 stat_mzxid: 16925469 stat_pzxid: 16926179 stat_version: 0 stat_cversion: 7 stat_dataLength: 0 stat_numChildren: 7 children: ['query-0000000006','query-0000000005','query-0000000004','query-0000000003','query-0000000002','query-0000000001','query-0000000000'] See Also ZooKeeperZooKeeper guide","keywords":""},{"title":"clickhouse-compressor","type":0,"sectionRef":"#","url":"docs/en/operations/utilities/clickhouse-compressor","content":"","keywords":""},{"title":"Examples​","type":1,"pageTitle":"clickhouse-compressor","url":"docs/en/operations/utilities/clickhouse-compressor#examples","content":"Compress data with LZ4: $ ./clickhouse-compressor &lt; input_file &gt; output_file  Decompress data from LZ4 format: $ ./clickhouse-compressor --decompress &lt; input_file &gt; output_file  Compress data with ZSTD at level 5: $ ./clickhouse-compressor --codec 'ZSTD(5)' &lt; input_file &gt; output_file  Compress data with Delta of four bytes and ZSTD level 10. $ ./clickhouse-compressor --codec 'Delta(4)' --codec 'ZSTD(10)' &lt; input_file &gt; output_file  "},{"title":"zookeeper","type":0,"sectionRef":"#","url":"docs/en/operations/system-tables/zookeeper","content":"zookeeper The table does not exist if ZooKeeper is not configured. Allows reading data from the ZooKeeper cluster defined in the config. The query must either have a ‘path =’ condition or a path IN condition set with the WHERE clause as shown below. This corresponds to the path of the children in ZooKeeper that you want to get data for. The query SELECT * FROM system.zookeeper WHERE path = '/clickhouse' outputs data for all children on the /clickhouse node. To output data for all root nodes, write path = ‘/’. If the path specified in ‘path’ does not exist, an exception will be thrown. The query SELECT * FROM system.zookeeper WHERE path IN ('/', '/clickhouse') outputs data for all children on the / and /clickhouse node. If in the specified ‘path’ collection has does not exist path, an exception will be thrown. It can be used to do a batch of ZooKeeper path queries. Columns: name (String) — The name of the node.path (String) — The path to the node.value (String) — Node value.dataLength (Int32) — Size of the value.numChildren (Int32) — Number of descendants.czxid (Int64) — ID of the transaction that created the node.mzxid (Int64) — ID of the transaction that last changed the node.pzxid (Int64) — ID of the transaction that last deleted or added descendants.ctime (DateTime) — Time of node creation.mtime (DateTime) — Time of the last modification of the node.version (Int32) — Node version: the number of times the node was changed.cversion (Int32) — Number of added or removed descendants.aversion (Int32) — Number of changes to the ACL.ephemeralOwner (Int64) — For ephemeral nodes, the ID of the session that owns this node. Example: SELECT * FROM system.zookeeper WHERE path = '/clickhouse/tables/01-08/visits/replicas' FORMAT Vertical Row 1: ────── name: example01-08-1 value: czxid: 932998691229 mzxid: 932998691229 ctime: 2015-03-27 16:49:51 mtime: 2015-03-27 16:49:51 version: 0 cversion: 47 aversion: 0 ephemeralOwner: 0 dataLength: 0 numChildren: 7 pzxid: 987021031383 path: /clickhouse/tables/01-08/visits/replicas Row 2: ────── name: example01-08-2 value: czxid: 933002738135 mzxid: 933002738135 ctime: 2015-03-27 16:57:01 mtime: 2015-03-27 16:57:01 version: 0 cversion: 37 aversion: 0 ephemeralOwner: 0 dataLength: 0 numChildren: 7 pzxid: 987021252247 path: /clickhouse/tables/01-08/visits/replicas Original article","keywords":""},{"title":"clickhouse-odbc-bridge","type":0,"sectionRef":"#","url":"docs/en/operations/utilities/odbc-bridge","content":"","keywords":""},{"title":"Usage​","type":1,"pageTitle":"clickhouse-odbc-bridge","url":"docs/en/operations/utilities/odbc-bridge#usage","content":"clickhouse-server use this tool inside odbc table function and StorageODBC. However it can be used as standalone tool from command line with the following parameters in POST-request URL: connection_string -- ODBC connection string.columns -- columns in ClickHouse NamesAndTypesList format, name in backticks, type as string. Name and type are space separated, rows separated with newline.max_block_size -- optional parameter, sets maximum size of single block. Query is send in post body. Response is returned in RowBinary format. "},{"title":"Example:​","type":1,"pageTitle":"clickhouse-odbc-bridge","url":"docs/en/operations/utilities/odbc-bridge#example","content":"$ clickhouse-odbc-bridge --http-port 9018 --daemon $ curl -d &quot;query=SELECT PageID, ImpID, AdType FROM Keys ORDER BY PageID, ImpID&quot; --data-urlencode &quot;connection_string=DSN=ClickHouse;DATABASE=stat&quot; --data-urlencode &quot;sample_block=columns format version: 1 3 columns: \\`PageID\\` String \\`ImpID\\` String \\`AdType\\` String &quot; &quot;http://localhost:9018/&quot; &gt; result.txt $ cat result.txt 12246623837185725195925621517  "},{"title":"clickhouse-format","type":0,"sectionRef":"#","url":"docs/en/operations/utilities/clickhouse-format","content":"","keywords":""},{"title":"Examples​","type":1,"pageTitle":"clickhouse-format","url":"docs/en/operations/utilities/clickhouse-format#examples","content":"Formatting a query: $ clickhouse-format --query &quot;select number from numbers(10) where number%2 order by number desc;&quot;  Result: SELECT number FROM numbers(10) WHERE number % 2 ORDER BY number DESC  Highlighting and single line: $ clickhouse-format --oneline --hilite &lt;&lt;&lt; &quot;SELECT sum(number) FROM numbers(5);&quot;  Result: SELECT sum(number) FROM numbers(5)  Multiqueries: $ clickhouse-format -n &lt;&lt;&lt; &quot;SELECT * FROM (SELECT 1 AS x UNION ALL SELECT 1 UNION DISTINCT SELECT 3);&quot;  Result: SELECT * FROM ( SELECT 1 AS x UNION ALL SELECT 1 UNION DISTINCT SELECT 3 ) ;  Obfuscating: $ clickhouse-format --seed Hello --obfuscate &lt;&lt;&lt; &quot;SELECT cost_first_screen BETWEEN a AND b, CASE WHEN x &gt;= 123 THEN y ELSE NULL END;&quot;  Result: SELECT treasury_mammoth_hazelnut BETWEEN nutmeg AND span, CASE WHEN chive &gt;= 116 THEN switching ELSE ANYTHING END;  Same query and another seed string: $ clickhouse-format --seed World --obfuscate &lt;&lt;&lt; &quot;SELECT cost_first_screen BETWEEN a AND b, CASE WHEN x &gt;= 123 THEN y ELSE NULL END;&quot;  Result: SELECT horse_tape_summer BETWEEN folklore AND moccasins, CASE WHEN intestine &gt;= 116 THEN nonconformist ELSE FORESTRY END;  Adding backslash: $ clickhouse-format --backslash &lt;&lt;&lt; &quot;SELECT * FROM (SELECT 1 AS x UNION ALL SELECT 1 UNION DISTINCT SELECT 3);&quot;  Result: SELECT * \\ FROM \\ ( \\ SELECT 1 AS x \\ UNION ALL \\ SELECT 1 \\ UNION DISTINCT \\ SELECT 3 \\ )  "},{"title":"Troubleshooting","type":0,"sectionRef":"#","url":"docs/en/operations/troubleshooting","content":"","keywords":""},{"title":"Installation​","type":1,"pageTitle":"Troubleshooting","url":"docs/en/operations/troubleshooting#troubleshooting-installation-errors","content":""},{"title":"You Cannot Get Deb Packages from ClickHouse Repository with Apt-get​","type":1,"pageTitle":"Troubleshooting","url":"docs/en/operations/troubleshooting#you-cannot-get-deb-packages-from-clickhouse-repository-with-apt-get","content":"Check firewall settings.If you cannot access the repository for any reason, download packages as described in the install guide article and install them manually using the sudo dpkg -i &lt;packages&gt; command. You will also need the tzdata package. "},{"title":"Connecting to the Server​","type":1,"pageTitle":"Troubleshooting","url":"docs/en/operations/troubleshooting#troubleshooting-accepts-no-connections","content":"Possible issues: The server is not running.Unexpected or wrong configuration parameters. "},{"title":"Server Is Not Running​","type":1,"pageTitle":"Troubleshooting","url":"docs/en/operations/troubleshooting#server-is-not-running","content":"Check if server is runnnig Command: $ sudo service clickhouse-server status  If the server is not running, start it with the command: $ sudo service clickhouse-server start  Check logs The main log of clickhouse-server is in /var/log/clickhouse-server/clickhouse-server.log by default. If the server started successfully, you should see the strings: &lt;Information&gt; Application: starting up. — Server started.&lt;Information&gt; Application: Ready for connections. — Server is running and ready for connections. If clickhouse-server start failed with a configuration error, you should see the &lt;Error&gt; string with an error description. For example: 2019.01.11 15:23:25.549505 [ 45 ] {} &lt;Error&gt; ExternalDictionaries: Failed reloading 'event2id' external dictionary: Poco::Exception. Code: 1000, e.code() = 111, e.displayText() = Connection refused, e.what() = Connection refused  If you do not see an error at the end of the file, look through the entire file starting from the string: &lt;Information&gt; Application: starting up.  If you try to start a second instance of clickhouse-server on the server, you see the following log: 2019.01.11 15:25:11.151730 [ 1 ] {} &lt;Information&gt; : Starting ClickHouse 19.1.0 with revision 54413 2019.01.11 15:25:11.154578 [ 1 ] {} &lt;Information&gt; Application: starting up 2019.01.11 15:25:11.156361 [ 1 ] {} &lt;Information&gt; StatusFile: Status file ./status already exists - unclean restart. Contents: PID: 8510 Started at: 2019-01-11 15:24:23 Revision: 54413 2019.01.11 15:25:11.156673 [ 1 ] {} &lt;Error&gt; Application: DB::Exception: Cannot lock file ./status. Another server instance in same directory is already running. 2019.01.11 15:25:11.156682 [ 1 ] {} &lt;Information&gt; Application: shutting down 2019.01.11 15:25:11.156686 [ 1 ] {} &lt;Debug&gt; Application: Uninitializing subsystem: Logging Subsystem 2019.01.11 15:25:11.156716 [ 2 ] {} &lt;Information&gt; BaseDaemon: Stop SignalListener thread  See system.d logs If you do not find any useful information in clickhouse-server logs or there aren’t any logs, you can view system.d logs using the command: $ sudo journalctl -u clickhouse-server  Start clickhouse-server in interactive mode $ sudo -u clickhouse /usr/bin/clickhouse-server --config-file /etc/clickhouse-server/config.xml  This command starts the server as an interactive app with standard parameters of the autostart script. In this mode clickhouse-server prints all the event messages in the console. "},{"title":"Configuration Parameters​","type":1,"pageTitle":"Troubleshooting","url":"docs/en/operations/troubleshooting#configuration-parameters","content":"Check: Docker settings. If you run ClickHouse in Docker in an IPv6 network, make sure that network=host is set. Endpoint settings. Check listen_host and tcp_port settings. ClickHouse server accepts localhost connections only by default. HTTP protocol settings. Check protocol settings for the HTTP API. Secure connection settings. Check: The tcp_port_secure setting.Settings for SSL certificates. Use proper parameters while connecting. For example, use the port_secure parameter with clickhouse_client. User settings. You might be using the wrong user name or password. "},{"title":"Query Processing​","type":1,"pageTitle":"Troubleshooting","url":"docs/en/operations/troubleshooting#troubleshooting-does-not-process-queries","content":"If ClickHouse is not able to process the query, it sends an error description to the client. In the clickhouse-client you get a description of the error in the console. If you are using the HTTP interface, ClickHouse sends the error description in the response body. For example: $ curl 'http://localhost:8123/' --data-binary &quot;SELECT a&quot; Code: 47, e.displayText() = DB::Exception: Unknown identifier: a. Note that there are no tables (FROM clause) in your query, context: required_names: 'a' source_tables: table_aliases: private_aliases: column_aliases: public_columns: 'a' masked_columns: array_join_columns: source_columns: , e.what() = DB::Exception  If you start clickhouse-client with the stack-trace parameter, ClickHouse returns the server stack trace with the description of an error. You might see a message about a broken connection. In this case, you can repeat the query. If the connection breaks every time you perform the query, check the server logs for errors. "},{"title":"Efficiency of Query Processing​","type":1,"pageTitle":"Troubleshooting","url":"docs/en/operations/troubleshooting#troubleshooting-too-slow","content":"If you see that ClickHouse is working too slowly, you need to profile the load on the server resources and network for your queries. You can use the clickhouse-benchmark utility to profile queries. It shows the number of queries processed per second, the number of rows processed per second, and percentiles of query processing times. "},{"title":"Aggregate Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/","content":"","keywords":""},{"title":"NULL Processing​","type":1,"pageTitle":"Aggregate Functions","url":"docs/en/sql-reference/aggregate-functions/#null-processing","content":"During aggregation, all NULLs are skipped. Examples: Consider this table: ┌─x─┬────y─┐ │ 1 │ 2 │ │ 2 │ ᴺᵁᴸᴸ │ │ 3 │ 2 │ │ 3 │ 3 │ │ 3 │ ᴺᵁᴸᴸ │ └───┴──────┘  Let’s say you need to total the values in the y column: SELECT sum(y) FROM t_null_big  ┌─sum(y)─┐ │ 7 │ └────────┘  Now you can use the groupArray function to create an array from the y column: SELECT groupArray(y) FROM t_null_big  ┌─groupArray(y)─┐ │ [2,2,3] │ └───────────────┘  groupArray does not include NULL in the resulting array. "},{"title":"clickhouse-local","type":0,"sectionRef":"#","url":"docs/en/operations/utilities/clickhouse-local","content":"","keywords":""},{"title":"Usage​","type":1,"pageTitle":"clickhouse-local","url":"docs/en/operations/utilities/clickhouse-local#usage","content":"Basic usage: $ clickhouse-local --structure &quot;table_structure&quot; --input-format &quot;format_of_incoming_data&quot; \\ --query &quot;query&quot;  Arguments: -S, --structure — table structure for input data.-if, --input-format — input format, TSV by default.-f, --file — path to data, stdin by default.-q, --query — queries to execute with ; as delimeter. You must specify either query or queries-file option.-qf, --queries-file - file path with queries to execute. You must specify either query or queries-file option.-N, --table — table name where to put output data, table by default.-of, --format, --output-format — output format, TSV by default.-d, --database — default database, _local by default.--stacktrace — whether to dump debug output in case of exception.--echo — print query before execution.--verbose — more details on query execution.--logger.console — Log to console.--logger.log — Log file name.--logger.level — Log level.--ignore-error — do not stop processing if a query failed.-c, --config-file — path to configuration file in same format as for ClickHouse server, by default the configuration empty.--no-system-tables — do not attach system tables.--help — arguments references for clickhouse-local.-V, --version — print version information and exit. Also there are arguments for each ClickHouse configuration variable which are more commonly used instead of --config-file. "},{"title":"Examples​","type":1,"pageTitle":"clickhouse-local","url":"docs/en/operations/utilities/clickhouse-local#examples","content":"$ echo -e &quot;1,2\\n3,4&quot; | clickhouse-local --structure &quot;a Int64, b Int64&quot; \\ --input-format &quot;CSV&quot; --query &quot;SELECT * FROM table&quot; Read 2 rows, 32.00 B in 0.000 sec., 5182 rows/sec., 80.97 KiB/sec. 1 2 3 4  Previous example is the same as: $ echo -e &quot;1,2\\n3,4&quot; | clickhouse-local --query &quot; CREATE TABLE table (a Int64, b Int64) ENGINE = File(CSV, stdin); SELECT a, b FROM table; DROP TABLE table&quot; Read 2 rows, 32.00 B in 0.000 sec., 4987 rows/sec., 77.93 KiB/sec. 1 2 3 4  You don't have to use stdin or --file argument, and can open any number of files using the file table function: $ echo 1 | tee 1.tsv 1 $ echo 2 | tee 2.tsv 2 $ clickhouse-local --query &quot; select * from file('1.tsv', TSV, 'a int') t1 cross join file('2.tsv', TSV, 'b int') t2&quot; 1 2  Now let’s output memory user for each Unix user: Query: $ ps aux | tail -n +2 | awk '{ printf(&quot;%s\\t%s\\n&quot;, $1, $4) }' \\ | clickhouse-local --structure &quot;user String, mem Float64&quot; \\ --query &quot;SELECT user, round(sum(mem), 2) as memTotal FROM table GROUP BY user ORDER BY memTotal DESC FORMAT Pretty&quot;  Result: Read 186 rows, 4.15 KiB in 0.035 sec., 5302 rows/sec., 118.34 KiB/sec. ┏━━━━━━━━━━┳━━━━━━━━━━┓ ┃ user ┃ memTotal ┃ ┡━━━━━━━━━━╇━━━━━━━━━━┩ │ bayonet │ 113.5 │ ├──────────┼──────────┤ │ root │ 8.8 │ ├──────────┼──────────┤ ...  Original article "},{"title":"clickhouse-obfuscator","type":0,"sectionRef":"#","url":"docs/en/operations/utilities/clickhouse-obfuscator","content":"clickhouse-obfuscator A simple tool for table data obfuscation. It reads an input table and produces an output table, that retains some properties of input, but contains different data. It allows publishing almost real production data for usage in benchmarks. It is designed to retain the following properties of data: cardinalities of values (number of distinct values) for every column and every tuple of columns; conditional cardinalities: number of distinct values of one column under the condition on the value of another column; probability distributions of the absolute value of integers; the sign of signed integers; exponent and sign for floats; probability distributions of the length of strings; probability of zero values of numbers; empty strings and arrays, NULLs; data compression ratio when compressed with LZ77 and entropy family of codecs; continuity (magnitude of difference) of time values across the table; continuity of floating-point values; date component of DateTime values; UTF-8 validity of string values; string values look natural. Most of the properties above are viable for performance testing: reading data, filtering, aggregatio, and sorting will work at almost the same speed as on original data due to saved cardinalities, magnitudes, compression ratios, etc. It works in a deterministic fashion: you define a seed value and the transformation is determined by input data and by seed. Some transformations are one to one and could be reversed, so you need to have a large seed and keep it in secret. It uses some cryptographic primitives to transform data but from the cryptographic point of view, it does not do it properly, that is why you should not consider the result as secure unless you have another reason. The result may retain some data you don't want to publish. It always leaves 0, 1, -1 numbers, dates, lengths of arrays, and null flags exactly as in source data. For example, you have a column IsMobile in your table with values 0 and 1. In transformed data, it will have the same value. So, the user will be able to count the exact ratio of mobile traffic. Let's give another example. When you have some private data in your table, like user email and you don't want to publish any single email address. If your table is large enough and contains multiple different emails and no email has a very high frequency than all others, it will anonymize all data. But if you have a small number of different values in a column, it can reproduce some of them. You should look at the working algorithm of this tool works, and fine-tune its command line parameters. This tool works fine only with an average amount of data (at least 1000s of rows).","keywords":""},{"title":"clickhouse-benchmark","type":0,"sectionRef":"#","url":"docs/en/operations/utilities/clickhouse-benchmark","content":"","keywords":""},{"title":"Keys​","type":1,"pageTitle":"clickhouse-benchmark","url":"docs/en/operations/utilities/clickhouse-benchmark#clickhouse-benchmark-keys","content":"--query=QUERY — Query to execute. If this parameter is not passed, clickhouse-benchmark will read queries from standard input.-c N, --concurrency=N — Number of queries that clickhouse-benchmark sends simultaneously. Default value: 1.-d N, --delay=N — Interval in seconds between intermediate reports (to disable reports set 0). Default value: 1.-h HOST, --host=HOST — Server host. Default value: localhost. For the comparison mode you can use multiple -h keys.-p N, --port=N — Server port. Default value: 9000. For the comparison mode you can use multiple -p keys.-i N, --iterations=N — Total number of queries. Default value: 0 (repeat forever).-r, --randomize — Random order of queries execution if there is more than one input query.-s, --secure — Using TLS connection.-t N, --timelimit=N — Time limit in seconds. clickhouse-benchmark stops sending queries when the specified time limit is reached. Default value: 0 (time limit disabled).--confidence=N — Level of confidence for T-test. Possible values: 0 (80%), 1 (90%), 2 (95%), 3 (98%), 4 (99%), 5 (99.5%). Default value: 5. In the comparison mode clickhouse-benchmark performs the Independent two-sample Student’s t-test to determine whether the two distributions aren’t different with the selected level of confidence.--cumulative — Printing cumulative data instead of data per interval.--database=DATABASE_NAME — ClickHouse database name. Default value: default.--json=FILEPATH — JSON output. When the key is set, clickhouse-benchmark outputs a report to the specified JSON-file.--user=USERNAME — ClickHouse user name. Default value: default.--password=PSWD — ClickHouse user password. Default value: empty string.--stacktrace — Stack traces output. When the key is set, clickhouse-bencmark outputs stack traces of exceptions.--stage=WORD — Query processing stage at server. ClickHouse stops query processing and returns an answer to clickhouse-benchmark at the specified stage. Possible values: complete, fetch_columns, with_mergeable_state. Default value: complete.--help — Shows the help message. If you want to apply some settings for queries, pass them as a key --&lt;session setting name&gt;= SETTING_VALUE. For example, --max_memory_usage=1048576. "},{"title":"Output​","type":1,"pageTitle":"clickhouse-benchmark","url":"docs/en/operations/utilities/clickhouse-benchmark#clickhouse-benchmark-output","content":"By default, clickhouse-benchmark reports for each --delay interval. Example of the report: Queries executed: 10. localhost:9000, queries 10, QPS: 6.772, RPS: 67904487.440, MiB/s: 518.070, result RPS: 67721584.984, result MiB/s: 516.675. 0.000% 0.145 sec. 10.000% 0.146 sec. 20.000% 0.146 sec. 30.000% 0.146 sec. 40.000% 0.147 sec. 50.000% 0.148 sec. 60.000% 0.148 sec. 70.000% 0.148 sec. 80.000% 0.149 sec. 90.000% 0.150 sec. 95.000% 0.150 sec. 99.000% 0.150 sec. 99.900% 0.150 sec. 99.990% 0.150 sec.  In the report you can find: Number of queries in the Queries executed: field. Status string containing (in order): Endpoint of ClickHouse server.Number of processed queries.QPS: How many queries the server performed per second during a period specified in the --delay argument.RPS: How many rows the server reads per second during a period specified in the --delay argument.MiB/s: How many mebibytes the server reads per second during a period specified in the --delay argument.result RPS: How many rows placed by the server to the result of a query per second during a period specified in the --delay argument.result MiB/s. How many mebibytes placed by the server to the result of a query per second during a period specified in the --delay argument. Percentiles of queries execution time. "},{"title":"Comparison Mode​","type":1,"pageTitle":"clickhouse-benchmark","url":"docs/en/operations/utilities/clickhouse-benchmark#clickhouse-benchmark-comparison-mode","content":"clickhouse-benchmark can compare performances for two running ClickHouse servers. To use the comparison mode, specify endpoints of both servers by two pairs of --host, --port keys. Keys matched together by position in arguments list, the first --host is matched with the first --port and so on. clickhouse-benchmark establishes connections to both servers, then sends queries. Each query addressed to a randomly selected server. The results are shown for each server separately. "},{"title":"Example​","type":1,"pageTitle":"clickhouse-benchmark","url":"docs/en/operations/utilities/clickhouse-benchmark#clickhouse-benchmark-example","content":"$ echo &quot;SELECT * FROM system.numbers LIMIT 10000000 OFFSET 10000000&quot; | clickhouse-benchmark -i 10  Loaded 1 queries. Queries executed: 6. localhost:9000, queries 6, QPS: 6.153, RPS: 123398340.957, MiB/s: 941.455, result RPS: 61532982.200, result MiB/s: 469.459. 0.000% 0.159 sec. 10.000% 0.159 sec. 20.000% 0.159 sec. 30.000% 0.160 sec. 40.000% 0.160 sec. 50.000% 0.162 sec. 60.000% 0.164 sec. 70.000% 0.165 sec. 80.000% 0.166 sec. 90.000% 0.166 sec. 95.000% 0.167 sec. 99.000% 0.167 sec. 99.900% 0.167 sec. 99.990% 0.167 sec. Queries executed: 10. localhost:9000, queries 10, QPS: 6.082, RPS: 121959604.568, MiB/s: 930.478, result RPS: 60815551.642, result MiB/s: 463.986. 0.000% 0.159 sec. 10.000% 0.159 sec. 20.000% 0.160 sec. 30.000% 0.163 sec. 40.000% 0.164 sec. 50.000% 0.165 sec. 60.000% 0.166 sec. 70.000% 0.166 sec. 80.000% 0.167 sec. 90.000% 0.167 sec. 95.000% 0.170 sec. 99.000% 0.172 sec. 99.900% 0.172 sec. 99.990% 0.172 sec.  Original article "},{"title":"Usage Recommendations","type":0,"sectionRef":"#","url":"docs/en/operations/tips","content":"","keywords":""},{"title":"CPU Scaling Governor​","type":1,"pageTitle":"Usage Recommendations","url":"docs/en/operations/tips#cpu-scaling-governor","content":"Always use the performance scaling governor. The on-demand scaling governor works much worse with constantly high demand. $ echo 'performance' | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor  "},{"title":"CPU Limitations​","type":1,"pageTitle":"Usage Recommendations","url":"docs/en/operations/tips#cpu-limitations","content":"Processors can overheat. Use dmesg to see if the CPU’s clock rate was limited due to overheating. The restriction can also be set externally at the datacenter level. You can use turbostat to monitor it under a load. "},{"title":"RAM​","type":1,"pageTitle":"Usage Recommendations","url":"docs/en/operations/tips#ram","content":"For small amounts of data (up to ~200 GB compressed), it is best to use as much memory as the volume of data. For large amounts of data and when processing interactive (online) queries, you should use a reasonable amount of RAM (128 GB or more) so the hot data subset will fit in the cache of pages. Even for data volumes of ~50 TB per server, using 128 GB of RAM significantly improves query performance compared to 64 GB. Do not disable overcommit. The value cat /proc/sys/vm/overcommit_memory should be 0 or 1. Run $ echo 0 | sudo tee /proc/sys/vm/overcommit_memory  Use perf top to watch the time spent in the kernel for memory management. Permanent huge pages also do not need to be allocated. warning If your system has less than 16 GB of RAM, you may experience various memory exceptions because default settings do not match this amount of memory. The recommended amount of RAM is 32 GB or more. You can use ClickHouse in a system with a small amount of RAM, even with 2 GB of RAM, but it requires additional tuning and can ingest at a low rate. "},{"title":"Storage Subsystem​","type":1,"pageTitle":"Usage Recommendations","url":"docs/en/operations/tips#storage-subsystem","content":"If your budget allows you to use SSD, use SSD. If not, use HDD. SATA HDDs 7200 RPM will do. Give preference to a lot of servers with local hard drives over a smaller number of servers with attached disk shelves. But for storing archives with rare queries, shelves will work. "},{"title":"RAID​","type":1,"pageTitle":"Usage Recommendations","url":"docs/en/operations/tips#raid","content":"When using HDD, you can combine their RAID-10, RAID-5, RAID-6 or RAID-50. For Linux, software RAID is better (with mdadm). We do not recommend using LVM. When creating RAID-10, select the far layout. If your budget allows, choose RAID-10. If you have more than 4 disks, use RAID-6 (preferred) or RAID-50, instead of RAID-5. When using RAID-5, RAID-6 or RAID-50, always increase stripe_cache_size, since the default value is usually not the best choice. $ echo 4096 | sudo tee /sys/block/md2/md/stripe_cache_size  Calculate the exact number from the number of devices and the block size, using the formula: 2 * num_devices * chunk_size_in_bytes / 4096. A block size of 64 KB is sufficient for most RAID configurations. The average clickhouse-server write size is approximately 1 MB (1024 KB), and thus the recommended stripe size is also 1 MB. The block size can be optimized if needed when set to 1 MB divided by the number of non-parity disks in the RAID array, such that each write is parallelized across all available non-parity disks. Never set the block size too small or too large. You can use RAID-0 on SSD. Regardless of RAID use, always use replication for data security. Enable NCQ with a long queue. For HDD, choose the CFQ scheduler, and for SSD, choose noop. Don’t reduce the ‘readahead’ setting. For HDD, enable the write cache. Make sure that fstrim is enabled for NVME and SSD disks in your OS (usually it's implemented using a cronjob or systemd service). "},{"title":"File System​","type":1,"pageTitle":"Usage Recommendations","url":"docs/en/operations/tips#file-system","content":"Ext4 is the most reliable option. Set the mount options noatime. XFS should be avoided. It works mostly fine but there are some reports about lower performance. Most other file systems should also work fine. Do not use compressed filesystems, because ClickHouse does compression on its own and better. It's not recommended to use encrypted filesystems, because you can use builtin encryption in ClickHouse, which is better. "},{"title":"Linux Kernel​","type":1,"pageTitle":"Usage Recommendations","url":"docs/en/operations/tips#linux-kernel","content":"Don’t use an outdated Linux kernel. "},{"title":"Network​","type":1,"pageTitle":"Usage Recommendations","url":"docs/en/operations/tips#network","content":"If you are using IPv6, increase the size of the route cache. The Linux kernel prior to 3.2 had a multitude of problems with IPv6 implementation. Use at least a 10 GB network, if possible. 1 Gb will also work, but it will be much worse for patching replicas with tens of terabytes of data, or for processing distributed queries with a large amount of intermediate data. "},{"title":"Huge Pages​","type":1,"pageTitle":"Usage Recommendations","url":"docs/en/operations/tips#huge-pages","content":"If you are using old Linux kernel, disable transparent huge pages. It interferes with memory allocators, which leads to significant performance degradation. On newer Linux kernels transparent huge pages are alright. $ echo 'madvise' | sudo tee /sys/kernel/mm/transparent_hugepage/enabled  "},{"title":"Hypervisor configuration​","type":1,"pageTitle":"Usage Recommendations","url":"docs/en/operations/tips#hypervisor-configuration","content":"If you are using OpenStack, set cpu_mode=host-passthrough  in nova.conf. If you are using libvirt, set &lt;cpu mode='host-passthrough'/&gt;  in XML configuration. This is important for ClickHouse to be able to get correct information with cpuid instruction. Otherwise you may get Illegal instruction crashes when hypervisor is run on old CPU models. "},{"title":"ZooKeeper​","type":1,"pageTitle":"Usage Recommendations","url":"docs/en/operations/tips#zookeeper","content":"You are probably already using ZooKeeper for other purposes. You can use the same installation of ZooKeeper, if it isn’t already overloaded. It’s best to use a fresh version of ZooKeeper – 3.4.9 or later. The version in stable Linux distributions may be outdated. You should never use manually written scripts to transfer data between different ZooKeeper clusters, because the result will be incorrect for sequential nodes. Never use the “zkcopy” utility for the same reason: https://github.com/ksprojects/zkcopy/issues/15 If you want to divide an existing ZooKeeper cluster into two, the correct way is to increase the number of its replicas and then reconfigure it as two independent clusters. Do not run ZooKeeper on the same servers as ClickHouse. Because ZooKeeper is very sensitive for latency and ClickHouse may utilize all available system resources. You can have ZooKeeper observers in an ensemble but ClickHouse servers should not interact with observers. Do not change minSessionTimeout setting, large values may affect ClickHouse restart stability. With the default settings, ZooKeeper is a time bomb: The ZooKeeper server won’t delete files from old snapshots and logs when using the default configuration (see autopurge), and this is the responsibility of the operator. This bomb must be defused. The ZooKeeper (3.5.1) configuration below is used in a large production environment: zoo.cfg: # http://hadoop.apache.org/zookeeper/docs/current/zookeeperAdmin.html # The number of milliseconds of each tick tickTime=2000 # The number of ticks that the initial # synchronization phase can take # This value is not quite motivated initLimit=300 # The number of ticks that can pass between # sending a request and getting an acknowledgement syncLimit=10 maxClientCnxns=2000 # It is the maximum value that client may request and the server will accept. # It is Ok to have high maxSessionTimeout on server to allow clients to work with high session timeout if they want. # But we request session timeout of 30 seconds by default (you can change it with session_timeout_ms in ClickHouse config). maxSessionTimeout=60000000 # the directory where the snapshot is stored. dataDir=/opt/zookeeper/{{ '{{' }} cluster['name'] {{ '}}' }}/data # Place the dataLogDir to a separate physical disc for better performance dataLogDir=/opt/zookeeper/{{ '{{' }} cluster['name'] {{ '}}' }}/logs autopurge.snapRetainCount=10 autopurge.purgeInterval=1 # To avoid seeks ZooKeeper allocates space in the transaction log file in # blocks of preAllocSize kilobytes. The default block size is 64M. One reason # for changing the size of the blocks is to reduce the block size if snapshots # are taken more often. (Also, see snapCount). preAllocSize=131072 # Clients can submit requests faster than ZooKeeper can process them, # especially if there are a lot of clients. To prevent ZooKeeper from running # out of memory due to queued requests, ZooKeeper will throttle clients so that # there is no more than globalOutstandingLimit outstanding requests in the # system. The default limit is 1,000.ZooKeeper logs transactions to a # transaction log. After snapCount transactions are written to a log file a # snapshot is started and a new transaction log file is started. The default # snapCount is 10,000. snapCount=3000000 # If this option is defined, requests will be will logged to a trace file named # traceFile.year.month.day. #traceFile= # Leader accepts client connections. Default value is &quot;yes&quot;. The leader machine # coordinates updates. For higher update throughput at thes slight expense of # read throughput the leader can be configured to not accept clients and focus # on coordination. leaderServes=yes standaloneEnabled=false dynamicConfigFile=/etc/zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }}/conf/zoo.cfg.dynamic  Java version: openjdk 11.0.5-shenandoah 2019-10-15 OpenJDK Runtime Environment (build 11.0.5-shenandoah+10-adhoc.heretic.src) OpenJDK 64-Bit Server VM (build 11.0.5-shenandoah+10-adhoc.heretic.src, mixed mode)  JVM parameters: NAME=zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }} ZOOCFGDIR=/etc/$NAME/conf # TODO this is really ugly # How to find out, which jars are needed? # seems, that log4j requires the log4j.properties file to be in the classpath CLASSPATH=&quot;$ZOOCFGDIR:/usr/build/classes:/usr/build/lib/*.jar:/usr/share/zookeeper-3.6.2/lib/audience-annotations-0.5.0.jar:/usr/share/zookeeper-3.6.2/lib/commons-cli-1.2.jar:/usr/share/zookeeper-3.6.2/lib/commons-lang-2.6.jar:/usr/share/zookeeper-3.6.2/lib/jackson-annotations-2.10.3.jar:/usr/share/zookeeper-3.6.2/lib/jackson-core-2.10.3.jar:/usr/share/zookeeper-3.6.2/lib/jackson-databind-2.10.3.jar:/usr/share/zookeeper-3.6.2/lib/javax.servlet-api-3.1.0.jar:/usr/share/zookeeper-3.6.2/lib/jetty-http-9.4.24.v20191120.jar:/usr/share/zookeeper-3.6.2/lib/jetty-io-9.4.24.v20191120.jar:/usr/share/zookeeper-3.6.2/lib/jetty-security-9.4.24.v20191120.jar:/usr/share/zookeeper-3.6.2/lib/jetty-server-9.4.24.v20191120.jar:/usr/share/zookeeper-3.6.2/lib/jetty-servlet-9.4.24.v20191120.jar:/usr/share/zookeeper-3.6.2/lib/jetty-util-9.4.24.v20191120.jar:/usr/share/zookeeper-3.6.2/lib/jline-2.14.6.jar:/usr/share/zookeeper-3.6.2/lib/json-simple-1.1.1.jar:/usr/share/zookeeper-3.6.2/lib/log4j-1.2.17.jar:/usr/share/zookeeper-3.6.2/lib/metrics-core-3.2.5.jar:/usr/share/zookeeper-3.6.2/lib/netty-buffer-4.1.50.Final.jar:/usr/share/zookeeper-3.6.2/lib/netty-codec-4.1.50.Final.jar:/usr/share/zookeeper-3.6.2/lib/netty-common-4.1.50.Final.jar:/usr/share/zookeeper-3.6.2/lib/netty-handler-4.1.50.Final.jar:/usr/share/zookeeper-3.6.2/lib/netty-resolver-4.1.50.Final.jar:/usr/share/zookeeper-3.6.2/lib/netty-transport-4.1.50.Final.jar:/usr/share/zookeeper-3.6.2/lib/netty-transport-native-epoll-4.1.50.Final.jar:/usr/share/zookeeper-3.6.2/lib/netty-transport-native-unix-common-4.1.50.Final.jar:/usr/share/zookeeper-3.6.2/lib/simpleclient-0.6.0.jar:/usr/share/zookeeper-3.6.2/lib/simpleclient_common-0.6.0.jar:/usr/share/zookeeper-3.6.2/lib/simpleclient_hotspot-0.6.0.jar:/usr/share/zookeeper-3.6.2/lib/simpleclient_servlet-0.6.0.jar:/usr/share/zookeeper-3.6.2/lib/slf4j-api-1.7.25.jar:/usr/share/zookeeper-3.6.2/lib/slf4j-log4j12-1.7.25.jar:/usr/share/zookeeper-3.6.2/lib/snappy-java-1.1.7.jar:/usr/share/zookeeper-3.6.2/lib/zookeeper-3.6.2.jar:/usr/share/zookeeper-3.6.2/lib/zookeeper-jute-3.6.2.jar:/usr/share/zookeeper-3.6.2/lib/zookeeper-prometheus-metrics-3.6.2.jar:/usr/share/zookeeper-3.6.2/etc&quot; ZOOCFG=&quot;$ZOOCFGDIR/zoo.cfg&quot; ZOO_LOG_DIR=/var/log/$NAME USER=zookeeper GROUP=zookeeper PIDDIR=/var/run/$NAME PIDFILE=$PIDDIR/$NAME.pid SCRIPTNAME=/etc/init.d/$NAME JAVA=/usr/local/jdk-11/bin/java ZOOMAIN=&quot;org.apache.zookeeper.server.quorum.QuorumPeerMain&quot; ZOO_LOG4J_PROP=&quot;INFO,ROLLINGFILE&quot; JMXLOCALONLY=false JAVA_OPTS=&quot;-Xms{{ '{{' }} cluster.get('xms','128M') {{ '}}' }} \\ -Xmx{{ '{{' }} cluster.get('xmx','1G') {{ '}}' }} \\ -Xlog:safepoint,gc*=info,age*=debug:file=/var/log/$NAME/zookeeper-gc.log:time,level,tags:filecount=16,filesize=16M -verbose:gc \\ -XX:+UseG1GC \\ -Djute.maxbuffer=8388608 \\ -XX:MaxGCPauseMillis=50&quot;  Salt init: description &quot;zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }} centralized coordination service&quot; start on runlevel [2345] stop on runlevel [!2345] respawn limit nofile 8192 8192 pre-start script [ -r &quot;/etc/zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }}/conf/environment&quot; ] || exit 0 . /etc/zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }}/conf/environment [ -d $ZOO_LOG_DIR ] || mkdir -p $ZOO_LOG_DIR chown $USER:$GROUP $ZOO_LOG_DIR end script script . /etc/zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }}/conf/environment [ -r /etc/default/zookeeper ] &amp;&amp; . /etc/default/zookeeper if [ -z &quot;$JMXDISABLE&quot; ]; then JAVA_OPTS=&quot;$JAVA_OPTS -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=$JMXLOCALONLY&quot; fi exec start-stop-daemon --start -c $USER --exec $JAVA --name zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }} \\ -- -cp $CLASSPATH $JAVA_OPTS -Dzookeeper.log.dir=${ZOO_LOG_DIR} \\ -Dzookeeper.root.logger=${ZOO_LOG4J_PROP} $ZOOMAIN $ZOOCFG end script  "},{"title":"Antivirus software​","type":1,"pageTitle":"Usage Recommendations","url":"docs/en/operations/tips#antivirus-software","content":"If you use antivirus software configure it to skip folders with Clickhouse datafiles (/var/lib/clickhouse) otherwise performance may be reduced and you may experience unexpected errors during data ingestion and background merges. Original article "},{"title":"SQL Reference","type":0,"sectionRef":"#","url":"docs/en/sql-reference/","content":"SQL Reference ClickHouse supports the following types of queries: SELECTINSERT INTOCREATEALTEROther types of queries Original article","keywords":""},{"title":"clickhouse-copier","type":0,"sectionRef":"#","url":"docs/en/operations/utilities/clickhouse-copier","content":"","keywords":""},{"title":"Running Clickhouse-copier​","type":1,"pageTitle":"clickhouse-copier","url":"docs/en/operations/utilities/clickhouse-copier#running-clickhouse-copier","content":"The utility should be run manually: $ clickhouse-copier --daemon --config zookeeper.xml --task-path /task/path --base-dir /path/to/dir  Parameters: daemon — Starts clickhouse-copier in daemon mode.config — The path to the zookeeper.xml file with the parameters for the connection to ZooKeeper.task-path — The path to the ZooKeeper node. This node is used for syncing clickhouse-copier processes and storing tasks. Tasks are stored in $task-path/description.task-file — Optional path to file with task configuration for initial upload to ZooKeeper.task-upload-force — Force upload task-file even if node already exists.base-dir — The path to logs and auxiliary files. When it starts, clickhouse-copier creates clickhouse-copier_YYYYMMHHSS_&lt;PID&gt; subdirectories in $base-dir. If this parameter is omitted, the directories are created in the directory where clickhouse-copier was launched. "},{"title":"Format of Zookeeper.xml​","type":1,"pageTitle":"clickhouse-copier","url":"docs/en/operations/utilities/clickhouse-copier#format-of-zookeeper-xml","content":"&lt;clickhouse&gt; &lt;logger&gt; &lt;level&gt;trace&lt;/level&gt; &lt;size&gt;100M&lt;/size&gt; &lt;count&gt;3&lt;/count&gt; &lt;/logger&gt; &lt;zookeeper&gt; &lt;node index=&quot;1&quot;&gt; &lt;host&gt;127.0.0.1&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;/zookeeper&gt; &lt;/clickhouse&gt;  "},{"title":"Configuration of Copying Tasks​","type":1,"pageTitle":"clickhouse-copier","url":"docs/en/operations/utilities/clickhouse-copier#configuration-of-copying-tasks","content":"&lt;clickhouse&gt; &lt;!-- Configuration of clusters as in an ordinary server config --&gt; &lt;remote_servers&gt; &lt;source_cluster&gt; &lt;!-- source cluster &amp; destination clusters accept exactly the same parameters as parameters for the usual Distributed table see https://clickhouse.com/docs/en/engines/table-engines/special/distributed/ --&gt; &lt;shard&gt; &lt;internal_replication&gt;false&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;127.0.0.1&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;!-- &lt;user&gt;default&lt;/user&gt; &lt;password&gt;default&lt;/password&gt; &lt;secure&gt;1&lt;/secure&gt; --&gt; &lt;/replica&gt; &lt;/shard&gt; ... &lt;/source_cluster&gt; &lt;destination_cluster&gt; ... &lt;/destination_cluster&gt; &lt;/remote_servers&gt; &lt;!-- How many simultaneously active workers are possible. If you run more workers superfluous workers will sleep. --&gt; &lt;max_workers&gt;2&lt;/max_workers&gt; &lt;!-- Setting used to fetch (pull) data from source cluster tables --&gt; &lt;settings_pull&gt; &lt;readonly&gt;1&lt;/readonly&gt; &lt;/settings_pull&gt; &lt;!-- Setting used to insert (push) data to destination cluster tables --&gt; &lt;settings_push&gt; &lt;readonly&gt;0&lt;/readonly&gt; &lt;/settings_push&gt; &lt;!-- Common setting for fetch (pull) and insert (push) operations. Also, copier process context uses it. They are overlaid by &lt;settings_pull/&gt; and &lt;settings_push/&gt; respectively. --&gt; &lt;settings&gt; &lt;connect_timeout&gt;3&lt;/connect_timeout&gt; &lt;!-- Sync insert is set forcibly, leave it here just in case. --&gt; &lt;insert_distributed_sync&gt;1&lt;/insert_distributed_sync&gt; &lt;/settings&gt; &lt;!-- Copying tasks description. You could specify several table task in the same task description (in the same ZooKeeper node), they will be performed sequentially. --&gt; &lt;tables&gt; &lt;!-- A table task, copies one table. --&gt; &lt;table_hits&gt; &lt;!-- Source cluster name (from &lt;remote_servers/&gt; section) and tables in it that should be copied --&gt; &lt;cluster_pull&gt;source_cluster&lt;/cluster_pull&gt; &lt;database_pull&gt;test&lt;/database_pull&gt; &lt;table_pull&gt;hits&lt;/table_pull&gt; &lt;!-- Destination cluster name and tables in which the data should be inserted --&gt; &lt;cluster_push&gt;destination_cluster&lt;/cluster_push&gt; &lt;database_push&gt;test&lt;/database_push&gt; &lt;table_push&gt;hits2&lt;/table_push&gt; &lt;!-- Engine of destination tables. If destination tables have not be created, workers create them using columns definition from source tables and engine definition from here. NOTE: If the first worker starts insert data and detects that destination partition is not empty then the partition will be dropped and refilled, take it into account if you already have some data in destination tables. You could directly specify partitions that should be copied in &lt;enabled_partitions/&gt;, they should be in quoted format like partition column of system.parts table. --&gt; &lt;engine&gt; ENGINE=ReplicatedMergeTree('/clickhouse/tables/{cluster}/{shard}/hits2', '{replica}') PARTITION BY toMonday(date) ORDER BY (CounterID, EventDate) &lt;/engine&gt; &lt;!-- Sharding key used to insert data to destination cluster --&gt; &lt;sharding_key&gt;jumpConsistentHash(intHash64(UserID), 2)&lt;/sharding_key&gt; &lt;!-- Optional expression that filter data while pull them from source servers --&gt; &lt;where_condition&gt;CounterID != 0&lt;/where_condition&gt; &lt;!-- This section specifies partitions that should be copied, other partition will be ignored. Partition names should have the same format as partition column of system.parts table (i.e. a quoted text). Since partition key of source and destination cluster could be different, these partition names specify destination partitions. NOTE: In spite of this section is optional (if it is not specified, all partitions will be copied), it is strictly recommended to specify them explicitly. If you already have some ready partitions on destination cluster they will be removed at the start of the copying since they will be interpeted as unfinished data from the previous copying!!! --&gt; &lt;enabled_partitions&gt; &lt;partition&gt;'2018-02-26'&lt;/partition&gt; &lt;partition&gt;'2018-03-05'&lt;/partition&gt; ... &lt;/enabled_partitions&gt; &lt;/table_hits&gt; &lt;!-- Next table to copy. It is not copied until previous table is copying. --&gt; &lt;table_visits&gt; ... &lt;/table_visits&gt; ... &lt;/tables&gt; &lt;/clickhouse&gt;  clickhouse-copier tracks the changes in /task/path/description and applies them on the fly. For instance, if you change the value of max_workers, the number of processes running tasks will also change. Original article "},{"title":"anyHeavy","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/anyheavy","content":"anyHeavy Selects a frequently occurring value using the heavy hitters algorithm. If there is a value that occurs more than in half the cases in each of the query’s execution threads, this value is returned. Normally, the result is nondeterministic. anyHeavy(column) Arguments column – The column name. Example Take the OnTime data set and select any frequently occurring value in the AirlineID column. SELECT anyHeavy(AirlineID) AS res FROM ontime ┌───res─┐ │ 19690 │ └───────┘ ","keywords":""},{"title":"anylast","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/anylast","content":"","keywords":""},{"title":"anyLast​","type":1,"pageTitle":"anylast","url":"docs/en/sql-reference/aggregate-functions/reference/anylast#anylastx","content":"Selects the last value encountered. The result is just as indeterminate as for the any function. "},{"title":"avg","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/avg","content":"avg Calculates the arithmetic mean. Syntax avg(x) Arguments x — input values, must be Integer, Float, or Decimal. Returned value The arithmetic mean, always as Float64.NaN if the input parameter x is empty. Example Query: SELECT avg(x) FROM values('x Int8', 0, 1, 2, 3, 4, 5); Result: ┌─avg(x)─┐ │ 2.5 │ └────────┘ Example Create a temp table: Query: CREATE table test (t UInt8) ENGINE = Memory; Get the arithmetic mean: Query: SELECT avg(t) FROM test; Result: ┌─avg(x)─┐ │ nan │ └────────┘ Original article","keywords":""},{"title":"argMax","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/argmax","content":"argMax Calculates the arg value for a maximum val value. If there are several different values of arg for maximum values of val, returns the first of these values encountered. Syntax argMax(arg, val) Arguments arg — Argument.val — Value. Returned value arg value that corresponds to maximum val value. Type: matches arg type. Example Input table: ┌─user─────┬─salary─┐ │ director │ 5000 │ │ manager │ 3000 │ │ worker │ 1000 │ └──────────┴────────┘ Query: SELECT argMax(user, salary) FROM salary; Result: ┌─argMax(user, salary)─┐ │ director │ └──────────────────────┘ ","keywords":""},{"title":"List of Aggregate Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/","content":"List of Aggregate Functions Standard aggregate functions: countminmaxsumavganystddevPopstddevSampvarPopvarSampcovarPopcovarSamp ClickHouse-specific aggregate functions: anyHeavyanyLastargMinargMaxavgWeightedtopKtopKWeightedgroupArraygroupUniqArraygroupArrayInsertAtgroupArrayMovingAvggroupArrayMovingSumgroupBitAndgroupBitOrgroupBitXorgroupBitmapgroupBitmapAndgroupBitmapOrgroupBitmapXorsumWithOverflowsumMapminMapmaxMapskewSampskewPopkurtSampkurtPopuniquniqExactuniqCombineduniqCombined64uniqHLL12quantilequantilesquantileExactquantileExactLowquantileExactHighquantileExactWeightedquantileTimingquantileTimingWeightedquantileDeterministicquantileTDigestquantileTDigestWeightedquantileBFloat16quantileBFloat16WeightedsimpleLinearRegressionstochasticLinearRegressionstochasticLogisticRegressioncategoricalInformationValue Original article","keywords":""},{"title":"categoricalInformationValue","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/categoricalinformationvalue","content":"categoricalInformationValue Calculates the value of (P(tag = 1) - P(tag = 0))(log(P(tag = 1)) - log(P(tag = 0))) for each category. categoricalInformationValue(category1, category2, ..., tag) The result indicates how a discrete (categorical) feature [category1, category2, ...] contribute to a learning model which predicting the value of tag.","keywords":""},{"title":"Aggregate Function Combinators","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/combinators","content":"","keywords":""},{"title":"-If​","type":1,"pageTitle":"Aggregate Function Combinators","url":"docs/en/sql-reference/aggregate-functions/combinators#agg-functions-combinator-if","content":"The suffix -If can be appended to the name of any aggregate function. In this case, the aggregate function accepts an extra argument – a condition (Uint8 type). The aggregate function processes only the rows that trigger the condition. If the condition was not triggered even once, it returns a default value (usually zeros or empty strings). Examples: sumIf(column, cond), countIf(cond), avgIf(x, cond), quantilesTimingIf(level1, level2)(x, cond), argMinIf(arg, val, cond) and so on. With conditional aggregate functions, you can calculate aggregates for several conditions at once, without using subqueries and JOINs. For example, conditional aggregate functions can be used to implement the segment comparison functionality. "},{"title":"-Array​","type":1,"pageTitle":"Aggregate Function Combinators","url":"docs/en/sql-reference/aggregate-functions/combinators#agg-functions-combinator-array","content":"The -Array suffix can be appended to any aggregate function. In this case, the aggregate function takes arguments of the ‘Array(T)’ type (arrays) instead of ‘T’ type arguments. If the aggregate function accepts multiple arguments, this must be arrays of equal lengths. When processing arrays, the aggregate function works like the original aggregate function across all array elements. Example 1: sumArray(arr) - Totals all the elements of all ‘arr’ arrays. In this example, it could have been written more simply: sum(arraySum(arr)). Example 2: uniqArray(arr) – Counts the number of unique elements in all ‘arr’ arrays. This could be done an easier way: uniq(arrayJoin(arr)), but it’s not always possible to add ‘arrayJoin’ to a query. -If and -Array can be combined. However, ‘Array’ must come first, then ‘If’. Examples: uniqArrayIf(arr, cond), quantilesTimingArrayIf(level1, level2)(arr, cond). Due to this order, the ‘cond’ argument won’t be an array. "},{"title":"-Map​","type":1,"pageTitle":"Aggregate Function Combinators","url":"docs/en/sql-reference/aggregate-functions/combinators#agg-functions-combinator-map","content":"The -Map suffix can be appended to any aggregate function. This will create an aggregate function which gets Map type as an argument, and aggregates values of each key of the map separately using the specified aggregate function. The result is also of a Map type. Examples: sumMap(map(1,1)), avgMap(map('a', 1)). "},{"title":"-SimpleState​","type":1,"pageTitle":"Aggregate Function Combinators","url":"docs/en/sql-reference/aggregate-functions/combinators#agg-functions-combinator-simplestate","content":"If you apply this combinator, the aggregate function returns the same value but with a different type. This is a SimpleAggregateFunction(...) that can be stored in a table to work with AggregatingMergeTree tables. Syntax &lt;aggFunction&gt;SimpleState(x)  Arguments x — Aggregate function parameters. Returned values The value of an aggregate function with the SimpleAggregateFunction(...) type. Example Query: WITH anySimpleState(number) AS c SELECT toTypeName(c), c FROM numbers(1);  Result: ┌─toTypeName(c)────────────────────────┬─c─┐ │ SimpleAggregateFunction(any, UInt64) │ 0 │ └──────────────────────────────────────┴───┘  "},{"title":"-State​","type":1,"pageTitle":"Aggregate Function Combinators","url":"docs/en/sql-reference/aggregate-functions/combinators#agg-functions-combinator-state","content":"If you apply this combinator, the aggregate function does not return the resulting value (such as the number of unique values for the uniq function), but an intermediate state of the aggregation (for uniq, this is the hash table for calculating the number of unique values). This is an AggregateFunction(...) that can be used for further processing or stored in a table to finish aggregating later. To work with these states, use: AggregatingMergeTree table engine.finalizeAggregation function.runningAccumulate function.-Merge combinator.-MergeState combinator. "},{"title":"-Merge​","type":1,"pageTitle":"Aggregate Function Combinators","url":"docs/en/sql-reference/aggregate-functions/combinators#aggregate_functions_combinators-merge","content":"If you apply this combinator, the aggregate function takes the intermediate aggregation state as an argument, combines the states to finish aggregation, and returns the resulting value. "},{"title":"-MergeState​","type":1,"pageTitle":"Aggregate Function Combinators","url":"docs/en/sql-reference/aggregate-functions/combinators#aggregate_functions_combinators-mergestate","content":"Merges the intermediate aggregation states in the same way as the -Merge combinator. However, it does not return the resulting value, but an intermediate aggregation state, similar to the -State combinator. "},{"title":"-ForEach​","type":1,"pageTitle":"Aggregate Function Combinators","url":"docs/en/sql-reference/aggregate-functions/combinators#agg-functions-combinator-foreach","content":"Converts an aggregate function for tables into an aggregate function for arrays that aggregates the corresponding array items and returns an array of results. For example, sumForEach for the arrays [1, 2], [3, 4, 5]and[6, 7]returns the result [10, 13, 5] after adding together the corresponding array items. "},{"title":"-Distinct​","type":1,"pageTitle":"Aggregate Function Combinators","url":"docs/en/sql-reference/aggregate-functions/combinators#agg-functions-combinator-distinct","content":"Every unique combination of arguments will be aggregated only once. Repeating values are ignored. Examples: sum(DISTINCT x), groupArray(DISTINCT x), corrStableDistinct(DISTINCT x, y) and so on. "},{"title":"-OrDefault​","type":1,"pageTitle":"Aggregate Function Combinators","url":"docs/en/sql-reference/aggregate-functions/combinators#agg-functions-combinator-ordefault","content":"Changes behavior of an aggregate function. If an aggregate function does not have input values, with this combinator it returns the default value for its return data type. Applies to the aggregate functions that can take empty input data. -OrDefault can be used with other combinators. Syntax &lt;aggFunction&gt;OrDefault(x)  Arguments x — Aggregate function parameters. Returned values Returns the default value of an aggregate function’s return type if there is nothing to aggregate. Type depends on the aggregate function used. Example Query: SELECT avg(number), avgOrDefault(number) FROM numbers(0)  Result: ┌─avg(number)─┬─avgOrDefault(number)─┐ │ nan │ 0 │ └─────────────┴──────────────────────┘  Also -OrDefault can be used with another combinators. It is useful when the aggregate function does not accept the empty input. Query: SELECT avgOrDefaultIf(x, x &gt; 10) FROM ( SELECT toDecimal32(1.23, 2) AS x )  Result: ┌─avgOrDefaultIf(x, greater(x, 10))─┐ │ 0.00 │ └───────────────────────────────────┘  "},{"title":"-OrNull​","type":1,"pageTitle":"Aggregate Function Combinators","url":"docs/en/sql-reference/aggregate-functions/combinators#agg-functions-combinator-ornull","content":"Changes behavior of an aggregate function. This combinator converts a result of an aggregate function to the Nullable data type. If the aggregate function does not have values to calculate it returns NULL. -OrNull can be used with other combinators. Syntax &lt;aggFunction&gt;OrNull(x)  Arguments x — Aggregate function parameters. Returned values The result of the aggregate function, converted to the Nullable data type.NULL, if there is nothing to aggregate. Type: Nullable(aggregate function return type). Example Add -orNull to the end of aggregate function. Query: SELECT sumOrNull(number), toTypeName(sumOrNull(number)) FROM numbers(10) WHERE number &gt; 10  Result: ┌─sumOrNull(number)─┬─toTypeName(sumOrNull(number))─┐ │ ᴺᵁᴸᴸ │ Nullable(UInt64) │ └───────────────────┴───────────────────────────────┘  Also -OrNull can be used with another combinators. It is useful when the aggregate function does not accept the empty input. Query: SELECT avgOrNullIf(x, x &gt; 10) FROM ( SELECT toDecimal32(1.23, 2) AS x )  Result: ┌─avgOrNullIf(x, greater(x, 10))─┐ │ ᴺᵁᴸᴸ │ └────────────────────────────────┘  "},{"title":"-Resample​","type":1,"pageTitle":"Aggregate Function Combinators","url":"docs/en/sql-reference/aggregate-functions/combinators#agg-functions-combinator-resample","content":"Lets you divide data into groups, and then separately aggregates the data in those groups. Groups are created by splitting the values from one column into intervals. &lt;aggFunction&gt;Resample(start, end, step)(&lt;aggFunction_params&gt;, resampling_key)  Arguments start — Starting value of the whole required interval for resampling_key values.stop — Ending value of the whole required interval for resampling_key values. The whole interval does not include the stop value [start, stop).step — Step for separating the whole interval into subintervals. The aggFunction is executed over each of those subintervals independently.resampling_key — Column whose values are used for separating data into intervals.aggFunction_params — aggFunction parameters. Returned values Array of aggFunction results for each subinterval. Example Consider the people table with the following data: ┌─name───┬─age─┬─wage─┐ │ John │ 16 │ 10 │ │ Alice │ 30 │ 15 │ │ Mary │ 35 │ 8 │ │ Evelyn │ 48 │ 11.5 │ │ David │ 62 │ 9.9 │ │ Brian │ 60 │ 16 │ └────────┴─────┴──────┘  Let’s get the names of the people whose age lies in the intervals of [30,60) and [60,75). Since we use integer representation for age, we get ages in the [30, 59] and [60,74] intervals. To aggregate names in an array, we use the groupArray aggregate function. It takes one argument. In our case, it’s the name column. The groupArrayResample function should use the age column to aggregate names by age. To define the required intervals, we pass the 30, 75, 30 arguments into the groupArrayResample function. SELECT groupArrayResample(30, 75, 30)(name, age) FROM people  ┌─groupArrayResample(30, 75, 30)(name, age)─────┐ │ [['Alice','Mary','Evelyn'],['David','Brian']] │ └───────────────────────────────────────────────┘  Consider the results. Jonh is out of the sample because he’s too young. Other people are distributed according to the specified age intervals. Now let’s count the total number of people and their average wage in the specified age intervals. SELECT countResample(30, 75, 30)(name, age) AS amount, avgResample(30, 75, 30)(wage, age) AS avg_wage FROM people  ┌─amount─┬─avg_wage──────────────────┐ │ [3,2] │ [11.5,12.949999809265137] │ └────────┴───────────────────────────┘  "},{"title":"covarPop","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/covarpop","content":"covarPop Syntax: covarPop(x, y) Calculates the value of Σ((x - x̅)(y - y̅)) / n. note This function uses a numerically unstable algorithm. If you need numerical stability in calculations, use the covarPopStable function. It works slower but provides a lower computational error.","keywords":""},{"title":"count","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/count","content":"count Counts the number of rows or not-NULL values. ClickHouse supports the following syntaxes for count: count(expr) or COUNT(DISTINCT expr).count() or COUNT(*). The count() syntax is ClickHouse-specific. Arguments The function can take: Zero parameters.One expression. Returned value If the function is called without parameters it counts the number of rows.If the expression is passed, then the function counts how many times this expression returned not null. If the expression returns a Nullable-type value, then the result of count stays not Nullable. The function returns 0 if the expression returned NULL for all the rows. In both cases the type of the returned value is UInt64. Details ClickHouse supports the COUNT(DISTINCT ...) syntax. The behavior of this construction depends on the count_distinct_implementation setting. It defines which of the uniq* functions is used to perform the operation. The default is the uniqExact function. The SELECT count() FROM table query is optimized by default using metadata from MergeTree. If you need to use row-level security, disable optimization using the optimize_trivial_count_query setting. However SELECT count(nullable_column) FROM table query can be optimized by enabling the optimize_functions_to_subcolumns setting. With optimize_functions_to_subcolumns = 1 the function reads only null subcolumn instead of reading and processing the whole column data. The query SELECT count(n) FROM table transforms to SELECT sum(NOT n.null) FROM table. Examples Example 1: SELECT count() FROM t ┌─count()─┐ │ 5 │ └─────────┘ Example 2: SELECT name, value FROM system.settings WHERE name = 'count_distinct_implementation' ┌─name──────────────────────────┬─value─────┐ │ count_distinct_implementation │ uniqExact │ └───────────────────────────────┴───────────┘ SELECT count(DISTINCT num) FROM t ┌─uniqExact(num)─┐ │ 3 │ └────────────────┘ This example shows that count(DISTINCT num) is performed by the uniqExact function according to the count_distinct_implementation setting value.","keywords":""},{"title":"deltaSumTimestamp","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/deltasumtimestamp","content":"deltaSumTimestamp Adds the difference between consecutive rows. If the difference is negative, it is ignored. This function is primarily for materialized views that are ordered by some time bucket-aligned timestamp, for example, a toStartOfMinute bucket. Because the rows in such a materialized view will all have the same timestamp, it is impossible for them to be merged in the &quot;right&quot; order. This function keeps track of the timestamp of the values it's seen, so it's possible to order the states correctly during merging. To calculate the delta sum across an ordered collection you can simply use the deltaSum function. Syntax deltaSumTimestamp(value, timestamp) Arguments value — Input values, must be some Integer type or Float type or a Date or DateTime.timestamp — The parameter for order values, must be some Integer type or Float type or a Date or DateTime. Returned value Accumulated differences between consecutive values, ordered by the timestamp parameter. Type: Integer or Float or Date or DateTime. Example Query: SELECT deltaSumTimestamp(value, timestamp) FROM (SELECT number AS timestamp, [0, 4, 8, 3, 0, 0, 0, 1, 3, 5][number] AS value FROM numbers(1, 10)); Result: ┌─deltaSumTimestamp(value, timestamp)─┐ │ 13 │ └─────────────────────────────────────┘ ","keywords":""},{"title":"entropy","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/entropy","content":"entropy Calculates Shannon entropy of a column of values. Syntax entropy(val) Arguments val — Column of values of any type. Returned value Shannon entropy. Type: Float64. Example Query: CREATE TABLE entropy (`vals` UInt32,`strings` String) ENGINE = Memory; INSERT INTO entropy VALUES (1, 'A'), (1, 'A'), (1,'A'), (1,'A'), (2,'B'), (2,'B'), (2,'C'), (2,'D'); SELECT entropy(vals), entropy(strings) FROM entropy; Result: ┌─entropy(vals)─┬─entropy(strings)─┐ │ 1 │ 1.75 │ └───────────────┴──────────────────┘ ","keywords":""},{"title":"any","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/any","content":"any Selects the first encountered value. The query can be executed in any order and even in a different order each time, so the result of this function is indeterminate. To get a determinate result, you can use the ‘min’ or ‘max’ function instead of ‘any’. In some cases, you can rely on the order of execution. This applies to cases when SELECT comes from a subquery that uses ORDER BY. When a SELECT query has the GROUP BY clause or at least one aggregate function, ClickHouse (in contrast to MySQL) requires that all expressions in the SELECT, HAVING, and ORDER BY clauses be calculated from keys or from aggregate functions. In other words, each column selected from the table must be used either in keys or inside aggregate functions. To get behavior like in MySQL, you can put the other columns in the any aggregate function.","keywords":""},{"title":"argMin","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/argmin","content":"argMin Calculates the arg value for a minimum val value. If there are several different values of arg for minimum values of val, returns the first of these values encountered. Syntax argMin(arg, val) Arguments arg — Argument.val — Value. Returned value arg value that corresponds to minimum val value. Type: matches arg type. Example Input table: ┌─user─────┬─salary─┐ │ director │ 5000 │ │ manager │ 3000 │ │ worker │ 1000 │ └──────────┴────────┘ Query: SELECT argMin(user, salary) FROM salary Result: ┌─argMin(user, salary)─┐ │ worker │ └──────────────────────┘ ","keywords":""},{"title":"avgWeighted","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/avgweighted","content":"avgWeighted Calculates the weighted arithmetic mean. Syntax avgWeighted(x, weight) Arguments x — Values.weight — Weights of the values. x and weight must both beInteger,floating-point, orDecimal, but may have different types. Returned value NaN if all the weights are equal to 0 or the supplied weights parameter is empty.Weighted mean otherwise. Return type is always Float64. Example Query: SELECT avgWeighted(x, w) FROM values('x Int8, w Int8', (4, 1), (1, 0), (10, 2)) Result: ┌─avgWeighted(x, weight)─┐ │ 8 │ └────────────────────────┘ Example Query: SELECT avgWeighted(x, w) FROM values('x Int8, w Float64', (4, 1), (1, 0), (10, 2)) Result: ┌─avgWeighted(x, weight)─┐ │ 8 │ └────────────────────────┘ Example Query: SELECT avgWeighted(x, w) FROM values('x Int8, w Int8', (0, 0), (1, 0), (10, 0)) Result: ┌─avgWeighted(x, weight)─┐ │ nan │ └────────────────────────┘ Example Query: CREATE table test (t UInt8) ENGINE = Memory; SELECT avgWeighted(t) FROM test Result: ┌─avgWeighted(x, weight)─┐ │ nan │ └────────────────────────┘ ","keywords":""},{"title":"corr","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/corr","content":"corr Syntax: corr(x, y) Calculates the Pearson correlation coefficient: Σ((x - x̅)(y - y̅)) / sqrt(Σ((x - x̅)^2) * Σ((y - y̅)^2)). note This function uses a numerically unstable algorithm. If you need numerical stability in calculations, use the corrStable function. It works slower but provides a lower computational error.","keywords":""},{"title":"covarSamp","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/covarsamp","content":"covarSamp Calculates the value of Σ((x - x̅)(y - y̅)) / (n - 1). Returns Float64. When n &lt;= 1, returns +∞. note This function uses a numerically unstable algorithm. If you need numerical stability in calculations, use the covarSampStable function. It works slower but provides a lower computational error.","keywords":""},{"title":"groupArrayInsertAt","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/grouparrayinsertat","content":"groupArrayInsertAt Inserts a value into the array at the specified position. Syntax groupArrayInsertAt(default_x, size)(x, pos) If in one query several values are inserted into the same position, the function behaves in the following ways: If a query is executed in a single thread, the first one of the inserted values is used.If a query is executed in multiple threads, the resulting value is an undetermined one of the inserted values. Arguments x — Value to be inserted. Expression resulting in one of the supported data types.pos — Position at which the specified element x is to be inserted. Index numbering in the array starts from zero. UInt32.default_x — Default value for substituting in empty positions. Optional parameter. Expression resulting in the data type configured for the x parameter. If default_x is not defined, the default values are used.size — Length of the resulting array. Optional parameter. When using this parameter, the default value default_x must be specified. UInt32. Returned value Array with inserted values. Type: Array. Example Query: SELECT groupArrayInsertAt(toString(number), number * 2) FROM numbers(5); Result: ┌─groupArrayInsertAt(toString(number), multiply(number, 2))─┐ │ ['0','','1','','2','','3','','4'] │ └───────────────────────────────────────────────────────────┘ Query: SELECT groupArrayInsertAt('-')(toString(number), number * 2) FROM numbers(5); Result: ┌─groupArrayInsertAt('-')(toString(number), multiply(number, 2))─┐ │ ['0','-','1','-','2','-','3','-','4'] │ └────────────────────────────────────────────────────────────────┘ Query: SELECT groupArrayInsertAt('-', 5)(toString(number), number * 2) FROM numbers(5); Result: ┌─groupArrayInsertAt('-', 5)(toString(number), multiply(number, 2))─┐ │ ['0','-','1','-','2'] │ └───────────────────────────────────────────────────────────────────┘ Multi-threaded insertion of elements into one position. Query: SELECT groupArrayInsertAt(number, 0) FROM numbers_mt(10) SETTINGS max_block_size = 1; As a result of this query you get random integer in the [0,9] range. For example: ┌─groupArrayInsertAt(number, 0)─┐ │ [7] │ └───────────────────────────────┘ ","keywords":""},{"title":"groupArrayMovingSum","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/grouparraymovingsum","content":"groupArrayMovingSum Calculates the moving sum of input values. groupArrayMovingSum(numbers_for_summing) groupArrayMovingSum(window_size)(numbers_for_summing) The function can take the window size as a parameter. If left unspecified, the function takes the window size equal to the number of rows in the column. Arguments numbers_for_summing — Expression resulting in a numeric data type value.window_size — Size of the calculation window. Returned values Array of the same size and type as the input data. Example The sample table: CREATE TABLE t ( `int` UInt8, `float` Float32, `dec` Decimal32(2) ) ENGINE = TinyLog ┌─int─┬─float─┬──dec─┐ │ 1 │ 1.1 │ 1.10 │ │ 2 │ 2.2 │ 2.20 │ │ 4 │ 4.4 │ 4.40 │ │ 7 │ 7.77 │ 7.77 │ └─────┴───────┴──────┘ The queries: SELECT groupArrayMovingSum(int) AS I, groupArrayMovingSum(float) AS F, groupArrayMovingSum(dec) AS D FROM t ┌─I──────────┬─F───────────────────────────────┬─D──────────────────────┐ │ [1,3,7,14] │ [1.1,3.3000002,7.7000003,15.47] │ [1.10,3.30,7.70,15.47] │ └────────────┴─────────────────────────────────┴────────────────────────┘ SELECT groupArrayMovingSum(2)(int) AS I, groupArrayMovingSum(2)(float) AS F, groupArrayMovingSum(2)(dec) AS D FROM t ┌─I──────────┬─F───────────────────────────────┬─D──────────────────────┐ │ [1,3,6,11] │ [1.1,3.3000002,6.6000004,12.17] │ [1.10,3.30,6.60,12.17] │ └────────────┴─────────────────────────────────┴────────────────────────┘ ","keywords":""},{"title":"groupArrayMovingAvg","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/grouparraymovingavg","content":"groupArrayMovingAvg Calculates the moving average of input values. groupArrayMovingAvg(numbers_for_summing) groupArrayMovingAvg(window_size)(numbers_for_summing) The function can take the window size as a parameter. If left unspecified, the function takes the window size equal to the number of rows in the column. Arguments numbers_for_summing — Expression resulting in a numeric data type value.window_size — Size of the calculation window. Returned values Array of the same size and type as the input data. The function uses rounding towards zero. It truncates the decimal places insignificant for the resulting data type. Example The sample table b: CREATE TABLE t ( `int` UInt8, `float` Float32, `dec` Decimal32(2) ) ENGINE = TinyLog ┌─int─┬─float─┬──dec─┐ │ 1 │ 1.1 │ 1.10 │ │ 2 │ 2.2 │ 2.20 │ │ 4 │ 4.4 │ 4.40 │ │ 7 │ 7.77 │ 7.77 │ └─────┴───────┴──────┘ The queries: SELECT groupArrayMovingAvg(int) AS I, groupArrayMovingAvg(float) AS F, groupArrayMovingAvg(dec) AS D FROM t ┌─I─────────┬─F───────────────────────────────────┬─D─────────────────────┐ │ [0,0,1,3] │ [0.275,0.82500005,1.9250001,3.8675] │ [0.27,0.82,1.92,3.86] │ └───────────┴─────────────────────────────────────┴───────────────────────┘ SELECT groupArrayMovingAvg(2)(int) AS I, groupArrayMovingAvg(2)(float) AS F, groupArrayMovingAvg(2)(dec) AS D FROM t ┌─I─────────┬─F────────────────────────────────┬─D─────────────────────┐ │ [0,1,3,5] │ [0.55,1.6500001,3.3000002,6.085] │ [0.55,1.65,3.30,6.08] │ └───────────┴──────────────────────────────────┴───────────────────────┘ ","keywords":""},{"title":"exponentialmovingaverage","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/exponentialmovingaverage","content":"","keywords":""},{"title":"exponentialMovingAverage​","type":1,"pageTitle":"exponentialmovingaverage","url":"docs/en/sql-reference/aggregate-functions/reference/exponentialmovingaverage#exponential-moving-average","content":"Сalculates the exponential moving average of values for the determined time. Syntax exponentialMovingAverage(x)(value, timestamp)  Each value corresponds to the determinate timestamp. The half-life x is the time lag at which the exponential weights decay by one-half. The function returns a weighted average: the older the time point, the less weight the corresponding value is considered to be. Arguments value — Value. Integer, Float or Decimal.timestamp — Timestamp. Integer, Float or Decimal. Parameters x — Half-life period. Integer, Float or Decimal. Returned values Returnes an exponentially smoothed moving average of the values for the past x time at the latest point of time. Type: Float64. Examples Input table: ┌──temperature─┬─timestamp──┐ │ 95 │ 1 │ │ 95 │ 2 │ │ 95 │ 3 │ │ 96 │ 4 │ │ 96 │ 5 │ │ 96 │ 6 │ │ 96 │ 7 │ │ 97 │ 8 │ │ 97 │ 9 │ │ 97 │ 10 │ │ 97 │ 11 │ │ 98 │ 12 │ │ 98 │ 13 │ │ 98 │ 14 │ │ 98 │ 15 │ │ 99 │ 16 │ │ 99 │ 17 │ │ 99 │ 18 │ │ 100 │ 19 │ │ 100 │ 20 │ └──────────────┴────────────┘  Query: SELECT exponentialMovingAverage(5)(temperature, timestamp);  Result: ┌──exponentialMovingAverage(5)(temperature, timestamp)──┐ │ 92.25779635374204 │ └───────────────────────────────────────────────────────┘  Query: SELECT value, time, round(exp_smooth, 3), bar(exp_smooth, 0, 1, 50) AS bar FROM ( SELECT (number = 0) OR (number &gt;= 25) AS value, number AS time, exponentialMovingAverage(10)(value, time) OVER (Rows BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS exp_smooth FROM numbers(50) )  Result: ┌─value─┬─time─┬─round(exp_smooth, 3)─┬─bar────────────────────────────────────────┐ │ 1 │ 0 │ 0.067 │ ███▎ │ │ 0 │ 1 │ 0.062 │ ███ │ │ 0 │ 2 │ 0.058 │ ██▊ │ │ 0 │ 3 │ 0.054 │ ██▋ │ │ 0 │ 4 │ 0.051 │ ██▌ │ │ 0 │ 5 │ 0.047 │ ██▎ │ │ 0 │ 6 │ 0.044 │ ██▏ │ │ 0 │ 7 │ 0.041 │ ██ │ │ 0 │ 8 │ 0.038 │ █▊ │ │ 0 │ 9 │ 0.036 │ █▋ │ │ 0 │ 10 │ 0.033 │ █▋ │ │ 0 │ 11 │ 0.031 │ █▌ │ │ 0 │ 12 │ 0.029 │ █▍ │ │ 0 │ 13 │ 0.027 │ █▎ │ │ 0 │ 14 │ 0.025 │ █▎ │ │ 0 │ 15 │ 0.024 │ █▏ │ │ 0 │ 16 │ 0.022 │ █ │ │ 0 │ 17 │ 0.021 │ █ │ │ 0 │ 18 │ 0.019 │ ▊ │ │ 0 │ 19 │ 0.018 │ ▊ │ │ 0 │ 20 │ 0.017 │ ▋ │ │ 0 │ 21 │ 0.016 │ ▋ │ │ 0 │ 22 │ 0.015 │ ▋ │ │ 0 │ 23 │ 0.014 │ ▋ │ │ 0 │ 24 │ 0.013 │ ▋ │ │ 1 │ 25 │ 0.079 │ ███▊ │ │ 1 │ 26 │ 0.14 │ ███████ │ │ 1 │ 27 │ 0.198 │ █████████▊ │ │ 1 │ 28 │ 0.252 │ ████████████▌ │ │ 1 │ 29 │ 0.302 │ ███████████████ │ │ 1 │ 30 │ 0.349 │ █████████████████▍ │ │ 1 │ 31 │ 0.392 │ ███████████████████▌ │ │ 1 │ 32 │ 0.433 │ █████████████████████▋ │ │ 1 │ 33 │ 0.471 │ ███████████████████████▌ │ │ 1 │ 34 │ 0.506 │ █████████████████████████▎ │ │ 1 │ 35 │ 0.539 │ ██████████████████████████▊ │ │ 1 │ 36 │ 0.57 │ ████████████████████████████▌ │ │ 1 │ 37 │ 0.599 │ █████████████████████████████▊ │ │ 1 │ 38 │ 0.626 │ ███████████████████████████████▎ │ │ 1 │ 39 │ 0.651 │ ████████████████████████████████▌ │ │ 1 │ 40 │ 0.674 │ █████████████████████████████████▋ │ │ 1 │ 41 │ 0.696 │ ██████████████████████████████████▋ │ │ 1 │ 42 │ 0.716 │ ███████████████████████████████████▋ │ │ 1 │ 43 │ 0.735 │ ████████████████████████████████████▋ │ │ 1 │ 44 │ 0.753 │ █████████████████████████████████████▋ │ │ 1 │ 45 │ 0.77 │ ██████████████████████████████████████▍ │ │ 1 │ 46 │ 0.785 │ ███████████████████████████████████████▎ │ │ 1 │ 47 │ 0.8 │ ███████████████████████████████████████▊ │ │ 1 │ 48 │ 0.813 │ ████████████████████████████████████████▋ │ │ 1 │ 49 │ 0.825 │ █████████████████████████████████████████▎│ └───────┴──────┴──────────────────────┴────────────────────────────────────────────┘  "},{"title":"Parametric Aggregate Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/parametric-functions","content":"","keywords":""},{"title":"histogram​","type":1,"pageTitle":"Parametric Aggregate Functions","url":"docs/en/sql-reference/aggregate-functions/parametric-functions#histogram","content":"Calculates an adaptive histogram. It does not guarantee precise results. histogram(number_of_bins)(values)  The functions uses A Streaming Parallel Decision Tree Algorithm. The borders of histogram bins are adjusted as new data enters a function. In common case, the widths of bins are not equal. Arguments values — Expression resulting in input values. Parameters number_of_bins — Upper limit for the number of bins in the histogram. The function automatically calculates the number of bins. It tries to reach the specified number of bins, but if it fails, it uses fewer bins. Returned values Array of Tuples of the following format: ``` [(lower_1, upper_1, height_1), ... (lower_N, upper_N, height_N)] ``` - `lower` — Lower bound of the bin. - `upper` — Upper bound of the bin. - `height` — Calculated height of the bin.  Example SELECT histogram(5)(number + 1) FROM ( SELECT * FROM system.numbers LIMIT 20 )  ┌─histogram(5)(plus(number, 1))───────────────────────────────────────────┐ │ [(1,4.5,4),(4.5,8.5,4),(8.5,12.75,4.125),(12.75,17,4.625),(17,20,3.25)] │ └─────────────────────────────────────────────────────────────────────────┘  You can visualize a histogram with the bar function, for example: WITH histogram(5)(rand() % 100) AS hist SELECT arrayJoin(hist).3 AS height, bar(height, 0, 6, 5) AS bar FROM ( SELECT * FROM system.numbers LIMIT 20 )  ┌─height─┬─bar───┐ │ 2.125 │ █▋ │ │ 3.25 │ ██▌ │ │ 5.625 │ ████▏ │ │ 5.625 │ ████▏ │ │ 3.375 │ ██▌ │ └────────┴───────┘  In this case, you should remember that you do not know the histogram bin borders. "},{"title":"sequenceMatch(pattern)(timestamp, cond1, cond2, …)​","type":1,"pageTitle":"Parametric Aggregate Functions","url":"docs/en/sql-reference/aggregate-functions/parametric-functions#function-sequencematch","content":"Checks whether the sequence contains an event chain that matches the pattern. sequenceMatch(pattern)(timestamp, cond1, cond2, ...)  warning Events that occur at the same second may lay in the sequence in an undefined order affecting the result. Arguments timestamp — Column considered to contain time data. Typical data types are Date and DateTime. You can also use any of the supported UInt data types. cond1, cond2 — Conditions that describe the chain of events. Data type: UInt8. You can pass up to 32 condition arguments. The function takes only the events described in these conditions into account. If the sequence contains data that isn’t described in a condition, the function skips them. Parameters pattern — Pattern string. See Pattern syntax. Returned values 1, if the pattern is matched.0, if the pattern isn’t matched. Type: UInt8.  **Pattern syntax** (?N) — Matches the condition argument at position N. Conditions are numbered in the [1, 32] range. For example, (?1) matches the argument passed to the cond1 parameter. .* — Matches any number of events. You do not need conditional arguments to match this element of the pattern. (?t operator value) — Sets the time in seconds that should separate two events. For example, pattern (?1)(?t&gt;1800)(?2) matches events that occur more than 1800 seconds from each other. An arbitrary number of any events can lay between these events. You can use the &gt;=, &gt;, &lt;, &lt;=, == operators. Examples Consider data in the t table: ┌─time─┬─number─┐ │ 1 │ 1 │ │ 2 │ 3 │ │ 3 │ 2 │ └──────┴────────┘  Perform the query: SELECT sequenceMatch('(?1)(?2)')(time, number = 1, number = 2) FROM t  ┌─sequenceMatch('(?1)(?2)')(time, equals(number, 1), equals(number, 2))─┐ │ 1 │ └───────────────────────────────────────────────────────────────────────┘  The function found the event chain where number 2 follows number 1. It skipped number 3 between them, because the number is not described as an event. If we want to take this number into account when searching for the event chain given in the example, we should make a condition for it. SELECT sequenceMatch('(?1)(?2)')(time, number = 1, number = 2, number = 3) FROM t  ┌─sequenceMatch('(?1)(?2)')(time, equals(number, 1), equals(number, 2), equals(number, 3))─┐ │ 0 │ └──────────────────────────────────────────────────────────────────────────────────────────┘  In this case, the function couldn’t find the event chain matching the pattern, because the event for number 3 occurred between 1 and 2. If in the same case we checked the condition for number 4, the sequence would match the pattern. SELECT sequenceMatch('(?1)(?2)')(time, number = 1, number = 2, number = 4) FROM t  ┌─sequenceMatch('(?1)(?2)')(time, equals(number, 1), equals(number, 2), equals(number, 4))─┐ │ 1 │ └──────────────────────────────────────────────────────────────────────────────────────────┘  See Also sequenceCount "},{"title":"sequenceCount(pattern)(time, cond1, cond2, …)​","type":1,"pageTitle":"Parametric Aggregate Functions","url":"docs/en/sql-reference/aggregate-functions/parametric-functions#function-sequencecount","content":"Counts the number of event chains that matched the pattern. The function searches event chains that do not overlap. It starts to search for the next chain after the current chain is matched. warning Events that occur at the same second may lay in the sequence in an undefined order affecting the result. sequenceCount(pattern)(timestamp, cond1, cond2, ...)  Arguments timestamp — Column considered to contain time data. Typical data types are Date and DateTime. You can also use any of the supported UInt data types. cond1, cond2 — Conditions that describe the chain of events. Data type: UInt8. You can pass up to 32 condition arguments. The function takes only the events described in these conditions into account. If the sequence contains data that isn’t described in a condition, the function skips them. Parameters pattern — Pattern string. See Pattern syntax. Returned values Number of non-overlapping event chains that are matched. Type: UInt64. Example Consider data in the t table: ┌─time─┬─number─┐ │ 1 │ 1 │ │ 2 │ 3 │ │ 3 │ 2 │ │ 4 │ 1 │ │ 5 │ 3 │ │ 6 │ 2 │ └──────┴────────┘  Count how many times the number 2 occurs after the number 1 with any amount of other numbers between them: SELECT sequenceCount('(?1).*(?2)')(time, number = 1, number = 2) FROM t  ┌─sequenceCount('(?1).*(?2)')(time, equals(number, 1), equals(number, 2))─┐ │ 2 │ └─────────────────────────────────────────────────────────────────────────┘  See Also sequenceMatch "},{"title":"windowFunnel​","type":1,"pageTitle":"Parametric Aggregate Functions","url":"docs/en/sql-reference/aggregate-functions/parametric-functions#windowfunnel","content":"Searches for event chains in a sliding time window and calculates the maximum number of events that occurred from the chain. The function works according to the algorithm: The function searches for data that triggers the first condition in the chain and sets the event counter to 1. This is the moment when the sliding window starts. If events from the chain occur sequentially within the window, the counter is incremented. If the sequence of events is disrupted, the counter isn’t incremented. If the data has multiple event chains at varying points of completion, the function will only output the size of the longest chain. Syntax windowFunnel(window, [mode, [mode, ... ]])(timestamp, cond1, cond2, ..., condN)  Arguments timestamp — Name of the column containing the timestamp. Data types supported: Date, DateTime and other unsigned integer types (note that even though timestamp supports the UInt64 type, it’s value can’t exceed the Int64 maximum, which is 2^63 - 1).cond — Conditions or data describing the chain of events. UInt8. Parameters window — Length of the sliding window, it is the time interval between the first and the last condition. The unit of window depends on the timestamp itself and varies. Determined using the expression timestamp of cond1 &lt;= timestamp of cond2 &lt;= ... &lt;= timestamp of condN &lt;= timestamp of cond1 + window.mode — It is an optional argument. One or more modes can be set. 'strict_deduplication' — If the same condition holds for the sequence of events, then such repeating event interrupts further processing.'strict_order' — Don't allow interventions of other events. E.g. in the case of A-&gt;B-&gt;D-&gt;C, it stops finding A-&gt;B-&gt;C at the D and the max event level is 2.'strict_increase' — Apply conditions only to events with strictly increasing timestamps. Returned value The maximum number of consecutive triggered conditions from the chain within the sliding time window. All the chains in the selection are analyzed. Type: Integer. Example Determine if a set period of time is enough for the user to select a phone and purchase it twice in the online store. Set the following chain of events: The user logged in to their account on the store (eventID = 1003).The user searches for a phone (eventID = 1007, product = 'phone').The user placed an order (eventID = 1009).The user made the order again (eventID = 1010). Input table: ┌─event_date─┬─user_id─┬───────────timestamp─┬─eventID─┬─product─┐ │ 2019-01-28 │ 1 │ 2019-01-29 10:00:00 │ 1003 │ phone │ └────────────┴─────────┴─────────────────────┴─────────┴─────────┘ ┌─event_date─┬─user_id─┬───────────timestamp─┬─eventID─┬─product─┐ │ 2019-01-31 │ 1 │ 2019-01-31 09:00:00 │ 1007 │ phone │ └────────────┴─────────┴─────────────────────┴─────────┴─────────┘ ┌─event_date─┬─user_id─┬───────────timestamp─┬─eventID─┬─product─┐ │ 2019-01-30 │ 1 │ 2019-01-30 08:00:00 │ 1009 │ phone │ └────────────┴─────────┴─────────────────────┴─────────┴─────────┘ ┌─event_date─┬─user_id─┬───────────timestamp─┬─eventID─┬─product─┐ │ 2019-02-01 │ 1 │ 2019-02-01 08:00:00 │ 1010 │ phone │ └────────────┴─────────┴─────────────────────┴─────────┴─────────┘  Find out how far the user user_id could get through the chain in a period in January-February of 2019. Query: SELECT level, count() AS c FROM ( SELECT user_id, windowFunnel(6048000000000000)(timestamp, eventID = 1003, eventID = 1009, eventID = 1007, eventID = 1010) AS level FROM trend WHERE (event_date &gt;= '2019-01-01') AND (event_date &lt;= '2019-02-02') GROUP BY user_id ) GROUP BY level ORDER BY level ASC;  Result: ┌─level─┬─c─┐ │ 4 │ 1 │ └───────┴───┘  "},{"title":"retention​","type":1,"pageTitle":"Parametric Aggregate Functions","url":"docs/en/sql-reference/aggregate-functions/parametric-functions#retention","content":"The function takes as arguments a set of conditions from 1 to 32 arguments of type UInt8 that indicate whether a certain condition was met for the event. Any condition can be specified as an argument (as in WHERE). The conditions, except the first, apply in pairs: the result of the second will be true if the first and second are true, of the third if the first and third are true, etc. Syntax retention(cond1, cond2, ..., cond32);  Arguments cond — An expression that returns a UInt8 result (1 or 0). Returned value The array of 1 or 0. 1 — Condition was met for the event.0 — Condition wasn’t met for the event. Type: UInt8. Example Let’s consider an example of calculating the retention function to determine site traffic. 1. Сreate a table to illustrate an example. CREATE TABLE retention_test(date Date, uid Int32) ENGINE = Memory; INSERT INTO retention_test SELECT '2020-01-01', number FROM numbers(5); INSERT INTO retention_test SELECT '2020-01-02', number FROM numbers(10); INSERT INTO retention_test SELECT '2020-01-03', number FROM numbers(15);  Input table: Query: SELECT * FROM retention_test  Result: ┌───────date─┬─uid─┐ │ 2020-01-01 │ 0 │ │ 2020-01-01 │ 1 │ │ 2020-01-01 │ 2 │ │ 2020-01-01 │ 3 │ │ 2020-01-01 │ 4 │ └────────────┴─────┘ ┌───────date─┬─uid─┐ │ 2020-01-02 │ 0 │ │ 2020-01-02 │ 1 │ │ 2020-01-02 │ 2 │ │ 2020-01-02 │ 3 │ │ 2020-01-02 │ 4 │ │ 2020-01-02 │ 5 │ │ 2020-01-02 │ 6 │ │ 2020-01-02 │ 7 │ │ 2020-01-02 │ 8 │ │ 2020-01-02 │ 9 │ └────────────┴─────┘ ┌───────date─┬─uid─┐ │ 2020-01-03 │ 0 │ │ 2020-01-03 │ 1 │ │ 2020-01-03 │ 2 │ │ 2020-01-03 │ 3 │ │ 2020-01-03 │ 4 │ │ 2020-01-03 │ 5 │ │ 2020-01-03 │ 6 │ │ 2020-01-03 │ 7 │ │ 2020-01-03 │ 8 │ │ 2020-01-03 │ 9 │ │ 2020-01-03 │ 10 │ │ 2020-01-03 │ 11 │ │ 2020-01-03 │ 12 │ │ 2020-01-03 │ 13 │ │ 2020-01-03 │ 14 │ └────────────┴─────┘  2. Group users by unique ID uid using the retention function. Query: SELECT uid, retention(date = '2020-01-01', date = '2020-01-02', date = '2020-01-03') AS r FROM retention_test WHERE date IN ('2020-01-01', '2020-01-02', '2020-01-03') GROUP BY uid ORDER BY uid ASC  Result: ┌─uid─┬─r───────┐ │ 0 │ [1,1,1] │ │ 1 │ [1,1,1] │ │ 2 │ [1,1,1] │ │ 3 │ [1,1,1] │ │ 4 │ [1,1,1] │ │ 5 │ [0,0,0] │ │ 6 │ [0,0,0] │ │ 7 │ [0,0,0] │ │ 8 │ [0,0,0] │ │ 9 │ [0,0,0] │ │ 10 │ [0,0,0] │ │ 11 │ [0,0,0] │ │ 12 │ [0,0,0] │ │ 13 │ [0,0,0] │ │ 14 │ [0,0,0] │ └─────┴─────────┘  3. Calculate the total number of site visits per day. Query: SELECT sum(r[1]) AS r1, sum(r[2]) AS r2, sum(r[3]) AS r3 FROM ( SELECT uid, retention(date = '2020-01-01', date = '2020-01-02', date = '2020-01-03') AS r FROM retention_test WHERE date IN ('2020-01-01', '2020-01-02', '2020-01-03') GROUP BY uid )  Result: ┌─r1─┬─r2─┬─r3─┐ │ 5 │ 5 │ 5 │ └────┴────┴────┘  Where: r1- the number of unique visitors who visited the site during 2020-01-01 (the cond1 condition).r2- the number of unique visitors who visited the site during a specific time period between 2020-01-01 and 2020-01-02 (cond1 and cond2 conditions).r3- the number of unique visitors who visited the site during a specific time period between 2020-01-01 and 2020-01-03 (cond1 and cond3 conditions). "},{"title":"uniqUpTo(N)(x)​","type":1,"pageTitle":"Parametric Aggregate Functions","url":"docs/en/sql-reference/aggregate-functions/parametric-functions#uniquptonx","content":"Calculates the number of different argument values ​​if it is less than or equal to N. If the number of different argument values is greater than N, it returns N + 1. Recommended for use with small Ns, up to 10. The maximum value of N is 100. For the state of an aggregate function, it uses the amount of memory equal to 1 + N * the size of one value of bytes. For strings, it stores a non-cryptographic hash of 8 bytes. That is, the calculation is approximated for strings. The function also works for several arguments. It works as fast as possible, except for cases when a large N value is used and the number of unique values is slightly less than N. Usage example: Problem: Generate a report that shows only keywords that produced at least 5 unique users. Solution: Write in the GROUP BY query SearchPhrase HAVING uniqUpTo(4)(UserID) &gt;= 5  "},{"title":"sumMapFiltered(keys_to_keep)(keys, values)​","type":1,"pageTitle":"Parametric Aggregate Functions","url":"docs/en/sql-reference/aggregate-functions/parametric-functions#summapfilteredkeys-to-keepkeys-values","content":"Same behavior as sumMap except that an array of keys is passed as a parameter. This can be especially useful when working with a high cardinality of keys. "},{"title":"sequenceNextNode​","type":1,"pageTitle":"Parametric Aggregate Functions","url":"docs/en/sql-reference/aggregate-functions/parametric-functions#sequenceNextNode","content":"Returns a value of the next event that matched an event chain. Experimental function, SET allow_experimental_funnel_functions = 1 to enable it. Syntax sequenceNextNode(direction, base)(timestamp, event_column, base_condition, event1, event2, event3, ...)  Parameters direction — Used to navigate to directions. forward — Moving forward.backward — Moving backward. base — Used to set the base point. head — Set the base point to the first event.tail — Set the base point to the last event.first_match — Set the base point to the first matched event1.last_match — Set the base point to the last matched event1. Arguments timestamp — Name of the column containing the timestamp. Data types supported: Date, DateTime and other unsigned integer types.event_column — Name of the column containing the value of the next event to be returned. Data types supported: String and Nullable(String).base_condition — Condition that the base point must fulfill.event1, event2, ... — Conditions describing the chain of events. UInt8. Returned values event_column[next_index] — If the pattern is matched and next value exists.NULL - If the pattern isn’t matched or next value doesn't exist. Type: Nullable(String). Example It can be used when events are A-&gt;B-&gt;C-&gt;D-&gt;E and you want to know the event following B-&gt;C, which is D. The query statement searching the event following A-&gt;B: CREATE TABLE test_flow ( dt DateTime, id int, page String) ENGINE = MergeTree() PARTITION BY toYYYYMMDD(dt) ORDER BY id; INSERT INTO test_flow VALUES (1, 1, 'A') (2, 1, 'B') (3, 1, 'C') (4, 1, 'D') (5, 1, 'E'); SELECT id, sequenceNextNode('forward', 'head')(dt, page, page = 'A', page = 'A', page = 'B') as next_flow FROM test_flow GROUP BY id;  Result: ┌─id─┬─next_flow─┐ │ 1 │ C │ └────┴───────────┘  Behavior for forward and head ALTER TABLE test_flow DELETE WHERE 1 = 1 settings mutations_sync = 1; INSERT INTO test_flow VALUES (1, 1, 'Home') (2, 1, 'Gift') (3, 1, 'Exit'); INSERT INTO test_flow VALUES (1, 2, 'Home') (2, 2, 'Home') (3, 2, 'Gift') (4, 2, 'Basket'); INSERT INTO test_flow VALUES (1, 3, 'Gift') (2, 3, 'Home') (3, 3, 'Gift') (4, 3, 'Basket');  SELECT id, sequenceNextNode('forward', 'head')(dt, page, page = 'Home', page = 'Home', page = 'Gift') FROM test_flow GROUP BY id; dt id page 1970-01-01 09:00:01 1 Home // Base point, Matched with Home 1970-01-01 09:00:02 1 Gift // Matched with Gift 1970-01-01 09:00:03 1 Exit // The result 1970-01-01 09:00:01 2 Home // Base point, Matched with Home 1970-01-01 09:00:02 2 Home // Unmatched with Gift 1970-01-01 09:00:03 2 Gift 1970-01-01 09:00:04 2 Basket 1970-01-01 09:00:01 3 Gift // Base point, Unmatched with Home 1970-01-01 09:00:02 3 Home 1970-01-01 09:00:03 3 Gift 1970-01-01 09:00:04 3 Basket  Behavior for backward and tail SELECT id, sequenceNextNode('backward', 'tail')(dt, page, page = 'Basket', page = 'Basket', page = 'Gift') FROM test_flow GROUP BY id; dt id page 1970-01-01 09:00:01 1 Home 1970-01-01 09:00:02 1 Gift 1970-01-01 09:00:03 1 Exit // Base point, Unmatched with Basket 1970-01-01 09:00:01 2 Home 1970-01-01 09:00:02 2 Home // The result 1970-01-01 09:00:03 2 Gift // Matched with Gift 1970-01-01 09:00:04 2 Basket // Base point, Matched with Basket 1970-01-01 09:00:01 3 Gift 1970-01-01 09:00:02 3 Home // The result 1970-01-01 09:00:03 3 Gift // Base point, Matched with Gift 1970-01-01 09:00:04 3 Basket // Base point, Matched with Basket  Behavior for forward and first_match SELECT id, sequenceNextNode('forward', 'first_match')(dt, page, page = 'Gift', page = 'Gift') FROM test_flow GROUP BY id; dt id page 1970-01-01 09:00:01 1 Home 1970-01-01 09:00:02 1 Gift // Base point 1970-01-01 09:00:03 1 Exit // The result 1970-01-01 09:00:01 2 Home 1970-01-01 09:00:02 2 Home 1970-01-01 09:00:03 2 Gift // Base point 1970-01-01 09:00:04 2 Basket The result 1970-01-01 09:00:01 3 Gift // Base point 1970-01-01 09:00:02 3 Home // The result 1970-01-01 09:00:03 3 Gift 1970-01-01 09:00:04 3 Basket  SELECT id, sequenceNextNode('forward', 'first_match')(dt, page, page = 'Gift', page = 'Gift', page = 'Home') FROM test_flow GROUP BY id; dt id page 1970-01-01 09:00:01 1 Home 1970-01-01 09:00:02 1 Gift // Base point 1970-01-01 09:00:03 1 Exit // Unmatched with Home 1970-01-01 09:00:01 2 Home 1970-01-01 09:00:02 2 Home 1970-01-01 09:00:03 2 Gift // Base point 1970-01-01 09:00:04 2 Basket // Unmatched with Home 1970-01-01 09:00:01 3 Gift // Base point 1970-01-01 09:00:02 3 Home // Matched with Home 1970-01-01 09:00:03 3 Gift // The result 1970-01-01 09:00:04 3 Basket  Behavior for backward and last_match SELECT id, sequenceNextNode('backward', 'last_match')(dt, page, page = 'Gift', page = 'Gift') FROM test_flow GROUP BY id; dt id page 1970-01-01 09:00:01 1 Home // The result 1970-01-01 09:00:02 1 Gift // Base point 1970-01-01 09:00:03 1 Exit 1970-01-01 09:00:01 2 Home 1970-01-01 09:00:02 2 Home // The result 1970-01-01 09:00:03 2 Gift // Base point 1970-01-01 09:00:04 2 Basket 1970-01-01 09:00:01 3 Gift 1970-01-01 09:00:02 3 Home // The result 1970-01-01 09:00:03 3 Gift // Base point 1970-01-01 09:00:04 3 Basket  SELECT id, sequenceNextNode('backward', 'last_match')(dt, page, page = 'Gift', page = 'Gift', page = 'Home') FROM test_flow GROUP BY id; dt id page 1970-01-01 09:00:01 1 Home // Matched with Home, the result is null 1970-01-01 09:00:02 1 Gift // Base point 1970-01-01 09:00:03 1 Exit 1970-01-01 09:00:01 2 Home // The result 1970-01-01 09:00:02 2 Home // Matched with Home 1970-01-01 09:00:03 2 Gift // Base point 1970-01-01 09:00:04 2 Basket 1970-01-01 09:00:01 3 Gift // The result 1970-01-01 09:00:02 3 Home // Matched with Home 1970-01-01 09:00:03 3 Gift // Base point 1970-01-01 09:00:04 3 Basket  Behavior for base_condition CREATE TABLE test_flow_basecond ( `dt` DateTime, `id` int, `page` String, `ref` String ) ENGINE = MergeTree PARTITION BY toYYYYMMDD(dt) ORDER BY id; INSERT INTO test_flow_basecond VALUES (1, 1, 'A', 'ref4') (2, 1, 'A', 'ref3') (3, 1, 'B', 'ref2') (4, 1, 'B', 'ref1');  SELECT id, sequenceNextNode('forward', 'head')(dt, page, ref = 'ref1', page = 'A') FROM test_flow_basecond GROUP BY id; dt id page ref 1970-01-01 09:00:01 1 A ref4 // The head can not be base point because the ref column of the head unmatched with 'ref1'. 1970-01-01 09:00:02 1 A ref3 1970-01-01 09:00:03 1 B ref2 1970-01-01 09:00:04 1 B ref1  SELECT id, sequenceNextNode('backward', 'tail')(dt, page, ref = 'ref4', page = 'B') FROM test_flow_basecond GROUP BY id; dt id page ref 1970-01-01 09:00:01 1 A ref4 1970-01-01 09:00:02 1 A ref3 1970-01-01 09:00:03 1 B ref2 1970-01-01 09:00:04 1 B ref1 // The tail can not be base point because the ref column of the tail unmatched with 'ref4'.  SELECT id, sequenceNextNode('forward', 'first_match')(dt, page, ref = 'ref3', page = 'A') FROM test_flow_basecond GROUP BY id; dt id page ref 1970-01-01 09:00:01 1 A ref4 // This row can not be base point because the ref column unmatched with 'ref3'. 1970-01-01 09:00:02 1 A ref3 // Base point 1970-01-01 09:00:03 1 B ref2 // The result 1970-01-01 09:00:04 1 B ref1  SELECT id, sequenceNextNode('backward', 'last_match')(dt, page, ref = 'ref2', page = 'B') FROM test_flow_basecond GROUP BY id; dt id page ref 1970-01-01 09:00:01 1 A ref4 1970-01-01 09:00:02 1 A ref3 // The result 1970-01-01 09:00:03 1 B ref2 // Base point 1970-01-01 09:00:04 1 B ref1 // This row can not be base point because the ref column unmatched with 'ref2'.  "},{"title":"deltaSum","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/deltasum","content":"","keywords":""},{"title":"See Also​","type":1,"pageTitle":"deltaSum","url":"docs/en/sql-reference/aggregate-functions/reference/deltasum#see-also","content":"runningDifference "},{"title":"groupArray","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/grouparray","content":"groupArray Syntax: groupArray(x) or groupArray(max_size)(x) Creates an array of argument values. Values can be added to the array in any (indeterminate) order. The second version (with the max_size parameter) limits the size of the resulting array to max_size elements. For example, groupArray(1)(x) is equivalent to [any (x)]. In some cases, you can still rely on the order of execution. This applies to cases when SELECT comes from a subquery that uses ORDER BY.","keywords":""},{"title":"groupArraySample","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/grouparraysample","content":"groupArraySample Creates an array of sample argument values. The size of the resulting array is limited to max_size elements. Argument values are selected and added to the array randomly. Syntax groupArraySample(max_size[, seed])(x) Arguments max_size — Maximum size of the resulting array. UInt64.seed — Seed for the random number generator. Optional. UInt64. Default value: 123456.x — Argument (column name or expression). Returned values Array of randomly selected x arguments. Type: Array. Examples Consider table colors: ┌─id─┬─color──┐ │ 1 │ red │ │ 2 │ blue │ │ 3 │ green │ │ 4 │ white │ │ 5 │ orange │ └────┴────────┘ Query with column name as argument: SELECT groupArraySample(3)(color) as newcolors FROM colors; Result: ┌─newcolors──────────────────┐ │ ['white','blue','green'] │ └────────────────────────────┘ Query with column name and different seed: SELECT groupArraySample(3, 987654321)(color) as newcolors FROM colors; Result: ┌─newcolors──────────────────┐ │ ['red','orange','green'] │ └────────────────────────────┘ Query with expression as argument: SELECT groupArraySample(3)(concat('light-', color)) as newcolors FROM colors; Result: ┌─newcolors───────────────────────────────────┐ │ ['light-blue','light-orange','light-green'] │ └─────────────────────────────────────────────┘ ","keywords":""},{"title":"groupBitmap","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/groupbitmap","content":"groupBitmap Bitmap or Aggregate calculations from a unsigned integer column, return cardinality of type UInt64, if add suffix -State, then return bitmap object. groupBitmap(expr) Arguments expr – An expression that results in UInt* type. Return value Value of the UInt64 type. Example Test data: UserID 1 1 2 3 Query: SELECT groupBitmap(UserID) as num FROM t Result: num 3 ","keywords":""},{"title":"groupUniqArray","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/groupuniqarray","content":"groupUniqArray Syntax: groupUniqArray(x) or groupUniqArray(max_size)(x) Creates an array from different argument values. Memory consumption is the same as for the uniqExact function. The second version (with the max_size parameter) limits the size of the resulting array to max_size elements. For example, groupUniqArray(1)(x) is equivalent to [any(x)].","keywords":""},{"title":"groupBitmapAnd","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/groupbitmapand","content":"groupBitmapAnd Calculations the AND of a bitmap column, return cardinality of type UInt64, if add suffix -State, then return bitmap object. groupBitmapAnd(expr) Arguments expr – An expression that results in AggregateFunction(groupBitmap, UInt*) type. Return value Value of the UInt64 type. Example DROP TABLE IF EXISTS bitmap_column_expr_test2; CREATE TABLE bitmap_column_expr_test2 ( tag_id String, z AggregateFunction(groupBitmap, UInt32) ) ENGINE = MergeTree ORDER BY tag_id; INSERT INTO bitmap_column_expr_test2 VALUES ('tag1', bitmapBuild(cast([1,2,3,4,5,6,7,8,9,10] as Array(UInt32)))); INSERT INTO bitmap_column_expr_test2 VALUES ('tag2', bitmapBuild(cast([6,7,8,9,10,11,12,13,14,15] as Array(UInt32)))); INSERT INTO bitmap_column_expr_test2 VALUES ('tag3', bitmapBuild(cast([2,4,6,8,10,12] as Array(UInt32)))); SELECT groupBitmapAnd(z) FROM bitmap_column_expr_test2 WHERE like(tag_id, 'tag%'); ┌─groupBitmapAnd(z)─┐ │ 3 │ └───────────────────┘ SELECT arraySort(bitmapToArray(groupBitmapAndState(z))) FROM bitmap_column_expr_test2 WHERE like(tag_id, 'tag%'); ┌─arraySort(bitmapToArray(groupBitmapAndState(z)))─┐ │ [6,8,10] │ └──────────────────────────────────────────────────┘ ","keywords":""},{"title":"groupBitAnd","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/groupbitand","content":"groupBitAnd Applies bitwise AND for series of numbers. groupBitAnd(expr) Arguments expr – An expression that results in UInt* type. Return value Value of the UInt* type. Example Test data: binary decimal 00101100 = 44 00011100 = 28 00001101 = 13 01010101 = 85 Query: SELECT groupBitAnd(num) FROM t Where num is the column with the test data. Result: binary decimal 00000100 = 4 ","keywords":""},{"title":"groupBitXor","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/groupbitxor","content":"groupBitXor Applies bitwise XOR for series of numbers. groupBitXor(expr) Arguments expr – An expression that results in UInt* type. Return value Value of the UInt* type. Example Test data: binary decimal 00101100 = 44 00011100 = 28 00001101 = 13 01010101 = 85 Query: SELECT groupBitXor(num) FROM t Where num is the column with the test data. Result: binary decimal 01101000 = 104 ","keywords":""},{"title":"groupBitOr","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/groupbitor","content":"groupBitOr Applies bitwise OR for series of numbers. groupBitOr(expr) Arguments expr – An expression that results in UInt* type. Returned value Value of the UInt* type. Example Test data: binary decimal 00101100 = 44 00011100 = 28 00001101 = 13 01010101 = 85 Query: SELECT groupBitOr(num) FROM t Where num is the column with the test data. Result: binary decimal 01111101 = 125 ","keywords":""},{"title":"groupArraySorted","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/grouparraysorted","content":"groupArraySorted Returns an array with the first N items in ascending order. groupArraySorted(N)(column) Arguments N – The number of elements to return. If the parameter is omitted, default value 10 is used. Arguments column – The value.expr — Optional. The field or expresion to sort by. If not set values are sorted by themselves. Example Gets the first 10 numbers: SELECT groupArraySorted(10)(number) FROM numbers(100) ┌─groupArraySorted(10)(number)─┐ │ [0,1,2,3,4,5,6,7,8,9] │ └──────────────────────────────┘ Or the last 10: SELECT groupArraySorted(10)(number, -number) FROM numbers(100) ┌─groupArraySorted(10)(number, negate(number))─┐ │ [99,98,97,96,95,94,93,92,91,90] │ └──────────────────────────────────────────────┘ ","keywords":""},{"title":"max","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/max","content":"max Aggregate function that calculates the maximum across a group of values. Example: SELECT max(salary) FROM employees; SELECT department, max(salary) FROM employees GROUP BY department; If you need non-aggregate function to choose a maximum of two values, see greatest: SELECT greatest(a, b) FROM table; ","keywords":""},{"title":"kurtPop","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/kurtpop","content":"kurtPop Computes the kurtosis of a sequence. kurtPop(expr) Arguments expr — Expression returning a number. Returned value The kurtosis of the given distribution. Type — Float64 Example SELECT kurtPop(value) FROM series_with_value_column; ","keywords":""},{"title":"groupBitmapXor","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/groupbitmapxor","content":"groupBitmapXor Calculations the XOR of a bitmap column, return cardinality of type UInt64, if add suffix -State, then return bitmap object. groupBitmapOr(expr) Arguments expr – An expression that results in AggregateFunction(groupBitmap, UInt*) type. Returned value Value of the UInt64 type. Example DROP TABLE IF EXISTS bitmap_column_expr_test2; CREATE TABLE bitmap_column_expr_test2 ( tag_id String, z AggregateFunction(groupBitmap, UInt32) ) ENGINE = MergeTree ORDER BY tag_id; INSERT INTO bitmap_column_expr_test2 VALUES ('tag1', bitmapBuild(cast([1,2,3,4,5,6,7,8,9,10] as Array(UInt32)))); INSERT INTO bitmap_column_expr_test2 VALUES ('tag2', bitmapBuild(cast([6,7,8,9,10,11,12,13,14,15] as Array(UInt32)))); INSERT INTO bitmap_column_expr_test2 VALUES ('tag3', bitmapBuild(cast([2,4,6,8,10,12] as Array(UInt32)))); SELECT groupBitmapXor(z) FROM bitmap_column_expr_test2 WHERE like(tag_id, 'tag%'); ┌─groupBitmapXor(z)─┐ │ 10 │ └───────────────────┘ SELECT arraySort(bitmapToArray(groupBitmapXorState(z))) FROM bitmap_column_expr_test2 WHERE like(tag_id, 'tag%'); ┌─arraySort(bitmapToArray(groupBitmapXorState(z)))─┐ │ [1,3,5,6,8,10,11,13,14,15] │ └──────────────────────────────────────────────────┘ ","keywords":""},{"title":"intervalLengthSum","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/intervalLengthSum","content":"intervalLengthSum Calculates the total length of union of all ranges (segments on numeric axis). Syntax intervalLengthSum(start, end) Arguments start — The starting value of the interval. Int32, Int64, UInt32, UInt64, Float32, Float64, DateTime or Date.end — The ending value of the interval. Int32, Int64, UInt32, UInt64, Float32, Float64, DateTime or Date. note Arguments must be of the same data type. Otherwise, an exception will be thrown. Returned value Total length of union of all ranges (segments on numeric axis). Depending on the type of the argument, the return value may be UInt64 or Float64 type. Examples Input table: ┌─id─┬─start─┬─end─┐ │ a │ 1.1 │ 2.9 │ │ a │ 2.5 │ 3.2 │ │ a │ 4 │ 5 │ └────┴───────┴─────┘ In this example, the arguments of the Float32 type are used. The function returns a value of the Float64 type. Result is the sum of lengths of intervals [1.1, 3.2] (union of [1.1, 2.9] and [2.5, 3.2]) and [4, 5] Query: SELECT id, intervalLengthSum(start, end), toTypeName(intervalLengthSum(start, end)) FROM fl_interval GROUP BY id ORDER BY id; Result: ┌─id─┬─intervalLengthSum(start, end)─┬─toTypeName(intervalLengthSum(start, end))─┐ │ a │ 3.1 │ Float64 │ └────┴───────────────────────────────┴───────────────────────────────────────────┘ Input table: ┌─id─┬───────────────start─┬─────────────────end─┐ │ a │ 2020-01-01 01:12:30 │ 2020-01-01 02:10:10 │ │ a │ 2020-01-01 02:05:30 │ 2020-01-01 02:50:31 │ │ a │ 2020-01-01 03:11:22 │ 2020-01-01 03:23:31 │ └────┴─────────────────────┴─────────────────────┘ In this example, the arguments of the DateTime type are used. The function returns a value in seconds. Query: SELECT id, intervalLengthSum(start, end), toTypeName(intervalLengthSum(start, end)) FROM dt_interval GROUP BY id ORDER BY id; Result: ┌─id─┬─intervalLengthSum(start, end)─┬─toTypeName(intervalLengthSum(start, end))─┐ │ a │ 6610 │ UInt64 │ └────┴───────────────────────────────┴───────────────────────────────────────────┘ Input table: ┌─id─┬──────start─┬────────end─┐ │ a │ 2020-01-01 │ 2020-01-04 │ │ a │ 2020-01-12 │ 2020-01-18 │ └────┴────────────┴────────────┘ In this example, the arguments of the Date type are used. The function returns a value in days. Query: SELECT id, intervalLengthSum(start, end), toTypeName(intervalLengthSum(start, end)) FROM date_interval GROUP BY id ORDER BY id; Result: ┌─id─┬─intervalLengthSum(start, end)─┬─toTypeName(intervalLengthSum(start, end))─┐ │ a │ 9 │ UInt64 │ └────┴───────────────────────────────┴───────────────────────────────────────────┘ ","keywords":""},{"title":"maxMap","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/maxmap","content":"maxMap Syntax: maxMap(key, value) or maxMap(Tuple(key, value)) Calculates the maximum from value array according to the keys specified in the key array. Passing a tuple of keys and value arrays is identical to passing two arrays of keys and values. The number of elements in key and value must be the same for each row that is totaled. Returns a tuple of two arrays: keys and values calculated for the corresponding keys. Example: SELECT maxMap(a, b) FROM values('a Array(Int32), b Array(Int64)', ([1, 2], [2, 2]), ([2, 3], [1, 1])) ┌─maxMap(a, b)──────┐ │ ([1,2,3],[2,2,1]) │ └───────────────────┘ ","keywords":""},{"title":"groupBitmapOr","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/groupbitmapor","content":"groupBitmapOr Calculations the OR of a bitmap column, return cardinality of type UInt64, if add suffix -State, then return bitmap object. This is equivalent to groupBitmapMerge. groupBitmapOr(expr) Arguments expr – An expression that results in AggregateFunction(groupBitmap, UInt*) type. Returned value Value of the UInt64 type. Example DROP TABLE IF EXISTS bitmap_column_expr_test2; CREATE TABLE bitmap_column_expr_test2 ( tag_id String, z AggregateFunction(groupBitmap, UInt32) ) ENGINE = MergeTree ORDER BY tag_id; INSERT INTO bitmap_column_expr_test2 VALUES ('tag1', bitmapBuild(cast([1,2,3,4,5,6,7,8,9,10] as Array(UInt32)))); INSERT INTO bitmap_column_expr_test2 VALUES ('tag2', bitmapBuild(cast([6,7,8,9,10,11,12,13,14,15] as Array(UInt32)))); INSERT INTO bitmap_column_expr_test2 VALUES ('tag3', bitmapBuild(cast([2,4,6,8,10,12] as Array(UInt32)))); SELECT groupBitmapOr(z) FROM bitmap_column_expr_test2 WHERE like(tag_id, 'tag%'); ┌─groupBitmapOr(z)─┐ │ 15 │ └──────────────────┘ SELECT arraySort(bitmapToArray(groupBitmapOrState(z))) FROM bitmap_column_expr_test2 WHERE like(tag_id, 'tag%'); ┌─arraySort(bitmapToArray(groupBitmapOrState(z)))─┐ │ [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15] │ └─────────────────────────────────────────────────┘ ","keywords":""},{"title":"mannWhitneyUTest","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/mannwhitneyutest","content":"mannWhitneyUTest Applies the Mann-Whitney rank test to samples from two populations. Syntax mannWhitneyUTest[(alternative[, continuity_correction])](sample_data, sample_index) Values of both samples are in the sample_data column. If sample_index equals to 0 then the value in that row belongs to the sample from the first population. Otherwise it belongs to the sample from the second population. The null hypothesis is that two populations are stochastically equal. Also one-sided hypothesises can be tested. This test does not assume that data have normal distribution. Arguments sample_data — sample data. Integer, Float or Decimal.sample_index — sample index. Integer. Parameters alternative — alternative hypothesis. (Optional, default: 'two-sided'.) String. 'two-sided';'greater';'less'. continuity_correction — if not 0 then continuity correction in the normal approximation for the p-value is applied. (Optional, default: 1.) UInt64. Returned values Tuple with two elements: calculated U-statistic. Float64.calculated p-value. Float64. Example Input table: ┌─sample_data─┬─sample_index─┐ │ 10 │ 0 │ │ 11 │ 0 │ │ 12 │ 0 │ │ 1 │ 1 │ │ 2 │ 1 │ │ 3 │ 1 │ └─────────────┴──────────────┘ Query: SELECT mannWhitneyUTest('greater')(sample_data, sample_index) FROM mww_ttest; Result: ┌─mannWhitneyUTest('greater')(sample_data, sample_index)─┐ │ (9,0.04042779918503192) │ └────────────────────────────────────────────────────────┘ See Also Mann–Whitney U testStochastic ordering Original article","keywords":""},{"title":"quantile","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/quantile","content":"quantile Computes an approximate quantile of a numeric data sequence. This function applies reservoir sampling with a reservoir size up to 8192 and a random number generator for sampling. The result is non-deterministic. To get an exact quantile, use the quantileExact function. When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function. Note that for an empty numeric sequence, quantile will return NaN, but its quantile* variants will return either NaN or a default value for the sequence type, depending on the variant. Syntax quantile(level)(expr) Alias: median. Arguments level — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr — Expression over the column values resulting in numeric data types, Date or DateTime. Returned value Approximate quantile of the specified level. Type: Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type. Example Input table: ┌─val─┐ │ 1 │ │ 1 │ │ 2 │ │ 3 │ └─────┘ Query: SELECT quantile(val) FROM t Result: ┌─quantile(val)─┐ │ 1.5 │ └───────────────┘ See Also medianquantiles","keywords":""},{"title":"kurtSamp","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/kurtsamp","content":"kurtSamp Computes the sample kurtosis of a sequence. It represents an unbiased estimate of the kurtosis of a random variable if passed values form its sample. kurtSamp(expr) Arguments expr — Expression returning a number. Returned value The kurtosis of the given distribution. Type — Float64. If n &lt;= 1 (n is a size of the sample), then the function returns nan. Example SELECT kurtSamp(value) FROM series_with_value_column; ","keywords":""},{"title":"meanZTest","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/meanztest","content":"meanZTest Applies mean z-test to samples from two populations. Syntax meanZTest(population_variance_x, population_variance_y, confidence_level)(sample_data, sample_index) Values of both samples are in the sample_data column. If sample_index equals to 0 then the value in that row belongs to the sample from the first population. Otherwise it belongs to the sample from the second population. The null hypothesis is that means of populations are equal. Normal distribution is assumed. Populations may have unequal variance and the variances are known. Arguments sample_data — Sample data. Integer, Float or Decimal.sample_index — Sample index. Integer. Parameters population_variance_x — Variance for population x. Float.population_variance_y — Variance for population y. Float.confidence_level — Confidence level in order to calculate confidence intervals. Float. Returned values Tuple with four elements: calculated t-statistic. Float64.calculated p-value. Float64.calculated confidence-interval-low. Float64.calculated confidence-interval-high. Float64. Example Input table: ┌─sample_data─┬─sample_index─┐ │ 20.3 │ 0 │ │ 21.9 │ 0 │ │ 22.1 │ 0 │ │ 18.9 │ 1 │ │ 19 │ 1 │ │ 20.3 │ 1 │ └─────────────┴──────────────┘ Query: SELECT meanZTest(0.7, 0.45, 0.95)(sample_data, sample_index) FROM mean_ztest Result: ┌─meanZTest(0.7, 0.45, 0.95)(sample_data, sample_index)────────────────────────────┐ │ (3.2841296025548123,0.0010229786769086013,0.8198428246768334,3.2468238419898365) │ └──────────────────────────────────────────────────────────────────────────────────┘ Original article","keywords":""},{"title":"quantileBFloat16","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/quantilebfloat16","content":"quantileBFloat16 Computes an approximate quantile of a sample consisting of bfloat16 numbers. bfloat16 is a floating-point data type with 1 sign bit, 8 exponent bits and 7 fraction bits. The function converts input values to 32-bit floats and takes the most significant 16 bits. Then it calculates bfloat16 quantile value and converts the result to a 64-bit float by appending zero bits. The function is a fast quantile estimator with a relative error no more than 0.390625%. Syntax quantileBFloat16[(level)](expr) Alias: medianBFloat16 Arguments expr — Column with numeric data. Integer, Float. Parameters level — Level of quantile. Optional. Possible values are in the range from 0 to 1. Default value: 0.5. Float. Returned value Approximate quantile of the specified level. Type: Float64. Example Input table has an integer and a float columns: ┌─a─┬─────b─┐ │ 1 │ 1.001 │ │ 2 │ 1.002 │ │ 3 │ 1.003 │ │ 4 │ 1.004 │ └───┴───────┘ Query to calculate 0.75-quantile (third quartile): SELECT quantileBFloat16(0.75)(a), quantileBFloat16(0.75)(b) FROM example_table; Result: ┌─quantileBFloat16(0.75)(a)─┬─quantileBFloat16(0.75)(b)─┐ │ 3 │ 1 │ └───────────────────────────┴───────────────────────────┘ Note that all floating point values in the example are truncated to 1.0 when converting to bfloat16. quantileBFloat16Weighted Like quantileBFloat16 but takes into account the weight of each sequence member. See Also medianquantiles","keywords":""},{"title":"min","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/min","content":"","keywords":""},{"title":"min​","type":1,"pageTitle":"min","url":"docs/en/sql-reference/aggregate-functions/reference/min#agg_function-min","content":"Aggregate function that calculates the minimum across a group of values. Example: SELECT min(salary) FROM employees;  SELECT department, min(salary) FROM employees GROUP BY department;  If you need non-aggregate function to choose a minimum of two values, see least: SELECT least(a, b) FROM table;  "},{"title":"median","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/median","content":"median The median* functions are the aliases for the corresponding quantile* functions. They calculate median of a numeric data sample. Functions: median — Alias for quantile.medianDeterministic — Alias for quantileDeterministic.medianExact — Alias for quantileExact.medianExactWeighted — Alias for quantileExactWeighted.medianTiming — Alias for quantileTiming.medianTimingWeighted — Alias for quantileTimingWeighted.medianTDigest — Alias for quantileTDigest.medianTDigestWeighted — Alias for quantileTDigestWeighted.medianBFloat16 — Alias for quantileBFloat16. Example Input table: ┌─val─┐ │ 1 │ │ 1 │ │ 2 │ │ 3 │ └─────┘ Query: SELECT medianDeterministic(val, 1) FROM t; Result: ┌─medianDeterministic(val, 1)─┐ │ 1.5 │ └─────────────────────────────┘ ","keywords":""},{"title":"quantileExactWeighted","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/quantileexactweighted","content":"quantileExactWeighted Exactly computes the quantile of a numeric data sequence, taking into account the weight of each element. To get exact value, all the passed values ​​are combined into an array, which is then partially sorted. Each value is counted with its weight, as if it is present weight times. A hash table is used in the algorithm. Because of this, if the passed values ​​are frequently repeated, the function consumes less RAM than quantileExact. You can use this function instead of quantileExact and specify the weight 1. When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function. Syntax quantileExactWeighted(level)(expr, weight) Alias: medianExactWeighted. Arguments level — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr — Expression over the column values resulting in numeric data types, Date or DateTime.weight — Column with weights of sequence members. Weight is a number of value occurrences. Returned value Quantile of the specified level. Type: Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type. Example Input table: ┌─n─┬─val─┐ │ 0 │ 3 │ │ 1 │ 2 │ │ 2 │ 1 │ │ 5 │ 4 │ └───┴─────┘ Query: SELECT quantileExactWeighted(n, val) FROM t Result: ┌─quantileExactWeighted(n, val)─┐ │ 1 │ └───────────────────────────────┘ See Also medianquantiles","keywords":""},{"title":"quantiles Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/quantiles","content":"","keywords":""},{"title":"quantiles​","type":1,"pageTitle":"quantiles Functions","url":"docs/en/sql-reference/aggregate-functions/reference/quantiles#quantiles","content":"Syntax: quantiles(level1, level2, …)(x) All the quantile functions also have corresponding quantiles functions: quantiles, quantilesDeterministic, quantilesTiming, quantilesTimingWeighted, quantilesExact, quantilesExactWeighted, quantilesTDigest, quantilesBFloat16. These functions calculate all the quantiles of the listed levels in one pass, and return an array of the resulting values. "},{"title":"quantilesExactExclusive​","type":1,"pageTitle":"quantiles Functions","url":"docs/en/sql-reference/aggregate-functions/reference/quantiles#quantilesexactexclusive","content":"Exactly computes the quantiles of a numeric data sequence. To get exact value, all the passed values ​​are combined into an array, which is then partially sorted. Therefore, the function consumes O(n) memory, where n is a number of values that were passed. However, for a small number of values, the function is very effective. This function is equivalent to PERCENTILE.EXC Excel function, (type R6). Works more efficiently with sets of levels than quantileExactExclusive. Syntax quantilesExactExclusive(level1, level2, ...)(expr)  Arguments expr — Expression over the column values resulting in numeric data types, Date or DateTime. Parameters level — Levels of quantiles. Possible values: (0, 1) — bounds not included. Float. Returned value Array of quantiles of the specified levels. Type of array values: Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type. Example Query: CREATE TABLE num AS numbers(1000); SELECT quantilesExactExclusive(0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 0.999)(x) FROM (SELECT number AS x FROM num);  Result: ┌─quantilesExactExclusive(0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 0.999)(x)─┐ │ [249.25,499.5,749.75,899.9,949.9499999999999,989.99,998.999] │ └─────────────────────────────────────────────────────────────────────┘  "},{"title":"quantilesExactInclusive​","type":1,"pageTitle":"quantiles Functions","url":"docs/en/sql-reference/aggregate-functions/reference/quantiles#quantilesexactinclusive","content":"Exactly computes the quantiles of a numeric data sequence. To get exact value, all the passed values ​​are combined into an array, which is then partially sorted. Therefore, the function consumes O(n) memory, where n is a number of values that were passed. However, for a small number of values, the function is very effective. This function is equivalent to PERCENTILE.INC Excel function, (type R7). Works more efficiently with sets of levels than quantileExactInclusive. Syntax quantilesExactInclusive(level1, level2, ...)(expr)  Arguments expr — Expression over the column values resulting in numeric data types, Date or DateTime. Parameters level — Levels of quantiles. Possible values: [0, 1] — bounds included. Float. Returned value Array of quantiles of the specified levels. Type of array values: Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type. Example Query: CREATE TABLE num AS numbers(1000); SELECT quantilesExactInclusive(0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 0.999)(x) FROM (SELECT number AS x FROM num);  Result: ┌─quantilesExactInclusive(0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 0.999)(x)─┐ │ [249.75,499.5,749.25,899.1,949.05,989.01,998.001] │ └─────────────────────────────────────────────────────────────────────┘  "},{"title":"minMap","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/minmap","content":"minMap Syntax: minMap(key, value) or minMap(Tuple(key, value)) Calculates the minimum from value array according to the keys specified in the key array. Passing a tuple of keys and value ​​arrays is identical to passing two arrays of keys and values. The number of elements in key and value must be the same for each row that is totaled. Returns a tuple of two arrays: keys in sorted order, and values calculated for the corresponding keys. Example: SELECT minMap(a, b) FROM values('a Array(Int32), b Array(Int64)', ([1, 2], [2, 2]), ([2, 3], [1, 1])) ┌─minMap(a, b)──────┐ │ ([1,2,3],[2,1,1]) │ └───────────────────┘ ","keywords":""},{"title":"quantileDeterministic","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/quantiledeterministic","content":"quantileDeterministic Computes an approximate quantile of a numeric data sequence. This function applies reservoir sampling with a reservoir size up to 8192 and deterministic algorithm of sampling. The result is deterministic. To get an exact quantile, use the quantileExact function. When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function. Syntax quantileDeterministic(level)(expr, determinator) Alias: medianDeterministic. Arguments level — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr — Expression over the column values resulting in numeric data types, Date or DateTime.determinator — Number whose hash is used instead of a random number generator in the reservoir sampling algorithm to make the result of sampling deterministic. As a determinator you can use any deterministic positive number, for example, a user id or an event id. If the same determinator value occures too often, the function works incorrectly. Returned value Approximate quantile of the specified level. Type: Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type. Example Input table: ┌─val─┐ │ 1 │ │ 1 │ │ 2 │ │ 3 │ └─────┘ Query: SELECT quantileDeterministic(val, 1) FROM t Result: ┌─quantileDeterministic(val, 1)─┐ │ 1.5 │ └───────────────────────────────┘ See Also medianquantiles","keywords":""},{"title":"rankCorr","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/rankCorr","content":"rankCorr Computes a rank correlation coefficient. Syntax rankCorr(x, y) Arguments x — Arbitrary value. Float32 or Float64.y — Arbitrary value. Float32 or Float64. Returned value(s) Returns a rank correlation coefficient of the ranks of x and y. The value of the correlation coefficient ranges from -1 to +1. If less than two arguments are passed, the function will return an exception. The value close to +1 denotes a high linear relationship, and with an increase of one random variable, the second random variable also increases. The value close to -1 denotes a high linear relationship, and with an increase of one random variable, the second random variable decreases. The value close or equal to 0 denotes no relationship between the two random variables. Type: Float64. Example Query: SELECT rankCorr(number, number) FROM numbers(100); Result: ┌─rankCorr(number, number)─┐ │ 1 │ └──────────────────────────┘ Query: SELECT roundBankers(rankCorr(exp(number), sin(number)), 3) FROM numbers(100); Result: ┌─roundBankers(rankCorr(exp(number), sin(number)), 3)─┐ │ -0.037 │ └─────────────────────────────────────────────────────┘ See Also Spearman's rank correlation coefficient","keywords":""},{"title":"quantileTDigest","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/quantiletdigest","content":"quantileTDigest Computes an approximate quantile of a numeric data sequence using the t-digest algorithm. Memory consumption is log(n), where n is a number of values. The result depends on the order of running the query, and is nondeterministic. The performance of the function is lower than performance of quantile or quantileTiming. In terms of the ratio of State size to precision, this function is much better than quantile. When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function. Syntax quantileTDigest(level)(expr) Alias: medianTDigest. Arguments level — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr — Expression over the column values resulting in numeric data types, Date or DateTime. Returned value Approximate quantile of the specified level. Type: Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type. Example Query: SELECT quantileTDigest(number) FROM numbers(10) Result: ┌─quantileTDigest(number)─┐ │ 4.5 │ └─────────────────────────┘ See Also medianquantiles","keywords":""},{"title":"skewSamp","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/skewsamp","content":"skewSamp Computes the sample skewness of a sequence. It represents an unbiased estimate of the skewness of a random variable if passed values form its sample. skewSamp(expr) Arguments expr — Expression returning a number. Returned value The skewness of the given distribution. Type — Float64. If n &lt;= 1 (n is the size of the sample), then the function returns nan. Example SELECT skewSamp(value) FROM series_with_value_column; ","keywords":""},{"title":"simpleLinearRegression","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/simplelinearregression","content":"simpleLinearRegression Performs simple (unidimensional) linear regression. simpleLinearRegression(x, y) Parameters: x — Column with dependent variable values.y — Column with explanatory variable values. Returned values: Constants (a, b) of the resulting line y = a*x + b. Examples SELECT arrayReduce('simpleLinearRegression', [0, 1, 2, 3], [0, 1, 2, 3]) ┌─arrayReduce('simpleLinearRegression', [0, 1, 2, 3], [0, 1, 2, 3])─┐ │ (1,0) │ └───────────────────────────────────────────────────────────────────┘ SELECT arrayReduce('simpleLinearRegression', [0, 1, 2, 3], [3, 4, 5, 6]) ┌─arrayReduce('simpleLinearRegression', [0, 1, 2, 3], [3, 4, 5, 6])─┐ │ (1,3) │ └───────────────────────────────────────────────────────────────────┘ ","keywords":""},{"title":"stddevPop","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/stddevpop","content":"stddevPop The result is equal to the square root of varPop. note This function uses a numerically unstable algorithm. If you need numerical stability in calculations, use the stddevPopStable function. It works slower but provides a lower computational error.","keywords":""},{"title":"sparkbar","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/sparkbar","content":"sparkbar sparkbar The function plots a frequency histogram for values x and the repetition rate y of these values over the interval [min_x, max_x]. If no interval is specified, then the minimum x is used as the interval start, and the maximum x — as the interval end. Syntax sparkbar(width[, min_x, max_x])(x, y) Parameters width — The number of segments. Type: Integer.min_x — The interval start. Optional parameter.max_x — The interval end. Optional parameter. Arguments x — The field with values.y — The field with the frequency of values. Returned value The frequency histogram. Example Query: CREATE TABLE spark_bar_data (`cnt` UInt64,`event_date` Date) ENGINE = MergeTree ORDER BY event_date SETTINGS index_granularity = 8192; INSERT INTO spark_bar_data VALUES(1,'2020-01-01'),(4,'2020-01-02'),(5,'2020-01-03'),(2,'2020-01-04'),(3,'2020-01-05'),(7,'2020-01-06'),(6,'2020-01-07'),(8,'2020-01-08'),(2,'2020-01-11'); SELECT sparkbar(9)(event_date,cnt) FROM spark_bar_data; SELECT sparkbar(9,toDate('2020-01-01'),toDate('2020-01-10'))(event_date,cnt) FROM spark_bar_data; Result: ┌─sparkbar(9)(event_date, cnt)─┐ │ │ │ ▁▅▄▃██▅ ▁ │ │ │ └──────────────────────────────┘ ┌─sparkbar(9, toDate('2020-01-01'), toDate('2020-01-10'))(event_date, cnt)─┐ │ │ │▁▄▄▂▅▇█▁ │ │ │ └──────────────────────────────────────────────────────────────────────────┘ ","keywords":""},{"title":"skewPop","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/skewpop","content":"skewPop Computes the skewness of a sequence. skewPop(expr) Arguments expr — Expression returning a number. Returned value The skewness of the given distribution. Type — Float64 Example SELECT skewPop(value) FROM series_with_value_column; ","keywords":""},{"title":"quantileTimingWeighted","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/quantiletimingweighted","content":"quantileTimingWeighted With the determined precision computes the quantile of a numeric data sequence according to the weight of each sequence member. The result is deterministic (it does not depend on the query processing order). The function is optimized for working with sequences which describe distributions like loading web pages times or backend response times. When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function. Syntax quantileTimingWeighted(level)(expr, weight) Alias: medianTimingWeighted. Arguments level — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median. expr — Expression over a column values returning a Float*-type number. - If negative values are passed to the function, the behavior is undefined. - If the value is greater than 30,000 (a page loading time of more than 30 seconds), it is assumed to be 30,000. weight — Column with weights of sequence elements. Weight is a number of value occurrences. Accuracy The calculation is accurate if: Total number of values does not exceed 5670.Total number of values exceeds 5670, but the page loading time is less than 1024ms. Otherwise, the result of the calculation is rounded to the nearest multiple of 16 ms. note For calculating page loading time quantiles, this function is more effective and accurate than quantile. Returned value Quantile of the specified level. Type: Float32. note If no values are passed to the function (when using quantileTimingIf), NaN is returned. The purpose of this is to differentiate these cases from cases that result in zero. See ORDER BY clause for notes on sorting NaN values. Example Input table: ┌─response_time─┬─weight─┐ │ 68 │ 1 │ │ 104 │ 2 │ │ 112 │ 3 │ │ 126 │ 2 │ │ 138 │ 1 │ │ 162 │ 1 │ └───────────────┴────────┘ Query: SELECT quantileTimingWeighted(response_time, weight) FROM t Result: ┌─quantileTimingWeighted(response_time, weight)─┐ │ 112 │ └───────────────────────────────────────────────┘ quantilesTimingWeighted Same as quantileTimingWeighted, but accept multiple parameters with quantile levels and return an Array filled with many values of that quantiles. Example Input table: ┌─response_time─┬─weight─┐ │ 68 │ 1 │ │ 104 │ 2 │ │ 112 │ 3 │ │ 126 │ 2 │ │ 138 │ 1 │ │ 162 │ 1 │ └───────────────┴────────┘ Query: SELECT quantilesTimingWeighted(0,5, 0.99)(response_time, weight) FROM t Result: ┌─quantilesTimingWeighted(0.5, 0.99)(response_time, weight)─┐ │ [112,162] │ └───────────────────────────────────────────────────────────┘ See Also medianquantiles","keywords":""},{"title":"stochasticLinearRegression","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/stochasticlinearregression","content":"","keywords":""},{"title":"Parameters​","type":1,"pageTitle":"stochasticLinearRegression","url":"docs/en/sql-reference/aggregate-functions/reference/stochasticlinearregression#agg_functions-stochasticlinearregression-parameters","content":"There are 4 customizable parameters. They are passed to the function sequentially, but there is no need to pass all four - default values will be used, however good model required some parameter tuning. stochasticLinearRegression(1.0, 1.0, 10, 'SGD')  learning rate is the coefficient on step length, when gradient descent step is performed. Too big learning rate may cause infinite weights of the model. Default is 0.00001.l2 regularization coefficient which may help to prevent overfitting. Default is 0.1.mini-batch size sets the number of elements, which gradients will be computed and summed to perform one step of gradient descent. Pure stochastic descent uses one element, however having small batches(about 10 elements) make gradient steps more stable. Default is 15.method for updating weights, they are: Adam (by default), SGD, Momentum, Nesterov. Momentum and Nesterov require little bit more computations and memory, however they happen to be useful in terms of speed of convergance and stability of stochastic gradient methods. "},{"title":"Usage​","type":1,"pageTitle":"stochasticLinearRegression","url":"docs/en/sql-reference/aggregate-functions/reference/stochasticlinearregression#agg_functions-stochasticlinearregression-usage","content":"stochasticLinearRegression is used in two steps: fitting the model and predicting on new data. In order to fit the model and save its state for later usage we use -State combinator, which basically saves the state (model weights, etc). To predict we use function evalMLMethod, which takes a state as an argument as well as features to predict on.  1. Fitting Such query may be used. CREATE TABLE IF NOT EXISTS train_data ( param1 Float64, param2 Float64, target Float64 ) ENGINE = Memory; CREATE TABLE your_model ENGINE = Memory AS SELECT stochasticLinearRegressionState(0.1, 0.0, 5, 'SGD')(target, param1, param2) AS state FROM train_data;  Here we also need to insert data into train_data table. The number of parameters is not fixed, it depends only on number of arguments, passed into linearRegressionState. They all must be numeric values. Note that the column with target value(which we would like to learn to predict) is inserted as the first argument. 2. Predicting After saving a state into the table, we may use it multiple times for prediction, or even merge with other states and create new even better models. WITH (SELECT state FROM your_model) AS model SELECT evalMLMethod(model, param1, param2) FROM test_data  The query will return a column of predicted values. Note that first argument of evalMLMethod is AggregateFunctionState object, next are columns of features. test_data is a table like train_data but may not contain target value. "},{"title":"Notes​","type":1,"pageTitle":"stochasticLinearRegression","url":"docs/en/sql-reference/aggregate-functions/reference/stochasticlinearregression#agg_functions-stochasticlinearregression-notes","content":"To merge two models user may create such query:sql SELECT state1 + state2 FROM your_modelswhere your_models table contains both models. This query will return new AggregateFunctionState object. User may fetch weights of the created model for its own purposes without saving the model if no -State combinator is used.sql SELECT stochasticLinearRegression(0.01)(target, param1, param2) FROM train_dataSuch query will fit the model and return its weights - first are weights, which correspond to the parameters of the model, the last one is bias. So in the example above the query will return a column with 3 values. See Also stochasticLogisticRegressionDifference between linear and logistic regressions "},{"title":"studentTTest","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/studentttest","content":"studentTTest Applies Student's t-test to samples from two populations. Syntax studentTTest([confidence_level])(sample_data, sample_index) Values of both samples are in the sample_data column. If sample_index equals to 0 then the value in that row belongs to the sample from the first population. Otherwise it belongs to the sample from the second population. The null hypothesis is that means of populations are equal. Normal distribution with equal variances is assumed. Arguments sample_data — Sample data. Integer, Float or Decimal.sample_index — Sample index. Integer. Parameters confidence_level — Confidence level in order to calculate confidence intervals. Float. Returned values Tuple with two or four elements (if the optional confidence_level is specified): calculated t-statistic. Float64.calculated p-value. Float64.[calculated confidence-interval-low.][Float64](/docs/testing/docs/en/sql-reference/data-types/float).[calculated confidence-interval-high.][Float64](/docs/testing/docs/en/sql-reference/data-types/float). Example Input table: ┌─sample_data─┬─sample_index─┐ │ 20.3 │ 0 │ │ 21.1 │ 0 │ │ 21.9 │ 1 │ │ 21.7 │ 0 │ │ 19.9 │ 1 │ │ 21.8 │ 1 │ └─────────────┴──────────────┘ Query: SELECT studentTTest(sample_data, sample_index) FROM student_ttest; Result: ┌─studentTTest(sample_data, sample_index)───┐ │ (-0.21739130434783777,0.8385421208415731) │ └───────────────────────────────────────────┘ See Also Student's t-testwelchTTest function Original article","keywords":""},{"title":"stochasticLogisticRegression","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/stochasticlogisticregression","content":"","keywords":""},{"title":"Parameters​","type":1,"pageTitle":"stochasticLogisticRegression","url":"docs/en/sql-reference/aggregate-functions/reference/stochasticlogisticregression#agg_functions-stochasticlogisticregression-parameters","content":"Parameters are exactly the same as in stochasticLinearRegression:learning rate, l2 regularization coefficient, mini-batch size, method for updating weights. For more information see parameters. stochasticLogisticRegression(1.0, 1.0, 10, 'SGD')  1. Fitting See the `Fitting` section in the [stochasticLinearRegression](#stochasticlinearregression-usage-fitting) description. Predicted labels have to be in \\[-1, 1\\].  2. Predicting Using saved state we can predict probability of object having label `1`. ``` sql WITH (SELECT state FROM your_model) AS model SELECT evalMLMethod(model, param1, param2) FROM test_data ``` The query will return a column of probabilities. Note that first argument of `evalMLMethod` is `AggregateFunctionState` object, next are columns of features. We can also set a bound of probability, which assigns elements to different labels. ``` sql SELECT ans &lt; 1.1 AND ans &gt; 0.5 FROM (WITH (SELECT state FROM your_model) AS model SELECT evalMLMethod(model, param1, param2) AS ans FROM test_data) ``` Then the result will be labels. `test_data` is a table like `train_data` but may not contain target value.  See Also stochasticLinearRegressionDifference between linear and logistic regressions. "},{"title":"sumKahan","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/sumkahan","content":"sumKahan Calculates the sum of the numbers with Kahan compensated summation algorithmSlower than sum function. The compensation works only for Float types. Syntax sumKahan(x) Arguments x — Input value, must be Integer, Float, or Decimal. Returned value the sum of numbers, with type Integer, Float, or Decimal depends on type of input arguments Example Query: SELECT sum(0.1), sumKahan(0.1) FROM numbers(10); Result: ┌───────────sum(0.1)─┬─sumKahan(0.1)─┐ │ 0.9999999999999999 │ 1 │ └────────────────────┴───────────────┘ ","keywords":""},{"title":"quantileTDigestWeighted","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/quantiletdigestweighted","content":"quantileTDigestWeighted Computes an approximate quantile of a numeric data sequence using the t-digest algorithm. The function takes into account the weight of each sequence member. The maximum error is 1%. Memory consumption is log(n), where n is a number of values. The performance of the function is lower than performance of quantile or quantileTiming. In terms of the ratio of State size to precision, this function is much better than quantile. The result depends on the order of running the query, and is nondeterministic. When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function. note Using quantileTDigestWeighted is not recommended for tiny data sets and can lead to significat error. In this case, consider possibility of using quantileTDigest instead. Syntax quantileTDigestWeighted(level)(expr, weight) Alias: medianTDigestWeighted. Arguments level — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr — Expression over the column values resulting in numeric data types, Date or DateTime.weight — Column with weights of sequence elements. Weight is a number of value occurrences. Returned value Approximate quantile of the specified level. Type: Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type. Example Query: SELECT quantileTDigestWeighted(number, 1) FROM numbers(10) Result: ┌─quantileTDigestWeighted(number, 1)─┐ │ 4.5 │ └────────────────────────────────────┘ See Also medianquantiles","keywords":""},{"title":"sumWithOverflow","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/sumwithoverflow","content":"sumWithOverflow Computes the sum of the numbers, using the same data type for the result as for the input parameters. If the sum exceeds the maximum value for this data type, it is calculated with overflow. Only works for numbers.","keywords":""},{"title":"sum","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/sum","content":"sum Calculates the sum. Only works for numbers.","keywords":""},{"title":"quantileExact Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/quantileexact","content":"","keywords":""},{"title":"quantileExact​","type":1,"pageTitle":"quantileExact Functions","url":"docs/en/sql-reference/aggregate-functions/reference/quantileexact#quantileexact","content":"Exactly computes the quantile of a numeric data sequence. To get exact value, all the passed values ​​are combined into an array, which is then partially sorted. Therefore, the function consumes O(n) memory, where n is a number of values that were passed. However, for a small number of values, the function is very effective. When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function. Syntax quantileExact(level)(expr)  Alias: medianExact. Arguments level — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr — Expression over the column values resulting in numeric data types, Date or DateTime. Returned value Quantile of the specified level. Type: Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type. Example Query: SELECT quantileExact(number) FROM numbers(10)  Result: ┌─quantileExact(number)─┐ │ 5 │ └───────────────────────┘  "},{"title":"quantileExactLow​","type":1,"pageTitle":"quantileExact Functions","url":"docs/en/sql-reference/aggregate-functions/reference/quantileexact#quantileexactlow","content":"Similar to quantileExact, this computes the exact quantile of a numeric data sequence. To get the exact value, all the passed values are combined into an array, which is then fully sorted. The sorting algorithm's complexity is O(N·log(N)), where N = std::distance(first, last) comparisons. The return value depends on the quantile level and the number of elements in the selection, i.e. if the level is 0.5, then the function returns the lower median value for an even number of elements and the middle median value for an odd number of elements. Median is calculated similarly to the median_low implementation which is used in python. For all other levels, the element at the index corresponding to the value of level * size_of_array is returned. For example: SELECT quantileExactLow(0.1)(number) FROM numbers(10) ┌─quantileExactLow(0.1)(number)─┐ │ 1 │ └───────────────────────────────┘  When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function. Syntax quantileExactLow(level)(expr)  Alias: medianExactLow. Arguments level — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr — Expression over the column values resulting in numeric data types, Date or DateTime. Returned value Quantile of the specified level. Type: Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type. Example Query: SELECT quantileExactLow(number) FROM numbers(10)  Result: ┌─quantileExactLow(number)─┐ │ 4 │ └──────────────────────────┘  "},{"title":"quantileExactHigh​","type":1,"pageTitle":"quantileExact Functions","url":"docs/en/sql-reference/aggregate-functions/reference/quantileexact#quantileexacthigh","content":"Similar to quantileExact, this computes the exact quantile of a numeric data sequence. All the passed values are combined into an array, which is then fully sorted, to get the exact value. The sorting algorithm's complexity is O(N·log(N)), where N = std::distance(first, last) comparisons. The return value depends on the quantile level and the number of elements in the selection, i.e. if the level is 0.5, then the function returns the higher median value for an even number of elements and the middle median value for an odd number of elements. Median is calculated similarly to the median_high implementation which is used in python. For all other levels, the element at the index corresponding to the value of level * size_of_array is returned. This implementation behaves exactly similar to the current quantileExact implementation. When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function. Syntax quantileExactHigh(level)(expr)  Alias: medianExactHigh. Arguments level — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr — Expression over the column values resulting in numeric data types, Date or DateTime. Returned value Quantile of the specified level. Type: Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type. Example Query: SELECT quantileExactHigh(number) FROM numbers(10)  Result: ┌─quantileExactHigh(number)─┐ │ 5 │ └───────────────────────────┘  "},{"title":"quantileExactExclusive​","type":1,"pageTitle":"quantileExact Functions","url":"docs/en/sql-reference/aggregate-functions/reference/quantileexact#quantileexactexclusive","content":"Exactly computes the quantile of a numeric data sequence. To get exact value, all the passed values ​​are combined into an array, which is then partially sorted. Therefore, the function consumes O(n) memory, where n is a number of values that were passed. However, for a small number of values, the function is very effective. This function is equivalent to PERCENTILE.EXC Excel function, (type R6). When using multiple quantileExactExclusive functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantilesExactExclusive function. Syntax quantileExactExclusive(level)(expr)  Arguments expr — Expression over the column values resulting in numeric data types, Date or DateTime. Parameters level — Level of quantile. Optional. Possible values: (0, 1) — bounds not included. Default value: 0.5. At level=0.5 the function calculates median. Float. Returned value Quantile of the specified level. Type: Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type. Example Query: CREATE TABLE num AS numbers(1000); SELECT quantileExactExclusive(0.6)(x) FROM (SELECT number AS x FROM num);  Result: ┌─quantileExactExclusive(0.6)(x)─┐ │ 599.6 │ └────────────────────────────────┘  "},{"title":"quantileExactInclusive​","type":1,"pageTitle":"quantileExact Functions","url":"docs/en/sql-reference/aggregate-functions/reference/quantileexact#quantileexactinclusive","content":"Exactly computes the quantile of a numeric data sequence. To get exact value, all the passed values ​​are combined into an array, which is then partially sorted. Therefore, the function consumes O(n) memory, where n is a number of values that were passed. However, for a small number of values, the function is very effective. This function is equivalent to PERCENTILE.INC Excel function, (type R7). When using multiple quantileExactInclusive functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantilesExactInclusive function. Syntax quantileExactInclusive(level)(expr)  Arguments expr — Expression over the column values resulting in numeric data types, Date or DateTime. Parameters level — Level of quantile. Optional. Possible values: [0, 1] — bounds included. Default value: 0.5. At level=0.5 the function calculates median. Float. Returned value Quantile of the specified level. Type: Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type. Example Query: CREATE TABLE num AS numbers(1000); SELECT quantileExactInclusive(0.6)(x) FROM (SELECT number AS x FROM num);  Result: ┌─quantileExactInclusive(0.6)(x)─┐ │ 599.4 │ └────────────────────────────────┘  See Also medianquantiles "},{"title":"quantileTiming","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/quantiletiming","content":"quantileTiming With the determined precision computes the quantile of a numeric data sequence. The result is deterministic (it does not depend on the query processing order). The function is optimized for working with sequences which describe distributions like loading web pages times or backend response times. When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function. Syntax quantileTiming(level)(expr) Alias: medianTiming. Arguments level — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median. expr — Expression over a column values returning a Float*-type number. If negative values are passed to the function, the behavior is undefined.If the value is greater than 30,000 (a page loading time of more than 30 seconds), it is assumed to be 30,000. Accuracy The calculation is accurate if: Total number of values does not exceed 5670.Total number of values exceeds 5670, but the page loading time is less than 1024ms. Otherwise, the result of the calculation is rounded to the nearest multiple of 16 ms. note For calculating page loading time quantiles, this function is more effective and accurate than quantile. Returned value Quantile of the specified level. Type: Float32. note If no values are passed to the function (when using quantileTimingIf), NaN is returned. The purpose of this is to differentiate these cases from cases that result in zero. See ORDER BY clause for notes on sorting NaN values. Example Input table: ┌─response_time─┐ │ 72 │ │ 112 │ │ 126 │ │ 145 │ │ 104 │ │ 242 │ │ 313 │ │ 168 │ │ 108 │ └───────────────┘ Query: SELECT quantileTiming(response_time) FROM t Result: ┌─quantileTiming(response_time)─┐ │ 126 │ └───────────────────────────────┘ See Also medianquantiles","keywords":""},{"title":"stddevSamp","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/stddevsamp","content":"stddevSamp The result is equal to the square root of varSamp. note This function uses a numerically unstable algorithm. If you need numerical stability in calculations, use the stddevSampStable function. It works slower but provides a lower computational error.","keywords":""},{"title":"uniq","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/uniq","content":"uniq Calculates the approximate number of different values of the argument. uniq(x[, ...]) Arguments The function takes a variable number of parameters. Parameters can be Tuple, Array, Date, DateTime, String, or numeric types. Returned value A UInt64-type number. Implementation details Function: Calculates a hash for all parameters in the aggregate, then uses it in calculations. Uses an adaptive sampling algorithm. For the calculation state, the function uses a sample of element hash values up to 65536. This algorithm is very accurate and very efficient on the CPU. When the query contains several of these functions, using uniq is almost as fast as using other aggregate functions. Provides the result deterministically (it does not depend on the query processing order). We recommend using this function in almost all scenarios. See Also uniqCombineduniqCombined64uniqHLL12uniqExactuniqTheta","keywords":""},{"title":"topK","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/topk","content":"topK Returns an array of the approximately most frequent values in the specified column. The resulting array is sorted in descending order of approximate frequency of values (not by the values themselves). Implements the Filtered Space-Saving algorithm for analyzing TopK, based on the reduce-and-combine algorithm from Parallel Space Saving. topK(N)(column) This function does not provide a guaranteed result. In certain situations, errors might occur and it might return frequent values that aren’t the most frequent values. We recommend using the N &lt; 10 value; performance is reduced with large N values. Maximum value of N = 65536. Arguments N – The number of elements to return. If the parameter is omitted, default value 10 is used. Arguments x – The value to calculate frequency. Example Take the OnTime data set and select the three most frequently occurring values in the AirlineID column. SELECT topK(3)(AirlineID) AS res FROM ontime ┌─res─────────────────┐ │ [19393,19790,19805] │ └─────────────────────┘ ","keywords":""},{"title":"uniqCombined64","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/uniqcombined64","content":"uniqCombined64 Same as uniqCombined, but uses 64-bit hash for all data types.","keywords":""},{"title":"sumCount","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/sumcount","content":"sumCount Calculates the sum of the numbers and counts the number of rows at the same time. The function is used by ClickHouse query optimizer: if there are multiple sum, count or avg functions in a query, they can be replaced to single sumCount function to reuse the calculations. The function is rarely needed to use explicitly. Syntax sumCount(x) Arguments x — Input value, must be Integer, Float, or Decimal. Returned value Tuple (sum, count), where sum is the sum of numbers and count is the number of rows with not-NULL values. Type: Tuple. Example Query: CREATE TABLE s_table (x Int8) Engine = Log; INSERT INTO s_table SELECT number FROM numbers(0, 20); INSERT INTO s_table VALUES (NULL); SELECT sumCount(x) from s_table; Result: ┌─sumCount(x)─┐ │ (190,20) │ └─────────────┘ See also optimize_syntax_fuse_functions setting.","keywords":""},{"title":"uniqHLL12","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/uniqhll12","content":"uniqHLL12 Calculates the approximate number of different argument values, using the HyperLogLog algorithm. uniqHLL12(x[, ...]) Arguments The function takes a variable number of parameters. Parameters can be Tuple, Array, Date, DateTime, String, or numeric types. Returned value A UInt64-type number. Implementation details Function: Calculates a hash for all parameters in the aggregate, then uses it in calculations. Uses the HyperLogLog algorithm to approximate the number of different argument values. 2^12 5-bit cells are used. The size of the state is slightly more than 2.5 KB. The result is not very accurate (up to ~10% error) for small data sets (&lt;10K elements). However, the result is fairly accurate for high-cardinality data sets (10K-100M), with a maximum error of ~1.6%. Starting from 100M, the estimation error increases, and the function will return very inaccurate results for data sets with extremely high cardinality (1B+ elements). Provides the determinate result (it does not depend on the query processing order). We do not recommend using this function. In most cases, use the uniq or uniqCombined function. See Also uniquniqCombineduniqExactuniqTheta","keywords":""},{"title":"uniqTheta","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/uniqthetasketch","content":"uniqTheta Calculates the approximate number of different argument values, using the Theta Sketch Framework. uniqTheta(x[, ...]) Arguments The function takes a variable number of parameters. Parameters can be Tuple, Array, Date, DateTime, String, or numeric types. Returned value A UInt64-type number. Implementation details Function: Calculates a hash for all parameters in the aggregate, then uses it in calculations. Uses the KMV algorithm to approximate the number of different argument values. 4096(2^12) 64-bit sketch are used. The size of the state is about 41 KB. The relative error is 3.125% (95% confidence), see the relative error table for detail. See Also uniquniqCombineduniqCombined64uniqHLL12uniqExact","keywords":""},{"title":"varPop(x)","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/varpop","content":"varPop(x) Calculates the amount Σ((x - x̅)^2) / n, where n is the sample size and x̅is the average value of x. In other words, dispersion for a set of values. Returns Float64. note This function uses a numerically unstable algorithm. If you need numerical stability in calculations, use the varPopStable function. It works slower but provides a lower computational error.","keywords":""},{"title":"varSamp","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/varsamp","content":"varSamp Calculates the amount Σ((x - x̅)^2) / (n - 1), where n is the sample size and x̅is the average value of x. It represents an unbiased estimate of the variance of a random variable if passed values form its sample. Returns Float64. When n &lt;= 1, returns +∞. note This function uses a numerically unstable algorithm. If you need numerical stability in calculations, use the varSampStable function. It works slower but provides a lower computational error.","keywords":""},{"title":"welchTTest","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/welchttest","content":"welchTTest Applies Welch's t-test to samples from two populations. Syntax welchTTest([confidence_level])(sample_data, sample_index) Values of both samples are in the sample_data column. If sample_index equals to 0 then the value in that row belongs to the sample from the first population. Otherwise it belongs to the sample from the second population. The null hypothesis is that means of populations are equal. Normal distribution is assumed. Populations may have unequal variance. Arguments sample_data — Sample data. Integer, Float or Decimal.sample_index — Sample index. Integer. Parameters confidence_level — Confidence level in order to calculate confidence intervals. Float. Returned values Tuple with two or four elements (if the optional confidence_level is specified) calculated t-statistic. Float64.calculated p-value. Float64.[calculated confidence-interval-low.][Float64](/docs/testing/docs/en/sql-reference/data-types/float).[calculated confidence-interval-high.][Float64](/docs/testing/docs/en/sql-reference/data-types/float). Example Input table: ┌─sample_data─┬─sample_index─┐ │ 20.3 │ 0 │ │ 22.1 │ 0 │ │ 21.9 │ 0 │ │ 18.9 │ 1 │ │ 20.3 │ 1 │ │ 19 │ 1 │ └─────────────┴──────────────┘ Query: SELECT welchTTest(sample_data, sample_index) FROM welch_ttest; Result: ┌─welchTTest(sample_data, sample_index)─────┐ │ (2.7988719532211235,0.051807360348581945) │ └───────────────────────────────────────────┘ See Also Welch's t-teststudentTTest function Original article","keywords":""},{"title":"AggregateFunction","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/aggregatefunction","content":"","keywords":""},{"title":"Usage​","type":1,"pageTitle":"AggregateFunction","url":"docs/en/sql-reference/data-types/aggregatefunction#usage","content":""},{"title":"Data Insertion​","type":1,"pageTitle":"AggregateFunction","url":"docs/en/sql-reference/data-types/aggregatefunction#data-insertion","content":"To insert data, use INSERT SELECT with aggregate -State- functions. Function examples uniqState(UserID) quantilesState(0.5, 0.9)(SendTiming)  In contrast to the corresponding functions uniq and quantiles, -State- functions return the state, instead of the final value. In other words, they return a value of AggregateFunction type. In the results of SELECT query, the values of AggregateFunction type have implementation-specific binary representation for all of the ClickHouse output formats. If dump data into, for example, TabSeparated format with SELECT query, then this dump can be loaded back using INSERT query. "},{"title":"Data Selection​","type":1,"pageTitle":"AggregateFunction","url":"docs/en/sql-reference/data-types/aggregatefunction#data-selection","content":"When selecting data from AggregatingMergeTree table, use GROUP BY clause and the same aggregate functions as when inserting data, but using -Mergesuffix. An aggregate function with -Merge suffix takes a set of states, combines them, and returns the result of complete data aggregation. For example, the following two queries return the same result: SELECT uniq(UserID) FROM table SELECT uniqMerge(state) FROM (SELECT uniqState(UserID) AS state FROM table GROUP BY RegionID)  "},{"title":"Usage Example​","type":1,"pageTitle":"AggregateFunction","url":"docs/en/sql-reference/data-types/aggregatefunction#usage-example","content":"See AggregatingMergeTree engine description. Original article "},{"title":"Data Types","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/","content":"Data Types ClickHouse can store various kinds of data in table cells. This section describes the supported data types and special considerations for using and/or implementing them if any. You can check whether data type name is case-sensitive in the system.data_type_families table. Original article","keywords":""},{"title":"sumMap","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/summap","content":"sumMap Syntax: sumMap(key, value) or sumMap(Tuple(key, value)) Totals the value array according to the keys specified in the key array. Passing tuple of keys and values arrays is a synonym to passing two arrays of keys and values. The number of elements in key and value must be the same for each row that is totaled. Returns a tuple of two arrays: keys in sorted order, and values ​​summed for the corresponding keys. Example: CREATE TABLE sum_map( date Date, timeslot DateTime, statusMap Nested( status UInt16, requests UInt64 ), statusMapTuple Tuple(Array(Int32), Array(Int32)) ) ENGINE = Log; INSERT INTO sum_map VALUES ('2000-01-01', '2000-01-01 00:00:00', [1, 2, 3], [10, 10, 10], ([1, 2, 3], [10, 10, 10])), ('2000-01-01', '2000-01-01 00:00:00', [3, 4, 5], [10, 10, 10], ([3, 4, 5], [10, 10, 10])), ('2000-01-01', '2000-01-01 00:01:00', [4, 5, 6], [10, 10, 10], ([4, 5, 6], [10, 10, 10])), ('2000-01-01', '2000-01-01 00:01:00', [6, 7, 8], [10, 10, 10], ([6, 7, 8], [10, 10, 10])); SELECT timeslot, sumMap(statusMap.status, statusMap.requests), sumMap(statusMapTuple) FROM sum_map GROUP BY timeslot ┌────────────timeslot─┬─sumMap(statusMap.status, statusMap.requests)─┬─sumMap(statusMapTuple)─────────┐ │ 2000-01-01 00:00:00 │ ([1,2,3,4,5],[10,10,20,10,10]) │ ([1,2,3,4,5],[10,10,20,10,10]) │ │ 2000-01-01 00:01:00 │ ([4,5,6,7,8],[10,10,20,10,10]) │ ([4,5,6,7,8],[10,10,20,10,10]) │ └─────────────────────┴──────────────────────────────────────────────┴────────────────────────────────┘ ","keywords":""},{"title":"Boolean Values","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/boolean","content":"Boolean Values There is no separate type for boolean values. Use UInt8 type, restricted to the values 0 or 1. Original article","keywords":""},{"title":"Datetime","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/datetime","content":"","keywords":""},{"title":"Usage Remarks​","type":1,"pageTitle":"Datetime","url":"docs/en/sql-reference/data-types/datetime#usage-remarks","content":"The point in time is saved as a Unix timestamp, regardless of the time zone or daylight saving time. The time zone affects how the values of the DateTime type values are displayed in text format and how the values specified as strings are parsed (‘2020-01-01 05:00:01’). Timezone agnostic unix timestamp is stored in tables, and the timezone is used to transform it to text format or back during data import/export or to make calendar calculations on the values (example: toDate, toHour functions et cetera). The time zone is not stored in the rows of the table (or in resultset), but is stored in the column metadata. A list of supported time zones can be found in the IANA Time Zone Database and also can be queried by SELECT * FROM system.time_zones. The list is also available at Wikipedia. You can explicitly set a time zone for DateTime-type columns when creating a table. Example: DateTime('UTC'). If the time zone isn’t set, ClickHouse uses the value of the timezone parameter in the server settings or the operating system settings at the moment of the ClickHouse server start. The clickhouse-client applies the server time zone by default if a time zone isn’t explicitly set when initializing the data type. To use the client time zone, run clickhouse-client with the --use_client_time_zone parameter. ClickHouse outputs values depending on the value of the date_time_output_format setting. YYYY-MM-DD hh:mm:ss text format by default. Additionaly you can change the output with the formatDateTime function. When inserting data into ClickHouse, you can use different formats of date and time strings, depending on the value of the date_time_input_format setting. "},{"title":"Examples​","type":1,"pageTitle":"Datetime","url":"docs/en/sql-reference/data-types/datetime#examples","content":"1. Creating a table with a DateTime-type column and inserting data into it: CREATE TABLE dt ( `timestamp` DateTime('Asia/Istanbul'), `event_id` UInt8 ) ENGINE = TinyLog;  INSERT INTO dt Values (1546300800, 1), ('2019-01-01 00:00:00', 2);  SELECT * FROM dt;  ┌───────────timestamp─┬─event_id─┐ │ 2019-01-01 03:00:00 │ 1 │ │ 2019-01-01 00:00:00 │ 2 │ └─────────────────────┴──────────┘  When inserting datetime as an integer, it is treated as Unix Timestamp (UTC). 1546300800 represents '2019-01-01 00:00:00' UTC. However, as timestamp column has Asia/Istanbul (UTC+3) timezone specified, when outputting as string the value will be shown as '2019-01-01 03:00:00'When inserting string value as datetime, it is treated as being in column timezone. '2019-01-01 00:00:00' will be treated as being in Asia/Istanbul timezone and saved as 1546290000. 2. Filtering on DateTime values SELECT * FROM dt WHERE timestamp = toDateTime('2019-01-01 00:00:00', 'Asia/Istanbul')  ┌───────────timestamp─┬─event_id─┐ │ 2019-01-01 00:00:00 │ 2 │ └─────────────────────┴──────────┘  DateTime column values can be filtered using a string value in WHERE predicate. It will be converted to DateTime automatically: SELECT * FROM dt WHERE timestamp = '2019-01-01 00:00:00'  ┌───────────timestamp─┬─event_id─┐ │ 2019-01-01 03:00:00 │ 1 │ └─────────────────────┴──────────┘  3. Getting a time zone for a DateTime-type column: SELECT toDateTime(now(), 'Asia/Istanbul') AS column, toTypeName(column) AS x  ┌──────────────column─┬─x─────────────────────────┐ │ 2019-10-16 04:12:04 │ DateTime('Asia/Istanbul') │ └─────────────────────┴───────────────────────────┘  4. Timezone conversion SELECT toDateTime(timestamp, 'Europe/London') as lon_time, toDateTime(timestamp, 'Asia/Istanbul') as mos_time FROM dt  ┌───────────lon_time──┬────────────mos_time─┐ │ 2019-01-01 00:00:00 │ 2019-01-01 03:00:00 │ │ 2018-12-31 21:00:00 │ 2019-01-01 00:00:00 │ └─────────────────────┴─────────────────────┘  As timezone conversion only changes the metadata, the operation has no computation cost. "},{"title":"Limitations on timezones support​","type":1,"pageTitle":"Datetime","url":"docs/en/sql-reference/data-types/datetime#limitations-on-timezones-support","content":"Some timezones may not be supported completely. There are a few cases: If the offset from UTC is not a multiple of 15 minutes, the calculation of hours and minutes can be incorrect. For example, the time zone in Monrovia, Liberia has offset UTC -0:44:30 before 7 Jan 1972. If you are doing calculations on the historical time in Monrovia timezone, the time processing functions may give incorrect results. The results after 7 Jan 1972 will be correct nevertheless. If the time transition (due to daylight saving time or for other reasons) was performed at a point of time that is not a multiple of 15 minutes, you can also get incorrect results at this specific day. Non-monotonic calendar dates. For example, in Happy Valley - Goose Bay, the time was transitioned one hour backwards at 00:01:00 7 Nov 2010 (one minute after midnight). So after 6th Nov has ended, people observed a whole one minute of 7th Nov, then time was changed back to 23:01 6th Nov and after another 59 minutes the 7th Nov started again. ClickHouse does not (yet) support this kind of fun. During these days the results of time processing functions may be slightly incorrect. Similar issue exists for Casey Antarctic station in year 2010. They changed time three hours back at 5 Mar, 02:00. If you are working in antarctic station, please don't afraid to use ClickHouse. Just make sure you set timezone to UTC or be aware of inaccuracies. Time shifts for multiple days. Some pacific islands changed their timezone offset from UTC+14 to UTC-12. That's alright but some inaccuracies may present if you do calculations with their timezone for historical time points at the days of conversion. "},{"title":"See Also​","type":1,"pageTitle":"Datetime","url":"docs/en/sql-reference/data-types/datetime#see-also","content":"Type conversion functionsFunctions for working with dates and timesFunctions for working with arraysThe date_time_input_format settingThe date_time_output_format settingThe timezone server configuration parameterOperators for working with dates and timesThe Date data type Original article "},{"title":"uniqExact","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/uniqexact","content":"uniqExact Calculates the exact number of different argument values. uniqExact(x[, ...]) Use the uniqExact function if you absolutely need an exact result. Otherwise use the uniq function. The uniqExact function uses more memory than uniq, because the size of the state has unbounded growth as the number of different values increases. Arguments The function takes a variable number of parameters. Parameters can be Tuple, Array, Date, DateTime, String, or numeric types. See Also uniquniqCombineduniqHLL12uniqTheta","keywords":""},{"title":"Decimal(P, S), Decimal32(S), Decimal64(S), Decimal128(S), Decimal256(S)","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/decimal","content":"","keywords":""},{"title":"Parameters​","type":1,"pageTitle":"Decimal(P, S), Decimal32(S), Decimal64(S), Decimal128(S), Decimal256(S)","url":"docs/en/sql-reference/data-types/decimal#parameters","content":"P - precision. Valid range: [ 1 : 76 ]. Determines how many decimal digits number can have (including fraction).S - scale. Valid range: [ 0 : P ]. Determines how many decimal digits fraction can have. Depending on P parameter value Decimal(P, S) is a synonym for: P from [ 1 : 9 ] - for Decimal32(S)P from [ 10 : 18 ] - for Decimal64(S)P from [ 19 : 38 ] - for Decimal128(S)P from [ 39 : 76 ] - for Decimal256(S) "},{"title":"Decimal Value Ranges​","type":1,"pageTitle":"Decimal(P, S), Decimal32(S), Decimal64(S), Decimal128(S), Decimal256(S)","url":"docs/en/sql-reference/data-types/decimal#decimal-value-ranges","content":"Decimal32(S) - ( -1 * 10^(9 - S), 1 * 10^(9 - S) )Decimal64(S) - ( -1 * 10^(18 - S), 1 * 10^(18 - S) )Decimal128(S) - ( -1 * 10^(38 - S), 1 * 10^(38 - S) )Decimal256(S) - ( -1 * 10^(76 - S), 1 * 10^(76 - S) ) For example, Decimal32(4) can contain numbers from -99999.9999 to 99999.9999 with 0.0001 step. "},{"title":"Internal Representation​","type":1,"pageTitle":"Decimal(P, S), Decimal32(S), Decimal64(S), Decimal128(S), Decimal256(S)","url":"docs/en/sql-reference/data-types/decimal#internal-representation","content":"Internally data is represented as normal signed integers with respective bit width. Real value ranges that can be stored in memory are a bit larger than specified above, which are checked only on conversion from a string. Because modern CPUs do not support 128-bit integers natively, operations on Decimal128 are emulated. Because of this Decimal128 works significantly slower than Decimal32/Decimal64. "},{"title":"Operations and Result Type​","type":1,"pageTitle":"Decimal(P, S), Decimal32(S), Decimal64(S), Decimal128(S), Decimal256(S)","url":"docs/en/sql-reference/data-types/decimal#operations-and-result-type","content":"Binary operations on Decimal result in wider result type (with any order of arguments). Decimal64(S1) &lt;op&gt; Decimal32(S2) -&gt; Decimal64(S)Decimal128(S1) &lt;op&gt; Decimal32(S2) -&gt; Decimal128(S)Decimal128(S1) &lt;op&gt; Decimal64(S2) -&gt; Decimal128(S)Decimal256(S1) &lt;op&gt; Decimal&lt;32|64|128&gt;(S2) -&gt; Decimal256(S) Rules for scale: add, subtract: S = max(S1, S2).multuply: S = S1 + S2.divide: S = S1. For similar operations between Decimal and integers, the result is Decimal of the same size as an argument. Operations between Decimal and Float32/Float64 are not defined. If you need them, you can explicitly cast one of argument using toDecimal32, toDecimal64, toDecimal128 or toFloat32, toFloat64 builtins. Keep in mind that the result will lose precision and type conversion is a computationally expensive operation. Some functions on Decimal return result as Float64 (for example, var or stddev). Intermediate calculations might still be performed in Decimal, which might lead to different results between Float64 and Decimal inputs with the same values. "},{"title":"Overflow Checks​","type":1,"pageTitle":"Decimal(P, S), Decimal32(S), Decimal64(S), Decimal128(S), Decimal256(S)","url":"docs/en/sql-reference/data-types/decimal#overflow-checks","content":"During calculations on Decimal, integer overflows might happen. Excessive digits in a fraction are discarded (not rounded). Excessive digits in integer part will lead to an exception. SELECT toDecimal32(2, 4) AS x, x / 3  ┌──────x─┬─divide(toDecimal32(2, 4), 3)─┐ │ 2.0000 │ 0.6666 │ └────────┴──────────────────────────────┘  SELECT toDecimal32(4.2, 8) AS x, x * x  DB::Exception: Scale is out of bounds.  SELECT toDecimal32(4.2, 8) AS x, 6 * x  DB::Exception: Decimal math overflow.  Overflow checks lead to operations slowdown. If it is known that overflows are not possible, it makes sense to disable checks using decimal_check_overflow setting. When checks are disabled and overflow happens, the result will be incorrect: SET decimal_check_overflow = 0; SELECT toDecimal32(4.2, 8) AS x, 6 * x  ┌──────────x─┬─multiply(6, toDecimal32(4.2, 8))─┐ │ 4.20000000 │ -17.74967296 │ └────────────┴──────────────────────────────────┘  Overflow checks happen not only on arithmetic operations but also on value comparison: SELECT toDecimal32(1, 8) &lt; 100  DB::Exception: Can't compare.  See also isDecimalOverflowcountDigits Original article "},{"title":"uniqCombined","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/uniqcombined","content":"uniqCombined Calculates the approximate number of different argument values. uniqCombined(HLL_precision)(x[, ...]) The uniqCombined function is a good choice for calculating the number of different values. Arguments The function takes a variable number of parameters. Parameters can be Tuple, Array, Date, DateTime, String, or numeric types. HLL_precision is the base-2 logarithm of the number of cells in HyperLogLog. Optional, you can use the function as uniqCombined(x[, ...]). The default value for HLL_precision is 17, which is effectively 96 KiB of space (2^17 cells, 6 bits each). Returned value A number UInt64-type number. Implementation details Function: Calculates a hash (64-bit hash for String and 32-bit otherwise) for all parameters in the aggregate, then uses it in calculations. Uses a combination of three algorithms: array, hash table, and HyperLogLog with an error correction table. For a small number of distinct elements, an array is used. When the set size is larger, a hash table is used. For a larger number of elements, HyperLogLog is used, which will occupy a fixed amount of memory. Provides the result deterministically (it does not depend on the query processing order). note Since it uses 32-bit hash for non-String type, the result will have very high error for cardinalities significantly larger than UINT_MAX (error will raise quickly after a few tens of billions of distinct values), hence in this case you should use uniqCombined64 Compared to the uniq function, the uniqCombined: Consumes several times less memory.Calculates with several times higher accuracy.Usually has slightly lower performance. In some scenarios, uniqCombined can perform better than uniq, for example, with distributed queries that transmit a large number of aggregation states over the network. See Also uniquniqCombined64uniqHLL12uniqExactuniqTheta","keywords":""},{"title":"Domains","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/domains/","content":"","keywords":""},{"title":"Extra Features of Domains​","type":1,"pageTitle":"Domains","url":"docs/en/sql-reference/data-types/domains/#extra-features-of-domains","content":"Explicit column type name in SHOW CREATE TABLE or DESCRIBE TABLEInput from human-friendly format with INSERT INTO domain_table(domain_column) VALUES(...)Output to human-friendly format for SELECT domain_column FROM domain_tableLoading data from an external source in the human-friendly format: INSERT INTO domain_table FORMAT CSV ... "},{"title":"Limitations​","type":1,"pageTitle":"Domains","url":"docs/en/sql-reference/data-types/domains/#limitations","content":"Can’t convert index column of base type to domain type via ALTER TABLE.Can’t implicitly convert string values into domain values when inserting data from another column or table.Domain adds no constrains on stored values. Original article "},{"title":"Date","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/date","content":"Date A date. Stored in two bytes as the number of days since 1970-01-01 (unsigned). Allows storing values from just after the beginning of the Unix Epoch to the upper threshold defined by a constant at the compilation stage (currently, this is until the year 2149, but the final fully-supported year is 2148). Supported range of values: [1970-01-01, 2149-06-06]. The date value is stored without the time zone. Example Creating a table with a Date-type column and inserting data into it: CREATE TABLE dt ( `timestamp` Date, `event_id` UInt8 ) ENGINE = TinyLog; INSERT INTO dt VALUES (1546300800, 1), ('2019-01-01', 2); SELECT * FROM dt; ┌──timestamp─┬─event_id─┐ │ 2019-01-01 │ 1 │ │ 2019-01-01 │ 2 │ └────────────┴──────────┘ See Also Functions for working with dates and timesOperators for working with dates and timesDateTime data type","keywords":""},{"title":"topKWeighted","type":0,"sectionRef":"#","url":"docs/en/sql-reference/aggregate-functions/reference/topkweighted","content":"topKWeighted Returns an array of the approximately most frequent values in the specified column. The resulting array is sorted in descending order of approximate frequency of values (not by the values themselves). Additionally, the weight of the value is taken into account. Syntax topKWeighted(N)(x, weight) Arguments N — The number of elements to return.x — The value.weight — The weight. Every value is accounted weight times for frequency calculation. UInt64. Returned value Returns an array of the values with maximum approximate sum of weights. Example Query: SELECT topKWeighted(10)(number, number) FROM numbers(1000) Result: ┌─topKWeighted(10)(number, number)──────────┐ │ [999,998,997,996,995,994,993,992,991,990] │ └───────────────────────────────────────────┘ See Also topK","keywords":""},{"title":"Datetime64","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/datetime64","content":"","keywords":""},{"title":"Examples​","type":1,"pageTitle":"Datetime64","url":"docs/en/sql-reference/data-types/datetime64#examples","content":"Creating a table with DateTime64-type column and inserting data into it: CREATE TABLE dt ( `timestamp` DateTime64(3, 'Asia/Istanbul'), `event_id` UInt8 ) ENGINE = TinyLog;  INSERT INTO dt Values (1546300800000, 1), ('2019-01-01 00:00:00', 2);  SELECT * FROM dt;  ┌───────────────timestamp─┬─event_id─┐ │ 2019-01-01 03:00:00.000 │ 1 │ │ 2019-01-01 00:00:00.000 │ 2 │ └─────────────────────────┴──────────┘  When inserting datetime as an integer, it is treated as an appropriately scaled Unix Timestamp (UTC). 1546300800000 (with precision 3) represents '2019-01-01 00:00:00' UTC. However, as timestamp column has Asia/Istanbul (UTC+3) timezone specified, when outputting as a string the value will be shown as '2019-01-01 03:00:00'.When inserting string value as datetime, it is treated as being in column timezone. '2019-01-01 00:00:00' will be treated as being in Asia/Istanbul timezone and stored as 1546290000000. Filtering on DateTime64 values SELECT * FROM dt WHERE timestamp = toDateTime64('2019-01-01 00:00:00', 3, 'Asia/Istanbul');  ┌───────────────timestamp─┬─event_id─┐ │ 2019-01-01 00:00:00.000 │ 2 │ └─────────────────────────┴──────────┘  Unlike DateTime, DateTime64 values are not converted from String automatically. Getting a time zone for a DateTime64-type value: SELECT toDateTime64(now(), 3, 'Asia/Istanbul') AS column, toTypeName(column) AS x;  ┌──────────────────column─┬─x──────────────────────────────┐ │ 2019-10-16 04:12:04.000 │ DateTime64(3, 'Asia/Istanbul') │ └─────────────────────────┴────────────────────────────────┘  Timezone conversion SELECT toDateTime64(timestamp, 3, 'Europe/London') as lon_time, toDateTime64(timestamp, 3, 'Asia/Istanbul') as mos_time FROM dt;  ┌───────────────lon_time──┬────────────────mos_time─┐ │ 2019-01-01 00:00:00.000 │ 2019-01-01 03:00:00.000 │ │ 2018-12-31 21:00:00.000 │ 2019-01-01 00:00:00.000 │ └─────────────────────────┴─────────────────────────┘  See Also Type conversion functionsFunctions for working with dates and timesFunctions for working with arraysThe date_time_input_format settingThe date_time_output_format settingThe timezone server configuration parameterOperators for working with dates and timesDate data typeDateTime data type "},{"title":"Date32","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/date32","content":"Date32 A date. Supports the date range same with Datetime64. Stored in four bytes as the number of days since 1925-01-01. Allows storing values till 2283-11-11. Examples Creating a table with a Date32-type column and inserting data into it: CREATE TABLE new ( `timestamp` Date32, `event_id` UInt8 ) ENGINE = TinyLog; INSERT INTO new VALUES (4102444800, 1), ('2100-01-01', 2); SELECT * FROM new; ┌──timestamp─┬─event_id─┐ │ 2100-01-01 │ 1 │ │ 2100-01-01 │ 2 │ └────────────┴──────────┘ See Also toDate32toDate32OrZerotoDate32OrNull","keywords":""},{"title":"Enum","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/enum","content":"","keywords":""},{"title":"Usage Examples​","type":1,"pageTitle":"Enum","url":"docs/en/sql-reference/data-types/enum#usage-examples","content":"Here we create a table with an Enum8('hello' = 1, 'world' = 2) type column: CREATE TABLE t_enum ( x Enum('hello' = 1, 'world' = 2) ) ENGINE = TinyLog  Column x can only store values that are listed in the type definition: 'hello' or 'world'. If you try to save any other value, ClickHouse will raise an exception. 8-bit size for this Enum is chosen automatically. INSERT INTO t_enum VALUES ('hello'), ('world'), ('hello')  Ok.  INSERT INTO t_enum values('a')  Exception on client: Code: 49. DB::Exception: Unknown element 'a' for type Enum('hello' = 1, 'world' = 2)  When you query data from the table, ClickHouse outputs the string values from Enum. SELECT * FROM t_enum  ┌─x─────┐ │ hello │ │ world │ │ hello │ └───────┘  If you need to see the numeric equivalents of the rows, you must cast the Enum value to integer type. SELECT CAST(x, 'Int8') FROM t_enum  ┌─CAST(x, 'Int8')─┐ │ 1 │ │ 2 │ │ 1 │ └─────────────────┘  To create an Enum value in a query, you also need to use CAST. SELECT toTypeName(CAST('a', 'Enum(\\'a\\' = 1, \\'b\\' = 2)'))  ┌─toTypeName(CAST('a', 'Enum(\\'a\\' = 1, \\'b\\' = 2)'))─┐ │ Enum8('a' = 1, 'b' = 2) │ └─────────────────────────────────────────────────────┘  "},{"title":"General Rules and Usage​","type":1,"pageTitle":"Enum","url":"docs/en/sql-reference/data-types/enum#general-rules-and-usage","content":"Each of the values is assigned a number in the range -128 ... 127 for Enum8 or in the range -32768 ... 32767 for Enum16. All the strings and numbers must be different. An empty string is allowed. If this type is specified (in a table definition), numbers can be in an arbitrary order. However, the order does not matter. Neither the string nor the numeric value in an Enum can be NULL. An Enum can be contained in Nullable type. So if you create a table using the query CREATE TABLE t_enum_nullable ( x Nullable( Enum8('hello' = 1, 'world' = 2) ) ) ENGINE = TinyLog  it can store not only 'hello' and 'world', but NULL, as well. INSERT INTO t_enum_nullable Values('hello'),('world'),(NULL)  In RAM, an Enum column is stored in the same way as Int8 or Int16 of the corresponding numerical values. When reading in text form, ClickHouse parses the value as a string and searches for the corresponding string from the set of Enum values. If it is not found, an exception is thrown. When reading in text format, the string is read and the corresponding numeric value is looked up. An exception will be thrown if it is not found. When writing in text form, it writes the value as the corresponding string. If column data contains garbage (numbers that are not from the valid set), an exception is thrown. When reading and writing in binary form, it works the same way as for Int8 and Int16 data types. The implicit default value is the value with the lowest number. During ORDER BY, GROUP BY, IN, DISTINCT and so on, Enums behave the same way as the corresponding numbers. For example, ORDER BY sorts them numerically. Equality and comparison operators work the same way on Enums as they do on the underlying numeric values. Enum values cannot be compared with numbers. Enums can be compared to a constant string. If the string compared to is not a valid value for the Enum, an exception will be thrown. The IN operator is supported with the Enum on the left-hand side and a set of strings on the right-hand side. The strings are the values of the corresponding Enum. Most numeric and string operations are not defined for Enum values, e.g. adding a number to an Enum or concatenating a string to an Enum. However, the Enum has a natural toString function that returns its string value. Enum values are also convertible to numeric types using the toT function, where T is a numeric type. When T corresponds to the enum’s underlying numeric type, this conversion is zero-cost. The Enum type can be changed without cost using ALTER, if only the set of values is changed. It is possible to both add and remove members of the Enum using ALTER (removing is safe only if the removed value has never been used in the table). As a safeguard, changing the numeric value of a previously defined Enum member will throw an exception. Using ALTER, it is possible to change an Enum8 to an Enum16 or vice versa, just like changing an Int8 to Int16. Original article "},{"title":"UInt8, UInt16, UInt32, UInt64, UInt128, UInt256, Int8, Int16, Int32, Int64, Int128, Int256","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/int-uint","content":"","keywords":""},{"title":"Int Ranges​","type":1,"pageTitle":"UInt8, UInt16, UInt32, UInt64, UInt128, UInt256, Int8, Int16, Int32, Int64, Int128, Int256","url":"docs/en/sql-reference/data-types/int-uint#int-ranges","content":"Int8 — [-128 : 127]Int16 — [-32768 : 32767]Int32 — [-2147483648 : 2147483647]Int64 — [-9223372036854775808 : 9223372036854775807]Int128 — [-170141183460469231731687303715884105728 : 170141183460469231731687303715884105727]Int256 — [-57896044618658097711785492504343953926634992332820282019728792003956564819968 : 57896044618658097711785492504343953926634992332820282019728792003956564819967] Aliases: Int8 — TINYINT, BOOL, BOOLEAN, INT1.Int16 — SMALLINT, INT2.Int32 — INT, INT4, INTEGER.Int64 — BIGINT. "},{"title":"UInt Ranges​","type":1,"pageTitle":"UInt8, UInt16, UInt32, UInt64, UInt128, UInt256, Int8, Int16, Int32, Int64, Int128, Int256","url":"docs/en/sql-reference/data-types/int-uint#uint-ranges","content":"UInt8 — [0 : 255]UInt16 — [0 : 65535]UInt32 — [0 : 4294967295]UInt64 — [0 : 18446744073709551615]UInt128 — [0 : 340282366920938463463374607431768211455]UInt256 — [0 : 115792089237316195423570985008687907853269984665640564039457584007913129639935] Original article "},{"title":"Array(t)","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/array","content":"","keywords":""},{"title":"Creating an Array​","type":1,"pageTitle":"Array(t)","url":"docs/en/sql-reference/data-types/array#creating-an-array","content":"You can use a function to create an array: array(T)  You can also use square brackets. []  Example of creating an array: SELECT array(1, 2) AS x, toTypeName(x)  ┌─x─────┬─toTypeName(array(1, 2))─┐ │ [1,2] │ Array(UInt8) │ └───────┴─────────────────────────┘  SELECT [1, 2] AS x, toTypeName(x)  ┌─x─────┬─toTypeName([1, 2])─┐ │ [1,2] │ Array(UInt8) │ └───────┴────────────────────┘  "},{"title":"Working with Data Types​","type":1,"pageTitle":"Array(t)","url":"docs/en/sql-reference/data-types/array#working-with-data-types","content":"The maximum size of an array is limited to one million elements. When creating an array on the fly, ClickHouse automatically defines the argument type as the narrowest data type that can store all the listed arguments. If there are any Nullable or literal NULL values, the type of an array element also becomes Nullable. If ClickHouse couldn’t determine the data type, it generates an exception. For instance, this happens when trying to create an array with strings and numbers simultaneously (SELECT array(1, 'a')). Examples of automatic data type detection: SELECT array(1, 2, NULL) AS x, toTypeName(x)  ┌─x──────────┬─toTypeName(array(1, 2, NULL))─┐ │ [1,2,NULL] │ Array(Nullable(UInt8)) │ └────────────┴───────────────────────────────┘  If you try to create an array of incompatible data types, ClickHouse throws an exception: SELECT array(1, 'a')  Received exception from server (version 1.1.54388): Code: 386. DB::Exception: Received from localhost:9000, 127.0.0.1. DB::Exception: There is no supertype for types UInt8, String because some of them are String/FixedString and some of them are not.  "},{"title":"Array Size​","type":1,"pageTitle":"Array(t)","url":"docs/en/sql-reference/data-types/array#array-size","content":"It is possible to find the size of an array by using the size0 subcolumn without reading the whole column. For multi-dimensional arrays you can use sizeN-1, where N is the wanted dimension. Example Query: CREATE TABLE t_arr (`arr` Array(Array(Array(UInt32)))) ENGINE = MergeTree ORDER BY tuple(); INSERT INTO t_arr VALUES ([[[12, 13, 0, 1],[12]]]); SELECT arr.size0, arr.size1, arr.size2 FROM t_arr;  Result: ┌─arr.size0─┬─arr.size1─┬─arr.size2─┐ │ 1 │ [2] │ [[4,1]] │ └───────────┴───────────┴───────────┘  "},{"title":"Geo Data Types","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/geo","content":"","keywords":""},{"title":"Point​","type":1,"pageTitle":"Geo Data Types","url":"docs/en/sql-reference/data-types/geo#point-data-type","content":"Point is represented by its X and Y coordinates, stored as a Tuple(Float64, Float64). Example Query: SET allow_experimental_geo_types = 1; CREATE TABLE geo_point (p Point) ENGINE = Memory(); INSERT INTO geo_point VALUES((10, 10)); SELECT p, toTypeName(p) FROM geo_point;  Result: ┌─p─────┬─toTypeName(p)─┐ │ (10,10) │ Point │ └───────┴───────────────┘  "},{"title":"Ring​","type":1,"pageTitle":"Geo Data Types","url":"docs/en/sql-reference/data-types/geo#ring-data-type","content":"Ring is a simple polygon without holes stored as an array of points: Array(Point). Example Query: SET allow_experimental_geo_types = 1; CREATE TABLE geo_ring (r Ring) ENGINE = Memory(); INSERT INTO geo_ring VALUES([(0, 0), (10, 0), (10, 10), (0, 10)]); SELECT r, toTypeName(r) FROM geo_ring;  Result: ┌─r─────────────────────────────┬─toTypeName(r)─┐ │ [(0,0),(10,0),(10,10),(0,10)] │ Ring │ └───────────────────────────────┴───────────────┘  "},{"title":"Polygon​","type":1,"pageTitle":"Geo Data Types","url":"docs/en/sql-reference/data-types/geo#polygon-data-type","content":"Polygon is a polygon with holes stored as an array of rings: Array(Ring). First element of outer array is the outer shape of polygon and all the following elements are holes. Example This is a polygon with one hole: SET allow_experimental_geo_types = 1; CREATE TABLE geo_polygon (pg Polygon) ENGINE = Memory(); INSERT INTO geo_polygon VALUES([[(20, 20), (50, 20), (50, 50), (20, 50)], [(30, 30), (50, 50), (50, 30)]]); SELECT pg, toTypeName(pg) FROM geo_polygon;  Result: ┌─pg────────────────────────────────────────────────────────────┬─toTypeName(pg)─┐ │ [[(20,20),(50,20),(50,50),(20,50)],[(30,30),(50,50),(50,30)]] │ Polygon │ └───────────────────────────────────────────────────────────────┴────────────────┘  "},{"title":"MultiPolygon​","type":1,"pageTitle":"Geo Data Types","url":"docs/en/sql-reference/data-types/geo#multipolygon-data-type","content":"MultiPolygon consists of multiple polygons and is stored as an array of polygons: Array(Polygon). Example This multipolygon consists of two separate polygons — the first one without holes, and the second with one hole: SET allow_experimental_geo_types = 1; CREATE TABLE geo_multipolygon (mpg MultiPolygon) ENGINE = Memory(); INSERT INTO geo_multipolygon VALUES([[[(0, 0), (10, 0), (10, 10), (0, 10)]], [[(20, 20), (50, 20), (50, 50), (20, 50)],[(30, 30), (50, 50), (50, 30)]]]); SELECT mpg, toTypeName(mpg) FROM geo_multipolygon;  Result: ┌─mpg─────────────────────────────────────────────────────────────────────────────────────────────┬─toTypeName(mpg)─┐ │ [[[(0,0),(10,0),(10,10),(0,10)]],[[(20,20),(50,20),(50,50),(20,50)],[(30,30),(50,50),(50,30)]]] │ MultiPolygon │ └─────────────────────────────────────────────────────────────────────────────────────────────────┴─────────────────┘  Original article "},{"title":"ANSI SQL Compatibility of ClickHouse SQL Dialect","type":0,"sectionRef":"#","url":"docs/en/sql-reference/ansi","content":"","keywords":""},{"title":"Differences in Behaviour​","type":1,"pageTitle":"ANSI SQL Compatibility of ClickHouse SQL Dialect","url":"docs/en/sql-reference/ansi#differences-in-behaviour","content":"The following table lists cases when query feature works in ClickHouse, but behaves not as specified in ANSI SQL. Feature ID\tFeature Name\tDifferenceE011\tNumeric data types\tNumeric literal with period is interpreted as approximate (Float64) instead of exact (Decimal) E051-05\tSelect items can be renamed\tItem renames have a wider visibility scope than just the SELECT result E141-01\tNOT NULL constraints\tNOT NULL is implied for table columns by default E011-04\tArithmetic operators\tClickHouse overflows instead of checked arithmetic and changes the result data type based on custom rules "},{"title":"Feature Status​","type":1,"pageTitle":"ANSI SQL Compatibility of ClickHouse SQL Dialect","url":"docs/en/sql-reference/ansi#feature-status","content":"Feature ID\tFeature Name\tStatus\tCommentE011\tNumeric data types\tPartial E011-01\tINTEGER and SMALLINT data types\tYes E011-02\tREAL, DOUBLE PRECISION and FLOAT data types data types\tYes E011-03\tDECIMAL and NUMERIC data types\tYes E011-04\tArithmetic operators\tYes E011-05\tNumeric comparison\tYes E011-06\tImplicit casting among the numeric data types\tNo\tANSI SQL allows arbitrary implicit cast between numeric types, while ClickHouse relies on functions having multiple overloads instead of implicit cast E021\tCharacter string types\tPartial E021-01\tCHARACTER data type\tYes E021-02\tCHARACTER VARYING data type\tYes E021-03\tCharacter literals\tYes E021-04\tCHARACTER_LENGTH function\tPartial\tNo USING clause E021-05\tOCTET_LENGTH function\tNo\tLENGTH behaves similarly E021-06\tSUBSTRING\tPartial\tNo support for SIMILAR and ESCAPE clauses, no SUBSTRING_REGEX variant E021-07\tCharacter concatenation\tPartial\tNo COLLATE clause E021-08\tUPPER and LOWER functions\tYes E021-09\tTRIM function\tYes E021-10\tImplicit casting among the fixed-length and variable-length character string types\tPartial\tANSI SQL allows arbitrary implicit cast between string types, while ClickHouse relies on functions having multiple overloads instead of implicit cast E021-11\tPOSITION function\tPartial\tNo support for IN and USING clauses, no POSITION_REGEX variant E021-12\tCharacter comparison\tYes E031\tIdentifiers\tPartial E031-01\tDelimited identifiers\tPartial\tUnicode literal support is limited E031-02\tLower case identifiers\tYes E031-03\tTrailing underscore\tYes E051\tBasic query specification\tPartial E051-01\tSELECT DISTINCT\tYes E051-02\tGROUP BY clause\tYes E051-04\tGROUP BY can contain columns not in &lt;select list&gt;\tYes E051-05\tSelect items can be renamed\tYes E051-06\tHAVING clause\tYes E051-07\tQualified * in select list\tYes E051-08\tCorrelation name in the FROM clause\tYes E051-09\tRename columns in the FROM clause\tNo E061\tBasic predicates and search conditions\tPartial E061-01\tComparison predicate\tYes E061-02\tBETWEEN predicate\tPartial\tNo SYMMETRIC and ASYMMETRIC clause E061-03\tIN predicate with list of values\tYes E061-04\tLIKE predicate\tYes E061-05\tLIKE predicate: ESCAPE clause\tNo E061-06\tNULL predicate\tYes E061-07\tQuantified comparison predicate\tNo E061-08\tEXISTS predicate\tNo E061-09\tSubqueries in comparison predicate\tYes E061-11\tSubqueries in IN predicate\tYes E061-12\tSubqueries in quantified comparison predicate\tNo E061-13\tCorrelated subqueries\tNo E061-14\tSearch condition\tYes E071\tBasic query expressions\tPartial E071-01\tUNION DISTINCT table operator\tYes E071-02\tUNION ALL table operator\tYes E071-03\tEXCEPT DISTINCT table operator\tNo E071-05\tColumns combined via table operators need not have exactly the same data type\tYes E071-06\tTable operators in subqueries\tYes E081\tBasic privileges\tYes E081-01\tSELECT privilege at the table level\tYes E081-02\tDELETE privilege E081-03\tINSERT privilege at the table level\tYes E081-04\tUPDATE privilege at the table level\tYes E081-05\tUPDATE privilege at the column level E081-06\tREFERENCES privilege at the table level E081-07\tREFERENCES privilege at the column level E081-08\tWITH GRANT OPTION\tYes E081-09\tUSAGE privilege E081-10\tEXECUTE privilege E091\tSet functions\tYes E091-01\tAVG\tYes E091-02\tCOUNT\tYes E091-03\tMAX\tYes E091-04\tMIN\tYes E091-05\tSUM\tYes E091-06\tALL quantifier\tYes E091-07\tDISTINCT quantifier\tYes\tNot all aggregate functions supported E101\tBasic data manipulation\tPartial E101-01\tINSERT statement\tYes\tNote: primary key in ClickHouse does not imply the UNIQUE constraint E101-03\tSearched UPDATE statement\tPartial\tThere’s an ALTER UPDATE statement for batch data modification E101-04\tSearched DELETE statement\tPartial\tThere’s an ALTER DELETE statement for batch data removal E111\tSingle row SELECT statement\tNo E121\tBasic cursor support\tNo E121-01\tDECLARE CURSOR\tNo E121-02\tORDER BY columns need not be in select list\tYes E121-03\tValue expressions in ORDER BY clause\tYes E121-04\tOPEN statement\tNo E121-06\tPositioned UPDATE statement\tNo E121-07\tPositioned DELETE statement\tNo E121-08\tCLOSE statement\tNo E121-10\tFETCH statement: implicit NEXT\tNo E121-17\tWITH HOLD cursors\tNo E131\tNull value support (nulls in lieu of values)\tYes\tSome restrictions apply E141\tBasic integrity constraints\tPartial E141-01\tNOT NULL constraints\tYes\tNote: NOT NULL is implied for table columns by default E141-02\tUNIQUE constraint of NOT NULL columns\tNo E141-03\tPRIMARY KEY constraints\tPartial E141-04\tBasic FOREIGN KEY constraint with the NO ACTION default for both referential delete action and referential update action\tNo E141-06\tCHECK constraint\tYes E141-07\tColumn defaults\tYes E141-08\tNOT NULL inferred on PRIMARY KEY\tYes E141-10\tNames in a foreign key can be specified in any order\tNo E151\tTransaction support\tNo E151-01\tCOMMIT statement\tNo E151-02\tROLLBACK statement\tNo E152\tBasic SET TRANSACTION statement\tNo E152-01\tSET TRANSACTION statement: ISOLATION LEVEL SERIALIZABLE clause\tNo E152-02\tSET TRANSACTION statement: READ ONLY and READ WRITE clauses\tNo E153\tUpdatable queries with subqueries\tYes E161\tSQL comments using leading double minus\tYes E171\tSQLSTATE support\tNo E182\tHost language binding\tNo F031\tBasic schema manipulation\tPartial F031-01\tCREATE TABLE statement to create persistent base tables\tPartial\tNo SYSTEM VERSIONING, ON COMMIT, GLOBAL, LOCAL, PRESERVE, DELETE, REF IS, WITH OPTIONS, UNDER, LIKE, PERIOD FOR clauses and no support for user resolved data types F031-02\tCREATE VIEW statement\tPartial\tNo RECURSIVE, CHECK, UNDER, WITH OPTIONS clauses and no support for user resolved data types F031-03\tGRANT statement\tYes F031-04\tALTER TABLE statement: ADD COLUMN clause\tYes\tNo support for GENERATED clause and system time period F031-13\tDROP TABLE statement: RESTRICT clause\tNo F031-16\tDROP VIEW statement: RESTRICT clause\tNo F031-19\tREVOKE statement: RESTRICT clause\tNo F041\tBasic joined table\tPartial F041-01\tInner join (but not necessarily the INNER keyword)\tYes F041-02\tINNER keyword\tYes F041-03\tLEFT OUTER JOIN\tYes F041-04\tRIGHT OUTER JOIN\tYes F041-05\tOuter joins can be nested\tYes F041-07\tThe inner table in a left or right outer join can also be used in an inner join\tYes F041-08\tAll comparison operators are supported (rather than just =)\tNo F051\tBasic date and time\tPartial F051-01\tDATE data type (including support of DATE literal)\tYes F051-02\tTIME data type (including support of TIME literal) with fractional seconds precision of at least 0\tNo F051-03\tTIMESTAMP data type (including support of TIMESTAMP literal) with fractional seconds precision of at least 0 and 6\tYes F051-04\tComparison predicate on DATE, TIME, and TIMESTAMP data types\tYes F051-05\tExplicit CAST between datetime types and character string types\tYes F051-06\tCURRENT_DATE\tNo\ttoday() is similar F051-07\tLOCALTIME\tNo\tnow() is similar F051-08\tLOCALTIMESTAMP\tNo F081\tUNION and EXCEPT in views\tPartial F131\tGrouped operations\tPartial F131-01\tWHERE, GROUP BY, and HAVING clauses supported in queries with grouped views\tYes F131-02\tMultiple tables supported in queries with grouped views\tYes F131-03\tSet functions supported in queries with grouped views\tYes F131-04\tSubqueries with GROUP BY and HAVING clauses and grouped views\tYes F131-05\tSingle row SELECT with GROUP BY and HAVING clauses and grouped views\tNo F181\tMultiple module support\tNo F201\tCAST function\tYes F221\tExplicit defaults\tNo F261\tCASE expression\tYes F261-01\tSimple CASE\tYes F261-02\tSearched CASE\tYes F261-03\tNULLIF\tYes F261-04\tCOALESCE\tYes F311\tSchema definition statement\tPartial F311-01\tCREATE SCHEMA\tPartial\tSee CREATE DATABASE F311-02\tCREATE TABLE for persistent base tables\tYes F311-03\tCREATE VIEW\tYes F311-04\tCREATE VIEW: WITH CHECK OPTION\tNo F311-05\tGRANT statement\tYes F471\tScalar subquery values\tYes F481\tExpanded NULL predicate\tYes F812\tBasic flagging\tNo S011\tDistinct data types T321\tBasic SQL-invoked routines\tNo T321-01\tUser-defined functions with no overloading\tNo T321-02\tUser-defined stored procedures with no overloading\tNo T321-03\tFunction invocation\tNo T321-04\tCALL statement\tNo T321-05\tRETURN statement\tNo T631\tIN predicate with one list element\tYes\t "},{"title":"LowCardinality Data Type","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/lowcardinality","content":"","keywords":""},{"title":"Syntax​","type":1,"pageTitle":"LowCardinality Data Type","url":"docs/en/sql-reference/data-types/lowcardinality#lowcardinality-syntax","content":"LowCardinality(data_type)  Parameters data_type — String, FixedString, Date, DateTime, and numbers excepting Decimal. LowCardinality is not efficient for some data types, see the allow_suspicious_low_cardinality_types setting description. "},{"title":"Description​","type":1,"pageTitle":"LowCardinality Data Type","url":"docs/en/sql-reference/data-types/lowcardinality#lowcardinality-dscr","content":"LowCardinality is a superstructure that changes a data storage method and rules of data processing. ClickHouse applies dictionary coding to LowCardinality-columns. Operating with dictionary encoded data significantly increases performance of SELECT queries for many applications. The efficiency of using LowCardinality data type depends on data diversity. If a dictionary contains less than 10,000 distinct values, then ClickHouse mostly shows higher efficiency of data reading and storing. If a dictionary contains more than 100,000 distinct values, then ClickHouse can perform worse in comparison with using ordinary data types. Consider using LowCardinality instead of Enum when working with strings. LowCardinality provides more flexibility in use and often reveals the same or higher efficiency. "},{"title":"Example​","type":1,"pageTitle":"LowCardinality Data Type","url":"docs/en/sql-reference/data-types/lowcardinality#example","content":"Create a table with a LowCardinality-column: CREATE TABLE lc_t ( `id` UInt16, `strings` LowCardinality(String) ) ENGINE = MergeTree() ORDER BY id  "},{"title":"Related Settings and Functions​","type":1,"pageTitle":"LowCardinality Data Type","url":"docs/en/sql-reference/data-types/lowcardinality#related-settings-and-functions","content":"Settings: low_cardinality_max_dictionary_sizelow_cardinality_use_single_dictionary_for_partlow_cardinality_allow_in_native_formatallow_suspicious_low_cardinality_typesoutput_format_arrow_low_cardinality_as_dictionary Functions: toLowCardinality "},{"title":"See Also​","type":1,"pageTitle":"LowCardinality Data Type","url":"docs/en/sql-reference/data-types/lowcardinality#see-also","content":"Reducing ClickHouse Storage Cost with the Low Cardinality Type – Lessons from an Instana Engineer.String Optimization (video presentation in Russian). Slides in English. "},{"title":"Fixedstring","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/fixedstring","content":"Fixedstring A fixed-length string of N bytes (neither characters nor code points). To declare a column of FixedString type, use the following syntax: &lt;column_name&gt; FixedString(N) Where N is a natural number. The FixedString type is efficient when data has the length of precisely N bytes. In all other cases, it is likely to reduce efficiency. Examples of the values that can be efficiently stored in FixedString-typed columns: The binary representation of IP addresses (FixedString(16) for IPv6).Language codes (ru_RU, en_US … ).Currency codes (USD, RUB … ).Binary representation of hashes (FixedString(16) for MD5, FixedString(32) for SHA256). To store UUID values, use the UUID data type. When inserting the data, ClickHouse: Complements a string with null bytes if the string contains fewer than N bytes.Throws the Too large value for FixedString(N) exception if the string contains more than N bytes. When selecting the data, ClickHouse does not remove the null bytes at the end of the string. If you use the WHERE clause, you should add null bytes manually to match the FixedString value. The following example illustrates how to use the WHERE clause with FixedString. Let’s consider the following table with the single FixedString(2) column: ┌─name──┐ │ b │ └───────┘ The query SELECT * FROM FixedStringTable WHERE a = 'b' does not return any data as a result. We should complement the filter pattern with null bytes. SELECT * FROM FixedStringTable WHERE a = 'b\\0' ┌─a─┐ │ b │ └───┘ This behaviour differs from MySQL for the CHAR type (where strings are padded with spaces, and the spaces are removed for output). Note that the length of the FixedString(N) value is constant. The length function returns N even if the FixedString(N) value is filled only with null bytes, but the empty function returns 1 in this case. Original article","keywords":""},{"title":"Nested","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/nested-data-structures/nested","content":"","keywords":""},{"title":"Nested(name1 Type1, Name2 Type2, …)​","type":1,"pageTitle":"Nested","url":"docs/en/sql-reference/data-types/nested-data-structures/nested#nestedname1-type1-name2-type2","content":"A nested data structure is like a table inside a cell. The parameters of a nested data structure – the column names and types – are specified the same way as in a CREATE TABLE query. Each table row can correspond to any number of rows in a nested data structure. Example: CREATE TABLE test.visits ( CounterID UInt32, StartDate Date, Sign Int8, IsNew UInt8, VisitID UInt64, UserID UInt64, ... Goals Nested ( ID UInt32, Serial UInt32, EventTime DateTime, Price Int64, OrderID String, CurrencyID UInt32 ), ... ) ENGINE = CollapsingMergeTree(StartDate, intHash32(UserID), (CounterID, StartDate, intHash32(UserID), VisitID), 8192, Sign)  This example declares the Goals nested data structure, which contains data about conversions (goals reached). Each row in the ‘visits’ table can correspond to zero or any number of conversions. When flatten_nested is set to 0 (which is not by default), arbitrary levels of nesting are supported. In most cases, when working with a nested data structure, its columns are specified with column names separated by a dot. These columns make up an array of matching types. All the column arrays of a single nested data structure have the same length. Example: SELECT Goals.ID, Goals.EventTime FROM test.visits WHERE CounterID = 101500 AND length(Goals.ID) &lt; 5 LIMIT 10  ┌─Goals.ID───────────────────────┬─Goals.EventTime───────────────────────────────────────────────────────────────────────────┐ │ [1073752,591325,591325] │ ['2014-03-17 16:38:10','2014-03-17 16:38:48','2014-03-17 16:42:27'] │ │ [1073752] │ ['2014-03-17 00:28:25'] │ │ [1073752] │ ['2014-03-17 10:46:20'] │ │ [1073752,591325,591325,591325] │ ['2014-03-17 13:59:20','2014-03-17 22:17:55','2014-03-17 22:18:07','2014-03-17 22:18:51'] │ │ [] │ [] │ │ [1073752,591325,591325] │ ['2014-03-17 11:37:06','2014-03-17 14:07:47','2014-03-17 14:36:21'] │ │ [] │ [] │ │ [] │ [] │ │ [591325,1073752] │ ['2014-03-17 00:46:05','2014-03-17 00:46:05'] │ │ [1073752,591325,591325,591325] │ ['2014-03-17 13:28:33','2014-03-17 13:30:26','2014-03-17 18:51:21','2014-03-17 18:51:45'] │ └────────────────────────────────┴───────────────────────────────────────────────────────────────────────────────────────────┘  It is easiest to think of a nested data structure as a set of multiple column arrays of the same length. The only place where a SELECT query can specify the name of an entire nested data structure instead of individual columns is the ARRAY JOIN clause. For more information, see “ARRAY JOIN clause”. Example: SELECT Goal.ID, Goal.EventTime FROM test.visits ARRAY JOIN Goals AS Goal WHERE CounterID = 101500 AND length(Goals.ID) &lt; 5 LIMIT 10  ┌─Goal.ID─┬──────Goal.EventTime─┐ │ 1073752 │ 2014-03-17 16:38:10 │ │ 591325 │ 2014-03-17 16:38:48 │ │ 591325 │ 2014-03-17 16:42:27 │ │ 1073752 │ 2014-03-17 00:28:25 │ │ 1073752 │ 2014-03-17 10:46:20 │ │ 1073752 │ 2014-03-17 13:59:20 │ │ 591325 │ 2014-03-17 22:17:55 │ │ 591325 │ 2014-03-17 22:18:07 │ │ 591325 │ 2014-03-17 22:18:51 │ │ 1073752 │ 2014-03-17 11:37:06 │ └─────────┴─────────────────────┘  You can’t perform SELECT for an entire nested data structure. You can only explicitly list individual columns that are part of it. For an INSERT query, you should pass all the component column arrays of a nested data structure separately (as if they were individual column arrays). During insertion, the system checks that they have the same length. For a DESCRIBE query, the columns in a nested data structure are listed separately in the same way. The ALTER query for elements in a nested data structure has limitations. Original article "},{"title":"Float32, Float64","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/float","content":"","keywords":""},{"title":"Using Floating-point Numbers​","type":1,"pageTitle":"Float32, Float64","url":"docs/en/sql-reference/data-types/float#using-floating-point-numbers","content":"Computations with floating-point numbers might produce a rounding error. SELECT 1 - 0.9  ┌───────minus(1, 0.9)─┐ │ 0.09999999999999998 │ └─────────────────────┘  The result of the calculation depends on the calculation method (the processor type and architecture of the computer system).Floating-point calculations might result in numbers such as infinity (Inf) and “not-a-number” (NaN). This should be taken into account when processing the results of calculations.When parsing floating-point numbers from text, the result might not be the nearest machine-representable number. "},{"title":"NaN and Inf​","type":1,"pageTitle":"Float32, Float64","url":"docs/en/sql-reference/data-types/float#data_type-float-nan-inf","content":"In contrast to standard SQL, ClickHouse supports the following categories of floating-point numbers: Inf – Infinity. SELECT 0.5 / 0  ┌─divide(0.5, 0)─┐ │ inf │ └────────────────┘  -Inf — Negative infinity. SELECT -0.5 / 0  ┌─divide(-0.5, 0)─┐ │ -inf │ └─────────────────┘  NaN — Not a number. SELECT 0 / 0  ┌─divide(0, 0)─┐ │ nan │ └──────────────┘  See the rules for NaN sorting in the section ORDER BY clause. Original article "},{"title":"Nullable(typename)","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/nullable","content":"","keywords":""},{"title":"Storage Features​","type":1,"pageTitle":"Nullable(typename)","url":"docs/en/sql-reference/data-types/nullable#storage-features","content":"To store Nullable type values in a table column, ClickHouse uses a separate file with NULL masks in addition to normal file with values. Entries in masks file allow ClickHouse to distinguish between NULL and a default value of corresponding data type for each table row. Because of an additional file, Nullable column consumes additional storage space compared to a similar normal one. note Using Nullable almost always negatively affects performance, keep this in mind when designing your databases. "},{"title":"Finding NULL​","type":1,"pageTitle":"Nullable(typename)","url":"docs/en/sql-reference/data-types/nullable#finding-null","content":"It is possible to find NULL values in a column by using null subcolumn without reading the whole column. It returns 1 if the corresponding value is NULL and 0 otherwise. Example Query: CREATE TABLE nullable (`n` Nullable(UInt32)) ENGINE = MergeTree ORDER BY tuple(); INSERT INTO nullable VALUES (1) (NULL) (2) (NULL); SELECT n.null FROM nullable;  Result: ┌─n.null─┐ │ 0 │ │ 1 │ │ 0 │ │ 1 │ └────────┘  "},{"title":"Usage Example​","type":1,"pageTitle":"Nullable(typename)","url":"docs/en/sql-reference/data-types/nullable#usage-example","content":"CREATE TABLE t_null(x Int8, y Nullable(Int8)) ENGINE TinyLog  INSERT INTO t_null VALUES (1, NULL), (2, 3)  SELECT x + y FROM t_null  ┌─plus(x, y)─┐ │ ᴺᵁᴸᴸ │ │ 5 │ └────────────┘  Original article "},{"title":"Special Data Types","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/special-data-types/","content":"Special Data Types Special data type values can’t be serialized for saving in a table or output in query results, but can be used as an intermediate result during query execution. Original article","keywords":""},{"title":"ipv4","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/domains/ipv4","content":"","keywords":""},{"title":"IPv4​","type":1,"pageTitle":"ipv4","url":"docs/en/sql-reference/data-types/domains/ipv4#ipv4","content":"IPv4 is a domain based on UInt32 type and serves as a typed replacement for storing IPv4 values. It provides compact storage with the human-friendly input-output format and column type information on inspection. "},{"title":"Basic Usage​","type":1,"pageTitle":"ipv4","url":"docs/en/sql-reference/data-types/domains/ipv4#basic-usage","content":"CREATE TABLE hits (url String, from IPv4) ENGINE = MergeTree() ORDER BY url; DESCRIBE TABLE hits;  ┌─name─┬─type───┬─default_type─┬─default_expression─┬─comment─┬─codec_expression─┐ │ url │ String │ │ │ │ │ │ from │ IPv4 │ │ │ │ │ └──────┴────────┴──────────────┴────────────────────┴─────────┴──────────────────┘  OR you can use IPv4 domain as a key: CREATE TABLE hits (url String, from IPv4) ENGINE = MergeTree() ORDER BY from;  IPv4 domain supports custom input format as IPv4-strings: INSERT INTO hits (url, from) VALUES ('https://wikipedia.org', '116.253.40.133')('https://clickhouse.com', '183.247.232.58')('https://clickhouse.com/docs/en/', '116.106.34.242'); SELECT * FROM hits;  ┌─url────────────────────────────────┬───────────from─┐ │ https://clickhouse.com/docs/en/ │ 116.106.34.242 │ │ https://wikipedia.org │ 116.253.40.133 │ │ https://clickhouse.com │ 183.247.232.58 │ └────────────────────────────────────┴────────────────┘  Values are stored in compact binary form: SELECT toTypeName(from), hex(from) FROM hits LIMIT 1;  ┌─toTypeName(from)─┬─hex(from)─┐ │ IPv4 │ B7F7E83A │ └──────────────────┴───────────┘  Domain values are not implicitly convertible to types other than UInt32. If you want to convert IPv4 value to a string, you have to do that explicitly with IPv4NumToString() function: SELECT toTypeName(s), IPv4NumToString(from) as s FROM hits LIMIT 1;  ┌─toTypeName(IPv4NumToString(from))─┬─s──────────────┐ │ String │ 183.247.232.58 │ └───────────────────────────────────┴────────────────┘  Or cast to a UInt32 value: SELECT toTypeName(i), CAST(from as UInt32) as i FROM hits LIMIT 1;  ┌─toTypeName(CAST(from, 'UInt32'))─┬──────────i─┐ │ UInt32 │ 3086477370 │ └──────────────────────────────────┴────────────┘  Original article "},{"title":"Expression","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/special-data-types/expression","content":"Expression Expressions are used for representing lambdas in high-order functions. Original article","keywords":""},{"title":"Map(key, value)","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/map","content":"","keywords":""},{"title":"Convert Tuple to Map Type​","type":1,"pageTitle":"Map(key, value)","url":"docs/en/sql-reference/data-types/map#map-and-tuple","content":"You can cast Tuple() as Map() using CAST function: SELECT CAST(([1, 2, 3], ['Ready', 'Steady', 'Go']), 'Map(UInt8, String)') AS map;  ┌─map───────────────────────────┐ │ {1:'Ready',2:'Steady',3:'Go'} │ └───────────────────────────────┘  "},{"title":"Map.keys and Map.values Subcolumns​","type":1,"pageTitle":"Map(key, value)","url":"docs/en/sql-reference/data-types/map#map-subcolumns","content":"To optimize Map column processing, in some cases you can use the keys and values subcolumns instead of reading the whole column. Example Query: CREATE TABLE t_map (`a` Map(String, UInt64)) ENGINE = Memory; INSERT INTO t_map VALUES (map('key1', 1, 'key2', 2, 'key3', 3)); SELECT a.keys FROM t_map; SELECT a.values FROM t_map;  Result: ┌─a.keys─────────────────┐ │ ['key1','key2','key3'] │ └────────────────────────┘ ┌─a.values─┐ │ [1,2,3] │ └──────────┘  See Also map() functionCAST() function Original article "},{"title":"Set","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/special-data-types/set","content":"Set Used for the right half of an IN expression. Original article","keywords":""},{"title":"SimpleAggregateFunction","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/simpleaggregatefunction","content":"SimpleAggregateFunction SimpleAggregateFunction(name, types_of_arguments…) data type stores current value of the aggregate function, and does not store its full state as AggregateFunction does. This optimization can be applied to functions for which the following property holds: the result of applying a function f to a row set S1 UNION ALL S2 can be obtained by applying f to parts of the row set separately, and then again applying f to the results: f(S1 UNION ALL S2) = f(f(S1) UNION ALL f(S2)). This property guarantees that partial aggregation results are enough to compute the combined one, so we do not have to store and process any extra data. The common way to produce an aggregate function value is by calling the aggregate function with the -SimpleState suffix. The following aggregate functions are supported: anyanyLastminmaxsumsumWithOverflowgroupBitAndgroupBitOrgroupBitXorgroupArrayArraygroupUniqArrayArraysumMapminMapmaxMap note Values of the SimpleAggregateFunction(func, Type) look and stored the same way as Type, so you do not need to apply functions with -Merge/-State suffixes. SimpleAggregateFunction has better performance than AggregateFunction with same aggregation function. Parameters Name of the aggregate function.Types of the aggregate function arguments. Example CREATE TABLE simple (id UInt64, val SimpleAggregateFunction(sum, Double)) ENGINE=AggregatingMergeTree ORDER BY id; Original article","keywords":""},{"title":"Interval","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/special-data-types/interval","content":"","keywords":""},{"title":"Usage Remarks​","type":1,"pageTitle":"Interval","url":"docs/en/sql-reference/data-types/special-data-types/interval#data-type-interval-usage-remarks","content":"You can use Interval-type values in arithmetical operations with Date and DateTime-type values. For example, you can add 4 days to the current time: SELECT now() as current_date_time, current_date_time + INTERVAL 4 DAY  ┌───current_date_time─┬─plus(now(), toIntervalDay(4))─┐ │ 2019-10-23 10:58:45 │ 2019-10-27 10:58:45 │ └─────────────────────┴───────────────────────────────┘  Intervals with different types can’t be combined. You can’t use intervals like 4 DAY 1 HOUR. Specify intervals in units that are smaller or equal to the smallest unit of the interval, for example, the interval 1 day and an hour interval can be expressed as 25 HOUR or 90000 SECOND. You can’t perform arithmetical operations with Interval-type values, but you can add intervals of different types consequently to values in Date or DateTime data types. For example: SELECT now() AS current_date_time, current_date_time + INTERVAL 4 DAY + INTERVAL 3 HOUR  ┌───current_date_time─┬─plus(plus(now(), toIntervalDay(4)), toIntervalHour(3))─┐ │ 2019-10-23 11:16:28 │ 2019-10-27 14:16:28 │ └─────────────────────┴────────────────────────────────────────────────────────┘  The following query causes an exception: select now() AS current_date_time, current_date_time + (INTERVAL 4 DAY + INTERVAL 3 HOUR)  Received exception from server (version 19.14.1): Code: 43. DB::Exception: Received from localhost:9000. DB::Exception: Wrong argument types for function plus: if one argument is Interval, then another must be Date or DateTime..  "},{"title":"See Also​","type":1,"pageTitle":"Interval","url":"docs/en/sql-reference/data-types/special-data-types/interval#see-also","content":"INTERVAL operatortoInterval type conversion functions "},{"title":"ipv6","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/domains/ipv6","content":"","keywords":""},{"title":"IPv6​","type":1,"pageTitle":"ipv6","url":"docs/en/sql-reference/data-types/domains/ipv6#ipv6","content":"IPv6 is a domain based on FixedString(16) type and serves as a typed replacement for storing IPv6 values. It provides compact storage with the human-friendly input-output format and column type information on inspection. "},{"title":"Basic Usage​","type":1,"pageTitle":"ipv6","url":"docs/en/sql-reference/data-types/domains/ipv6#basic-usage","content":"CREATE TABLE hits (url String, from IPv6) ENGINE = MergeTree() ORDER BY url; DESCRIBE TABLE hits;  ┌─name─┬─type───┬─default_type─┬─default_expression─┬─comment─┬─codec_expression─┐ │ url │ String │ │ │ │ │ │ from │ IPv6 │ │ │ │ │ └──────┴────────┴──────────────┴────────────────────┴─────────┴──────────────────┘  OR you can use IPv6 domain as a key: CREATE TABLE hits (url String, from IPv6) ENGINE = MergeTree() ORDER BY from;  IPv6 domain supports custom input as IPv6-strings: INSERT INTO hits (url, from) VALUES ('https://wikipedia.org', '2a02:aa08:e000:3100::2')('https://clickhouse.com', '2001:44c8:129:2632:33:0:252:2')('https://clickhouse.com/docs/en/', '2a02:e980:1e::1'); SELECT * FROM hits;  ┌─url────────────────────────────────┬─from──────────────────────────┐ │ https://clickhouse.com │ 2001:44c8:129:2632:33:0:252:2 │ │ https://clickhouse.com/docs/en/ │ 2a02:e980:1e::1 │ │ https://wikipedia.org │ 2a02:aa08:e000:3100::2 │ └────────────────────────────────────┴───────────────────────────────┘  Values are stored in compact binary form: SELECT toTypeName(from), hex(from) FROM hits LIMIT 1;  ┌─toTypeName(from)─┬─hex(from)────────────────────────┐ │ IPv6 │ 200144C8012926320033000002520002 │ └──────────────────┴──────────────────────────────────┘  Domain values are not implicitly convertible to types other than FixedString(16). If you want to convert IPv6 value to a string, you have to do that explicitly with IPv6NumToString() function: SELECT toTypeName(s), IPv6NumToString(from) as s FROM hits LIMIT 1;  ┌─toTypeName(IPv6NumToString(from))─┬─s─────────────────────────────┐ │ String │ 2001:44c8:129:2632:33:0:252:2 │ └───────────────────────────────────┴───────────────────────────────┘  Or cast to a FixedString(16) value: SELECT toTypeName(i), CAST(from as FixedString(16)) as i FROM hits LIMIT 1;  ┌─toTypeName(CAST(from, 'FixedString(16)'))─┬─i───────┐ │ FixedString(16) │ ��� │ └───────────────────────────────────────────┴─────────┘  Original article "},{"title":"Multiword Types","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/multiword-types","content":"","keywords":""},{"title":"Multiword Types Support​","type":1,"pageTitle":"Multiword Types","url":"docs/en/sql-reference/data-types/multiword-types#multiword-types-support","content":"Multiword types\tSimple typesDOUBLE PRECISION\tFloat64 CHAR LARGE OBJECT\tString CHAR VARYING\tString CHARACTER LARGE OBJECT\tString CHARACTER VARYING\tString NCHAR LARGE OBJECT\tString NCHAR VARYING\tString NATIONAL CHARACTER LARGE OBJECT\tString NATIONAL CHARACTER VARYING\tString NATIONAL CHAR VARYING\tString NATIONAL CHARACTER\tString NATIONAL CHAR\tString BINARY LARGE OBJECT\tString BINARY VARYING\tString Original article "},{"title":"UUID","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/uuid","content":"","keywords":""},{"title":"How to Generate​","type":1,"pageTitle":"UUID","url":"docs/en/sql-reference/data-types/uuid#how-to-generate","content":"To generate the UUID value, ClickHouse provides the generateUUIDv4 function. "},{"title":"Usage Example​","type":1,"pageTitle":"UUID","url":"docs/en/sql-reference/data-types/uuid#usage-example","content":"Example 1 This example demonstrates creating a table with the UUID type column and inserting a value into the table. CREATE TABLE t_uuid (x UUID, y String) ENGINE=TinyLog  INSERT INTO t_uuid SELECT generateUUIDv4(), 'Example 1'  SELECT * FROM t_uuid  ┌────────────────────────────────────x─┬─y─────────┐ │ 417ddc5d-e556-4d27-95dd-a34d84e46a50 │ Example 1 │ └──────────────────────────────────────┴───────────┘  Example 2 In this example, the UUID column value is not specified when inserting a new record. INSERT INTO t_uuid (y) VALUES ('Example 2')  SELECT * FROM t_uuid  ┌────────────────────────────────────x─┬─y─────────┐ │ 417ddc5d-e556-4d27-95dd-a34d84e46a50 │ Example 1 │ │ 00000000-0000-0000-0000-000000000000 │ Example 2 │ └──────────────────────────────────────┴───────────┘  "},{"title":"Restrictions​","type":1,"pageTitle":"UUID","url":"docs/en/sql-reference/data-types/uuid#restrictions","content":"The UUID data type only supports functions which String data type also supports (for example, min, max, and count). The UUID data type is not supported by arithmetic operations (for example, abs) or aggregate functions, such as sum and avg. Original article "},{"title":"Nested Data Structures","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/nested-data-structures/","content":"Nested Data Structures Original article","keywords":""},{"title":"String","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/string","content":"","keywords":""},{"title":"Encodings​","type":1,"pageTitle":"String","url":"docs/en/sql-reference/data-types/string#encodings","content":"ClickHouse does not have the concept of encodings. Strings can contain an arbitrary set of bytes, which are stored and output as-is. If you need to store texts, we recommend using UTF-8 encoding. At the very least, if your terminal uses UTF-8 (as recommended), you can read and write your values without making conversions. Similarly, certain functions for working with strings have separate variations that work under the assumption that the string contains a set of bytes representing a UTF-8 encoded text. For example, the length function calculates the string length in bytes, while the lengthUTF8 function calculates the string length in Unicode code points, assuming that the value is UTF-8 encoded. Original article "},{"title":"Configuring an External Dictionary","type":0,"sectionRef":"#","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict","content":"Configuring an External Dictionary If dictionary is configured using xml file, than dictionary configuration has the following structure: &lt;dictionary&gt; &lt;name&gt;dict_name&lt;/name&gt; &lt;structure&gt; &lt;!-- Complex key configuration --&gt; &lt;/structure&gt; &lt;source&gt; &lt;!-- Source configuration --&gt; &lt;/source&gt; &lt;layout&gt; &lt;!-- Memory layout configuration --&gt; &lt;/layout&gt; &lt;lifetime&gt; &lt;!-- Lifetime of dictionary in memory --&gt; &lt;/lifetime&gt; &lt;/dictionary&gt; Corresponding DDL-query has the following structure: CREATE DICTIONARY dict_name ( ... -- attributes ) PRIMARY KEY ... -- complex or single key configuration SOURCE(...) -- Source configuration LAYOUT(...) -- Memory layout configuration LIFETIME(...) -- Lifetime of dictionary in memory name – The identifier that can be used to access the dictionary. Use the characters [a-zA-Z0-9_\\-].source — Source of the dictionary.layout — Dictionary layout in memory.structure — Structure of the dictionary . A key and attributes that can be retrieved by this key.lifetime — Frequency of dictionary updates.","keywords":""},{"title":"Dictionaries","type":0,"sectionRef":"#","url":"docs/en/sql-reference/dictionaries/","content":"Dictionaries A dictionary is a mapping (key -&gt; attributes) that is convenient for various types of reference lists. ClickHouse supports special functions for working with dictionaries that can be used in queries. It is easier and more efficient to use dictionaries with functions than a JOIN with reference tables. ClickHouse supports: Built-in dictionaries with a specific set of functions.Plug-in (external) dictionaries with a set of functions.","keywords":""},{"title":"Tuple(t1, T2, …)","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/tuple","content":"","keywords":""},{"title":"Creating a Tuple​","type":1,"pageTitle":"Tuple(t1, T2, …)","url":"docs/en/sql-reference/data-types/tuple#creating-a-tuple","content":"You can use a function to create a tuple: tuple(T1, T2, ...)  Example of creating a tuple: SELECT tuple(1,'a') AS x, toTypeName(x)  ┌─x───────┬─toTypeName(tuple(1, 'a'))─┐ │ (1,'a') │ Tuple(UInt8, String) │ └─────────┴───────────────────────────┘  "},{"title":"Working with Data Types​","type":1,"pageTitle":"Tuple(t1, T2, …)","url":"docs/en/sql-reference/data-types/tuple#working-with-data-types","content":"When creating a tuple on the fly, ClickHouse automatically detects the type of each argument as the minimum of the types which can store the argument value. If the argument is NULL, the type of the tuple element is Nullable. Example of automatic data type detection: SELECT tuple(1, NULL) AS x, toTypeName(x)  ┌─x────────┬─toTypeName(tuple(1, NULL))──────┐ │ (1,NULL) │ Tuple(UInt8, Nullable(Nothing)) │ └──────────┴─────────────────────────────────┘  "},{"title":"Addressing Tuple Elements​","type":1,"pageTitle":"Tuple(t1, T2, …)","url":"docs/en/sql-reference/data-types/tuple#addressing-tuple-elements","content":"It is possible to read elements of named tuples using indexes and names: CREATE TABLE named_tuples (`a` Tuple(s String, i Int64)) ENGINE = Memory; INSERT INTO named_tuples VALUES (('y', 10)), (('x',-10)); SELECT a.s FROM named_tuples; SELECT a.2 FROM named_tuples;  Result: ┌─a.s─┐ │ y │ │ x │ └─────┘ ┌─tupleElement(a, 2)─┐ │ 10 │ │ -10 │ └────────────────────┘  Original article "},{"title":"External Dictionaries","type":0,"sectionRef":"#","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts","content":"","keywords":""},{"title":"See Also​","type":1,"pageTitle":"External Dictionaries","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts#ext-dicts-see-also","content":"Configuring an External DictionaryStoring Dictionaries in MemoryDictionary UpdatesSources of External DictionariesDictionary Key and FieldsFunctions for Working with External Dictionaries "},{"title":"Polygon dictionaries","type":0,"sectionRef":"#","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-polygon","content":"Polygon dictionaries Polygon dictionaries allow you to efficiently search for the polygon containing specified points. For example: defining a city area by geographical coordinates. Example of a polygon dictionary configuration: &lt;dictionary&gt; &lt;structure&gt; &lt;key&gt; &lt;name&gt;key&lt;/name&gt; &lt;type&gt;Array(Array(Array(Array(Float64))))&lt;/type&gt; &lt;/key&gt; &lt;attribute&gt; &lt;name&gt;name&lt;/name&gt; &lt;type&gt;String&lt;/type&gt; &lt;null_value&gt;&lt;/null_value&gt; &lt;/attribute&gt; &lt;attribute&gt; &lt;name&gt;value&lt;/name&gt; &lt;type&gt;UInt64&lt;/type&gt; &lt;null_value&gt;0&lt;/null_value&gt; &lt;/attribute&gt; &lt;/structure&gt; &lt;layout&gt; &lt;polygon&gt; &lt;store_polygon_key_column&gt;1&lt;/store_polygon_key_column&gt; &lt;/polygon&gt; &lt;/layout&gt; ... &lt;/dictionary&gt; The corresponding DDL-query: CREATE DICTIONARY polygon_dict_name ( key Array(Array(Array(Array(Float64)))), name String, value UInt64 ) PRIMARY KEY key LAYOUT(POLYGON(STORE_POLYGON_KEY_COLUMN 1)) ... When configuring the polygon dictionary, the key must have one of two types: A simple polygon. It is an array of points.MultiPolygon. It is an array of polygons. Each polygon is a two-dimensional array of points. The first element of this array is the outer boundary of the polygon, and subsequent elements specify areas to be excluded from it. Points can be specified as an array or a tuple of their coordinates. In the current implementation, only two-dimensional points are supported. The user can upload their own data in all formats supported by ClickHouse. There are 3 types of in-memory storage available: POLYGON_SIMPLE. This is a naive implementation, where a linear pass through all polygons is made for each query, and membership is checked for each one without using additional indexes. POLYGON_INDEX_EACH. A separate index is built for each polygon, which allows you to quickly check whether it belongs in most cases (optimized for geographical regions). Also, a grid is superimposed on the area under consideration, which significantly narrows the number of polygons under consideration. The grid is created by recursively dividing the cell into 16 equal parts and is configured with two parameters. The division stops when the recursion depth reaches MAX_DEPTH or when the cell crosses no more than MIN_INTERSECTIONS polygons. To respond to the query, there is a corresponding cell, and the index for the polygons stored in it is accessed alternately. POLYGON_INDEX_CELL. This placement also creates the grid described above. The same options are available. For each sheet cell, an index is built on all pieces of polygons that fall into it, which allows you to quickly respond to a request. POLYGON. Synonym to POLYGON_INDEX_CELL. Dictionary queries are carried out using standard functions for working with external dictionaries. An important difference is that here the keys will be the points for which you want to find the polygon containing them. Example Example of working with the dictionary defined above: CREATE TABLE points ( x Float64, y Float64 ) ... SELECT tuple(x, y) AS key, dictGet(dict_name, 'name', key), dictGet(dict_name, 'value', key) FROM points ORDER BY x, y; As a result of executing the last command for each point in the 'points' table, a minimum area polygon containing this point will be found, and the requested attributes will be output. Example You can read columns from polygon dictionaries via SELECT query, just turn on the store_polygon_key_column = 1 in the dictionary configuration or corresponding DDL-query. Query: CREATE TABLE polygons_test_table ( key Array(Array(Array(Tuple(Float64, Float64)))), name String ) ENGINE = TinyLog; INSERT INTO polygons_test_table VALUES ([[[(3, 1), (0, 1), (0, -1), (3, -1)]]], 'Value'); CREATE DICTIONARY polygons_test_dictionary ( key Array(Array(Array(Tuple(Float64, Float64)))), name String ) PRIMARY KEY key SOURCE(CLICKHOUSE(TABLE 'polygons_test_table')) LAYOUT(POLYGON(STORE_POLYGON_KEY_COLUMN 1)) LIFETIME(0); SELECT * FROM polygons_test_dictionary; Result: ┌─key─────────────────────────────┬─name──┐ │ [[[(3,1),(0,1),(0,-1),(3,-1)]]] │ Value │ └─────────────────────────────────┴───────┘ ","keywords":""},{"title":"Dictionary Updates","type":0,"sectionRef":"#","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-lifetime","content":"Dictionary Updates ClickHouse periodically updates the dictionaries. The update interval for fully downloaded dictionaries and the invalidation interval for cached dictionaries are defined in the lifetime tag in seconds. Dictionary updates (other than loading for first use) do not block queries. During updates, the old version of a dictionary is used. If an error occurs during an update, the error is written to the server log, and queries continue using the old version of dictionaries. Example of settings: &lt;dictionary&gt; ... &lt;lifetime&gt;300&lt;/lifetime&gt; ... &lt;/dictionary&gt; or CREATE DICTIONARY (...) ... LIFETIME(300) ... Setting &lt;lifetime&gt;0&lt;/lifetime&gt; (LIFETIME(0)) prevents dictionaries from updating. You can set a time interval for updates, and ClickHouse will choose a uniformly random time within this range. This is necessary in order to distribute the load on the dictionary source when updating on a large number of servers. Example of settings: &lt;dictionary&gt; ... &lt;lifetime&gt; &lt;min&gt;300&lt;/min&gt; &lt;max&gt;360&lt;/max&gt; &lt;/lifetime&gt; ... &lt;/dictionary&gt; or LIFETIME(MIN 300 MAX 360) If &lt;min&gt;0&lt;/min&gt; and &lt;max&gt;0&lt;/max&gt;, ClickHouse does not reload the dictionary by timeout. In this case, ClickHouse can reload the dictionary earlier if the dictionary configuration file was changed or the SYSTEM RELOAD DICTIONARY command was executed. When updating the dictionaries, the ClickHouse server applies different logic depending on the type of source: For a text file, it checks the time of modification. If the time differs from the previously recorded time, the dictionary is updated.For MySQL source, the time of modification is checked using a SHOW TABLE STATUS query (in case of MySQL 8 you need to disable meta-information caching in MySQL by set global information_schema_stats_expiry=0).Dictionaries from other sources are updated every time by default. For other sources (ODBC, PostgreSQL, ClickHouse, etc), you can set up a query that will update the dictionaries only if they really changed, rather than each time. To do this, follow these steps: The dictionary table must have a field that always changes when the source data is updated.The settings of the source must specify a query that retrieves the changing field. The ClickHouse server interprets the query result as a row, and if this row has changed relative to its previous state, the dictionary is updated. Specify the query in the &lt;invalidate_query&gt; field in the settings for the source. Example of settings: &lt;dictionary&gt; ... &lt;odbc&gt; ... &lt;invalidate_query&gt;SELECT update_time FROM dictionary_source where id = 1&lt;/invalidate_query&gt; &lt;/odbc&gt; ... &lt;/dictionary&gt; or ... SOURCE(ODBC(... invalidate_query 'SELECT update_time FROM dictionary_source where id = 1')) ... For Cache, ComplexKeyCache, SSDCache, and SSDComplexKeyCache dictionaries both synchronious and asynchronious updates are supported. It is also possible for Flat, Hashed, ComplexKeyHashed dictionaries to only request data that was changed after the previous update. If update_field is specified as part of the dictionary source configuration, value of the previous update time in seconds will be added to the data request. Depends on source type (Executable, HTTP, MySQL, PostgreSQL, ClickHouse, or ODBC) different logic will be applied to update_field before request data from an external source. If the source is HTTP then update_field will be added as a query parameter with the last update time as the parameter value.If the source is Executable then update_field will be added as an executable script argument with the last update time as the argument value.If the source is ClickHouse, MySQL, PostgreSQL, ODBC there will be an additional part of WHERE, where update_field is compared as greater or equal with the last update time. If update_field option is set, additional option update_lag can be set. Value of update_lag option is subtracted from previous update time before request updated data. Example of settings: &lt;dictionary&gt; ... &lt;clickhouse&gt; ... &lt;update_field&gt;added_time&lt;/update_field&gt; &lt;update_lag&gt;15&lt;/update_lag&gt; &lt;/clickhouse&gt; ... &lt;/dictionary&gt; or ... SOURCE(CLICKHOUSE(... update_field 'added_time' update_lag 15)) ... ","keywords":""},{"title":"Distributed DDL Queries (ON CLUSTER Clause)","type":0,"sectionRef":"#","url":"docs/en/sql-reference/distributed-ddl","content":"Distributed DDL Queries (ON CLUSTER Clause) By default the CREATE, DROP, ALTER, and RENAME queries affect only the current server where they are executed. In a cluster setup, it is possible to run such queries in a distributed manner with the ON CLUSTER clause. For example, the following query creates the all_hits Distributed table on each host in cluster: CREATE TABLE IF NOT EXISTS all_hits ON CLUSTER cluster (p Date, i Int32) ENGINE = Distributed(cluster, default, hits) In order to run these queries correctly, each host must have the same cluster definition (to simplify syncing configs, you can use substitutions from ZooKeeper). They must also connect to the ZooKeeper servers. The local version of the query will eventually be executed on each host in the cluster, even if some hosts are currently not available. warning The order for executing queries within a single host is guaranteed.","keywords":""},{"title":"Nothing","type":0,"sectionRef":"#","url":"docs/en/sql-reference/data-types/special-data-types/nothing","content":"Nothing The only purpose of this data type is to represent cases where a value is not expected. So you can’t create a Nothing type value. For example, literal NULL has type of Nullable(Nothing). See more about Nullable. The Nothing type can also used to denote empty arrays: SELECT toTypeName(array()) ┌─toTypeName(array())─┐ │ Array(Nothing) │ └─────────────────────┘ Original article","keywords":""},{"title":"Hierarchical Dictionaries","type":0,"sectionRef":"#","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-hierarchical","content":"Hierarchical Dictionaries ClickHouse supports hierarchical dictionaries with a numeric key. Look at the following hierarchical structure: 0 (Common parent) │ ├── 1 (Russia) │ │ │ └── 2 (Moscow) │ │ │ └── 3 (Center) │ └── 4 (Great Britain) │ └── 5 (London) This hierarchy can be expressed as the following dictionary table. region_id\tparent_region\tregion_name1\t0\tRussia 2\t1\tMoscow 3\t2\tCenter 4\t0\tGreat Britain 5\t4\tLondon This table contains a column parent_region that contains the key of the nearest parent for the element. ClickHouse supports the hierarchical property for external dictionary attributes. This property allows you to configure the hierarchical dictionary similar to described above. The dictGetHierarchy function allows you to get the parent chain of an element. For our example, the structure of dictionary can be the following: &lt;dictionary&gt; &lt;structure&gt; &lt;id&gt; &lt;name&gt;region_id&lt;/name&gt; &lt;/id&gt; &lt;attribute&gt; &lt;name&gt;parent_region&lt;/name&gt; &lt;type&gt;UInt64&lt;/type&gt; &lt;null_value&gt;0&lt;/null_value&gt; &lt;hierarchical&gt;true&lt;/hierarchical&gt; &lt;/attribute&gt; &lt;attribute&gt; &lt;name&gt;region_name&lt;/name&gt; &lt;type&gt;String&lt;/type&gt; &lt;null_value&gt;&lt;/null_value&gt; &lt;/attribute&gt; &lt;/structure&gt; &lt;/dictionary&gt; ","keywords":""},{"title":"Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/","content":"","keywords":""},{"title":"Strong Typing​","type":1,"pageTitle":"Functions","url":"docs/en/sql-reference/functions/#strong-typing","content":"In contrast to standard SQL, ClickHouse has strong typing. In other words, it does not make implicit conversions between types. Each function works for a specific set of types. This means that sometimes you need to use type conversion functions. "},{"title":"Common Subexpression Elimination​","type":1,"pageTitle":"Functions","url":"docs/en/sql-reference/functions/#common-subexpression-elimination","content":"All expressions in a query that have the same AST (the same record or same result of syntactic parsing) are considered to have identical values. Such expressions are concatenated and executed once. Identical subqueries are also eliminated this way. "},{"title":"Types of Results​","type":1,"pageTitle":"Functions","url":"docs/en/sql-reference/functions/#types-of-results","content":"All functions return a single return as the result (not several values, and not zero values). The type of result is usually defined only by the types of arguments, not by the values. Exceptions are the tupleElement function (the a.N operator), and the toFixedString function. "},{"title":"Constants​","type":1,"pageTitle":"Functions","url":"docs/en/sql-reference/functions/#constants","content":"For simplicity, certain functions can only work with constants for some arguments. For example, the right argument of the LIKE operator must be a constant. Almost all functions return a constant for constant arguments. The exception is functions that generate random numbers. The ‘now’ function returns different values for queries that were run at different times, but the result is considered a constant, since constancy is only important within a single query. A constant expression is also considered a constant (for example, the right half of the LIKE operator can be constructed from multiple constants). Functions can be implemented in different ways for constant and non-constant arguments (different code is executed). But the results for a constant and for a true column containing only the same value should match each other. "},{"title":"NULL Processing​","type":1,"pageTitle":"Functions","url":"docs/en/sql-reference/functions/#null-processing","content":"Functions have the following behaviors: If at least one of the arguments of the function is NULL, the function result is also NULL.Special behavior that is specified individually in the description of each function. In the ClickHouse source code, these functions have UseDefaultImplementationForNulls=false. "},{"title":"Constancy​","type":1,"pageTitle":"Functions","url":"docs/en/sql-reference/functions/#constancy","content":"Functions can’t change the values of their arguments – any changes are returned as the result. Thus, the result of calculating separate functions does not depend on the order in which the functions are written in the query. "},{"title":"Higher-order functions, -> operator and lambda(params, expr) function​","type":1,"pageTitle":"Functions","url":"docs/en/sql-reference/functions/#higher-order-functions","content":"Higher-order functions can only accept lambda functions as their functional argument. To pass a lambda function to a higher-order function use -&gt; operator. The left side of the arrow has a formal parameter, which is any ID, or multiple formal parameters – any IDs in a tuple. The right side of the arrow has an expression that can use these formal parameters, as well as any table columns. Examples: x -&gt; 2 * x str -&gt; str != Referer  A lambda function that accepts multiple arguments can also be passed to a higher-order function. In this case, the higher-order function is passed several arrays of identical length that these arguments will correspond to. For some functions the first argument (the lambda function) can be omitted. In this case, identical mapping is assumed. "},{"title":"SQL User Defined Functions​","type":1,"pageTitle":"Functions","url":"docs/en/sql-reference/functions/#user-defined-functions","content":"Custom functions from lambda expressions can be created using the CREATE FUNCTION statement. To delete these functions use the DROP FUNCTION statement. "},{"title":"Executable User Defined Functions​","type":1,"pageTitle":"Functions","url":"docs/en/sql-reference/functions/#executable-user-defined-functions","content":"ClickHouse can call any external executable program or script to process data. Describe such functions in a configuration file and add the path of that file to the main configuration in user_defined_executable_functions_config setting. If a wildcard symbol * is used in the path, then all files matching the pattern are loaded. Example: &lt;user_defined_executable_functions_config&gt;*_function.xml&lt;/user_defined_executable_functions_config&gt;  User defined function configurations are searched relative to the path specified in the user_files_path setting. A function configuration contains the following settings: name - a function name.command - script name to execute or command if execute_direct is false.argument - argument description with the type, and optional name of an argument. Each argument is described in a separate setting. Specifying name is necessary if argument names are part of serialization for user defined function format like Native or JSONEachRow. Default argument name value is c + argument_number.format - a format in which arguments are passed to the command.return_type - the type of a returned value.return_name - name of retuned value. Specifying return name is necessary if return name is part of serialization for user defined function format like Native or JSONEachRow. Optional. Default value is result.type - an executable type. If type is set to executable then single command is started. If it is set to executable_pool then a pool of commands is created.max_command_execution_time - maximum execution time in seconds for processing block of data. This setting is valid for executable_pool commands only. Optional. Default value is 10.command_termination_timeout - time in seconds during which a command should finish after its pipe is closed. After that time SIGTERM is sent to the process executing the command. Optional. Default value is 10.command_read_timeout - timeout for reading data from command stdout in milliseconds. Default value 10000. Optional parameter.command_write_timeout - timeout for writing data to command stdin in milliseconds. Default value 10000. Optional parameter.pool_size - the size of a command pool. Optional. Default value is 16.send_chunk_header - controls whether to send row count before sending a chunk of data to process. Optional. Default value is false.execute_direct - If execute_direct = 1, then command will be searched inside user_scripts folder. Additional script arguments can be specified using whitespace separator. Example: script_name arg1 arg2. If execute_direct = 0, command is passed as argument for bin/sh -c. Default value is 1. Optional parameter.lifetime - the reload interval of a function in seconds. If it is set to 0 then the function is not reloaded. Default value is 0. Optional parameter. The command must read arguments from STDIN and must output the result to STDOUT. The command must process arguments iteratively. That is after processing a chunk of arguments it must wait for the next chunk. ExampleCreating test_function using XML configuration. File test_function.xml. &lt;functions&gt; &lt;function&gt; &lt;type&gt;executable&lt;/type&gt; &lt;name&gt;test_function_python&lt;/name&gt; &lt;return_type&gt;String&lt;/return_type&gt; &lt;argument&gt; &lt;type&gt;UInt64&lt;/type&gt; &lt;name&gt;value&lt;/name&gt; &lt;/argument&gt; &lt;format&gt;TabSeparated&lt;/format&gt; &lt;command&gt;test_function.py&lt;/command&gt; &lt;/function&gt; &lt;/functions&gt;  Script file inside user_scripts folder test_function.py. #!/usr/bin/python3 import sys if __name__ == '__main__': for line in sys.stdin: print(&quot;Value &quot; + line, end='') sys.stdout.flush()  Query: SELECT test_function_python(toUInt64(2));  Result: ┌─test_function_python(2)─┐ │ Value 2 │ └─────────────────────────┘  Creating test_function_sum manually specifying execute_direct to 0 using XML configuration. File test_function.xml. &lt;functions&gt; &lt;function&gt; &lt;type&gt;executable&lt;/type&gt; &lt;name&gt;test_function_sum&lt;/name&gt; &lt;return_type&gt;UInt64&lt;/return_type&gt; &lt;argument&gt; &lt;type&gt;UInt64&lt;/type&gt; &lt;name&gt;lhs&lt;/name&gt; &lt;/argument&gt; &lt;argument&gt; &lt;type&gt;UInt64&lt;/type&gt; &lt;name&gt;rhs&lt;/name&gt; &lt;/argument&gt; &lt;format&gt;TabSeparated&lt;/format&gt; &lt;command&gt;cd /; clickhouse-local --input-format TabSeparated --output-format TabSeparated --structure 'x UInt64, y UInt64' --query &quot;SELECT x + y FROM table&quot;&lt;/command&gt; &lt;execute_direct&gt;0&lt;/execute_direct&gt; &lt;/function&gt; &lt;/functions&gt;  Query: SELECT test_function_sum(2, 2);  Result: ┌─test_function_sum(2, 2)─┐ │ 4 │ └─────────────────────────┘  Creating test_function_sum_json with named arguments and format JSONEachRow using XML configuration. File test_function.xml. &lt;function&gt; &lt;type&gt;executable&lt;/type&gt; &lt;name&gt;test_function_sum_json&lt;/name&gt; &lt;return_type&gt;UInt64&lt;/return_type&gt; &lt;return_name&gt;result_name&lt;/return_name&gt; &lt;argument&gt; &lt;type&gt;UInt64&lt;/type&gt; &lt;name&gt;argument_1&lt;/name&gt; &lt;/argument&gt; &lt;argument&gt; &lt;type&gt;UInt64&lt;/type&gt; &lt;name&gt;argument_2&lt;/name&gt; &lt;/argument&gt; &lt;format&gt;JSONEachRow&lt;/format&gt; &lt;command&gt;test_function_sum_json.py&lt;/command&gt; &lt;/function&gt;  Script file inside user_scripts folder test_function_sum_json.py. #!/usr/bin/python3 import sys import json if __name__ == '__main__': for line in sys.stdin: value = json.loads(line) first_arg = int(value['argument_1']) second_arg = int(value['argument_2']) result = {'result_name': first_arg + second_arg} print(json.dumps(result), end='\\n') sys.stdout.flush()  Query: SELECT test_function_sum_json(2, 2);  Result: ┌─test_function_sum_json(2, 2)─┐ │ 4 │ └──────────────────────────────┘  "},{"title":"Error Handling​","type":1,"pageTitle":"Functions","url":"docs/en/sql-reference/functions/#error-handling","content":"Some functions might throw an exception if the data is invalid. In this case, the query is canceled and an error text is returned to the client. For distributed processing, when an exception occurs on one of the servers, the other servers also attempt to abort the query. "},{"title":"Evaluation of Argument Expressions​","type":1,"pageTitle":"Functions","url":"docs/en/sql-reference/functions/#evaluation-of-argument-expressions","content":"In almost all programming languages, one of the arguments might not be evaluated for certain operators. This is usually the operators &amp;&amp;, ||, and ?:. But in ClickHouse, arguments of functions (operators) are always evaluated. This is because entire parts of columns are evaluated at once, instead of calculating each row separately. "},{"title":"Performing Functions for Distributed Query Processing​","type":1,"pageTitle":"Functions","url":"docs/en/sql-reference/functions/#performing-functions-for-distributed-query-processing","content":"For distributed query processing, as many stages of query processing as possible are performed on remote servers, and the rest of the stages (merging intermediate results and everything after that) are performed on the requestor server. This means that functions can be performed on different servers. For example, in the query SELECT f(sum(g(x))) FROM distributed_table GROUP BY h(y), if a distributed_table has at least two shards, the functions ‘g’ and ‘h’ are performed on remote servers, and the function ‘f’ is performed on the requestor server.if a distributed_table has only one shard, all the ‘f’, ‘g’, and ‘h’ functions are performed on this shard’s server. The result of a function usually does not depend on which server it is performed on. However, sometimes this is important. For example, functions that work with dictionaries use the dictionary that exists on the server they are running on. Another example is the hostName function, which returns the name of the server it is running on in order to make GROUP BY by servers in a SELECT query. If a function in a query is performed on the requestor server, but you need to perform it on remote servers, you can wrap it in an ‘any’ aggregate function or add it to a key in GROUP BY. "},{"title":"Internal Dictionaries","type":0,"sectionRef":"#","url":"docs/en/sql-reference/dictionaries/internal-dicts","content":"Internal Dictionaries ClickHouse contains a built-in feature for working with a geobase. This allows you to: Use a region’s ID to get its name in the desired language.Use a region’s ID to get the ID of a city, area, federal district, country, or continent.Check whether a region is part of another region.Get a chain of parent regions. All the functions support “translocality,” the ability to simultaneously use different perspectives on region ownership. For more information, see the section “Functions for working with web analytics dictionaries”. The internal dictionaries are disabled in the default package. To enable them, uncomment the parameters path_to_regions_hierarchy_file and path_to_regions_names_files in the server configuration file. The geobase is loaded from text files. Place the regions_hierarchy*.txt files into the path_to_regions_hierarchy_file directory. This configuration parameter must contain the path to the regions_hierarchy.txt file (the default regional hierarchy), and the other files (regions_hierarchy_ua.txt) must be located in the same directory. Put the regions_names_*.txt files in the path_to_regions_names_files directory. You can also create these files yourself. The file format is as follows: regions_hierarchy*.txt: TabSeparated (no header), columns: region ID (UInt32)parent region ID (UInt32)region type (UInt8): 1 - continent, 3 - country, 4 - federal district, 5 - region, 6 - city; other types do not have valuespopulation (UInt32) — optional column regions_names_*.txt: TabSeparated (no header), columns: region ID (UInt32)region name (String) — Can’t contain tabs or line feeds, even escaped ones. A flat array is used for storing in RAM. For this reason, IDs shouldn’t be more than a million. Dictionaries can be updated without restarting the server. However, the set of available dictionaries is not updated. For updates, the file modification times are checked. If a file has changed, the dictionary is updated. The interval to check for changes is configured in the builtin_dictionaries_reload_interval parameter. Dictionary updates (other than loading at first use) do not block queries. During updates, queries use the old versions of dictionaries. If an error occurs during an update, the error is written to the server log, and queries continue using the old version of dictionaries. We recommend periodically updating the dictionaries with the geobase. During an update, generate new files and write them to a separate location. When everything is ready, rename them to the files used by the server. There are also functions for working with OS identifiers and search engines, but they shouldn’t be used.","keywords":""},{"title":"Storing Dictionaries in Memory","type":0,"sectionRef":"#","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout","content":"","keywords":""},{"title":"Ways to Store Dictionaries in Memory​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#ways-to-store-dictionaries-in-memory","content":"flathashedsparse_hashedcomplex_key_hashedcomplex_key_sparse_hashedhashed_arraycomplex_key_hashed_arrayrange_hashedcomplex_key_range_hashedcachecomplex_key_cachessd_cachecomplex_key_ssd_cachedirectcomplex_key_directip_trie "},{"title":"flat​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#flat","content":"The dictionary is completely stored in memory in the form of flat arrays. How much memory does the dictionary use? The amount is proportional to the size of the largest key (in space used). The dictionary key has the UInt64 type and the value is limited to max_array_size (by default — 500,000). If a larger key is discovered when creating the dictionary, ClickHouse throws an exception and does not create the dictionary. Dictionary flat arrays initial size is controlled by initial_array_size setting (by default — 1024). All types of sources are supported. When updating, data (from a file or from a table) is read in it entirety. This method provides the best performance among all available methods of storing the dictionary. Configuration example: &lt;layout&gt; &lt;flat&gt; &lt;initial_array_size&gt;50000&lt;/initial_array_size&gt; &lt;max_array_size&gt;5000000&lt;/max_array_size&gt; &lt;/flat&gt; &lt;/layout&gt;  or LAYOUT(FLAT(INITIAL_ARRAY_SIZE 50000 MAX_ARRAY_SIZE 5000000))  "},{"title":"hashed​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#dicts-external_dicts_dict_layout-hashed","content":"The dictionary is completely stored in memory in the form of a hash table. The dictionary can contain any number of elements with any identifiers In practice, the number of keys can reach tens of millions of items. If preallocate is true (default is false) the hash table will be preallocated (this will make the dictionary load faster). But note that you should use it only if: The source support an approximate number of elements (for now it is supported only by the ClickHouse source).There are no duplicates in the data (otherwise it may increase memory usage for the hashtable). All types of sources are supported. When updating, data (from a file or from a table) is read in its entirety. Configuration example: &lt;layout&gt; &lt;hashed&gt; &lt;preallocate&gt;0&lt;/preallocate&gt; &lt;/hashed&gt; &lt;/layout&gt;  or LAYOUT(HASHED(PREALLOCATE 0))  "},{"title":"sparse_hashed​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#dicts-external_dicts_dict_layout-sparse_hashed","content":"Similar to hashed, but uses less memory in favor more CPU usage. It will be also preallocated so as hashed (with preallocate set to true), and note that it is even more significant for sparse_hashed. Configuration example: &lt;layout&gt; &lt;sparse_hashed /&gt; &lt;/layout&gt;  or LAYOUT(SPARSE_HASHED([PREALLOCATE 0]))  "},{"title":"complex_key_hashed​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#complex-key-hashed","content":"This type of storage is for use with composite keys. Similar to hashed. Configuration example: &lt;layout&gt; &lt;complex_key_hashed /&gt; &lt;/layout&gt;  or LAYOUT(COMPLEX_KEY_HASHED())  "},{"title":"complex_key_sparse_hashed​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#complex-key-sparse-hashed","content":"This type of storage is for use with composite keys. Similar to sparse_hashed. Configuration example: &lt;layout&gt; &lt;complex_key_sparse_hashed /&gt; &lt;/layout&gt;  or LAYOUT(COMPLEX_KEY_SPARSE_HASHED())  "},{"title":"hashed_array​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#dicts-external_dicts_dict_layout-hashed-array","content":"The dictionary is completely stored in memory. Each attribute is stored in an array. The key attribute is stored in the form of a hashed table where value is an index in the attributes array. The dictionary can contain any number of elements with any identifiers. In practice, the number of keys can reach tens of millions of items. All types of sources are supported. When updating, data (from a file or from a table) is read in its entirety. Configuration example: &lt;layout&gt; &lt;hashed_array&gt; &lt;/hashed_array&gt; &lt;/layout&gt;  or LAYOUT(HASHED_ARRAY())  "},{"title":"complex_key_hashed_array​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#complex-key-hashed-array","content":"This type of storage is for use with composite keys. Similar to hashed_array. Configuration example: &lt;layout&gt; &lt;complex_key_hashed_array /&gt; &lt;/layout&gt;  or LAYOUT(COMPLEX_KEY_HASHED_ARRAY())  "},{"title":"range_hashed​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#range-hashed","content":"The dictionary is stored in memory in the form of a hash table with an ordered array of ranges and their corresponding values. This storage method works the same way as hashed and allows using date/time (arbitrary numeric type) ranges in addition to the key. Example: The table contains discounts for each advertiser in the format: +---------|-------------|-------------|------+ | advertiser id | discount start date | discount end date | amount | +===============+=====================+===================+========+ | 123 | 2015-01-01 | 2015-01-15 | 0.15 | +---------|-------------|-------------|------+ | 123 | 2015-01-16 | 2015-01-31 | 0.25 | +---------|-------------|-------------|------+ | 456 | 2015-01-01 | 2015-01-15 | 0.05 | +---------|-------------|-------------|------+  To use a sample for date ranges, define the range_min and range_max elements in the structure. These elements must contain elements name and type (if type is not specified, the default type will be used - Date). type can be any numeric type (Date / DateTime / UInt64 / Int32 / others). warning Values of range_min and range_max should fit in Int64 type. Example: &lt;structure&gt; &lt;id&gt; &lt;name&gt;Id&lt;/name&gt; &lt;/id&gt; &lt;range_min&gt; &lt;name&gt;first&lt;/name&gt; &lt;type&gt;Date&lt;/type&gt; &lt;/range_min&gt; &lt;range_max&gt; &lt;name&gt;last&lt;/name&gt; &lt;type&gt;Date&lt;/type&gt; &lt;/range_max&gt; ...  or CREATE DICTIONARY somedict ( id UInt64, first Date, last Date ) PRIMARY KEY id LAYOUT(RANGE_HASHED()) RANGE(MIN first MAX last)  To work with these dictionaries, you need to pass an additional argument to the dictGetT function, for which a range is selected: dictGetT('dict_name', 'attr_name', id, date)  This function returns the value for the specified ids and the date range that includes the passed date. Details of the algorithm: If the id is not found or a range is not found for the id, it returns the default value for the dictionary.If there are overlapping ranges, it returns value for any (random) range.If the range delimiter is NULL or an invalid date (such as 1900-01-01), the range is open. The range can be open on both sides. Configuration example: &lt;clickhouse&gt; &lt;dictionary&gt; ... &lt;layout&gt; &lt;range_hashed /&gt; &lt;/layout&gt; &lt;structure&gt; &lt;id&gt; &lt;name&gt;Abcdef&lt;/name&gt; &lt;/id&gt; &lt;range_min&gt; &lt;name&gt;StartTimeStamp&lt;/name&gt; &lt;type&gt;UInt64&lt;/type&gt; &lt;/range_min&gt; &lt;range_max&gt; &lt;name&gt;EndTimeStamp&lt;/name&gt; &lt;type&gt;UInt64&lt;/type&gt; &lt;/range_max&gt; &lt;attribute&gt; &lt;name&gt;XXXType&lt;/name&gt; &lt;type&gt;String&lt;/type&gt; &lt;null_value /&gt; &lt;/attribute&gt; &lt;/structure&gt; &lt;/dictionary&gt; &lt;/clickhouse&gt;  or CREATE DICTIONARY somedict( Abcdef UInt64, StartTimeStamp UInt64, EndTimeStamp UInt64, XXXType String DEFAULT '' ) PRIMARY KEY Abcdef RANGE(MIN StartTimeStamp MAX EndTimeStamp)  "},{"title":"complex_key_range_hashed​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#complex-key-range-hashed","content":"The dictionary is stored in memory in the form of a hash table with an ordered array of ranges and their corresponding values (see range_hashed). This type of storage is for use with composite keys. Configuration example: CREATE DICTIONARY range_dictionary ( CountryID UInt64, CountryKey String, StartDate Date, EndDate Date, Tax Float64 DEFAULT 0.2 ) PRIMARY KEY CountryID, CountryKey SOURCE(CLICKHOUSE(TABLE 'date_table')) LIFETIME(MIN 1 MAX 1000) LAYOUT(COMPLEX_KEY_RANGE_HASHED()) RANGE(MIN StartDate MAX EndDate);  "},{"title":"cache​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#cache","content":"The dictionary is stored in a cache that has a fixed number of cells. These cells contain frequently used elements. When searching for a dictionary, the cache is searched first. For each block of data, all keys that are not found in the cache or are outdated are requested from the source using SELECT attrs... FROM db.table WHERE id IN (k1, k2, ...). The received data is then written to the cache. If keys are not found in dictionary, then update cache task is created and added into update queue. Update queue properties can be controlled with settings max_update_queue_size, update_queue_push_timeout_milliseconds, query_wait_timeout_milliseconds, max_threads_for_updates. For cache dictionaries, the expiration lifetime of data in the cache can be set. If more time than lifetime has passed since loading the data in a cell, the cell’s value is not used and key becomes expired. The key is re-requested the next time it needs to be used. This behaviour can be configured with setting allow_read_expired_keys. This is the least effective of all the ways to store dictionaries. The speed of the cache depends strongly on correct settings and the usage scenario. A cache type dictionary performs well only when the hit rates are high enough (recommended 99% and higher). You can view the average hit rate in the system.dictionaries table. If setting allow_read_expired_keys is set to 1, by default 0. Then dictionary can support asynchronous updates. If a client requests keys and all of them are in cache, but some of them are expired, then dictionary will return expired keys for a client and request them asynchronously from the source. To improve cache performance, use a subquery with LIMIT, and call the function with the dictionary externally. Supported sources: MySQL, ClickHouse, executable, HTTP. Example of settings: &lt;layout&gt; &lt;cache&gt; &lt;!-- The size of the cache, in number of cells. Rounded up to a power of two. --&gt; &lt;size_in_cells&gt;1000000000&lt;/size_in_cells&gt; &lt;!-- Allows to read expired keys. --&gt; &lt;allow_read_expired_keys&gt;0&lt;/allow_read_expired_keys&gt; &lt;!-- Max size of update queue. --&gt; &lt;max_update_queue_size&gt;100000&lt;/max_update_queue_size&gt; &lt;!-- Max timeout in milliseconds for push update task into queue. --&gt; &lt;update_queue_push_timeout_milliseconds&gt;10&lt;/update_queue_push_timeout_milliseconds&gt; &lt;!-- Max wait timeout in milliseconds for update task to complete. --&gt; &lt;query_wait_timeout_milliseconds&gt;60000&lt;/query_wait_timeout_milliseconds&gt; &lt;!-- Max threads for cache dictionary update. --&gt; &lt;max_threads_for_updates&gt;4&lt;/max_threads_for_updates&gt; &lt;/cache&gt; &lt;/layout&gt;  or LAYOUT(CACHE(SIZE_IN_CELLS 1000000000))  Set a large enough cache size. You need to experiment to select the number of cells: Set some value.Run queries until the cache is completely full.Assess memory consumption using the system.dictionaries table.Increase or decrease the number of cells until the required memory consumption is reached. warning Do not use ClickHouse as a source, because it is slow to process queries with random reads. "},{"title":"complex_key_cache​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#complex-key-cache","content":"This type of storage is for use with composite keys. Similar to cache. "},{"title":"ssd_cache​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#ssd-cache","content":"Similar to cache, but stores data on SSD and index in RAM. All cache dictionary settings related to update queue can also be applied to SSD cache dictionaries. &lt;layout&gt; &lt;ssd_cache&gt; &lt;!-- Size of elementary read block in bytes. Recommended to be equal to SSD's page size. --&gt; &lt;block_size&gt;4096&lt;/block_size&gt; &lt;!-- Max cache file size in bytes. --&gt; &lt;file_size&gt;16777216&lt;/file_size&gt; &lt;!-- Size of RAM buffer in bytes for reading elements from SSD. --&gt; &lt;read_buffer_size&gt;131072&lt;/read_buffer_size&gt; &lt;!-- Size of RAM buffer in bytes for aggregating elements before flushing to SSD. --&gt; &lt;write_buffer_size&gt;1048576&lt;/write_buffer_size&gt; &lt;!-- Path where cache file will be stored. --&gt; &lt;path&gt;/var/lib/clickhouse/user_files/test_dict&lt;/path&gt; &lt;/ssd_cache&gt; &lt;/layout&gt;  or LAYOUT(SSD_CACHE(BLOCK_SIZE 4096 FILE_SIZE 16777216 READ_BUFFER_SIZE 1048576 PATH '/var/lib/clickhouse/user_files/test_dict'))  "},{"title":"complex_key_ssd_cache​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#complex-key-ssd-cache","content":"This type of storage is for use with composite keys. Similar to ssd_cache. "},{"title":"direct​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#direct","content":"The dictionary is not stored in memory and directly goes to the source during the processing of a request. The dictionary key has the UInt64 type. All types of sources, except local files, are supported. Configuration example: &lt;layout&gt; &lt;direct /&gt; &lt;/layout&gt;  or LAYOUT(DIRECT())  "},{"title":"complex_key_direct​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#complex-key-direct","content":"This type of storage is for use with composite keys. Similar to direct. "},{"title":"ip_trie​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#ip-trie","content":"This type of storage is for mapping network prefixes (IP addresses) to metadata such as ASN. Example: The table contains network prefixes and their corresponding AS number and country code:  +-----------|-----|------+ | prefix | asn | cca2 | +=================+=======+========+ | 202.79.32.0/20 | 17501 | NP | +-----------|-----|------+ | 2620:0:870::/48 | 3856 | US | +-----------|-----|------+ | 2a02:6b8:1::/48 | 13238 | RU | +-----------|-----|------+ | 2001:db8::/32 | 65536 | ZZ | +-----------|-----|------+  When using this type of layout, the structure must have a composite key. Example: &lt;structure&gt; &lt;key&gt; &lt;attribute&gt; &lt;name&gt;prefix&lt;/name&gt; &lt;type&gt;String&lt;/type&gt; &lt;/attribute&gt; &lt;/key&gt; &lt;attribute&gt; &lt;name&gt;asn&lt;/name&gt; &lt;type&gt;UInt32&lt;/type&gt; &lt;null_value /&gt; &lt;/attribute&gt; &lt;attribute&gt; &lt;name&gt;cca2&lt;/name&gt; &lt;type&gt;String&lt;/type&gt; &lt;null_value&gt;??&lt;/null_value&gt; &lt;/attribute&gt; ... &lt;/structure&gt; &lt;layout&gt; &lt;ip_trie&gt; &lt;!-- Key attribute `prefix` can be retrieved via dictGetString. --&gt; &lt;!-- This option increases memory usage. --&gt; &lt;access_to_key_from_attributes&gt;true&lt;/access_to_key_from_attributes&gt; &lt;/ip_trie&gt; &lt;/layout&gt;  or CREATE DICTIONARY somedict ( prefix String, asn UInt32, cca2 String DEFAULT '??' ) PRIMARY KEY prefix  The key must have only one String type attribute that contains an allowed IP prefix. Other types are not supported yet. For queries, you must use the same functions (dictGetT with a tuple) as for dictionaries with composite keys: dictGetT('dict_name', 'attr_name', tuple(ip))  The function takes either UInt32 for IPv4, or FixedString(16) for IPv6: dictGetString('prefix', 'asn', tuple(IPv6StringToNum('2001:db8::1')))  Other types are not supported yet. The function returns the attribute for the prefix that corresponds to this IP address. If there are overlapping prefixes, the most specific one is returned. Data must completely fit into RAM. "},{"title":"Dictionary Key and Fields","type":0,"sectionRef":"#","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure","content":"","keywords":""},{"title":"Key​","type":1,"pageTitle":"Dictionary Key and Fields","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure#ext_dict_structure-key","content":"ClickHouse supports the following types of keys: Numeric key. UInt64. Defined in the &lt;id&gt; tag or using PRIMARY KEY keyword.Composite key. Set of values of different types. Defined in the tag &lt;key&gt; or PRIMARY KEY keyword. An xml structure can contain either &lt;id&gt; or &lt;key&gt;. DDL-query must contain single PRIMARY KEY. warning You must not describe key as an attribute. "},{"title":"Numeric Key​","type":1,"pageTitle":"Dictionary Key and Fields","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure#numeric-key","content":"Type: UInt64. Configuration example: &lt;id&gt; &lt;name&gt;Id&lt;/name&gt; &lt;/id&gt;  Configuration fields: name – The name of the column with keys. For DDL-query: CREATE DICTIONARY ( Id UInt64, ... ) PRIMARY KEY Id ...  PRIMARY KEY – The name of the column with keys. "},{"title":"Composite Key​","type":1,"pageTitle":"Dictionary Key and Fields","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure#composite-key","content":"The key can be a tuple from any types of fields. The layout in this case must be complex_key_hashed or complex_key_cache. tip A composite key can consist of a single element. This makes it possible to use a string as the key, for instance. The key structure is set in the element &lt;key&gt;. Key fields are specified in the same format as the dictionary attributes. Example: &lt;structure&gt; &lt;key&gt; &lt;attribute&gt; &lt;name&gt;field1&lt;/name&gt; &lt;type&gt;String&lt;/type&gt; &lt;/attribute&gt; &lt;attribute&gt; &lt;name&gt;field2&lt;/name&gt; &lt;type&gt;UInt32&lt;/type&gt; &lt;/attribute&gt; ... &lt;/key&gt; ...  or CREATE DICTIONARY ( field1 String, field2 String ... ) PRIMARY KEY field1, field2 ...  For a query to the dictGet* function, a tuple is passed as the key. Example: dictGetString('dict_name', 'attr_name', tuple('string for field1', num_for_field2)). "},{"title":"Attributes​","type":1,"pageTitle":"Dictionary Key and Fields","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure#ext_dict_structure-attributes","content":"Configuration example: &lt;structure&gt; ... &lt;attribute&gt; &lt;name&gt;Name&lt;/name&gt; &lt;type&gt;ClickHouseDataType&lt;/type&gt; &lt;null_value&gt;&lt;/null_value&gt; &lt;expression&gt;rand64()&lt;/expression&gt; &lt;hierarchical&gt;true&lt;/hierarchical&gt; &lt;injective&gt;true&lt;/injective&gt; &lt;is_object_id&gt;true&lt;/is_object_id&gt; &lt;/attribute&gt; &lt;/structure&gt;  or CREATE DICTIONARY somename ( Name ClickHouseDataType DEFAULT '' EXPRESSION rand64() HIERARCHICAL INJECTIVE IS_OBJECT_ID )  Configuration fields: Tag\tDescription\tRequiredname\tColumn name.\tYes type\tClickHouse data type: UInt8, UInt16, UInt32, UInt64, Int8, Int16, Int32, Int64, Float32, Float64, UUID, Decimal32, Decimal64, Decimal128, Decimal256,Date, Date32, DateTime, DateTime64, String, Array. ClickHouse tries to cast value from dictionary to the specified data type. For example, for MySQL, the field might be TEXT, VARCHAR, or BLOB in the MySQL source table, but it can be uploaded as String in ClickHouse. Nullable is currently supported for Flat, Hashed, ComplexKeyHashed, Direct, ComplexKeyDirect, RangeHashed, Polygon, Cache, ComplexKeyCache, SSDCache, SSDComplexKeyCache dictionaries. In IPTrie dictionaries Nullable types are not supported.\tYes null_value\tDefault value for a non-existing element. In the example, it is an empty string. NULL value can be used only for the Nullable types (see the previous line with types description).\tYes expression\tExpression that ClickHouse executes on the value. The expression can be a column name in the remote SQL database. Thus, you can use it to create an alias for the remote column. Default value: no expression.\tNo hierarchical\tIf true, the attribute contains the value of a parent key for the current key. See Hierarchical Dictionaries. Default value: false.\tNo injective\tFlag that shows whether the id -&gt; attribute image is injective. If true, ClickHouse can automatically place after the GROUP BY clause the requests to dictionaries with injection. Usually it significantly reduces the amount of such requests. Default value: false.\tNo is_object_id\tFlag that shows whether the query is executed for a MongoDB document by ObjectID. Default value: false.\tNo See Also Functions for working with external dictionaries. "},{"title":"Comparison Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/comparison-functions","content":"","keywords":""},{"title":"equals, a = b and a == b operator​","type":1,"pageTitle":"Comparison Functions","url":"docs/en/sql-reference/functions/comparison-functions#function-equals","content":""},{"title":"notEquals, a != b and a \\<> b operator​","type":1,"pageTitle":"Comparison Functions","url":"docs/en/sql-reference/functions/comparison-functions#function-notequals","content":""},{"title":"less, \\< operator​","type":1,"pageTitle":"Comparison Functions","url":"docs/en/sql-reference/functions/comparison-functions#function-less","content":""},{"title":"greater, > operator​","type":1,"pageTitle":"Comparison Functions","url":"docs/en/sql-reference/functions/comparison-functions#function-greater","content":""},{"title":"lessOrEquals, \\<= operator​","type":1,"pageTitle":"Comparison Functions","url":"docs/en/sql-reference/functions/comparison-functions#function-lessorequals","content":""},{"title":"greaterOrEquals, >= operator​","type":1,"pageTitle":"Comparison Functions","url":"docs/en/sql-reference/functions/comparison-functions#function-greaterorequals","content":""},{"title":"Conditional Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/conditional-functions","content":"","keywords":""},{"title":"if​","type":1,"pageTitle":"Conditional Functions","url":"docs/en/sql-reference/functions/conditional-functions#if","content":"Controls conditional branching. Unlike most systems, ClickHouse always evaluate both expressions then and else. Syntax if(cond, then, else)  If the condition cond evaluates to a non-zero value, returns the result of the expression then, and the result of the expression else, if present, is skipped. If the cond is zero or NULL, then the result of the then expression is skipped and the result of the else expression, if present, is returned. You can use the short_circuit_function_evaluation setting to calculate the if function according to a short scheme. If this setting is enabled, then expression is evaluated only on rows where cond is true, else expression – where cond is false. For example, an exception about division by zero is not thrown when executing the query SELECT if(number = 0, 0, intDiv(42, number)) FROM numbers(10), because intDiv(42, number) will be evaluated only for numbers that doesn't satisfy condition number = 0. Arguments cond – The condition for evaluation that can be zero or not. The type is UInt8, Nullable(UInt8) or NULL.then – The expression to return if condition is met.else – The expression to return if condition is not met. Returned values The function executes then and else expressions and returns its result, depending on whether the condition cond ended up being zero or not. Example Query: SELECT if(1, plus(2, 2), plus(2, 6));  Result: ┌─plus(2, 2)─┐ │ 4 │ └────────────┘  Query: SELECT if(0, plus(2, 2), plus(2, 6));  Result: ┌─plus(2, 6)─┐ │ 8 │ └────────────┘  then and else must have the lowest common type. Example: Take this LEFT_RIGHT table: SELECT * FROM LEFT_RIGHT ┌─left─┬─right─┐ │ ᴺᵁᴸᴸ │ 4 │ │ 1 │ 3 │ │ 2 │ 2 │ │ 3 │ 1 │ │ 4 │ ᴺᵁᴸᴸ │ └──────┴───────┘  The following query compares left and right values: SELECT left, right, if(left &lt; right, 'left is smaller than right', 'right is greater or equal than left') AS is_smaller FROM LEFT_RIGHT WHERE isNotNull(left) AND isNotNull(right) ┌─left─┬─right─┬─is_smaller──────────────────────────┐ │ 1 │ 3 │ left is smaller than right │ │ 2 │ 2 │ right is greater or equal than left │ │ 3 │ 1 │ right is greater or equal than left │ └──────┴───────┴─────────────────────────────────────┘  Note: NULL values are not used in this example, check NULL values in conditionals section. "},{"title":"Ternary Operator​","type":1,"pageTitle":"Conditional Functions","url":"docs/en/sql-reference/functions/conditional-functions#ternary-operator","content":"It works same as if function. Syntax: cond ? then : else Returns then if the cond evaluates to be true (greater than zero), otherwise returns else. cond must be of type of UInt8, and then and else must have the lowest common type. then and else can be NULL See also ifNotFinite. "},{"title":"multiIf​","type":1,"pageTitle":"Conditional Functions","url":"docs/en/sql-reference/functions/conditional-functions#multiif","content":"Allows you to write the CASE operator more compactly in the query. Syntax multiIf(cond_1, then_1, cond_2, then_2, ..., else)  You can use the short_circuit_function_evaluation setting to calculate the multiIf function according to a short scheme. If this setting is enabled, then_i expression is evaluated only on rows where ((NOT cond_1) AND (NOT cond_2) AND ... AND (NOT cond_{i-1}) AND cond_i) is true, cond_i will be evaluated only on rows where ((NOT cond_1) AND (NOT cond_2) AND ... AND (NOT cond_{i-1})) is true. For example, an exception about division by zero is not thrown when executing the query SELECT multiIf(number = 2, intDiv(1, number), number = 5) FROM numbers(10). Arguments cond_N — The condition for the function to return then_N.then_N — The result of the function when executed.else — The result of the function if none of the conditions is met. The function accepts 2N+1 parameters. Returned values The function returns one of the values then_N or else, depending on the conditions cond_N. Example Again using LEFT_RIGHT table. SELECT left, right, multiIf(left &lt; right, 'left is smaller', left &gt; right, 'left is greater', left = right, 'Both equal', 'Null value') AS result FROM LEFT_RIGHT ┌─left─┬─right─┬─result──────────┐ │ ᴺᵁᴸᴸ │ 4 │ Null value │ │ 1 │ 3 │ left is smaller │ │ 2 │ 2 │ Both equal │ │ 3 │ 1 │ left is greater │ │ 4 │ ᴺᵁᴸᴸ │ Null value │ └──────┴───────┴─────────────────┘  "},{"title":"Using Conditional Results Directly​","type":1,"pageTitle":"Conditional Functions","url":"docs/en/sql-reference/functions/conditional-functions#using-conditional-results-directly","content":"Conditionals always result to 0, 1 or NULL. So you can use conditional results directly like this: SELECT left &lt; right AS is_small FROM LEFT_RIGHT ┌─is_small─┐ │ ᴺᵁᴸᴸ │ │ 1 │ │ 0 │ │ 0 │ │ ᴺᵁᴸᴸ │ └──────────┘  "},{"title":"NULL Values in Conditionals​","type":1,"pageTitle":"Conditional Functions","url":"docs/en/sql-reference/functions/conditional-functions#null-values-in-conditionals","content":"When NULL values are involved in conditionals, the result will also be NULL. SELECT NULL &lt; 1, 2 &lt; NULL, NULL &lt; NULL, NULL = NULL ┌─less(NULL, 1)─┬─less(2, NULL)─┬─less(NULL, NULL)─┬─equals(NULL, NULL)─┐ │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ └───────────────┴───────────────┴──────────────────┴────────────────────┘  So you should construct your queries carefully if the types are Nullable. The following example demonstrates this by failing to add equals condition to multiIf. SELECT left, right, multiIf(left &lt; right, 'left is smaller', left &gt; right, 'right is smaller', 'Both equal') AS faulty_result FROM LEFT_RIGHT ┌─left─┬─right─┬─faulty_result────┐ │ ᴺᵁᴸᴸ │ 4 │ Both equal │ │ 1 │ 3 │ left is smaller │ │ 2 │ 2 │ Both equal │ │ 3 │ 1 │ right is smaller │ │ 4 │ ᴺᵁᴸᴸ │ Both equal │ └──────┴───────┴──────────────────┘  "},{"title":"arrayJoin function","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/array-join","content":"arrayJoin function This is a very unusual function. Normal functions do not change a set of rows, but just change the values in each row (map). Aggregate functions compress a set of rows (fold or reduce). The ‘arrayJoin’ function takes each row and generates a set of rows (unfold). This function takes an array as an argument, and propagates the source row to multiple rows for the number of elements in the array. All the values in columns are simply copied, except the values in the column where this function is applied; it is replaced with the corresponding array value. A query can use multiple arrayJoin functions. In this case, the transformation is performed multiple times. Note the ARRAY JOIN syntax in the SELECT query, which provides broader possibilities. Example: SELECT arrayJoin([1, 2, 3] AS src) AS dst, 'Hello', src ┌─dst─┬─\\'Hello\\'─┬─src─────┐ │ 1 │ Hello │ [1,2,3] │ │ 2 │ Hello │ [1,2,3] │ │ 3 │ Hello │ [1,2,3] │ └─────┴───────────┴─────────┘ ","keywords":""},{"title":"Bit Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/bit-functions","content":"","keywords":""},{"title":"bitAnd(a, b)​","type":1,"pageTitle":"Bit Functions","url":"docs/en/sql-reference/functions/bit-functions#bitanda-b","content":""},{"title":"bitOr(a, b)​","type":1,"pageTitle":"Bit Functions","url":"docs/en/sql-reference/functions/bit-functions#bitora-b","content":""},{"title":"bitXor(a, b)​","type":1,"pageTitle":"Bit Functions","url":"docs/en/sql-reference/functions/bit-functions#bitxora-b","content":""},{"title":"bitNot(a)​","type":1,"pageTitle":"Bit Functions","url":"docs/en/sql-reference/functions/bit-functions#bitnota","content":""},{"title":"bitShiftLeft(a, b)​","type":1,"pageTitle":"Bit Functions","url":"docs/en/sql-reference/functions/bit-functions#bitshiftlefta-b","content":"Shifts the binary representation of a value to the left by a specified number of bit positions. A FixedString or a String is treated as a single multibyte value. Bits of a FixedString value are lost as they are shifted out. On the contrary, a String value is extended with additional bytes, so no bits are lost. Syntax bitShiftLeft(a, b)  Arguments a — A value to shift. Integer types, String or FixedString.b — The number of shift positions. Unsigned integer types, 64 bit types or less are allowed. Returned value Shifted value. The type of the returned value is the same as the type of the input value. Example In the following queries bin and hex functions are used to show bits of shifted values. SELECT 99 AS a, bin(a), bitShiftLeft(a, 2) AS a_shifted, bin(a_shifted); SELECT 'abc' AS a, hex(a), bitShiftLeft(a, 4) AS a_shifted, hex(a_shifted); SELECT toFixedString('abc', 3) AS a, hex(a), bitShiftLeft(a, 4) AS a_shifted, hex(a_shifted);  Result: ┌──a─┬─bin(99)──┬─a_shifted─┬─bin(bitShiftLeft(99, 2))─┐ │ 99 │ 01100011 │ 140 │ 10001100 │ └────┴──────────┴───────────┴──────────────────────────┘ ┌─a───┬─hex('abc')─┬─a_shifted─┬─hex(bitShiftLeft('abc', 4))─┐ │ abc │ 616263 │ &amp;0 │ 06162630 │ └─────┴────────────┴───────────┴─────────────────────────────┘ ┌─a───┬─hex(toFixedString('abc', 3))─┬─a_shifted─┬─hex(bitShiftLeft(toFixedString('abc', 3), 4))─┐ │ abc │ 616263 │ &amp;0 │ 162630 │ └─────┴──────────────────────────────┴───────────┴───────────────────────────────────────────────┘  "},{"title":"bitShiftRight(a, b)​","type":1,"pageTitle":"Bit Functions","url":"docs/en/sql-reference/functions/bit-functions#bitshiftrighta-b","content":"Shifts the binary representation of a value to the right by a specified number of bit positions. A FixedString or a String is treated as a single multibyte value. Note that the length of a String value is reduced as bits are shifted out. Syntax bitShiftRight(a, b)  Arguments a — A value to shift. Integer types, String or FixedString.b — The number of shift positions. Unsigned integer types, 64 bit types or less are allowed. Returned value Shifted value. The type of the returned value is the same as the type of the input value. Example Query: SELECT 101 AS a, bin(a), bitShiftRight(a, 2) AS a_shifted, bin(a_shifted); SELECT 'abc' AS a, hex(a), bitShiftRight(a, 12) AS a_shifted, hex(a_shifted); SELECT toFixedString('abc', 3) AS a, hex(a), bitShiftRight(a, 12) AS a_shifted, hex(a_shifted);  Result: ┌───a─┬─bin(101)─┬─a_shifted─┬─bin(bitShiftRight(101, 2))─┐ │ 101 │ 01100101 │ 25 │ 00011001 │ └─────┴──────────┴───────────┴────────────────────────────┘ ┌─a───┬─hex('abc')─┬─a_shifted─┬─hex(bitShiftRight('abc', 12))─┐ │ abc │ 616263 │ │ 0616 │ └─────┴────────────┴───────────┴───────────────────────────────┘ ┌─a───┬─hex(toFixedString('abc', 3))─┬─a_shifted─┬─hex(bitShiftRight(toFixedString('abc', 3), 12))─┐ │ abc │ 616263 │ │ 000616 │ └─────┴──────────────────────────────┴───────────┴─────────────────────────────────────────────────┘  "},{"title":"bitRotateLeft(a, b)​","type":1,"pageTitle":"Bit Functions","url":"docs/en/sql-reference/functions/bit-functions#bitrotatelefta-b","content":""},{"title":"bitRotateRight(a, b)​","type":1,"pageTitle":"Bit Functions","url":"docs/en/sql-reference/functions/bit-functions#bitrotaterighta-b","content":""},{"title":"bitSlice(s, offset, length)​","type":1,"pageTitle":"Bit Functions","url":"docs/en/sql-reference/functions/bit-functions#bitslices-offset-length","content":"Returns a substring starting with the bit from the ‘offset’ index that is ‘length’ bits long. bits indexing starts from 1 Syntax bitSlice(s, offset[, length])  Arguments s — s is Stringor FixedString.offset — The start index with bit, A positive value indicates an offset on the left, and a negative value is an indent on the right. Numbering of the bits begins with 1.length — The length of substring with bit. If you specify a negative value, the function returns an open substring [ offset, array_length - length). If you omit the value, the function returns the substring [offset, the_end_string]. If length exceeds s, it will be truncate.If length isn't multiple of 8, will fill 0 on the right. Returned value The substring. String Example Query: select bin('Hello'), bin(bitSlice('Hello', 1, 8)) select bin('Hello'), bin(bitSlice('Hello', 1, 2)) select bin('Hello'), bin(bitSlice('Hello', 1, 9)) select bin('Hello'), bin(bitSlice('Hello', -4, 8))  Result: ┌─bin('Hello')─────────────────────────────┬─bin(bitSlice('Hello', 1, 8))─┐ │ 0100100001100101011011000110110001101111 │ 01001000 │ └──────────────────────────────────────────┴──────────────────────────────┘ ┌─bin('Hello')─────────────────────────────┬─bin(bitSlice('Hello', 1, 2))─┐ │ 0100100001100101011011000110110001101111 │ 01000000 │ └──────────────────────────────────────────┴──────────────────────────────┘ ┌─bin('Hello')─────────────────────────────┬─bin(bitSlice('Hello', 1, 9))─┐ │ 0100100001100101011011000110110001101111 │ 0100100000000000 │ └──────────────────────────────────────────┴──────────────────────────────┘ ┌─bin('Hello')─────────────────────────────┬─bin(bitSlice('Hello', -4, 8))─┐ │ 0100100001100101011011000110110001101111 │ 11110000 │ └──────────────────────────────────────────┴───────────────────────────────┘  "},{"title":"bitTest​","type":1,"pageTitle":"Bit Functions","url":"docs/en/sql-reference/functions/bit-functions#bittest","content":"Takes any integer and converts it into binary form, returns the value of a bit at specified position. The countdown starts from 0 from the right to the left. Syntax SELECT bitTest(number, index)  Arguments number – Integer number.index – Position of bit. Returned values Returns a value of bit at specified position. Type: UInt8. Example For example, the number 43 in base-2 (binary) numeral system is 101011. Query: SELECT bitTest(43, 1);  Result: ┌─bitTest(43, 1)─┐ │ 1 │ └────────────────┘  Another example: Query: SELECT bitTest(43, 2);  Result: ┌─bitTest(43, 2)─┐ │ 0 │ └────────────────┘  "},{"title":"bitTestAll​","type":1,"pageTitle":"Bit Functions","url":"docs/en/sql-reference/functions/bit-functions#bittestall","content":"Returns result of logical conjuction (AND operator) of all bits at given positions. The countdown starts from 0 from the right to the left. The conjuction for bitwise operations: 0 AND 0 = 0 0 AND 1 = 0 1 AND 0 = 0 1 AND 1 = 1 Syntax SELECT bitTestAll(number, index1, index2, index3, index4, ...)  Arguments number – Integer number.index1, index2, index3, index4 – Positions of bit. For example, for set of positions (index1, index2, index3, index4) is true if and only if all of its positions are true (index1 ⋀ index2, ⋀ index3 ⋀ index4). Returned values Returns result of logical conjuction. Type: UInt8. Example For example, the number 43 in base-2 (binary) numeral system is 101011. Query: SELECT bitTestAll(43, 0, 1, 3, 5);  Result: ┌─bitTestAll(43, 0, 1, 3, 5)─┐ │ 1 │ └────────────────────────────┘  Another example: Query: SELECT bitTestAll(43, 0, 1, 3, 5, 2);  Result: ┌─bitTestAll(43, 0, 1, 3, 5, 2)─┐ │ 0 │ └───────────────────────────────┘  "},{"title":"bitTestAny​","type":1,"pageTitle":"Bit Functions","url":"docs/en/sql-reference/functions/bit-functions#bittestany","content":"Returns result of logical disjunction (OR operator) of all bits at given positions. The countdown starts from 0 from the right to the left. The disjunction for bitwise operations: 0 OR 0 = 0 0 OR 1 = 1 1 OR 0 = 1 1 OR 1 = 1 Syntax SELECT bitTestAny(number, index1, index2, index3, index4, ...)  Arguments number – Integer number.index1, index2, index3, index4 – Positions of bit. Returned values Returns result of logical disjuction. Type: UInt8. Example For example, the number 43 in base-2 (binary) numeral system is 101011. Query: SELECT bitTestAny(43, 0, 2);  Result: ┌─bitTestAny(43, 0, 2)─┐ │ 1 │ └──────────────────────┘  Another example: Query: SELECT bitTestAny(43, 4, 2);  Result: ┌─bitTestAny(43, 4, 2)─┐ │ 0 │ └──────────────────────┘  "},{"title":"bitCount​","type":1,"pageTitle":"Bit Functions","url":"docs/en/sql-reference/functions/bit-functions#bitcount","content":"Calculates the number of bits set to one in the binary representation of a number. Syntax bitCount(x)  Arguments x — Integer or floating-point number. The function uses the value representation in memory. It allows supporting floating-point numbers. Returned value Number of bits set to one in the input number. The function does not convert input value to a larger type (sign extension). So, for example, bitCount(toUInt8(-1)) = 8. Type: UInt8. Example Take for example the number 333. Its binary representation: 0000000101001101. Query: SELECT bitCount(333);  Result: ┌─bitCount(333)─┐ │ 5 │ └───────────────┘  "},{"title":"bitHammingDistance​","type":1,"pageTitle":"Bit Functions","url":"docs/en/sql-reference/functions/bit-functions#bithammingdistance","content":"Returns the Hamming Distance between the bit representations of two integer values. Can be used with SimHash functions for detection of semi-duplicate strings. The smaller is the distance, the more likely those strings are the same. Syntax bitHammingDistance(int1, int2)  Arguments int1 — First integer value. Int64.int2 — Second integer value. Int64. Returned value The Hamming distance. Type: UInt8. Examples Query: SELECT bitHammingDistance(111, 121);  Result: ┌─bitHammingDistance(111, 121)─┐ │ 3 │ └──────────────────────────────┘  With SimHash: SELECT bitHammingDistance(ngramSimHash('cat ate rat'), ngramSimHash('rat ate cat'));  Result: ┌─bitHammingDistance(ngramSimHash('cat ate rat'), ngramSimHash('rat ate cat'))─┐ │ 5 │ └──────────────────────────────────────────────────────────────────────────────┘  "},{"title":"Bitmap Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/bitmap-functions","content":"","keywords":""},{"title":"bitmapBuild​","type":1,"pageTitle":"Bitmap Functions","url":"docs/en/sql-reference/functions/bitmap-functions#bitmap_functions-bitmapbuild","content":"Build a bitmap from unsigned integer array. bitmapBuild(array)  Arguments array – Unsigned integer array. Example SELECT bitmapBuild([1, 2, 3, 4, 5]) AS res, toTypeName(res);  ┌─res─┬─toTypeName(bitmapBuild([1, 2, 3, 4, 5]))─────┐ │ │ AggregateFunction(groupBitmap, UInt8) │ └─────┴──────────────────────────────────────────────┘  "},{"title":"bitmapToArray​","type":1,"pageTitle":"Bitmap Functions","url":"docs/en/sql-reference/functions/bitmap-functions#bitmaptoarray","content":"Convert bitmap to integer array. bitmapToArray(bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapToArray(bitmapBuild([1, 2, 3, 4, 5])) AS res;  ┌─res─────────┐ │ [1,2,3,4,5] │ └─────────────┘  "},{"title":"bitmapSubsetInRange​","type":1,"pageTitle":"Bitmap Functions","url":"docs/en/sql-reference/functions/bitmap-functions#bitmap-functions-bitmapsubsetinrange","content":"Return subset in specified range (not include the range_end). bitmapSubsetInRange(bitmap, range_start, range_end)  Arguments bitmap – Bitmap object.range_start – Range start point. Type: UInt32.range_end – Range end point (excluded). Type: UInt32. Example SELECT bitmapToArray(bitmapSubsetInRange(bitmapBuild([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,100,200,500]), toUInt32(30), toUInt32(200))) AS res;  ┌─res───────────────┐ │ [30,31,32,33,100] │ └───────────────────┘  "},{"title":"bitmapSubsetLimit​","type":1,"pageTitle":"Bitmap Functions","url":"docs/en/sql-reference/functions/bitmap-functions#bitmapsubsetlimit","content":"Creates a subset of bitmap with n elements taken between range_start and cardinality_limit. Syntax bitmapSubsetLimit(bitmap, range_start, cardinality_limit)  Arguments bitmap – Bitmap object.range_start – The subset starting point. Type: UInt32.cardinality_limit – The subset cardinality upper limit. Type: UInt32. Returned value The subset. Type: Bitmap object. Example Query: SELECT bitmapToArray(bitmapSubsetLimit(bitmapBuild([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,100,200,500]), toUInt32(30), toUInt32(200))) AS res;  Result: ┌─res───────────────────────┐ │ [30,31,32,33,100,200,500] │ └───────────────────────────┘  "},{"title":"subBitmap​","type":1,"pageTitle":"Bitmap Functions","url":"docs/en/sql-reference/functions/bitmap-functions#subbitmap","content":"Returns the bitmap elements, starting from the offset position. The number of returned elements is limited by the cardinality_limit parameter. Analog of the substring) string function, but for bitmap. Syntax subBitmap(bitmap, offset, cardinality_limit)  Arguments bitmap – The bitmap. Type: Bitmap object.offset – The position of the first element of the subset. Type: UInt32.cardinality_limit – The maximum number of elements in the subset. Type: UInt32. Returned value The subset. Type: Bitmap object. Example Query: SELECT bitmapToArray(subBitmap(bitmapBuild([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,100,200,500]), toUInt32(10), toUInt32(10))) AS res;  Result: ┌─res─────────────────────────────┐ │ [10,11,12,13,14,15,16,17,18,19] │ └─────────────────────────────────┘  "},{"title":"bitmapContains​","type":1,"pageTitle":"Bitmap Functions","url":"docs/en/sql-reference/functions/bitmap-functions#bitmap_functions-bitmapcontains","content":"Checks whether the bitmap contains an element. bitmapContains(haystack, needle)  Arguments haystack – Bitmap object, where the function searches.needle – Value that the function searches. Type: UInt32. Returned values 0 — If haystack does not contain needle.1 — If haystack contains needle. Type: UInt8. Example SELECT bitmapContains(bitmapBuild([1,5,7,9]), toUInt32(9)) AS res;  ┌─res─┐ │ 1 │ └─────┘  "},{"title":"bitmapHasAny​","type":1,"pageTitle":"Bitmap Functions","url":"docs/en/sql-reference/functions/bitmap-functions#bitmaphasany","content":"Checks whether two bitmaps have intersection by some elements. bitmapHasAny(bitmap1, bitmap2)  If you are sure that bitmap2 contains strictly one element, consider using the bitmapContains function. It works more efficiently. Arguments bitmap* – Bitmap object. Return values 1, if bitmap1 and bitmap2 have one similar element at least.0, otherwise. Example SELECT bitmapHasAny(bitmapBuild([1,2,3]),bitmapBuild([3,4,5])) AS res;  ┌─res─┐ │ 1 │ └─────┘  "},{"title":"bitmapHasAll​","type":1,"pageTitle":"Bitmap Functions","url":"docs/en/sql-reference/functions/bitmap-functions#bitmaphasall","content":"Analogous to hasAll(array, array) returns 1 if the first bitmap contains all the elements of the second one, 0 otherwise. If the second argument is an empty bitmap then returns 1. bitmapHasAll(bitmap,bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapHasAll(bitmapBuild([1,2,3]),bitmapBuild([3,4,5])) AS res;  ┌─res─┐ │ 0 │ └─────┘  "},{"title":"bitmapCardinality​","type":1,"pageTitle":"Bitmap Functions","url":"docs/en/sql-reference/functions/bitmap-functions#bitmapcardinality","content":"Retrun bitmap cardinality of type UInt64. bitmapCardinality(bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapCardinality(bitmapBuild([1, 2, 3, 4, 5])) AS res;  ┌─res─┐ │ 5 │ └─────┘  "},{"title":"bitmapMin​","type":1,"pageTitle":"Bitmap Functions","url":"docs/en/sql-reference/functions/bitmap-functions#bitmapmin","content":"Retrun the smallest value of type UInt64 in the set, UINT32_MAX if the set is empty. bitmapMin(bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapMin(bitmapBuild([1, 2, 3, 4, 5])) AS res;   ┌─res─┐ │ 1 │ └─────┘  "},{"title":"bitmapMax​","type":1,"pageTitle":"Bitmap Functions","url":"docs/en/sql-reference/functions/bitmap-functions#bitmapmax","content":"Retrun the greatest value of type UInt64 in the set, 0 if the set is empty. bitmapMax(bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapMax(bitmapBuild([1, 2, 3, 4, 5])) AS res;   ┌─res─┐ │ 5 │ └─────┘  "},{"title":"bitmapTransform​","type":1,"pageTitle":"Bitmap Functions","url":"docs/en/sql-reference/functions/bitmap-functions#bitmaptransform","content":"Transform an array of values in a bitmap to another array of values, the result is a new bitmap. bitmapTransform(bitmap, from_array, to_array)  Arguments bitmap – Bitmap object.from_array – UInt32 array. For idx in range [0, from_array.size()), if bitmap contains from_array[idx], then replace it with to_array[idx]. Note that the result depends on array ordering if there are common elements between from_array and to_array.to_array – UInt32 array, its size shall be the same to from_array. Example SELECT bitmapToArray(bitmapTransform(bitmapBuild([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]), cast([5,999,2] as Array(UInt32)), cast([2,888,20] as Array(UInt32)))) AS res;   ┌─res───────────────────┐ │ [1,3,4,6,7,8,9,10,20] │ └───────────────────────┘  "},{"title":"bitmapAnd​","type":1,"pageTitle":"Bitmap Functions","url":"docs/en/sql-reference/functions/bitmap-functions#bitmapand","content":"Two bitmap and calculation, the result is a new bitmap. bitmapAnd(bitmap,bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapToArray(bitmapAnd(bitmapBuild([1,2,3]),bitmapBuild([3,4,5]))) AS res;  ┌─res─┐ │ [3] │ └─────┘  "},{"title":"bitmapOr​","type":1,"pageTitle":"Bitmap Functions","url":"docs/en/sql-reference/functions/bitmap-functions#bitmapor","content":"Two bitmap or calculation, the result is a new bitmap. bitmapOr(bitmap,bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapToArray(bitmapOr(bitmapBuild([1,2,3]),bitmapBuild([3,4,5]))) AS res;  ┌─res─────────┐ │ [1,2,3,4,5] │ └─────────────┘  "},{"title":"bitmapXor​","type":1,"pageTitle":"Bitmap Functions","url":"docs/en/sql-reference/functions/bitmap-functions#bitmapxor","content":"Two bitmap xor calculation, the result is a new bitmap. bitmapXor(bitmap,bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapToArray(bitmapXor(bitmapBuild([1,2,3]),bitmapBuild([3,4,5]))) AS res;  ┌─res───────┐ │ [1,2,4,5] │ └───────────┘  "},{"title":"bitmapAndnot​","type":1,"pageTitle":"Bitmap Functions","url":"docs/en/sql-reference/functions/bitmap-functions#bitmapandnot","content":"Two bitmap andnot calculation, the result is a new bitmap. bitmapAndnot(bitmap,bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapToArray(bitmapAndnot(bitmapBuild([1,2,3]),bitmapBuild([3,4,5]))) AS res;  ┌─res───┐ │ [1,2] │ └───────┘  "},{"title":"bitmapAndCardinality​","type":1,"pageTitle":"Bitmap Functions","url":"docs/en/sql-reference/functions/bitmap-functions#bitmapandcardinality","content":"Two bitmap and calculation, return cardinality of type UInt64. bitmapAndCardinality(bitmap,bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapAndCardinality(bitmapBuild([1,2,3]),bitmapBuild([3,4,5])) AS res;  ┌─res─┐ │ 1 │ └─────┘  "},{"title":"bitmapOrCardinality​","type":1,"pageTitle":"Bitmap Functions","url":"docs/en/sql-reference/functions/bitmap-functions#bitmaporcardinality","content":"Two bitmap or calculation, return cardinality of type UInt64. bitmapOrCardinality(bitmap,bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapOrCardinality(bitmapBuild([1,2,3]),bitmapBuild([3,4,5])) AS res;  ┌─res─┐ │ 5 │ └─────┘  "},{"title":"bitmapXorCardinality​","type":1,"pageTitle":"Bitmap Functions","url":"docs/en/sql-reference/functions/bitmap-functions#bitmapxorcardinality","content":"Two bitmap xor calculation, return cardinality of type UInt64. bitmapXorCardinality(bitmap,bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapXorCardinality(bitmapBuild([1,2,3]),bitmapBuild([3,4,5])) AS res;  ┌─res─┐ │ 4 │ └─────┘  "},{"title":"bitmapAndnotCardinality​","type":1,"pageTitle":"Bitmap Functions","url":"docs/en/sql-reference/functions/bitmap-functions#bitmapandnotcardinality","content":"Two bitmap andnot calculation, return cardinality of type UInt64. bitmapAndnotCardinality(bitmap,bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapAndnotCardinality(bitmapBuild([1,2,3]),bitmapBuild([3,4,5])) AS res;  ┌─res─┐ │ 2 │ └─────┘  "},{"title":"Arithmetic Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/arithmetic-functions","content":"","keywords":""},{"title":"plus(a, b), a + b operator​","type":1,"pageTitle":"Arithmetic Functions","url":"docs/en/sql-reference/functions/arithmetic-functions#plusa-b-a-b-operator","content":"Calculates the sum of the numbers. You can also add integer numbers with a date or date and time. In the case of a date, adding an integer means adding the corresponding number of days. For a date with time, it means adding the corresponding number of seconds. "},{"title":"minus(a, b), a - b operator​","type":1,"pageTitle":"Arithmetic Functions","url":"docs/en/sql-reference/functions/arithmetic-functions#minusa-b-a-b-operator","content":"Calculates the difference. The result is always signed. You can also calculate integer numbers from a date or date with time. The idea is the same – see above for ‘plus’. "},{"title":"multiply(a, b), a * b operator​","type":1,"pageTitle":"Arithmetic Functions","url":"docs/en/sql-reference/functions/arithmetic-functions#multiplya-b-a-b-operator","content":"Calculates the product of the numbers. "},{"title":"divide(a, b), a / b operator​","type":1,"pageTitle":"Arithmetic Functions","url":"docs/en/sql-reference/functions/arithmetic-functions#dividea-b-a-b-operator","content":"Calculates the quotient of the numbers. The result type is always a floating-point type. It is not integer division. For integer division, use the ‘intDiv’ function. When dividing by zero you get ‘inf’, ‘-inf’, or ‘nan’. "},{"title":"intDiv(a, b)​","type":1,"pageTitle":"Arithmetic Functions","url":"docs/en/sql-reference/functions/arithmetic-functions#intdiva-b","content":"Calculates the quotient of the numbers. Divides into integers, rounding down (by the absolute value). An exception is thrown when dividing by zero or when dividing a minimal negative number by minus one. "},{"title":"intDivOrZero(a, b)​","type":1,"pageTitle":"Arithmetic Functions","url":"docs/en/sql-reference/functions/arithmetic-functions#intdivorzeroa-b","content":"Differs from ‘intDiv’ in that it returns zero when dividing by zero or when dividing a minimal negative number by minus one. "},{"title":"modulo(a, b), a % b operator​","type":1,"pageTitle":"Arithmetic Functions","url":"docs/en/sql-reference/functions/arithmetic-functions#modulo","content":"Calculates the remainder after division. If arguments are floating-point numbers, they are pre-converted to integers by dropping the decimal portion. The remainder is taken in the same sense as in C++. Truncated division is used for negative numbers. An exception is thrown when dividing by zero or when dividing a minimal negative number by minus one. "},{"title":"moduloOrZero(a, b)​","type":1,"pageTitle":"Arithmetic Functions","url":"docs/en/sql-reference/functions/arithmetic-functions#modulo-or-zero","content":"Differs from modulo in that it returns zero when the divisor is zero. "},{"title":"negate(a), -a operator​","type":1,"pageTitle":"Arithmetic Functions","url":"docs/en/sql-reference/functions/arithmetic-functions#negatea-a-operator","content":"Calculates a number with the reverse sign. The result is always signed. "},{"title":"abs(a)​","type":1,"pageTitle":"Arithmetic Functions","url":"docs/en/sql-reference/functions/arithmetic-functions#arithm_func-abs","content":"Calculates the absolute value of the number (a). That is, if a \\&lt; 0, it returns -a. For unsigned types it does not do anything. For signed integer types, it returns an unsigned number. "},{"title":"gcd(a, b)​","type":1,"pageTitle":"Arithmetic Functions","url":"docs/en/sql-reference/functions/arithmetic-functions#gcda-b","content":"Returns the greatest common divisor of the numbers. An exception is thrown when dividing by zero or when dividing a minimal negative number by minus one. "},{"title":"lcm(a, b)​","type":1,"pageTitle":"Arithmetic Functions","url":"docs/en/sql-reference/functions/arithmetic-functions#lcma-b","content":"Returns the least common multiple of the numbers. An exception is thrown when dividing by zero or when dividing a minimal negative number by minus one. "},{"title":"max2​","type":1,"pageTitle":"Arithmetic Functions","url":"docs/en/sql-reference/functions/arithmetic-functions#max2","content":"Compares two values and returns the maximum. The returned value is converted to Float64. Syntax max2(value1, value2)  Arguments value1 — First value. Int/UInt or Float.value2 — Second value. Int/UInt or Float. Returned value The maximum of two values. Type: Float. Example Query: SELECT max2(-1, 2);  Result: ┌─max2(-1, 2)─┐ │ 2 │ └─────────────┘  "},{"title":"min2​","type":1,"pageTitle":"Arithmetic Functions","url":"docs/en/sql-reference/functions/arithmetic-functions#min2","content":"Compares two values and returns the minimum. The returned value is converted to Float64. Syntax max2(value1, value2)  Arguments value1 — First value. Int/UInt or Float.value2 — Second value. Int/UInt or Float. Returned value The minimum of two values. Type: Float. Example Query: SELECT min2(-1, 2);  Result: ┌─min2(-1, 2)─┐ │ -1 │ └─────────────┘  "},{"title":"Sources of External Dictionaries","type":0,"sectionRef":"#","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources","content":"","keywords":""},{"title":"Local File​","type":1,"pageTitle":"Sources of External Dictionaries","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#dicts-external_dicts_dict_sources-local_file","content":"Example of settings: &lt;source&gt; &lt;file&gt; &lt;path&gt;/opt/dictionaries/os.tsv&lt;/path&gt; &lt;format&gt;TabSeparated&lt;/format&gt; &lt;/file&gt; &lt;/source&gt;  or SOURCE(FILE(path './user_files/os.tsv' format 'TabSeparated'))  Setting fields: path – The absolute path to the file.format – The file format. All the formats described in Formats are supported. When dictionary with source FILE is created via DDL command (CREATE DICTIONARY ...), the source file needs to be located in user_files directory, to prevent DB users accessing arbitrary file on ClickHouse node. See Also Dictionary function "},{"title":"Executable File​","type":1,"pageTitle":"Sources of External Dictionaries","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#dicts-external_dicts_dict_sources-executable","content":"Working with executable files depends on how the dictionary is stored in memory. If the dictionary is stored using cache and complex_key_cache, ClickHouse requests the necessary keys by sending a request to the executable file’s STDIN. Otherwise, ClickHouse starts executable file and treats its output as dictionary data. Example of settings: &lt;source&gt; &lt;executable&gt; &lt;command&gt;cat /opt/dictionaries/os.tsv&lt;/command&gt; &lt;format&gt;TabSeparated&lt;/format&gt; &lt;implicit_key&gt;false&lt;/implicit_key&gt; &lt;/executable&gt; &lt;/source&gt;  Setting fields: command — The absolute path to the executable file, or the file name (if the program directory is written to PATH).format — The file format. All the formats described in Formats are supported.command_termination_timeout — executable script should contain main read-write loop. After dictionary is destroyed, pipe is closed, and executable file will have command_termination_timeout seconds to shutdown, before ClickHouse will send SIGTERM signal to child process. Specified in seconds. Default value is 10. Optional parameter.command_read_timeout - timeout for reading data from command stdout in milliseconds. Default value 10000. Optional parameter.command_write_timeout - timeout for writing data to command stdin in milliseconds. Default value 10000. Optional parameter.implicit_key — The executable source file can return only values, and the correspondence to the requested keys is determined implicitly — by the order of rows in the result. Default value is false.execute_direct - If execute_direct = 1, then command will be searched inside user_scripts folder. Additional script arguments can be specified using whitespace separator. Example: script_name arg1 arg2. If execute_direct = 0, command is passed as argument for bin/sh -c. Default value is 0. Optional parameter.send_chunk_header - controls whether to send row count before sending a chunk of data to process. Optional. Default value is false. That dictionary source can be configured only via XML configuration. Creating dictionaries with executable source via DDL is disabled, otherwise, the DB user would be able to execute arbitrary binary on ClickHouse node. "},{"title":"Executable Pool​","type":1,"pageTitle":"Sources of External Dictionaries","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#dicts-external_dicts_dict_sources-executable_pool","content":"Executable pool allows loading data from pool of processes. This source does not work with dictionary layouts that need to load all data from source. Executable pool works if the dictionary is stored using cache, complex_key_cache, ssd_cache, complex_key_ssd_cache, direct, complex_key_direct layouts. Executable pool will spawn pool of processes with specified command and keep them running until they exit. The program should read data from STDIN while it is available and output result to STDOUT, and it can wait for next block of data on STDIN. ClickHouse will not close STDIN after processing a block of data but will pipe another chunk of data when needed. The executable script should be ready for this way of data processing — it should poll STDIN and flush data to STDOUT early. Example of settings: &lt;source&gt; &lt;executable_pool&gt; &lt;command&gt;&lt;command&gt;while read key; do printf &quot;$key\\tData for key $key\\n&quot;; done&lt;/command&lt;/command&gt; &lt;format&gt;TabSeparated&lt;/format&gt; &lt;pool_size&gt;10&lt;/pool_size&gt; &lt;max_command_execution_time&gt;10&lt;max_command_execution_time&gt; &lt;implicit_key&gt;false&lt;/implicit_key&gt; &lt;/executable_pool&gt; &lt;/source&gt;  Setting fields: command — The absolute path to the executable file, or the file name (if the program directory is written to PATH).format — The file format. All the formats described in “Formats” are supported.pool_size — Size of pool. If 0 is specified as pool_size then there is no pool size restrictions. Default value is 16.command_termination_timeout — executable script should contain main read-write loop. After dictionary is destroyed, pipe is closed, and executable file will have command_termination_timeout seconds to shutdown, before ClickHouse will send SIGTERM signal to child process. Specified in seconds. Default value is 10. Optional parameter.max_command_execution_time — Maximum executable script command execution time for processing block of data. Specified in seconds. Default value is 10. Optional parameter.command_read_timeout - timeout for reading data from command stdout in milliseconds. Default value 10000. Optional parameter.command_write_timeout - timeout for writing data to command stdin in milliseconds. Default value 10000. Optional parameter.implicit_key — The executable source file can return only values, and the correspondence to the requested keys is determined implicitly — by the order of rows in the result. Default value is false. Optional parameter.execute_direct - If execute_direct = 1, then command will be searched inside user_scripts folder. Additional script arguments can be specified using whitespace separator. Example: script_name arg1 arg2. If execute_direct = 0, command is passed as argument for bin/sh -c. Default value is 1. Optional parameter.send_chunk_header - controls whether to send row count before sending a chunk of data to process. Optional. Default value is false. That dictionary source can be configured only via XML configuration. Creating dictionaries with executable source via DDL is disabled, otherwise, the DB user would be able to execute arbitrary binary on ClickHouse node. "},{"title":"Http(s)​","type":1,"pageTitle":"Sources of External Dictionaries","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#dicts-external_dicts_dict_sources-http","content":"Working with an HTTP(s) server depends on how the dictionary is stored in memory. If the dictionary is stored using cache and complex_key_cache, ClickHouse requests the necessary keys by sending a request via the POST method. Example of settings: &lt;source&gt; &lt;http&gt; &lt;url&gt;http://[::1]/os.tsv&lt;/url&gt; &lt;format&gt;TabSeparated&lt;/format&gt; &lt;credentials&gt; &lt;user&gt;user&lt;/user&gt; &lt;password&gt;password&lt;/password&gt; &lt;/credentials&gt; &lt;headers&gt; &lt;header&gt; &lt;name&gt;API-KEY&lt;/name&gt; &lt;value&gt;key&lt;/value&gt; &lt;/header&gt; &lt;/headers&gt; &lt;/http&gt; &lt;/source&gt;  or SOURCE(HTTP( url 'http://[::1]/os.tsv' format 'TabSeparated' credentials(user 'user' password 'password') headers(header(name 'API-KEY' value 'key')) ))  In order for ClickHouse to access an HTTPS resource, you must configure openSSL in the server configuration. Setting fields: url – The source URL.format – The file format. All the formats described in “Formats” are supported.credentials – Basic HTTP authentication. Optional parameter.user – Username required for the authentication.password – Password required for the authentication.headers – All custom HTTP headers entries used for the HTTP request. Optional parameter.header – Single HTTP header entry.name – Identifiant name used for the header send on the request.value – Value set for a specific identifiant name. When creating a dictionary using the DDL command (CREATE DICTIONARY ...) remote hosts for HTTP dictionaries are checked against the contents of remote_url_allow_hosts section from config to prevent database users to access arbitrary HTTP server. "},{"title":"Known Vulnerability of the ODBC Dictionary Functionality​","type":1,"pageTitle":"Sources of External Dictionaries","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#known-vulnerability-of-the-odbc-dictionary-functionality","content":"note When connecting to the database through the ODBC driver connection parameter Servername can be substituted. In this case values of USERNAME and PASSWORD from odbc.ini are sent to the remote server and can be compromised. Example of insecure use Let’s configure unixODBC for PostgreSQL. Content of /etc/odbc.ini: [gregtest] Driver = /usr/lib/psqlodbca.so Servername = localhost PORT = 5432 DATABASE = test_db #OPTION = 3 USERNAME = test PASSWORD = test  If you then make a query such as SELECT * FROM odbc('DSN=gregtest;Servername=some-server.com', 'test_db');  ODBC driver will send values of USERNAME and PASSWORD from odbc.ini to some-server.com. "},{"title":"Example of Connecting Postgresql​","type":1,"pageTitle":"Sources of External Dictionaries","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#example-of-connecting-postgresql","content":"Ubuntu OS. Installing unixODBC and the ODBC driver for PostgreSQL: $ sudo apt-get install -y unixodbc odbcinst odbc-postgresql  Configuring /etc/odbc.ini (or ~/.odbc.ini if you signed in under a user that runs ClickHouse):  [DEFAULT] Driver = myconnection [myconnection] Description = PostgreSQL connection to my_db Driver = PostgreSQL Unicode Database = my_db Servername = 127.0.0.1 UserName = username Password = password Port = 5432 Protocol = 9.3 ReadOnly = No RowVersioning = No ShowSystemTables = No ConnSettings =  The dictionary configuration in ClickHouse: &lt;clickhouse&gt; &lt;dictionary&gt; &lt;name&gt;table_name&lt;/name&gt; &lt;source&gt; &lt;odbc&gt; &lt;!-- You can specify the following parameters in connection_string: --&gt; &lt;!-- DSN=myconnection;UID=username;PWD=password;HOST=127.0.0.1;PORT=5432;DATABASE=my_db --&gt; &lt;connection_string&gt;DSN=myconnection&lt;/connection_string&gt; &lt;table&gt;postgresql_table&lt;/table&gt; &lt;/odbc&gt; &lt;/source&gt; &lt;lifetime&gt; &lt;min&gt;300&lt;/min&gt; &lt;max&gt;360&lt;/max&gt; &lt;/lifetime&gt; &lt;layout&gt; &lt;hashed/&gt; &lt;/layout&gt; &lt;structure&gt; &lt;id&gt; &lt;name&gt;id&lt;/name&gt; &lt;/id&gt; &lt;attribute&gt; &lt;name&gt;some_column&lt;/name&gt; &lt;type&gt;UInt64&lt;/type&gt; &lt;null_value&gt;0&lt;/null_value&gt; &lt;/attribute&gt; &lt;/structure&gt; &lt;/dictionary&gt; &lt;/clickhouse&gt;  or CREATE DICTIONARY table_name ( id UInt64, some_column UInt64 DEFAULT 0 ) PRIMARY KEY id SOURCE(ODBC(connection_string 'DSN=myconnection' table 'postgresql_table')) LAYOUT(HASHED()) LIFETIME(MIN 300 MAX 360)  You may need to edit odbc.ini to specify the full path to the library with the driver DRIVER=/usr/local/lib/psqlodbcw.so. "},{"title":"Example of Connecting MS SQL Server​","type":1,"pageTitle":"Sources of External Dictionaries","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#example-of-connecting-ms-sql-server","content":"Ubuntu OS. Installing the ODBC driver for connecting to MS SQL: $ sudo apt-get install tdsodbc freetds-bin sqsh  Configuring the driver:  $ cat /etc/freetds/freetds.conf ... [MSSQL] host = 192.168.56.101 port = 1433 tds version = 7.0 client charset = UTF-8 # test TDS connection $ sqsh -S MSSQL -D database -U user -P password $ cat /etc/odbcinst.ini [FreeTDS] Description = FreeTDS Driver = /usr/lib/x86_64-linux-gnu/odbc/libtdsodbc.so Setup = /usr/lib/x86_64-linux-gnu/odbc/libtdsS.so FileUsage = 1 UsageCount = 5 $ cat /etc/odbc.ini # $ cat ~/.odbc.ini # if you signed in under a user that runs ClickHouse [MSSQL] Description = FreeTDS Driver = FreeTDS Servername = MSSQL Database = test UID = test PWD = test Port = 1433 # (optional) test ODBC connection (to use isql-tool install the [unixodbc](https://packages.debian.org/sid/unixodbc)-package) $ isql -v MSSQL &quot;user&quot; &quot;password&quot;  Remarks: to determine the earliest TDS version that is supported by a particular SQL Server version, refer to the product documentation or look at MS-TDS Product Behavior Configuring the dictionary in ClickHouse: &lt;clickhouse&gt; &lt;dictionary&gt; &lt;name&gt;test&lt;/name&gt; &lt;source&gt; &lt;odbc&gt; &lt;table&gt;dict&lt;/table&gt; &lt;connection_string&gt;DSN=MSSQL;UID=test;PWD=test&lt;/connection_string&gt; &lt;/odbc&gt; &lt;/source&gt; &lt;lifetime&gt; &lt;min&gt;300&lt;/min&gt; &lt;max&gt;360&lt;/max&gt; &lt;/lifetime&gt; &lt;layout&gt; &lt;flat /&gt; &lt;/layout&gt; &lt;structure&gt; &lt;id&gt; &lt;name&gt;k&lt;/name&gt; &lt;/id&gt; &lt;attribute&gt; &lt;name&gt;s&lt;/name&gt; &lt;type&gt;String&lt;/type&gt; &lt;null_value&gt;&lt;/null_value&gt; &lt;/attribute&gt; &lt;/structure&gt; &lt;/dictionary&gt; &lt;/clickhouse&gt;  or CREATE DICTIONARY test ( k UInt64, s String DEFAULT '' ) PRIMARY KEY k SOURCE(ODBC(table 'dict' connection_string 'DSN=MSSQL;UID=test;PWD=test')) LAYOUT(FLAT()) LIFETIME(MIN 300 MAX 360)  "},{"title":"DBMS​","type":1,"pageTitle":"Sources of External Dictionaries","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#dbms","content":""},{"title":"ODBC​","type":1,"pageTitle":"Sources of External Dictionaries","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#dicts-external_dicts_dict_sources-odbc","content":"You can use this method to connect any database that has an ODBC driver. Example of settings: &lt;source&gt; &lt;odbc&gt; &lt;db&gt;DatabaseName&lt;/db&gt; &lt;table&gt;ShemaName.TableName&lt;/table&gt; &lt;connection_string&gt;DSN=some_parameters&lt;/connection_string&gt; &lt;invalidate_query&gt;SQL_QUERY&lt;/invalidate_query&gt; &lt;query&gt;SELECT id, value_1, value_2 FROM ShemaName.TableName&lt;/query&gt; &lt;/odbc&gt; &lt;/source&gt;  or SOURCE(ODBC( db 'DatabaseName' table 'SchemaName.TableName' connection_string 'DSN=some_parameters' invalidate_query 'SQL_QUERY' query 'SELECT id, value_1, value_2 FROM db_name.table_name' ))  Setting fields: db – Name of the database. Omit it if the database name is set in the &lt;connection_string&gt; parameters.table – Name of the table and schema if exists.connection_string – Connection string.invalidate_query – Query for checking the dictionary status. Optional parameter. Read more in the section Updating dictionaries.query – The custom query. Optional parameter. note The table and query fields cannot be used together. And either one of the table or query fields must be declared. ClickHouse receives quoting symbols from ODBC-driver and quote all settings in queries to driver, so it’s necessary to set table name accordingly to table name case in database. If you have a problems with encodings when using Oracle, see the corresponding FAQ item. "},{"title":"Mysql​","type":1,"pageTitle":"Sources of External Dictionaries","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#dicts-external_dicts_dict_sources-mysql","content":"Example of settings: &lt;source&gt; &lt;mysql&gt; &lt;port&gt;3306&lt;/port&gt; &lt;user&gt;clickhouse&lt;/user&gt; &lt;password&gt;qwerty&lt;/password&gt; &lt;replica&gt; &lt;host&gt;example01-1&lt;/host&gt; &lt;priority&gt;1&lt;/priority&gt; &lt;/replica&gt; &lt;replica&gt; &lt;host&gt;example01-2&lt;/host&gt; &lt;priority&gt;1&lt;/priority&gt; &lt;/replica&gt; &lt;db&gt;db_name&lt;/db&gt; &lt;table&gt;table_name&lt;/table&gt; &lt;where&gt;id=10&lt;/where&gt; &lt;invalidate_query&gt;SQL_QUERY&lt;/invalidate_query&gt; &lt;fail_on_connection_loss&gt;true&lt;/fail_on_connection_loss&gt; &lt;query&gt;SELECT id, value_1, value_2 FROM db_name.table_name&lt;/query&gt; &lt;/mysql&gt; &lt;/source&gt;  or SOURCE(MYSQL( port 3306 user 'clickhouse' password 'qwerty' replica(host 'example01-1' priority 1) replica(host 'example01-2' priority 1) db 'db_name' table 'table_name' where 'id=10' invalidate_query 'SQL_QUERY' fail_on_connection_loss 'true' query 'SELECT id, value_1, value_2 FROM db_name.table_name' ))  Setting fields: port – The port on the MySQL server. You can specify it for all replicas, or for each one individually (inside &lt;replica&gt;). user – Name of the MySQL user. You can specify it for all replicas, or for each one individually (inside &lt;replica&gt;). password – Password of the MySQL user. You can specify it for all replicas, or for each one individually (inside &lt;replica&gt;). replica – Section of replica configurations. There can be multiple sections. - `replica/host` – The MySQL host. - `replica/priority` – The replica priority. When attempting to connect, ClickHouse traverses the replicas in order of priority. The lower the number, the higher the priority. db – Name of the database. table – Name of the table. where – The selection criteria. The syntax for conditions is the same as for WHERE clause in MySQL, for example, id &gt; 10 AND id &lt; 20. Optional parameter. invalidate_query – Query for checking the dictionary status. Optional parameter. Read more in the section Updating dictionaries. fail_on_connection_loss – The configuration parameter that controls behavior of the server on connection loss. If true, an exception is thrown immediately if the connection between client and server was lost. If false, the ClickHouse server retries to execute the query three times before throwing an exception. Note that retrying leads to increased response times. Default value: false. query – The custom query. Optional parameter. note The table or where fields cannot be used together with the query field. And either one of the table or query fields must be declared. MySQL can be connected on a local host via sockets. To do this, set host and socket. Example of settings: &lt;source&gt; &lt;mysql&gt; &lt;host&gt;localhost&lt;/host&gt; &lt;socket&gt;/path/to/socket/file.sock&lt;/socket&gt; &lt;user&gt;clickhouse&lt;/user&gt; &lt;password&gt;qwerty&lt;/password&gt; &lt;db&gt;db_name&lt;/db&gt; &lt;table&gt;table_name&lt;/table&gt; &lt;where&gt;id=10&lt;/where&gt; &lt;invalidate_query&gt;SQL_QUERY&lt;/invalidate_query&gt; &lt;fail_on_connection_loss&gt;true&lt;/fail_on_connection_loss&gt; &lt;query&gt;SELECT id, value_1, value_2 FROM db_name.table_name&lt;/query&gt; &lt;/mysql&gt; &lt;/source&gt;  or SOURCE(MYSQL( host 'localhost' socket '/path/to/socket/file.sock' user 'clickhouse' password 'qwerty' db 'db_name' table 'table_name' where 'id=10' invalidate_query 'SQL_QUERY' fail_on_connection_loss 'true' query 'SELECT id, value_1, value_2 FROM db_name.table_name' ))  "},{"title":"ClickHouse​","type":1,"pageTitle":"Sources of External Dictionaries","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#dicts-external_dicts_dict_sources-clickhouse","content":"Example of settings: &lt;source&gt; &lt;clickhouse&gt; &lt;host&gt;example01-01-1&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;&lt;/password&gt; &lt;db&gt;default&lt;/db&gt; &lt;table&gt;ids&lt;/table&gt; &lt;where&gt;id=10&lt;/where&gt; &lt;secure&gt;1&lt;/secure&gt; &lt;query&gt;SELECT id, value_1, value_2 FROM default.ids&lt;/query&gt; &lt;/clickhouse&gt; &lt;/source&gt;  or SOURCE(CLICKHOUSE( host 'example01-01-1' port 9000 user 'default' password '' db 'default' table 'ids' where 'id=10' secure 1 query 'SELECT id, value_1, value_2 FROM default.ids' ));  Setting fields: host – The ClickHouse host. If it is a local host, the query is processed without any network activity. To improve fault tolerance, you can create a Distributed table and enter it in subsequent configurations.port – The port on the ClickHouse server.user – Name of the ClickHouse user.password – Password of the ClickHouse user.db – Name of the database.table – Name of the table.where – The selection criteria. May be omitted.invalidate_query – Query for checking the dictionary status. Optional parameter. Read more in the section Updating dictionaries.secure - Use ssl for connection.query – The custom query. Optional parameter. note The table or where fields cannot be used together with the query field. And either one of the table or query fields must be declared. "},{"title":"Mongodb​","type":1,"pageTitle":"Sources of External Dictionaries","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#dicts-external_dicts_dict_sources-mongodb","content":"Example of settings: &lt;source&gt; &lt;mongodb&gt; &lt;host&gt;localhost&lt;/host&gt; &lt;port&gt;27017&lt;/port&gt; &lt;user&gt;&lt;/user&gt; &lt;password&gt;&lt;/password&gt; &lt;db&gt;test&lt;/db&gt; &lt;collection&gt;dictionary_source&lt;/collection&gt; &lt;/mongodb&gt; &lt;/source&gt;  or SOURCE(MONGODB( host 'localhost' port 27017 user '' password '' db 'test' collection 'dictionary_source' ))  Setting fields: host – The MongoDB host.port – The port on the MongoDB server.user – Name of the MongoDB user.password – Password of the MongoDB user.db – Name of the database.collection – Name of the collection. "},{"title":"Redis​","type":1,"pageTitle":"Sources of External Dictionaries","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#dicts-external_dicts_dict_sources-redis","content":"Example of settings: &lt;source&gt; &lt;redis&gt; &lt;host&gt;localhost&lt;/host&gt; &lt;port&gt;6379&lt;/port&gt; &lt;storage_type&gt;simple&lt;/storage_type&gt; &lt;db_index&gt;0&lt;/db_index&gt; &lt;/redis&gt; &lt;/source&gt;  or SOURCE(REDIS( host 'localhost' port 6379 storage_type 'simple' db_index 0 ))  Setting fields: host – The Redis host.port – The port on the Redis server.storage_type – The structure of internal Redis storage using for work with keys. simple is for simple sources and for hashed single key sources, hash_map is for hashed sources with two keys. Ranged sources and cache sources with complex key are unsupported. May be omitted, default value is simple.db_index – The specific numeric index of Redis logical database. May be omitted, default value is 0. "},{"title":"Cassandra​","type":1,"pageTitle":"Sources of External Dictionaries","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#dicts-external_dicts_dict_sources-cassandra","content":"Example of settings: &lt;source&gt; &lt;cassandra&gt; &lt;host&gt;localhost&lt;/host&gt; &lt;port&gt;9042&lt;/port&gt; &lt;user&gt;username&lt;/user&gt; &lt;password&gt;qwerty123&lt;/password&gt; &lt;keyspase&gt;database_name&lt;/keyspase&gt; &lt;column_family&gt;table_name&lt;/column_family&gt; &lt;allow_filering&gt;1&lt;/allow_filering&gt; &lt;partition_key_prefix&gt;1&lt;/partition_key_prefix&gt; &lt;consistency&gt;One&lt;/consistency&gt; &lt;where&gt;&quot;SomeColumn&quot; = 42&lt;/where&gt; &lt;max_threads&gt;8&lt;/max_threads&gt; &lt;query&gt;SELECT id, value_1, value_2 FROM database_name.table_name&lt;/query&gt; &lt;/cassandra&gt; &lt;/source&gt;  Setting fields: host – The Cassandra host or comma-separated list of hosts.port – The port on the Cassandra servers. If not specified, default port 9042 is used.user – Name of the Cassandra user.password – Password of the Cassandra user.keyspace – Name of the keyspace (database).column_family – Name of the column family (table).allow_filering – Flag to allow or not potentially expensive conditions on clustering key columns. Default value is 1.partition_key_prefix – Number of partition key columns in primary key of the Cassandra table. Required for compose key dictionaries. Order of key columns in the dictionary definition must be the same as in Cassandra. Default value is 1 (the first key column is a partition key and other key columns are clustering key).consistency – Consistency level. Possible values: One, Two, Three, All, EachQuorum, Quorum, LocalQuorum, LocalOne, Serial, LocalSerial. Default value is One.where – Optional selection criteria.max_threads – The maximum number of threads to use for loading data from multiple partitions in compose key dictionaries.query – The custom query. Optional parameter. note The column_family or where fields cannot be used together with the query field. And either one of the column_family or query fields must be declared. "},{"title":"PostgreSQL​","type":1,"pageTitle":"Sources of External Dictionaries","url":"docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#dicts-external_dicts_dict_sources-postgresql","content":"Example of settings: &lt;source&gt; &lt;postgresql&gt; &lt;port&gt;5432&lt;/port&gt; &lt;user&gt;clickhouse&lt;/user&gt; &lt;password&gt;qwerty&lt;/password&gt; &lt;db&gt;db_name&lt;/db&gt; &lt;table&gt;table_name&lt;/table&gt; &lt;where&gt;id=10&lt;/where&gt; &lt;invalidate_query&gt;SQL_QUERY&lt;/invalidate_query&gt; &lt;query&gt;SELECT id, value_1, value_2 FROM db_name.table_name&lt;/query&gt; &lt;/postgresql&gt; &lt;/source&gt;  or SOURCE(POSTGRESQL( port 5432 host 'postgresql-hostname' user 'postgres_user' password 'postgres_password' db 'db_name' table 'table_name' replica(host 'example01-1' port 5432 priority 1) replica(host 'example01-2' port 5432 priority 2) where 'id=10' invalidate_query 'SQL_QUERY' query 'SELECT id, value_1, value_2 FROM db_name.table_name' ))  Setting fields: host – The host on the PostgreSQL server. You can specify it for all replicas, or for each one individually (inside &lt;replica&gt;).port – The port on the PostgreSQL server. You can specify it for all replicas, or for each one individually (inside &lt;replica&gt;).user – Name of the PostgreSQL user. You can specify it for all replicas, or for each one individually (inside &lt;replica&gt;).password – Password of the PostgreSQL user. You can specify it for all replicas, or for each one individually (inside &lt;replica&gt;).replica – Section of replica configurations. There can be multiple sections: replica/host – The PostgreSQL host.replica/port – The PostgreSQL port.replica/priority – The replica priority. When attempting to connect, ClickHouse traverses the replicas in order of priority. The lower the number, the higher the priority. db – Name of the database.table – Name of the table.where – The selection criteria. The syntax for conditions is the same as for WHERE clause in PostgreSQL. For example, id &gt; 10 AND id &lt; 20. Optional parameter.invalidate_query – Query for checking the dictionary status. Optional parameter. Read more in the section Updating dictionaries.query – The custom query. Optional parameter. note The table or where fields cannot be used together with the query field. And either one of the table or query fields must be declared. "},{"title":"ext-dict-functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/ext-dict-functions","content":"","keywords":""},{"title":"dictGet, dictGetOrDefault, dictGetOrNull​","type":1,"pageTitle":"ext-dict-functions","url":"docs/en/sql-reference/functions/ext-dict-functions#dictget","content":"Retrieves values from an external dictionary. dictGet('dict_name', attr_names, id_expr) dictGetOrDefault('dict_name', attr_names, id_expr, default_value_expr) dictGetOrNull('dict_name', attr_name, id_expr)  Arguments dict_name — Name of the dictionary. String literal.attr_names — Name of the column of the dictionary, String literal, or tuple of column names, Tuple(String literal).id_expr — Key value. Expression returning dictionary key-type value or Tuple-type value depending on the dictionary configuration.default_value_expr — Values returned if the dictionary does not contain a row with the id_expr key. Expression or Tuple(Expression), returning the value (or values) in the data types configured for the attr_names attribute. Returned value If ClickHouse parses the attribute successfully in the attribute’s data type, functions return the value of the dictionary attribute that corresponds to id_expr. If there is no the key, corresponding to id_expr, in the dictionary, then: - `dictGet` returns the content of the `&lt;null_value&gt;` element specified for the attribute in the dictionary configuration. - `dictGetOrDefault` returns the value passed as the `default_value_expr` parameter. - `dictGetOrNull` returns `NULL` in case key was not found in dictionary.  ClickHouse throws an exception if it cannot parse the value of the attribute or the value does not match the attribute data type. Example for simple key dictionary Create a text file ext-dict-test.csv containing the following: 1,1 2,2  The first column is id, the second column is c1. Configure the external dictionary: &lt;clickhouse&gt; &lt;dictionary&gt; &lt;name&gt;ext-dict-test&lt;/name&gt; &lt;source&gt; &lt;file&gt; &lt;path&gt;/path-to/ext-dict-test.csv&lt;/path&gt; &lt;format&gt;CSV&lt;/format&gt; &lt;/file&gt; &lt;/source&gt; &lt;layout&gt; &lt;flat /&gt; &lt;/layout&gt; &lt;structure&gt; &lt;id&gt; &lt;name&gt;id&lt;/name&gt; &lt;/id&gt; &lt;attribute&gt; &lt;name&gt;c1&lt;/name&gt; &lt;type&gt;UInt32&lt;/type&gt; &lt;null_value&gt;&lt;/null_value&gt; &lt;/attribute&gt; &lt;/structure&gt; &lt;lifetime&gt;0&lt;/lifetime&gt; &lt;/dictionary&gt; &lt;/clickhouse&gt;  Perform the query: SELECT dictGetOrDefault('ext-dict-test', 'c1', number + 1, toUInt32(number * 10)) AS val, toTypeName(val) AS type FROM system.numbers LIMIT 3;  ┌─val─┬─type───┐ │ 1 │ UInt32 │ │ 2 │ UInt32 │ │ 20 │ UInt32 │ └─────┴────────┘  Example for complex key dictionary Create a text file ext-dict-mult.csv containing the following: 1,1,'1' 2,2,'2' 3,3,'3'  The first column is id, the second is c1, the third is c2. Configure the external dictionary: &lt;clickhouse&gt; &lt;dictionary&gt; &lt;name&gt;ext-dict-mult&lt;/name&gt; &lt;source&gt; &lt;file&gt; &lt;path&gt;/path-to/ext-dict-mult.csv&lt;/path&gt; &lt;format&gt;CSV&lt;/format&gt; &lt;/file&gt; &lt;/source&gt; &lt;layout&gt; &lt;flat /&gt; &lt;/layout&gt; &lt;structure&gt; &lt;id&gt; &lt;name&gt;id&lt;/name&gt; &lt;/id&gt; &lt;attribute&gt; &lt;name&gt;c1&lt;/name&gt; &lt;type&gt;UInt32&lt;/type&gt; &lt;null_value&gt;&lt;/null_value&gt; &lt;/attribute&gt; &lt;attribute&gt; &lt;name&gt;c2&lt;/name&gt; &lt;type&gt;String&lt;/type&gt; &lt;null_value&gt;&lt;/null_value&gt; &lt;/attribute&gt; &lt;/structure&gt; &lt;lifetime&gt;0&lt;/lifetime&gt; &lt;/dictionary&gt; &lt;/clickhouse&gt;  Perform the query: SELECT dictGet('ext-dict-mult', ('c1','c2'), number) AS val, toTypeName(val) AS type FROM system.numbers LIMIT 3;  ┌─val─────┬─type──────────────────┐ │ (1,'1') │ Tuple(UInt8, String) │ │ (2,'2') │ Tuple(UInt8, String) │ │ (3,'3') │ Tuple(UInt8, String) │ └─────────┴───────────────────────┘  Example for range key dictionary Input table: CREATE TABLE range_key_dictionary_source_table ( key UInt64, start_date Date, end_date Date, value String, value_nullable Nullable(String) ) ENGINE = TinyLog(); INSERT INTO range_key_dictionary_source_table VALUES(1, toDate('2019-05-20'), toDate('2019-05-20'), 'First', 'First'); INSERT INTO range_key_dictionary_source_table VALUES(2, toDate('2019-05-20'), toDate('2019-05-20'), 'Second', NULL); INSERT INTO range_key_dictionary_source_table VALUES(3, toDate('2019-05-20'), toDate('2019-05-20'), 'Third', 'Third');  Create the external dictionary: CREATE DICTIONARY range_key_dictionary ( key UInt64, start_date Date, end_date Date, value String, value_nullable Nullable(String) ) PRIMARY KEY key SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() TABLE 'range_key_dictionary_source_table')) LIFETIME(MIN 1 MAX 1000) LAYOUT(RANGE_HASHED()) RANGE(MIN start_date MAX end_date);  Perform the query: SELECT (number, toDate('2019-05-20')), dictHas('range_key_dictionary', number, toDate('2019-05-20')), dictGetOrNull('range_key_dictionary', 'value', number, toDate('2019-05-20')), dictGetOrNull('range_key_dictionary', 'value_nullable', number, toDate('2019-05-20')), dictGetOrNull('range_key_dictionary', ('value', 'value_nullable'), number, toDate('2019-05-20')) FROM system.numbers LIMIT 5 FORMAT TabSeparated;  Result: (0,'2019-05-20') 0 \\N \\N (NULL,NULL) (1,'2019-05-20') 1 First First ('First','First') (2,'2019-05-20') 1 Second \\N ('Second',NULL) (3,'2019-05-20') 1 Third Third ('Third','Third') (4,'2019-05-20') 0 \\N \\N (NULL,NULL)  See Also External Dictionaries "},{"title":"dictHas​","type":1,"pageTitle":"ext-dict-functions","url":"docs/en/sql-reference/functions/ext-dict-functions#dicthas","content":"Checks whether a key is present in a dictionary. dictHas('dict_name', id_expr)  Arguments dict_name — Name of the dictionary. String literal.id_expr — Key value. Expression returning dictionary key-type value or Tuple-type value depending on the dictionary configuration. Returned value 0, if there is no key.1, if there is a key. Type: UInt8. "},{"title":"dictGetHierarchy​","type":1,"pageTitle":"ext-dict-functions","url":"docs/en/sql-reference/functions/ext-dict-functions#dictgethierarchy","content":"Creates an array, containing all the parents of a key in the hierarchical dictionary. Syntax dictGetHierarchy('dict_name', key)  Arguments dict_name — Name of the dictionary. String literal.key — Key value. Expression returning a UInt64-type value. Returned value Parents for the key. Type: Array(UInt64). "},{"title":"dictIsIn​","type":1,"pageTitle":"ext-dict-functions","url":"docs/en/sql-reference/functions/ext-dict-functions#dictisin","content":"Checks the ancestor of a key through the whole hierarchical chain in the dictionary. dictIsIn('dict_name', child_id_expr, ancestor_id_expr)  Arguments dict_name — Name of the dictionary. String literal.child_id_expr — Key to be checked. Expression returning a UInt64-type value.ancestor_id_expr — Alleged ancestor of the child_id_expr key. Expression returning a UInt64-type value. Returned value 0, if child_id_expr is not a child of ancestor_id_expr.1, if child_id_expr is a child of ancestor_id_expr or if child_id_expr is an ancestor_id_expr. Type: UInt8. "},{"title":"dictGetChildren​","type":1,"pageTitle":"ext-dict-functions","url":"docs/en/sql-reference/functions/ext-dict-functions#dictgetchildren","content":"Returns first-level children as an array of indexes. It is the inverse transformation for dictGetHierarchy. Syntax dictGetChildren(dict_name, key)  Arguments dict_name — Name of the dictionary. String literal.key — Key value. Expression returning a UInt64-type value. Returned values First-level descendants for the key. Type: Array(UInt64). Example Consider the hierarchic dictionary: ┌─id─┬─parent_id─┐ │ 1 │ 0 │ │ 2 │ 1 │ │ 3 │ 1 │ │ 4 │ 2 │ └────┴───────────┘  First-level children: SELECT dictGetChildren('hierarchy_flat_dictionary', number) FROM system.numbers LIMIT 4;  ┌─dictGetChildren('hierarchy_flat_dictionary', number)─┐ │ [1] │ │ [2,3] │ │ [4] │ │ [] │ └──────────────────────────────────────────────────────┘  "},{"title":"dictGetDescendant​","type":1,"pageTitle":"ext-dict-functions","url":"docs/en/sql-reference/functions/ext-dict-functions#dictgetdescendant","content":"Returns all descendants as if dictGetChildren function was applied level times recursively. Syntax dictGetDescendants(dict_name, key, level)  Arguments dict_name — Name of the dictionary. String literal.key — Key value. Expression returning a UInt64-type value.level — Hierarchy level. If level = 0 returns all descendants to the end. UInt8. Returned values Descendants for the key. Type: Array(UInt64). Example Consider the hierarchic dictionary: ┌─id─┬─parent_id─┐ │ 1 │ 0 │ │ 2 │ 1 │ │ 3 │ 1 │ │ 4 │ 2 │ └────┴───────────┘  All descendants: SELECT dictGetDescendants('hierarchy_flat_dictionary', number) FROM system.numbers LIMIT 4;  ┌─dictGetDescendants('hierarchy_flat_dictionary', number)─┐ │ [1,2,3,4] │ │ [2,3,4] │ │ [4] │ │ [] │ └─────────────────────────────────────────────────────────┘  First-level descendants: SELECT dictGetDescendants('hierarchy_flat_dictionary', number, 1) FROM system.numbers LIMIT 4;  ┌─dictGetDescendants('hierarchy_flat_dictionary', number, 1)─┐ │ [1] │ │ [2,3] │ │ [4] │ │ [] │ └────────────────────────────────────────────────────────────┘  "},{"title":"Other Functions​","type":1,"pageTitle":"ext-dict-functions","url":"docs/en/sql-reference/functions/ext-dict-functions#ext_dict_functions-other","content":"ClickHouse supports specialized functions that convert dictionary attribute values to a specific data type regardless of the dictionary configuration. Functions: dictGetInt8, dictGetInt16, dictGetInt32, dictGetInt64dictGetUInt8, dictGetUInt16, dictGetUInt32, dictGetUInt64dictGetFloat32, dictGetFloat64dictGetDatedictGetDateTimedictGetUUIDdictGetString All these functions have the OrDefault modification. For example, dictGetDateOrDefault. Syntax: dictGet[Type]('dict_name', 'attr_name', id_expr) dictGet[Type]OrDefault('dict_name', 'attr_name', id_expr, default_value_expr)  Arguments dict_name — Name of the dictionary. String literal.attr_name — Name of the column of the dictionary. String literal.id_expr — Key value. Expression returning a UInt64 or Tuple-type value depending on the dictionary configuration.default_value_expr — Value returned if the dictionary does not contain a row with the id_expr key. Expression returning the value in the data type configured for the attr_name attribute. Returned value If ClickHouse parses the attribute successfully in the attribute’s data type, functions return the value of the dictionary attribute that corresponds to id_expr. If there is no requested id_expr in the dictionary then: - `dictGet[Type]` returns the content of the `&lt;null_value&gt;` element specified for the attribute in the dictionary configuration. - `dictGet[Type]OrDefault` returns the value passed as the `default_value_expr` parameter.  ClickHouse throws an exception if it cannot parse the value of the attribute or the value does not match the attribute data type. "},{"title":"Encryption functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/encryption-functions","content":"","keywords":""},{"title":"encrypt​","type":1,"pageTitle":"Encryption functions","url":"docs/en/sql-reference/functions/encryption-functions#encrypt","content":"This function encrypts data using these modes: aes-128-ecb, aes-192-ecb, aes-256-ecbaes-128-cbc, aes-192-cbc, aes-256-cbcaes-128-cfb1, aes-192-cfb1, aes-256-cfb1aes-128-cfb8, aes-192-cfb8, aes-256-cfb8aes-128-cfb128, aes-192-cfb128, aes-256-cfb128aes-128-ofb, aes-192-ofb, aes-256-ofbaes-128-gcm, aes-192-gcm, aes-256-gcm Syntax encrypt('mode', 'plaintext', 'key' [, iv, aad])  Arguments mode — Encryption mode. String.plaintext — Text thats need to be encrypted. String.key — Encryption key. String.iv — Initialization vector. Required for -gcm modes, optinal for others. String.aad — Additional authenticated data. It isn't encrypted, but it affects decryption. Works only in -gcm modes, for others would throw an exception. String. Returned value Ciphertext binary string. String. Examples Create this table: Query: CREATE TABLE encryption_test ( `comment` String, `secret` String ) ENGINE = Memory;  Insert some data (please avoid storing the keys/ivs in the database as this undermines the whole concept of encryption), also storing 'hints' is unsafe too and used only for illustrative purposes: Query: INSERT INTO encryption_test VALUES('aes-256-cfb128 no IV', encrypt('aes-256-cfb128', 'Secret', '12345678910121314151617181920212')),\\ ('aes-256-cfb128 no IV, different key', encrypt('aes-256-cfb128', 'Secret', 'keykeykeykeykeykeykeykeykeykeyke')),\\ ('aes-256-cfb128 with IV', encrypt('aes-256-cfb128', 'Secret', '12345678910121314151617181920212', 'iviviviviviviviv')),\\ ('aes-256-cbc no IV', encrypt('aes-256-cbc', 'Secret', '12345678910121314151617181920212'));  Query: SELECT comment, hex(secret) FROM encryption_test;  Result: ┌─comment─────────────────────────────┬─hex(secret)──────────────────────┐ │ aes-256-cfb128 no IV │ B4972BDC4459 │ │ aes-256-cfb128 no IV, different key │ 2FF57C092DC9 │ │ aes-256-cfb128 with IV │ 5E6CB398F653 │ │ aes-256-cbc no IV │ 1BC0629A92450D9E73A00E7D02CF4142 │ └─────────────────────────────────────┴──────────────────────────────────┘  Example with -gcm: Query: INSERT INTO encryption_test VALUES('aes-256-gcm', encrypt('aes-256-gcm', 'Secret', '12345678910121314151617181920212', 'iviviviviviviviv')), \\ ('aes-256-gcm with AAD', encrypt('aes-256-gcm', 'Secret', '12345678910121314151617181920212', 'iviviviviviviviv', 'aad')); SELECT comment, hex(secret) FROM encryption_test WHERE comment LIKE '%gcm%';  Result: ┌─comment──────────────┬─hex(secret)──────────────────────────────────┐ │ aes-256-gcm │ A8A3CCBC6426CFEEB60E4EAE03D3E94204C1B09E0254 │ │ aes-256-gcm with AAD │ A8A3CCBC6426D9A1017A0A932322F1852260A4AD6837 │ └──────────────────────┴──────────────────────────────────────────────┘  "},{"title":"aes_encrypt_mysql​","type":1,"pageTitle":"Encryption functions","url":"docs/en/sql-reference/functions/encryption-functions#aes_encrypt_mysql","content":"Compatible with mysql encryption and resulting ciphertext can be decrypted with AES_DECRYPT function. Will produce the same ciphertext as encrypt on equal inputs. But when key or iv are longer than they should normally be, aes_encrypt_mysql will stick to what MySQL's aes_encrypt does: 'fold' key and ignore excess bits of iv. Supported encryption modes: aes-128-ecb, aes-192-ecb, aes-256-ecbaes-128-cbc, aes-192-cbc, aes-256-cbcaes-128-cfb1, aes-192-cfb1, aes-256-cfb1aes-128-cfb8, aes-192-cfb8, aes-256-cfb8aes-128-cfb128, aes-192-cfb128, aes-256-cfb128aes-128-ofb, aes-192-ofb, aes-256-ofb Syntax aes_encrypt_mysql('mode', 'plaintext', 'key' [, iv])  Arguments mode — Encryption mode. String.plaintext — Text that needs to be encrypted. String.key — Encryption key. If key is longer than required by mode, MySQL-specific key folding is performed. String.iv — Initialization vector. Optional, only first 16 bytes are taken into account String. Returned value Ciphertext binary string. String. Examples Given equal input encrypt and aes_encrypt_mysql produce the same ciphertext: Query: SELECT encrypt('aes-256-cfb128', 'Secret', '12345678910121314151617181920212', 'iviviviviviviviv') = aes_encrypt_mysql('aes-256-cfb128', 'Secret', '12345678910121314151617181920212', 'iviviviviviviviv') AS ciphertexts_equal;  Result: ┌─ciphertexts_equal─┐ │ 1 │ └───────────────────┘  But encrypt fails when key or iv is longer than expected: Query: SELECT encrypt('aes-256-cfb128', 'Secret', '123456789101213141516171819202122', 'iviviviviviviviv123');  Result: Received exception from server (version 21.1.2): Code: 36. DB::Exception: Received from localhost:9000. DB::Exception: Invalid key size: 33 expected 32: While processing encrypt('aes-256-cfb128', 'Secret', '123456789101213141516171819202122', 'iviviviviviviviv123').  While aes_encrypt_mysql produces MySQL-compatitalbe output: Query: SELECT hex(aes_encrypt_mysql('aes-256-cfb128', 'Secret', '123456789101213141516171819202122', 'iviviviviviviviv123')) AS ciphertext;  Result: ┌─ciphertext───┐ │ 24E9E4966469 │ └──────────────┘  Notice how supplying even longer IV produces the same result Query: SELECT hex(aes_encrypt_mysql('aes-256-cfb128', 'Secret', '123456789101213141516171819202122', 'iviviviviviviviv123456')) AS ciphertext  Result: ┌─ciphertext───┐ │ 24E9E4966469 │ └──────────────┘  Which is binary equal to what MySQL produces on same inputs: mysql&gt; SET block_encryption_mode='aes-256-cfb128'; Query OK, 0 rows affected (0.00 sec) mysql&gt; SELECT aes_encrypt('Secret', '123456789101213141516171819202122', 'iviviviviviviviv123456') as ciphertext; +------------------------+ | ciphertext | +------------------------+ | 0x24E9E4966469 | +------------------------+ 1 row in set (0.00 sec)  "},{"title":"decrypt​","type":1,"pageTitle":"Encryption functions","url":"docs/en/sql-reference/functions/encryption-functions#decrypt","content":"This function decrypts ciphertext into a plaintext using these modes: aes-128-ecb, aes-192-ecb, aes-256-ecbaes-128-cbc, aes-192-cbc, aes-256-cbcaes-128-cfb1, aes-192-cfb1, aes-256-cfb1aes-128-cfb8, aes-192-cfb8, aes-256-cfb8aes-128-cfb128, aes-192-cfb128, aes-256-cfb128aes-128-ofb, aes-192-ofb, aes-256-ofbaes-128-gcm, aes-192-gcm, aes-256-gcm Syntax decrypt('mode', 'ciphertext', 'key' [, iv, aad])  Arguments mode — Decryption mode. String.ciphertext — Encrypted text that needs to be decrypted. String.key — Decryption key. String.iv — Initialization vector. Required for -gcm modes, optinal for others. String.aad — Additional authenticated data. Won't decrypt if this value is incorrect. Works only in -gcm modes, for others would throw an exception. String. Returned value Decrypted String. String. Examples Re-using table from encrypt. Query: SELECT comment, hex(secret) FROM encryption_test;  Result: ┌─comment──────────────┬─hex(secret)──────────────────────────────────┐ │ aes-256-gcm │ A8A3CCBC6426CFEEB60E4EAE03D3E94204C1B09E0254 │ │ aes-256-gcm with AAD │ A8A3CCBC6426D9A1017A0A932322F1852260A4AD6837 │ └──────────────────────┴──────────────────────────────────────────────┘ ┌─comment─────────────────────────────┬─hex(secret)──────────────────────┐ │ aes-256-cfb128 no IV │ B4972BDC4459 │ │ aes-256-cfb128 no IV, different key │ 2FF57C092DC9 │ │ aes-256-cfb128 with IV │ 5E6CB398F653 │ │ aes-256-cbc no IV │ 1BC0629A92450D9E73A00E7D02CF4142 │ └─────────────────────────────────────┴──────────────────────────────────┘  Now let's try to decrypt all that data. Query: SELECT comment, decrypt('aes-256-cfb128', secret, '12345678910121314151617181920212') as plaintext FROM encryption_test  Result: ┌─comment─────────────────────────────┬─plaintext─┐ │ aes-256-cfb128 no IV │ Secret │ │ aes-256-cfb128 no IV, different key │ �4� � │ │ aes-256-cfb128 with IV │ ���6�~ │ │aes-256-cbc no IV │ �2*4�h3c�4w��@ └─────────────────────────────────────┴───────────┘  Notice how only a portion of the data was properly decrypted, and the rest is gibberish since either mode, key, or iv were different upon encryption. "},{"title":"aes_decrypt_mysql​","type":1,"pageTitle":"Encryption functions","url":"docs/en/sql-reference/functions/encryption-functions#aes_decrypt_mysql","content":"Compatible with mysql encryption and decrypts data encrypted with AES_ENCRYPT function. Will produce same plaintext as decrypt on equal inputs. But when key or iv are longer than they should normally be, aes_decrypt_mysql will stick to what MySQL's aes_decrypt does: 'fold' key and ignore excess bits of IV. Supported decryption modes: aes-128-ecb, aes-192-ecb, aes-256-ecbaes-128-cbc, aes-192-cbc, aes-256-cbcaes-128-cfb1, aes-192-cfb1, aes-256-cfb1aes-128-cfb8, aes-192-cfb8, aes-256-cfb8aes-128-cfb128, aes-192-cfb128, aes-256-cfb128aes-128-ofb, aes-192-ofb, aes-256-ofb Syntax aes_decrypt_mysql('mode', 'ciphertext', 'key' [, iv])  Arguments mode — Decryption mode. String.ciphertext — Encrypted text that needs to be decrypted. String.key — Decryption key. String.iv — Initialization vector. Optinal. String. Returned value Decrypted String. String. Examples Let's decrypt data we've previously encrypted with MySQL: mysql&gt; SET block_encryption_mode='aes-256-cfb128'; Query OK, 0 rows affected (0.00 sec) mysql&gt; SELECT aes_encrypt('Secret', '123456789101213141516171819202122', 'iviviviviviviviv123456') as ciphertext; +------------------------+ | ciphertext | +------------------------+ | 0x24E9E4966469 | +------------------------+ 1 row in set (0.00 sec)  Query: SELECT aes_decrypt_mysql('aes-256-cfb128', unhex('24E9E4966469'), '123456789101213141516171819202122', 'iviviviviviviviv123456') AS plaintext  Result: ┌─plaintext─┐ │ Secret │ └───────────┘  Original article "},{"title":"Functions for Working with Geographical Coordinates","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/geo/coordinates","content":"","keywords":""},{"title":"greatCircleDistance​","type":1,"pageTitle":"Functions for Working with Geographical Coordinates","url":"docs/en/sql-reference/functions/geo/coordinates#greatcircledistance","content":"Calculates the distance between two points on the Earth’s surface using the great-circle formula. greatCircleDistance(lon1Deg, lat1Deg, lon2Deg, lat2Deg)  Input parameters lon1Deg — Longitude of the first point in degrees. Range: [-180°, 180°].lat1Deg — Latitude of the first point in degrees. Range: [-90°, 90°].lon2Deg — Longitude of the second point in degrees. Range: [-180°, 180°].lat2Deg — Latitude of the second point in degrees. Range: [-90°, 90°]. Positive values correspond to North latitude and East longitude, and negative values correspond to South latitude and West longitude. Returned value The distance between two points on the Earth’s surface, in meters. Generates an exception when the input parameter values fall outside of the range. Example SELECT greatCircleDistance(55.755831, 37.617673, -55.755831, -37.617673)  ┌─greatCircleDistance(55.755831, 37.617673, -55.755831, -37.617673)─┐ │ 14132374.194975413 │ └───────────────────────────────────────────────────────────────────┘  "},{"title":"geoDistance​","type":1,"pageTitle":"Functions for Working with Geographical Coordinates","url":"docs/en/sql-reference/functions/geo/coordinates#geodistance","content":"Similar to greatCircleDistance but calculates the distance on WGS-84 ellipsoid instead of sphere. This is more precise approximation of the Earth Geoid. The performance is the same as for greatCircleDistance (no performance drawback). It is recommended to use geoDistance to calculate the distances on Earth. Technical note: for close enough points we calculate the distance using planar approximation with the metric on the tangent plane at the midpoint of the coordinates. "},{"title":"greatCircleAngle​","type":1,"pageTitle":"Functions for Working with Geographical Coordinates","url":"docs/en/sql-reference/functions/geo/coordinates#greatcircleangle","content":"Calculates the central angle between two points on the Earth’s surface using the great-circle formula. greatCircleAngle(lon1Deg, lat1Deg, lon2Deg, lat2Deg)  Input parameters lon1Deg — Longitude of the first point in degrees.lat1Deg — Latitude of the first point in degrees.lon2Deg — Longitude of the second point in degrees.lat2Deg — Latitude of the second point in degrees. Returned value The central angle between two points in degrees. Example SELECT greatCircleAngle(0, 0, 45, 0) AS arc  ┌─arc─┐ │ 45 │ └─────┘  "},{"title":"pointInEllipses​","type":1,"pageTitle":"Functions for Working with Geographical Coordinates","url":"docs/en/sql-reference/functions/geo/coordinates#pointinellipses","content":"Checks whether the point belongs to at least one of the ellipses. Coordinates are geometric in the Cartesian coordinate system. pointInEllipses(x, y, x₀, y₀, a₀, b₀,...,xₙ, yₙ, aₙ, bₙ)  Input parameters x, y — Coordinates of a point on the plane.xᵢ, yᵢ — Coordinates of the center of the i-th ellipsis.aᵢ, bᵢ — Axes of the i-th ellipsis in units of x, y coordinates. The input parameters must be 2+4⋅n, where n is the number of ellipses. Returned values 1 if the point is inside at least one of the ellipses; 0if it is not. Example SELECT pointInEllipses(10., 10., 10., 9.1, 1., 0.9999)  ┌─pointInEllipses(10., 10., 10., 9.1, 1., 0.9999)─┐ │ 1 │ └─────────────────────────────────────────────────┘  "},{"title":"pointInPolygon​","type":1,"pageTitle":"Functions for Working with Geographical Coordinates","url":"docs/en/sql-reference/functions/geo/coordinates#pointinpolygon","content":"Checks whether the point belongs to the polygon on the plane. pointInPolygon((x, y), [(a, b), (c, d) ...], ...)  Input values (x, y) — Coordinates of a point on the plane. Data type — Tuple — A tuple of two numbers.[(a, b), (c, d) ...] — Polygon vertices. Data type — Array. Each vertex is represented by a pair of coordinates (a, b). Vertices should be specified in a clockwise or counterclockwise order. The minimum number of vertices is 3. The polygon must be constant.The function also supports polygons with holes (cut out sections). In this case, add polygons that define the cut out sections using additional arguments of the function. The function does not support non-simply-connected polygons. Returned values 1 if the point is inside the polygon, 0 if it is not. If the point is on the polygon boundary, the function may return either 0 or 1. Example SELECT pointInPolygon((3., 3.), [(6, 0), (8, 4), (5, 8), (0, 2)]) AS res  ┌─res─┐ │ 1 │ └─────┘  Original article "},{"title":"Functions for Working with Files","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/files","content":"","keywords":""},{"title":"file​","type":1,"pageTitle":"Functions for Working with Files","url":"docs/en/sql-reference/functions/files#file","content":"Reads file as a String. The file content is not parsed, so any information is read as one string and placed into the specified column. Syntax file(path)  Arguments path — The relative path to the file from user_files_path. Path to file support following wildcards: *, ?, {abc,def} and {N..M} where N, M — numbers, 'abc', 'def' — strings. Example Inserting data from files a.txt and b.txt into a table as strings: Query: INSERT INTO table SELECT file('a.txt'), file('b.txt');  See Also user_files_pathfile "},{"title":"Functions for Working with Geohash","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/geo/geohash","content":"","keywords":""},{"title":"geohashEncode​","type":1,"pageTitle":"Functions for Working with Geohash","url":"docs/en/sql-reference/functions/geo/geohash#geohashencode","content":"Encodes latitude and longitude as a geohash-string. geohashEncode(longitude, latitude, [precision])  Input values longitude - longitude part of the coordinate you want to encode. Floating in range[-180°, 180°]latitude - latitude part of the coordinate you want to encode. Floating in range [-90°, 90°]precision - Optional, length of the resulting encoded string, defaults to 12. Integer in range [1, 12]. Any value less than 1 or greater than 12 is silently converted to 12. Returned values alphanumeric String of encoded coordinate (modified version of the base32-encoding alphabet is used). Example SELECT geohashEncode(-5.60302734375, 42.593994140625, 0) AS res;  ┌─res──────────┐ │ ezs42d000000 │ └──────────────┘  "},{"title":"geohashDecode​","type":1,"pageTitle":"Functions for Working with Geohash","url":"docs/en/sql-reference/functions/geo/geohash#geohashdecode","content":"Decodes any geohash-encoded string into longitude and latitude. Input values encoded string - geohash-encoded string. Returned values (longitude, latitude) - 2-tuple of Float64 values of longitude and latitude. Example SELECT geohashDecode('ezs42') AS res;  ┌─res─────────────────────────────┐ │ (-5.60302734375,42.60498046875) │ └─────────────────────────────────┘  "},{"title":"geohashesInBox​","type":1,"pageTitle":"Functions for Working with Geohash","url":"docs/en/sql-reference/functions/geo/geohash#geohashesinbox","content":"Returns an array of geohash-encoded strings of given precision that fall inside and intersect boundaries of given box, basically a 2D grid flattened into array. Syntax geohashesInBox(longitude_min, latitude_min, longitude_max, latitude_max, precision)  Arguments longitude_min — Minimum longitude. Range: [-180°, 180°]. Type: Float.latitude_min — Minimum latitude. Range: [-90°, 90°]. Type: Float.longitude_max — Maximum longitude. Range: [-180°, 180°]. Type: Float.latitude_max — Maximum latitude. Range: [-90°, 90°]. Type: Float.precision — Geohash precision. Range: [1, 12]. Type: UInt8. note All coordinate parameters must be of the same type: either Float32 or Float64. Returned values Array of precision-long strings of geohash-boxes covering provided area, you should not rely on order of items.[] - Empty array if minimum latitude and longitude values aren’t less than corresponding maximum values. Type: Array(String). note Function throws an exception if resulting array is over 10’000’000 items long. Example Query: SELECT geohashesInBox(24.48, 40.56, 24.785, 40.81, 4) AS thasos;  Result: ┌─thasos──────────────────────────────────────┐ │ ['sx1q','sx1r','sx32','sx1w','sx1x','sx38'] │ └─────────────────────────────────────────────┘  Original article "},{"title":"Functions for Working with S2 Index","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/geo/s2","content":"","keywords":""},{"title":"geoToS2​","type":1,"pageTitle":"Functions for Working with S2 Index","url":"docs/en/sql-reference/functions/geo/s2#geotos2","content":"Returns S2 point index corresponding to the provided coordinates (longitude, latitude). Syntax geoToS2(lon, lat)  Arguments lon — Longitude. Float64.lat — Latitude. Float64. Returned values S2 point index. Type: UInt64. Example Query: SELECT geoToS2(37.79506683, 55.71290588) AS s2Index;  Result: ┌─────────────s2Index─┐ │ 4704772434919038107 │ └─────────────────────┘  "},{"title":"s2ToGeo​","type":1,"pageTitle":"Functions for Working with S2 Index","url":"docs/en/sql-reference/functions/geo/s2#s2togeo","content":"Returns geo coordinates (longitude, latitude) corresponding to the provided S2 point index. Syntax s2ToGeo(s2index)  Arguments s2index — S2 Index. UInt64. Returned values A tuple consisting of two values: tuple(lon,lat). Type: lon — Float64. lat — Float64. Example Query: SELECT s2ToGeo(4704772434919038107) AS s2Coodrinates;  Result: ┌─s2Coodrinates────────────────────────┐ │ (37.79506681471008,55.7129059052841) │ └──────────────────────────────────────┘  "},{"title":"s2GetNeighbors​","type":1,"pageTitle":"Functions for Working with S2 Index","url":"docs/en/sql-reference/functions/geo/s2#s2getneighbors","content":"Returns S2 neighbor indixes corresponding to the provided S2. Each cell in the S2 system is a quadrilateral bounded by four geodesics. So, each cell has 4 neighbors. Syntax s2GetNeighbors(s2index)  Arguments s2index — S2 Index. UInt64. Returned values An array consisting of 4 neighbor indexes: array[s2index1, s2index3, s2index2, s2index4]. Type: UInt64. Example Query: SELECT s2GetNeighbors(5074766849661468672) AS s2Neighbors;  Result: ┌─s2Neighbors───────────────────────────────────────────────────────────────────────┐ │ [5074766987100422144,5074766712222515200,5074767536856236032,5074767261978329088] │ └───────────────────────────────────────────────────────────────────────────────────┘  "},{"title":"s2CellsIntersect​","type":1,"pageTitle":"Functions for Working with S2 Index","url":"docs/en/sql-reference/functions/geo/s2#s2cellsintersect","content":"Determines if the two provided S2 cells intersect or not. Syntax s2CellsIntersect(s2index1, s2index2)  Arguments siIndex1, s2index2 — S2 Index. UInt64. Returned values 1 — If the cells intersect.0 — If the cells don't intersect. Type: UInt8. Example Query: SELECT s2CellsIntersect(9926595209846587392, 9926594385212866560) AS intersect;  Result: ┌─intersect─┐ │ 1 │ └───────────┘  "},{"title":"s2CapContains​","type":1,"pageTitle":"Functions for Working with S2 Index","url":"docs/en/sql-reference/functions/geo/s2#s2capcontains","content":"Determines if a cap contains a S2 point. A cap represents a part of the sphere that has been cut off by a plane. It is defined by a point on a sphere and a radius in degrees. Syntax s2CapContains(center, degrees, point)  Arguments center — S2 point index corresponding to the cap. UInt64.degrees — Radius of the cap in degrees. Float64.point — S2 point index. UInt64. Returned values 1 — If the cap contains the S2 point index.0 — If the cap doesn't contain the S2 point index. Type: UInt8. Example Query: SELECT s2CapContains(1157339245694594829, 1.0, 1157347770437378819) AS capContains;  Result: ┌─capContains─┐ │ 1 │ └─────────────┘  "},{"title":"s2CapUnion​","type":1,"pageTitle":"Functions for Working with S2 Index","url":"docs/en/sql-reference/functions/geo/s2#s2capunion","content":"Determines the smallest cap that contains the given two input caps. A cap represents a portion of the sphere that has been cut off by a plane. It is defined by a point on a sphere and a radius in degrees. Syntax s2CapUnion(center1, radius1, center2, radius2)  Arguments center1, center2 — S2 point indixes corresponding to the two input caps. UInt64.radius1, radius2 — Radius of the two input caps in degrees. Float64. Returned values center — S2 point index corresponding the center of the smallest cap containing the two input caps. Type: UInt64.radius — Radius of the smallest cap containing the two input caps. Type: Float64. Example Query: SELECT s2CapUnion(3814912406305146967, 1.0, 1157347770437378819, 1.0) AS capUnion;  Result: ┌─capUnion───────────────────────────────┐ │ (4534655147792050737,60.2088283994957) │ └────────────────────────────────────────┘  "},{"title":"s2RectAdd​","type":1,"pageTitle":"Functions for Working with S2 Index","url":"docs/en/sql-reference/functions/geo/s2#s2rectadd","content":"Increases the size of the bounding rectangle to include the given S2 point. In the S2 system, a rectangle is represented by a type of S2Region called a S2LatLngRect that represents a rectangle in latitude-longitude space. Syntax s2RectAdd(s2pointLow, s2pointHigh, s2Point)  Arguments s2PointLow — Low S2 point index corresponding to the rectangle. UInt64.s2PointHigh — High S2 point index corresponding to the rectangle. UInt64.s2Point — Target S2 point index that the bound rectangle should be grown to include. UInt64. Returned values s2PointLow — Low S2 cell id corresponding to the grown rectangle. Type: UInt64.s2PointHigh — Hight S2 cell id corresponding to the grown rectangle. Type: UInt64. Example Query: SELECT s2RectAdd(5178914411069187297, 5177056748191934217, 5179056748191934217) AS rectAdd;  Result: ┌─rectAdd───────────────────────────────────┐ │ (5179062030687166815,5177056748191934217) │ └───────────────────────────────────────────┘  "},{"title":"s2RectContains​","type":1,"pageTitle":"Functions for Working with S2 Index","url":"docs/en/sql-reference/functions/geo/s2#s2rectcontains","content":"Determines if a given rectangle contains a S2 point. In the S2 system, a rectangle is represented by a type of S2Region called a S2LatLngRect that represents a rectangle in latitude-longitude space. Syntax s2RectContains(s2PointLow, s2PointHi, s2Point)  Arguments s2PointLow — Low S2 point index corresponding to the rectangle. UInt64.s2PointHigh — High S2 point index corresponding to the rectangle. UInt64.s2Point — Target S2 point index. UInt64. Returned values 1 — If the rectangle contains the given S2 point.0 — If the rectangle doesn't contain the given S2 point. Example Query: SELECT s2RectContains(5179062030687166815, 5177056748191934217, 5177914411069187297) AS rectContains;  Result: ┌─rectContains─┐ │ 0 │ └──────────────┘  "},{"title":"s2RectUinion​","type":1,"pageTitle":"Functions for Working with S2 Index","url":"docs/en/sql-reference/functions/geo/s2#s2rectunion","content":"Returns the smallest rectangle containing the union of this rectangle and the given rectangle. In the S2 system, a rectangle is represented by a type of S2Region called a S2LatLngRect that represents a rectangle in latitude-longitude space. Syntax s2RectUnion(s2Rect1PointLow, s2Rect1PointHi, s2Rect2PointLow, s2Rect2PointHi)  Arguments s2Rect1PointLow, s2Rect1PointHi — Low and High S2 point indexes corresponding to the first rectangle. UInt64.s2Rect2PointLow, s2Rect2PointHi — Low and High S2 point indexes corresponding to the second rectangle. UInt64. Returned values s2UnionRect2PointLow — Low S2 cell id corresponding to the union rectangle. Type: UInt64.s2UnionRect2PointHi — High S2 cell id corresponding to the union rectangle. Type: UInt64. Example Query: SELECT s2RectUnion(5178914411069187297, 5177056748191934217, 5179062030687166815, 5177056748191934217) AS rectUnion;  Result: ┌─rectUnion─────────────────────────────────┐ │ (5179062030687166815,5177056748191934217) │ └───────────────────────────────────────────┘  "},{"title":"s2RectIntersection​","type":1,"pageTitle":"Functions for Working with S2 Index","url":"docs/en/sql-reference/functions/geo/s2#s2rectintersection","content":"Returns the smallest rectangle containing the intersection of this rectangle and the given rectangle. In the S2 system, a rectangle is represented by a type of S2Region called a S2LatLngRect that represents a rectangle in latitude-longitude space. Syntax s2RectIntersection(s2Rect1PointLow, s2Rect1PointHi, s2Rect2PointLow, s2Rect2PointHi)  Arguments s2Rect1PointLow, s2Rect1PointHi — Low and High S2 point indexes corresponding to the first rectangle. UInt64.s2Rect2PointLow, s2Rect2PointHi — Low and High S2 point indexes corresponding to the second rectangle. UInt64. Returned values s2UnionRect2PointLow — Low S2 cell id corresponding to the rectangle containing the intersection of the given rectangles. Type: UInt64.s2UnionRect2PointHi — High S2 cell id corresponding to the rectangle containing the intersection of the given rectangles. Type: UInt64. Example Query: SELECT s2RectIntersection(5178914411069187297, 5177056748191934217, 5179062030687166815, 5177056748191934217) AS rectIntersection;  Result: ┌─rectIntersection──────────────────────────┐ │ (5178914411069187297,5177056748191934217) │ └───────────────────────────────────────────┘  "},{"title":"Functions for Implementing the IN Operator","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/in-functions","content":"","keywords":""},{"title":"in, notIn, globalIn, globalNotIn​","type":1,"pageTitle":"Functions for Implementing the IN Operator","url":"docs/en/sql-reference/functions/in-functions#in-functions","content":"See the section IN operators. "},{"title":"Functions for Working with Dates and Times","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/date-time-functions","content":"","keywords":""},{"title":"timeZone​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#timezone","content":"Returns the timezone of the server. If it is executed in the context of a distributed table, then it generates a normal column with values relevant to each shard. Otherwise it produces a constant value. Syntax timeZone()  Alias: timezone. Returned value Timezone. Type: String. "},{"title":"toTimeZone​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#totimezone","content":"Converts time or date and time to the specified time zone. The time zone is an attribute of the Date and DateTime data types. The internal value (number of seconds) of the table field or of the resultset's column does not change, the column's type changes and its string representation changes accordingly. Syntax toTimezone(value, timezone)  Alias: toTimezone. Arguments value — Time or date and time. DateTime64.timezone — Timezone for the returned value. String. This argument is a constant, because toTimezone changes the timezone of a column (timezone is an attribute of DateTime* types). Returned value Date and time. Type: DateTime. Example Query: SELECT toDateTime('2019-01-01 00:00:00', 'UTC') AS time_utc, toTypeName(time_utc) AS type_utc, toInt32(time_utc) AS int32utc, toTimeZone(time_utc, 'Asia/Yekaterinburg') AS time_yekat, toTypeName(time_yekat) AS type_yekat, toInt32(time_yekat) AS int32yekat, toTimeZone(time_utc, 'US/Samoa') AS time_samoa, toTypeName(time_samoa) AS type_samoa, toInt32(time_samoa) AS int32samoa FORMAT Vertical;  Result: Row 1: ────── time_utc: 2019-01-01 00:00:00 type_utc: DateTime('UTC') int32utc: 1546300800 time_yekat: 2019-01-01 05:00:00 type_yekat: DateTime('Asia/Yekaterinburg') int32yekat: 1546300800 time_samoa: 2018-12-31 13:00:00 type_samoa: DateTime('US/Samoa') int32samoa: 1546300800  toTimeZone(time_utc, 'Asia/Yekaterinburg') changes the DateTime('UTC') type to DateTime('Asia/Yekaterinburg'). The value (Unixtimestamp) 1546300800 stays the same, but the string representation (the result of the toString() function) changes from time_utc: 2019-01-01 00:00:00 to time_yekat: 2019-01-01 05:00:00. "},{"title":"timeZoneOf​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#timezoneof","content":"Returns the timezone name of DateTime or DateTime64 data types. Syntax timeZoneOf(value)  Alias: timezoneOf. Arguments value — Date and time. DateTime or DateTime64. Returned value Timezone name. Type: String. Example Query: SELECT timezoneOf(now());  Result: ┌─timezoneOf(now())─┐ │ Etc/UTC │ └───────────────────┘  "},{"title":"timeZoneOffset​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#timezoneoffset","content":"Returns a timezone offset in seconds from UTC. The function takes into account daylight saving time and historical timezone changes at the specified date and time.IANA timezone database is used to calculate the offset. Syntax timeZoneOffset(value)  Alias: timezoneOffset. Arguments value — Date and time. DateTime or DateTime64. Returned value Offset from UTC in seconds. Type: Int32. Example Query: SELECT toDateTime('2021-04-21 10:20:30', 'America/New_York') AS Time, toTypeName(Time) AS Type, timeZoneOffset(Time) AS Offset_in_seconds, (Offset_in_seconds / 3600) AS Offset_in_hours;  Result: ┌────────────────Time─┬─Type─────────────────────────┬─Offset_in_seconds─┬─Offset_in_hours─┐ │ 2021-04-21 10:20:30 │ DateTime('America/New_York') │ -14400 │ -4 │ └─────────────────────┴──────────────────────────────┴───────────────────┴─────────────────┘  "},{"title":"toYear​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#toyear","content":"Converts a date or date with time to a UInt16 number containing the year number (AD). Alias: YEAR. "},{"title":"toQuarter​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#toquarter","content":"Converts a date or date with time to a UInt8 number containing the quarter number. Alias: QUARTER. "},{"title":"toMonth​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#tomonth","content":"Converts a date or date with time to a UInt8 number containing the month number (1-12). Alias: MONTH. "},{"title":"toDayOfYear​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#todayofyear","content":"Converts a date or date with time to a UInt16 number containing the number of the day of the year (1-366). Alias: DAYOFYEAR. "},{"title":"toDayOfMonth​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#todayofmonth","content":"Converts a date or date with time to a UInt8 number containing the number of the day of the month (1-31). Aliases: DAYOFMONTH, DAY. "},{"title":"toDayOfWeek​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#todayofweek","content":"Converts a date or date with time to a UInt8 number containing the number of the day of the week (Monday is 1, and Sunday is 7). Alias: DAYOFWEEK. "},{"title":"toHour​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#tohour","content":"Converts a date with time to a UInt8 number containing the number of the hour in 24-hour time (0-23). This function assumes that if clocks are moved ahead, it is by one hour and occurs at 2 a.m., and if clocks are moved back, it is by one hour and occurs at 3 a.m. (which is not always true – even in Moscow the clocks were twice changed at a different time). Alias: HOUR. "},{"title":"toMinute​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#tominute","content":"Converts a date with time to a UInt8 number containing the number of the minute of the hour (0-59). Alias: MINUTE. "},{"title":"toSecond​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#tosecond","content":"Converts a date with time to a UInt8 number containing the number of the second in the minute (0-59). Leap seconds are not accounted for. Alias: SECOND. "},{"title":"toUnixTimestamp​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#to-unix-timestamp","content":"For DateTime argument: converts value to the number with type UInt32 -- Unix Timestamp (https://en.wikipedia.org/wiki/Unix_time). For String argument: converts the input string to the datetime according to the timezone (optional second argument, server timezone is used by default) and returns the corresponding unix timestamp. Syntax toUnixTimestamp(datetime) toUnixTimestamp(str, [timezone])  Returned value Returns the unix timestamp. Type: UInt32. Example Query: SELECT toUnixTimestamp('2017-11-05 08:07:47', 'Asia/Tokyo') AS unix_timestamp  Result: ┌─unix_timestamp─┐ │ 1509836867 │ └────────────────┘  note The return type toStartOf* functions described below is Date or DateTime. Though these functions can take DateTime64 as an argument, passing them a DateTime64 that is out of the normal range (years 1925 - 2283) will give an incorrect result. "},{"title":"toStartOfYear​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#tostartofyear","content":"Rounds down a date or date with time to the first day of the year. Returns the date. "},{"title":"toStartOfISOYear​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#tostartofisoyear","content":"Rounds down a date or date with time to the first day of ISO year. Returns the date. "},{"title":"toStartOfQuarter​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#tostartofquarter","content":"Rounds down a date or date with time to the first day of the quarter. The first day of the quarter is either 1 January, 1 April, 1 July, or 1 October. Returns the date. "},{"title":"toStartOfMonth​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#tostartofmonth","content":"Rounds down a date or date with time to the first day of the month. Returns the date. note The behavior of parsing incorrect dates is implementation specific. ClickHouse may return zero date, throw an exception or do “natural” overflow. "},{"title":"toMonday​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#tomonday","content":"Rounds down a date or date with time to the nearest Monday. Returns the date. "},{"title":"toStartOfWeek(t[,mode])​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#tostartofweektmode","content":"Rounds down a date or date with time to the nearest Sunday or Monday by mode. Returns the date. The mode argument works exactly like the mode argument to toWeek(). For the single-argument syntax, a mode value of 0 is used. "},{"title":"toStartOfDay​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#tostartofday","content":"Rounds down a date with time to the start of the day. "},{"title":"toStartOfHour​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#tostartofhour","content":"Rounds down a date with time to the start of the hour. "},{"title":"toStartOfMinute​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#tostartofminute","content":"Rounds down a date with time to the start of the minute. "},{"title":"toStartOfSecond​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#tostartofsecond","content":"Truncates sub-seconds. Syntax toStartOfSecond(value, [timezone])  Arguments value — Date and time. DateTime64.timezone — Timezone for the returned value (optional). If not specified, the function uses the timezone of the value parameter. String. Returned value Input value without sub-seconds. Type: DateTime64. Examples Query without timezone: WITH toDateTime64('2020-01-01 10:20:30.999', 3) AS dt64 SELECT toStartOfSecond(dt64);  Result: ┌───toStartOfSecond(dt64)─┐ │ 2020-01-01 10:20:30.000 │ └─────────────────────────┘  Query with timezone: WITH toDateTime64('2020-01-01 10:20:30.999', 3) AS dt64 SELECT toStartOfSecond(dt64, 'Asia/Istanbul');  Result: ┌─toStartOfSecond(dt64, 'Asia/Istanbul')─┐ │ 2020-01-01 13:20:30.000 │ └────────────────────────────────────────┘  See also Timezone server configuration parameter. "},{"title":"toStartOfFiveMinute​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#tostartoffiveminute","content":"Rounds down a date with time to the start of the five-minute interval. "},{"title":"toStartOfTenMinutes​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#tostartoftenminutes","content":"Rounds down a date with time to the start of the ten-minute interval. "},{"title":"toStartOfFifteenMinutes​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#tostartoffifteenminutes","content":"Rounds down the date with time to the start of the fifteen-minute interval. "},{"title":"toStartOfInterval(time_or_data, INTERVAL x unit [, time_zone])​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#tostartofintervaltime-or-data-interval-x-unit-time-zone","content":"This is a generalization of other functions named toStartOf*. For example,toStartOfInterval(t, INTERVAL 1 year) returns the same as toStartOfYear(t),toStartOfInterval(t, INTERVAL 1 month) returns the same as toStartOfMonth(t),toStartOfInterval(t, INTERVAL 1 day) returns the same as toStartOfDay(t),toStartOfInterval(t, INTERVAL 15 minute) returns the same as toStartOfFifteenMinutes(t) etc. "},{"title":"toTime​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#totime","content":"Converts a date with time to a certain fixed date, while preserving the time. "},{"title":"toRelativeYearNum​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#torelativeyearnum","content":"Converts a date with time or date to the number of the year, starting from a certain fixed point in the past. "},{"title":"toRelativeQuarterNum​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#torelativequarternum","content":"Converts a date with time or date to the number of the quarter, starting from a certain fixed point in the past. "},{"title":"toRelativeMonthNum​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#torelativemonthnum","content":"Converts a date with time or date to the number of the month, starting from a certain fixed point in the past. "},{"title":"toRelativeWeekNum​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#torelativeweeknum","content":"Converts a date with time or date to the number of the week, starting from a certain fixed point in the past. "},{"title":"toRelativeDayNum​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#torelativedaynum","content":"Converts a date with time or date to the number of the day, starting from a certain fixed point in the past. "},{"title":"toRelativeHourNum​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#torelativehournum","content":"Converts a date with time or date to the number of the hour, starting from a certain fixed point in the past. "},{"title":"toRelativeMinuteNum​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#torelativeminutenum","content":"Converts a date with time or date to the number of the minute, starting from a certain fixed point in the past. "},{"title":"toRelativeSecondNum​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#torelativesecondnum","content":"Converts a date with time or date to the number of the second, starting from a certain fixed point in the past. "},{"title":"toISOYear​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#toisoyear","content":"Converts a date or date with time to a UInt16 number containing the ISO Year number. "},{"title":"toISOWeek​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#toisoweek","content":"Converts a date or date with time to a UInt8 number containing the ISO Week number. "},{"title":"toWeek(date[,mode])​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#toweekdatemode","content":"This function returns the week number for date or datetime. The two-argument form of toWeek() enables you to specify whether the week starts on Sunday or Monday and whether the return value should be in the range from 0 to 53 or from 1 to 53. If the mode argument is omitted, the default mode is 0.toISOWeek()is a compatibility function that is equivalent to toWeek(date,3). The following table describes how the mode argument works. Mode\tFirst day of week\tRange\tWeek 1 is the first week …0\tSunday\t0-53\twith a Sunday in this year 1\tMonday\t0-53\twith 4 or more days this year 2\tSunday\t1-53\twith a Sunday in this year 3\tMonday\t1-53\twith 4 or more days this year 4\tSunday\t0-53\twith 4 or more days this year 5\tMonday\t0-53\twith a Monday in this year 6\tSunday\t1-53\twith 4 or more days this year 7\tMonday\t1-53\twith a Monday in this year 8\tSunday\t1-53\tcontains January 1 9\tMonday\t1-53\tcontains January 1 For mode values with a meaning of “with 4 or more days this year,” weeks are numbered according to ISO 8601:1988: If the week containing January 1 has 4 or more days in the new year, it is week 1. Otherwise, it is the last week of the previous year, and the next week is week 1. For mode values with a meaning of “contains January 1”, the week contains January 1 is week 1. It does not matter how many days in the new year the week contained, even if it contained only one day. toWeek(date, [, mode][, Timezone])  Arguments date – Date or DateTime.mode – Optional parameter, Range of values is [0,9], default is 0.Timezone – Optional parameter, it behaves like any other conversion function. Example SELECT toDate('2016-12-27') AS date, toWeek(date) AS week0, toWeek(date,1) AS week1, toWeek(date,9) AS week9;  ┌───────date─┬─week0─┬─week1─┬─week9─┐ │ 2016-12-27 │ 52 │ 52 │ 1 │ └────────────┴───────┴───────┴───────┘  "},{"title":"toYearWeek(date[,mode])​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#toyearweekdatemode","content":"Returns year and week for a date. The year in the result may be different from the year in the date argument for the first and the last week of the year. The mode argument works exactly like the mode argument to toWeek(). For the single-argument syntax, a mode value of 0 is used. toISOYear()is a compatibility function that is equivalent to intDiv(toYearWeek(date,3),100). Example SELECT toDate('2016-12-27') AS date, toYearWeek(date) AS yearWeek0, toYearWeek(date,1) AS yearWeek1, toYearWeek(date,9) AS yearWeek9;  ┌───────date─┬─yearWeek0─┬─yearWeek1─┬─yearWeek9─┐ │ 2016-12-27 │ 201652 │ 201652 │ 201701 │ └────────────┴───────────┴───────────┴───────────┘  "},{"title":"date_trunc​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#date_trunc","content":"Truncates date and time data to the specified part of date. Syntax date_trunc(unit, value[, timezone])  Alias: dateTrunc. Arguments unit — The type of interval to truncate the result. String Literal. Possible values: secondminutehourdayweekmonthquarteryear value — Date and time. DateTime or DateTime64. timezone — Timezone name for the returned value (optional). If not specified, the function uses the timezone of the value parameter. String. Returned value Value, truncated to the specified part of date. Type: Datetime. Example Query without timezone: SELECT now(), date_trunc('hour', now());  Result: ┌───────────────now()─┬─date_trunc('hour', now())─┐ │ 2020-09-28 10:40:45 │ 2020-09-28 10:00:00 │ └─────────────────────┴───────────────────────────┘  Query with the specified timezone: SELECT now(), date_trunc('hour', now(), 'Asia/Istanbul');  Result: ┌───────────────now()─┬─date_trunc('hour', now(), 'Asia/Istanbul')─┐ │ 2020-09-28 10:46:26 │ 2020-09-28 13:00:00 │ └─────────────────────┴────────────────────────────────────────────┘  See Also toStartOfInterval "},{"title":"date_add​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#date_add","content":"Adds the time interval or date interval to the provided date or date with time. Syntax date_add(unit, value, date)  Aliases: dateAdd, DATE_ADD. Arguments unit — The type of interval to add. String. Possible values: secondminutehourdayweekmonthquarteryear value — Value of interval to add. Int. date — The date or date with time to which value is added. Date or DateTime. Returned value Date or date with time obtained by adding value, expressed in unit, to date. Type: Date or DateTime. Example Query: SELECT date_add(YEAR, 3, toDate('2018-01-01'));  Result: ┌─plus(toDate('2018-01-01'), toIntervalYear(3))─┐ │ 2021-01-01 │ └───────────────────────────────────────────────┘  "},{"title":"date_diff​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#date_diff","content":"Returns the difference between two dates or dates with time values. Syntax date_diff('unit', startdate, enddate, [timezone])  Aliases: dateDiff, DATE_DIFF. Arguments unit — The type of interval for result. String. Possible values: secondminutehourdayweekmonthquarteryear startdate — The first time value to subtract (the subtrahend). Date or DateTime. enddate — The second time value to subtract from (the minuend). Date or DateTime. timezone — Timezone name (optional). If specified, it is applied to both startdate and enddate. If not specified, timezones of startdate and enddate are used. If they are not the same, the result is unspecified. String. Returned value Difference between enddate and startdate expressed in unit. Type: Int. Example Query: SELECT dateDiff('hour', toDateTime('2018-01-01 22:00:00'), toDateTime('2018-01-02 23:00:00'));  Result: ┌─dateDiff('hour', toDateTime('2018-01-01 22:00:00'), toDateTime('2018-01-02 23:00:00'))─┐ │ 25 │ └────────────────────────────────────────────────────────────────────────────────────────┘  "},{"title":"date_sub​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#date_sub","content":"Subtracts the time interval or date interval from the provided date or date with time. Syntax date_sub(unit, value, date)  Aliases: dateSub, DATE_SUB. Arguments unit — The type of interval to subtract. String. Possible values: secondminutehourdayweekmonthquarteryear value — Value of interval to subtract. Int. date — The date or date with time from which value is subtracted. Date or DateTime. Returned value Date or date with time obtained by subtracting value, expressed in unit, from date. Type: Date or DateTime. Example Query: SELECT date_sub(YEAR, 3, toDate('2018-01-01'));  Result: ┌─minus(toDate('2018-01-01'), toIntervalYear(3))─┐ │ 2015-01-01 │ └────────────────────────────────────────────────┘  "},{"title":"timestamp_add​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#timestamp_add","content":"Adds the specified time value with the provided date or date time value. Syntax timestamp_add(date, INTERVAL value unit)  Aliases: timeStampAdd, TIMESTAMP_ADD. Arguments date — Date or date with time. Date or DateTime. value — Value of interval to add. Int. unit — The type of interval to add. String. Possible values: secondminutehourdayweekmonthquarteryear Returned value Date or date with time with the specified value expressed in unit added to date. Type: Date or DateTime. Example Query: select timestamp_add(toDate('2018-01-01'), INTERVAL 3 MONTH);  Result: ┌─plus(toDate('2018-01-01'), toIntervalMonth(3))─┐ │ 2018-04-01 │ └────────────────────────────────────────────────┘  "},{"title":"timestamp_sub​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#timestamp_sub","content":"Subtracts the time interval from the provided date or date with time. Syntax timestamp_sub(unit, value, date)  Aliases: timeStampSub, TIMESTAMP_SUB. Arguments unit — The type of interval to subtract. String. Possible values: secondminutehourdayweekmonthquarteryear value — Value of interval to subtract. Int. date — Date or date with time. Date or DateTime. Returned value Date or date with time obtained by subtracting value, expressed in unit, from date. Type: Date or DateTime. Example Query: select timestamp_sub(MONTH, 5, toDateTime('2018-12-18 01:02:03'));  Result: ┌─minus(toDateTime('2018-12-18 01:02:03'), toIntervalMonth(5))─┐ │ 2018-07-18 01:02:03 │ └──────────────────────────────────────────────────────────────┘  "},{"title":"now​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#now","content":"Returns the current date and time. Syntax now([timezone])  Arguments timezone — Timezone name for the returned value (optional). String. Returned value Current date and time. Type: Datetime. Example Query without timezone: SELECT now();  Result: ┌───────────────now()─┐ │ 2020-10-17 07:42:09 │ └─────────────────────┘  Query with the specified timezone: SELECT now('Asia/Istanbul');  Result: ┌─now('Asia/Istanbul')─┐ │ 2020-10-17 10:42:23 │ └──────────────────────┘  "},{"title":"today​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#today","content":"Accepts zero arguments and returns the current date at one of the moments of request execution. The same as ‘toDate(now())’. "},{"title":"yesterday​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#yesterday","content":"Accepts zero arguments and returns yesterday’s date at one of the moments of request execution. The same as ‘today() - 1’. "},{"title":"timeSlot​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#timeslot","content":"Rounds the time to the half hour. "},{"title":"toYYYYMM​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#toyyyymm","content":"Converts a date or date with time to a UInt32 number containing the year and month number (YYYY * 100 + MM). "},{"title":"toYYYYMMDD​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#toyyyymmdd","content":"Converts a date or date with time to a UInt32 number containing the year and month number (YYYY * 10000 + MM * 100 + DD). "},{"title":"toYYYYMMDDhhmmss​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#toyyyymmddhhmmss","content":"Converts a date or date with time to a UInt64 number containing the year and month number (YYYY * 10000000000 + MM * 100000000 + DD * 1000000 + hh * 10000 + mm * 100 + ss). "},{"title":"addYears, addMonths, addWeeks, addDays, addHours, addMinutes, addSeconds, addQuarters​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#addyears-addmonths-addweeks-adddays-addhours-addminutes-addseconds-addquarters","content":"Function adds a Date/DateTime interval to a Date/DateTime and then return the Date/DateTime. For example: WITH toDate('2018-01-01') AS date, toDateTime('2018-01-01 00:00:00') AS date_time SELECT addYears(date, 1) AS add_years_with_date, addYears(date_time, 1) AS add_years_with_date_time  ┌─add_years_with_date─┬─add_years_with_date_time─┐ │ 2019-01-01 │ 2019-01-01 00:00:00 │ └─────────────────────┴──────────────────────────┘  "},{"title":"subtractYears, subtractMonths, subtractWeeks, subtractDays, subtractHours, subtractMinutes, subtractSeconds, subtractQuarters​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#subtractyears-subtractmonths-subtractweeks-subtractdays-subtracthours-subtractminutes-subtractseconds-subtractquarters","content":"Function subtract a Date/DateTime interval to a Date/DateTime and then return the Date/DateTime. For example: WITH toDate('2019-01-01') AS date, toDateTime('2019-01-01 00:00:00') AS date_time SELECT subtractYears(date, 1) AS subtract_years_with_date, subtractYears(date_time, 1) AS subtract_years_with_date_time  ┌─subtract_years_with_date─┬─subtract_years_with_date_time─┐ │ 2018-01-01 │ 2018-01-01 00:00:00 │ └──────────────────────────┴───────────────────────────────┘  "},{"title":"timeSlots(StartTime, Duration,[, Size])​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#timeslotsstarttime-duration-size","content":"For a time interval starting at ‘StartTime’ and continuing for ‘Duration’ seconds, it returns an array of moments in time, consisting of points from this interval rounded down to the ‘Size’ in seconds. ‘Size’ is an optional parameter: a constant UInt32, set to 1800 by default. For example, timeSlots(toDateTime('2012-01-01 12:20:00'), 600) = [toDateTime('2012-01-01 12:00:00'), toDateTime('2012-01-01 12:30:00')]. This is necessary for searching for pageviews in the corresponding session. "},{"title":"formatDateTime​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#formatdatetime","content":"Formats a Time according to the given Format string. Format is a constant expression, so you cannot have multiple formats for a single result column. Syntax formatDateTime(Time, Format\\[, Timezone\\])  Returned value(s) Returns time and date values according to the determined format. Replacement fieldsUsing replacement fields, you can define a pattern for the resulting string. “Example” column shows formatting result for 2018-01-02 22:33:44. Placeholder\tDescription\tExample%C\tyear divided by 100 and truncated to integer (00-99)\t20 %d\tday of the month, zero-padded (01-31)\t02 %D\tShort MM/DD/YY date, equivalent to %m/%d/%y\t01/02/18 %e\tday of the month, space-padded ( 1-31) 2 %F\tshort YYYY-MM-DD date, equivalent to %Y-%m-%d\t2018-01-02 %G\tfour-digit year format for ISO week number, calculated from the week-based year defined by the ISO 8601 standard, normally useful only with %V\t2018 %g\ttwo-digit year format, aligned to ISO 8601, abbreviated from four-digit notation\t18 %H\thour in 24h format (00-23)\t22 %I\thour in 12h format (01-12)\t10 %j\tday of the year (001-366)\t002 %m\tmonth as a decimal number (01-12)\t01 %M\tminute (00-59)\t33 %n\tnew-line character (‘’) %p\tAM or PM designation\tPM %Q\tQuarter (1-4)\t1 %R\t24-hour HH:MM time, equivalent to %H:%M\t22:33 %S\tsecond (00-59)\t44 %t\thorizontal-tab character (’) %T\tISO 8601 time format (HH:MM:SS), equivalent to %H:%M:%S\t22:33:44 %u\tISO 8601 weekday as number with Monday as 1 (1-7)\t2 %V\tISO 8601 week number (01-53)\t01 %w\tweekday as a decimal number with Sunday as 0 (0-6)\t2 %y\tYear, last two digits (00-99)\t18 %Y\tYear\t2018 %%\ta % sign\t% Example Query: SELECT formatDateTime(toDate('2010-01-04'), '%g')  Result: ┌─formatDateTime(toDate('2010-01-04'), '%g')─┐ │ 10 │ └────────────────────────────────────────────┘  "},{"title":"dateName​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#dataname","content":"Returns specified part of date. Syntax dateName(date_part, date)  Arguments date_part — Date part. Possible values: 'year', 'quarter', 'month', 'week', 'dayofyear', 'day', 'weekday', 'hour', 'minute', 'second'. String.date — Date. Date, DateTime or DateTime64.timezone — Timezone. Optional. String. Returned value The specified part of date. Type: String Example Query: WITH toDateTime('2021-04-14 11:22:33') AS date_value SELECT dateName('year', date_value), dateName('month', date_value), dateName('day', date_value);  Result: ┌─dateName('year', date_value)─┬─dateName('month', date_value)─┬─dateName('day', date_value)─┐ │ 2021 │ April │ 14 │ └──────────────────────────────┴───────────────────────────────┴─────────────────────────────  "},{"title":"FROM_UNIXTIME​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#fromunixfime","content":"Function converts Unix timestamp to a calendar date and a time of a day. When there is only a single argument of Integer type, it acts in the same way as toDateTime and return DateTime type. Example: Query: SELECT FROM_UNIXTIME(423543535);  Result: ┌─FROM_UNIXTIME(423543535)─┐ │ 1983-06-04 10:58:55 │ └──────────────────────────┘  When there are two arguments: first is an Integer or DateTime, second is a constant format string — it acts in the same way as formatDateTime and return String type. For example: SELECT FROM_UNIXTIME(1234334543, '%Y-%m-%d %R:%S') AS DateTime;  ┌─DateTime────────────┐ │ 2009-02-11 14:42:23 │ └─────────────────────┘  "},{"title":"toModifiedJulianDay​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#tomodifiedjulianday","content":"Converts a Proleptic Gregorian calendar date in text form YYYY-MM-DD to a Modified Julian Day number in Int32. This function supports date from 0000-01-01 to 9999-12-31. It raises an exception if the argument cannot be parsed as a date, or the date is invalid. Syntax toModifiedJulianDay(date)  Arguments date — Date in text form. String or FixedString. Returned value Modified Julian Day number. Type: Int32. Example Query: SELECT toModifiedJulianDay('2020-01-01');  Result: ┌─toModifiedJulianDay('2020-01-01')─┐ │ 58849 │ └───────────────────────────────────┘  "},{"title":"toModifiedJulianDayOrNull​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#tomodifiedjuliandayornull","content":"Similar to toModifiedJulianDay(), but instead of raising exceptions it returns NULL. Syntax toModifiedJulianDayOrNull(date)  Arguments date — Date in text form. String or FixedString. Returned value Modified Julian Day number. Type: Nullable(Int32). Example Query: SELECT toModifiedJulianDayOrNull('2020-01-01');  Result: ┌─toModifiedJulianDayOrNull('2020-01-01')─┐ │ 58849 │ └─────────────────────────────────────────┘  "},{"title":"fromModifiedJulianDay​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#frommodifiedjulianday","content":"Converts a Modified Julian Day number to a Proleptic Gregorian calendar date in text form YYYY-MM-DD. This function supports day number from -678941 to 2973119 (which represent 0000-01-01 and 9999-12-31 respectively). It raises an exception if the day number is outside of the supported range. Syntax fromModifiedJulianDay(day)  Arguments day — Modified Julian Day number. Any integral types. Returned value Date in text form. Type: String Example Query: SELECT fromModifiedJulianDay(58849);  Result: ┌─fromModifiedJulianDay(58849)─┐ │ 2020-01-01 │ └──────────────────────────────┘  "},{"title":"fromModifiedJulianDayOrNull​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"docs/en/sql-reference/functions/date-time-functions#frommodifiedjuliandayornull","content":"Similar to fromModifiedJulianDayOrNull(), but instead of raising exceptions it returns NULL. Syntax fromModifiedJulianDayOrNull(day)  Arguments day — Modified Julian Day number. Any integral types. Returned value Date in text form. Type: Nullable(String) Example Query: SELECT fromModifiedJulianDayOrNull(58849);  Result: ┌─fromModifiedJulianDayOrNull(58849)─┐ │ 2020-01-01 │ └────────────────────────────────────┘  "},{"title":"Encoding Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/encoding-functions","content":"","keywords":""},{"title":"char​","type":1,"pageTitle":"Encoding Functions","url":"docs/en/sql-reference/functions/encoding-functions#char","content":"Returns the string with the length as the number of passed arguments and each byte has the value of corresponding argument. Accepts multiple arguments of numeric types. If the value of argument is out of range of UInt8 data type, it is converted to UInt8 with possible rounding and overflow. Syntax char(number_1, [number_2, ..., number_n]);  Arguments number_1, number_2, ..., number_n — Numerical arguments interpreted as integers. Types: Int, Float. Returned value a string of given bytes. Type: String. Example Query: SELECT char(104.1, 101, 108.9, 108.9, 111) AS hello;  Result: ┌─hello─┐ │ hello │ └───────┘  You can construct a string of arbitrary encoding by passing the corresponding bytes. Here is example for UTF-8: Query: SELECT char(0xD0, 0xBF, 0xD1, 0x80, 0xD0, 0xB8, 0xD0, 0xB2, 0xD0, 0xB5, 0xD1, 0x82) AS hello;  Result: ┌─hello──┐ │ привет │ └────────┘  Query: SELECT char(0xE4, 0xBD, 0xA0, 0xE5, 0xA5, 0xBD) AS hello;  Result: ┌─hello─┐ │ 你好 │ └───────┘  "},{"title":"hex​","type":1,"pageTitle":"Encoding Functions","url":"docs/en/sql-reference/functions/encoding-functions#hex","content":"Returns a string containing the argument’s hexadecimal representation. Alias: HEX. Syntax hex(arg)  The function is using uppercase letters A-F and not using any prefixes (like 0x) or suffixes (like h). For integer arguments, it prints hex digits (“nibbles”) from the most significant to least significant (big-endian or “human-readable” order). It starts with the most significant non-zero byte (leading zero bytes are omitted) but always prints both digits of every byte even if the leading digit is zero. Values of type Date and DateTime are formatted as corresponding integers (the number of days since Epoch for Date and the value of Unix Timestamp for DateTime). For String and FixedString, all bytes are simply encoded as two hexadecimal numbers. Zero bytes are not omitted. Values of Float and Decimal types are encoded as their representation in memory. As we support little-endian architecture, they are encoded in little-endian. Zero leading/trailing bytes are not omitted. Values of UUID type are encoded as big-endian order string. Arguments arg — A value to convert to hexadecimal. Types: String, UInt, Float, Decimal, Date or DateTime. Returned value A string with the hexadecimal representation of the argument. Type: String. Examples Query: SELECT hex(1);  Result: 01  Query: SELECT hex(toFloat32(number)) AS hex_presentation FROM numbers(15, 2);  Result: ┌─hex_presentation─┐ │ 00007041 │ │ 00008041 │ └──────────────────┘  Query: SELECT hex(toFloat64(number)) AS hex_presentation FROM numbers(15, 2);  Result: ┌─hex_presentation─┐ │ 0000000000002E40 │ │ 0000000000003040 │ └──────────────────┘  Query: SELECT lower(hex(toUUID('61f0c404-5cb3-11e7-907b-a6006ad3dba0'))) as uuid_hex  Result: ┌─uuid_hex─────────────────────────┐ │ 61f0c4045cb311e7907ba6006ad3dba0 │ └──────────────────────────────────┘  "},{"title":"unhex​","type":1,"pageTitle":"Encoding Functions","url":"docs/en/sql-reference/functions/encoding-functions#unhexstr","content":"Performs the opposite operation of hex. It interprets each pair of hexadecimal digits (in the argument) as a number and converts it to the byte represented by the number. The return value is a binary string (BLOB). If you want to convert the result to a number, you can use the reverse and reinterpretAs&lt;Type&gt; functions. note If unhex is invoked from within the clickhouse-client, binary strings display using UTF-8. Alias: UNHEX. Syntax unhex(arg)  Arguments arg — A string containing any number of hexadecimal digits. Type: String. Supports both uppercase and lowercase letters A-F. The number of hexadecimal digits does not have to be even. If it is odd, the last digit is interpreted as the least significant half of the 00-0F byte. If the argument string contains anything other than hexadecimal digits, some implementation-defined result is returned (an exception isn’t thrown). For a numeric argument the inverse of hex(N) is not performed by unhex(). Returned value A binary string (BLOB). Type: String. Example Query: SELECT unhex('303132'), UNHEX('4D7953514C');  Result: ┌─unhex('303132')─┬─unhex('4D7953514C')─┐ │ 012 │ MySQL │ └─────────────────┴─────────────────────┘  Query: SELECT reinterpretAsUInt64(reverse(unhex('FFF'))) AS num;  Result: ┌──num─┐ │ 4095 │ └──────┘  "},{"title":"bin​","type":1,"pageTitle":"Encoding Functions","url":"docs/en/sql-reference/functions/encoding-functions#bin","content":"Returns a string containing the argument’s binary representation. Syntax bin(arg)  Alias: BIN. For integer arguments, it prints bin digits from the most significant to least significant (big-endian or “human-readable” order). It starts with the most significant non-zero byte (leading zero bytes are omitted) but always prints eight digits of every byte if the leading digit is zero. Values of type Date and DateTime are formatted as corresponding integers (the number of days since Epoch for Date and the value of Unix Timestamp for DateTime). For String and FixedString, all bytes are simply encoded as eight binary numbers. Zero bytes are not omitted. Values of Float and Decimal types are encoded as their representation in memory. As we support little-endian architecture, they are encoded in little-endian. Zero leading/trailing bytes are not omitted. Values of UUID type are encoded as big-endian order string. Arguments arg — A value to convert to binary. String, FixedString, UInt, Float, Decimal, Date, or DateTime. Returned value A string with the binary representation of the argument. Type: String. Examples Query: SELECT bin(14);  Result: ┌─bin(14)──┐ │ 00001110 │ └──────────┘  Query: SELECT bin(toFloat32(number)) AS bin_presentation FROM numbers(15, 2);  Result: ┌─bin_presentation─────────────────┐ │ 00000000000000000111000001000001 │ │ 00000000000000001000000001000001 │ └──────────────────────────────────┘  Query: SELECT bin(toFloat64(number)) AS bin_presentation FROM numbers(15, 2);  Result: ┌─bin_presentation─────────────────────────────────────────────────┐ │ 0000000000000000000000000000000000000000000000000010111001000000 │ │ 0000000000000000000000000000000000000000000000000011000001000000 │ └──────────────────────────────────────────────────────────────────┘  Query: SELECT bin(toUUID('61f0c404-5cb3-11e7-907b-a6006ad3dba0')) as bin_uuid  Result: ┌─bin_uuid─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ 01100001111100001100010000000100010111001011001100010001111001111001000001111011101001100000000001101010110100111101101110100000 │ └──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘  "},{"title":"unbin​","type":1,"pageTitle":"Encoding Functions","url":"docs/en/sql-reference/functions/encoding-functions#unbinstr","content":"Interprets each pair of binary digits (in the argument) as a number and converts it to the byte represented by the number. The functions performs the opposite operation to bin. Syntax unbin(arg)  Alias: UNBIN. For a numeric argument unbin() does not return the inverse of bin(). If you want to convert the result to a number, you can use the reverse and reinterpretAs&lt;Type&gt; functions. note If unbin is invoked from within the clickhouse-client, binary strings are displayed using UTF-8. Supports binary digits 0 and 1. The number of binary digits does not have to be multiples of eight. If the argument string contains anything other than binary digits, some implementation-defined result is returned (an exception isn’t thrown). Arguments arg — A string containing any number of binary digits. String. Returned value A binary string (BLOB). Type: String. Examples Query: SELECT UNBIN('001100000011000100110010'), UNBIN('0100110101111001010100110101000101001100');  Result: ┌─unbin('001100000011000100110010')─┬─unbin('0100110101111001010100110101000101001100')─┐ │ 012 │ MySQL │ └───────────────────────────────────┴───────────────────────────────────────────────────┘  Query: SELECT reinterpretAsUInt64(reverse(unbin('1110'))) AS num;  Result: ┌─num─┐ │ 14 │ └─────┘  "},{"title":"UUIDStringToNum(str)​","type":1,"pageTitle":"Encoding Functions","url":"docs/en/sql-reference/functions/encoding-functions#uuidstringtonumstr","content":"Accepts a string containing 36 characters in the format 123e4567-e89b-12d3-a456-426655440000, and returns it as a set of bytes in a FixedString(16). "},{"title":"UUIDNumToString(str)​","type":1,"pageTitle":"Encoding Functions","url":"docs/en/sql-reference/functions/encoding-functions#uuidnumtostringstr","content":"Accepts a FixedString(16) value. Returns a string containing 36 characters in text format. "},{"title":"bitmaskToList(num)​","type":1,"pageTitle":"Encoding Functions","url":"docs/en/sql-reference/functions/encoding-functions#bitmasktolistnum","content":"Accepts an integer. Returns a string containing the list of powers of two that total the source number when summed. They are comma-separated without spaces in text format, in ascending order. "},{"title":"bitmaskToArray(num)​","type":1,"pageTitle":"Encoding Functions","url":"docs/en/sql-reference/functions/encoding-functions#bitmasktoarraynum","content":"Accepts an integer. Returns an array of UInt64 numbers containing the list of powers of two that total the source number when summed. Numbers in the array are in ascending order. "},{"title":"bitPositionsToArray(num)​","type":1,"pageTitle":"Encoding Functions","url":"docs/en/sql-reference/functions/encoding-functions#bitpositionstoarraynum","content":"Accepts an integer and converts it to an unsigned integer. Returns an array of UInt64 numbers containing the list of positions of bits of arg that equal 1, in ascending order. Syntax bitPositionsToArray(arg)  Arguments arg — Integer value. Int/UInt. Returned value An array containing a list of positions of bits that equal 1, in ascending order. Type: Array(UInt64). Example Query: SELECT bitPositionsToArray(toInt8(1)) AS bit_positions;  Result: ┌─bit_positions─┐ │ [0] │ └───────────────┘  Query: SELECT bitPositionsToArray(toInt8(-1)) AS bit_positions;  Result: ┌─bit_positions─────┐ │ [0,1,2,3,4,5,6,7] │ └───────────────────┘  "},{"title":"Functions for Working with H3 Indexes","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/geo/h3","content":"","keywords":""},{"title":"h3IsValid​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3isvalid","content":"Verifies whether the number is a valid H3 index. Syntax h3IsValid(h3index)  Parameter h3index — Hexagon index number. Type: UInt64. Returned values 1 — The number is a valid H3 index.0 — The number is not a valid H3 index. Type: UInt8. Example Query: SELECT h3IsValid(630814730351855103) AS h3IsValid;  Result: ┌─h3IsValid─┐ │ 1 │ └───────────┘  "},{"title":"h3GetResolution​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3getresolution","content":"Defines the resolution of the given H3 index. Syntax h3GetResolution(h3index)  Parameter h3index — Hexagon index number. Type: UInt64. Returned values Index resolution. Range: [0, 15].If the index is not valid, the function returns a random value. Use h3IsValid to verify the index. Type: UInt8. Example Query: SELECT h3GetResolution(639821929606596015) AS resolution;  Result: ┌─resolution─┐ │ 14 │ └────────────┘  "},{"title":"h3EdgeAngle​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3edgeangle","content":"Calculates the average length of the H3 hexagon edge in grades. Syntax h3EdgeAngle(resolution)  Parameter resolution — Index resolution. Type: UInt8. Range: [0, 15]. Returned values The average length of the H3 hexagon edge in grades. Type: Float64. Example Query: SELECT h3EdgeAngle(10) AS edgeAngle;  Result: ┌───────h3EdgeAngle(10)─┐ │ 0.0005927224846720883 │ └───────────────────────┘  "},{"title":"h3EdgeLengthM​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3edgelengthm","content":"Calculates the average length of the H3 hexagon edge in meters. Syntax h3EdgeLengthM(resolution)  Parameter resolution — Index resolution. Type: UInt8. Range: [0, 15]. Returned values The average length of the H3 hexagon edge in meters. Type: Float64. Example Query: SELECT h3EdgeLengthM(15) AS edgeLengthM;  Result: ┌─edgeLengthM─┐ │ 0.509713273 │ └─────────────┘  "},{"title":"h3EdgeLengthKm​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3edgelengthkm","content":"Calculates the average length of the H3 hexagon edge in kilometers. Syntax h3EdgeLengthKm(resolution)  Parameter resolution — Index resolution. Type: UInt8. Range: [0, 15]. Returned values The average length of the H3 hexagon edge in kilometers. Type: Float64. Example Query: SELECT h3EdgeLengthKm(15) AS edgeLengthKm;  Result: ┌─edgeLengthKm─┐ │ 0.000509713 │ └──────────────┘  "},{"title":"geoToH3​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#geotoh3","content":"Returns H3 point index (lon, lat) with specified resolution. Syntax geoToH3(lon, lat, resolution)  Arguments lon — Longitude. Type: Float64.lat — Latitude. Type: Float64.resolution — Index resolution. Range: [0, 15]. Type: UInt8. Returned values Hexagon index number.0 in case of error. Type: UInt64. Example Query: SELECT geoToH3(37.79506683, 55.71290588, 15) AS h3Index;  Result: ┌────────────h3Index─┐ │ 644325524701193974 │ └────────────────────┘  "},{"title":"h3ToGeo​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3togeo","content":"Returns the centroid longitude and latitude corresponding to the provided H3 index. Syntax h3ToGeo(h3Index)  Arguments h3Index — H3 Index. UInt64. Returned values A tuple consisting of two values: tuple(lon,lat). lon — Longitude. Float64. lat — Latitude. Float64. Example Query: SELECT h3ToGeo(644325524701193974) AS coordinates;  Result: ┌─coordinates───────────────────────────┐ │ (37.79506616830252,55.71290243145668) │ └───────────────────────────────────────┘  "},{"title":"h3ToGeoBoundary​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3togeoboundary","content":"Returns array of pairs (lon, lat), which corresponds to the boundary of the provided H3 index. Syntax h3ToGeoBoundary(h3Index)  Arguments h3Index — H3 Index. Type: UInt64. Returned values Array of pairs '(lon, lat)'. Type: Array(Float64, Float64). Example Query: SELECT h3ToGeoBoundary(644325524701193974) AS coordinates;  Result: ┌─h3ToGeoBoundary(599686042433355775)────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ [(37.2713558667319,-121.91508032705622),(37.353926450852256,-121.8622232890249),(37.42834118609435,-121.92354999630156),(37.42012867767779,-122.03773496427027),(37.33755608435299,-122.090428929044),(37.26319797461824,-122.02910130919001)] │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘  "},{"title":"h3kRing​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3kring","content":"Lists all the H3 hexagons in the raduis of k from the given hexagon in random order. Syntax h3kRing(h3index, k)  Arguments h3index — Hexagon index number. Type: UInt64.k — Radius. Type: integer Returned values Array of H3 indexes. Type: Array(UInt64). Example Query: SELECT arrayJoin(h3kRing(644325529233966508, 1)) AS h3index;  Result: ┌────────────h3index─┐ │ 644325529233966508 │ │ 644325529233966497 │ │ 644325529233966510 │ │ 644325529233966504 │ │ 644325529233966509 │ │ 644325529233966355 │ │ 644325529233966354 │ └────────────────────┘  "},{"title":"h3GetBaseCell​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3getbasecell","content":"Returns the base cell number of the H3 index. Syntax h3GetBaseCell(index)  Parameter index — Hexagon index number. Type: UInt64. Returned value Hexagon base cell number. Type: UInt8. Example Query: SELECT h3GetBaseCell(612916788725809151) AS basecell;  Result: ┌─basecell─┐ │ 12 │ └──────────┘  "},{"title":"h3HexAreaM2​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3hexaream2","content":"Returns average hexagon area in square meters at the given resolution. Syntax h3HexAreaM2(resolution)  Parameter resolution — Index resolution. Range: [0, 15]. Type: UInt8. Returned value Area in square meters. Type: Float64. Example Query: SELECT h3HexAreaM2(13) AS area;  Result: ┌─area─┐ │ 43.9 │ └──────┘  "},{"title":"h3HexAreaKm2​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3hexareakm2","content":"Returns average hexagon area in square kilometers at the given resolution. Syntax h3HexAreaKm2(resolution)  Parameter resolution — Index resolution. Range: [0, 15]. Type: UInt8. Returned value Area in square kilometers. Type: Float64. Example Query: SELECT h3HexAreaKm2(13) AS area;  Result: ┌──────area─┐ │ 0.0000439 │ └───────────┘  "},{"title":"h3IndexesAreNeighbors​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3indexesareneighbors","content":"Returns whether or not the provided H3 indexes are neighbors. Syntax h3IndexesAreNeighbors(index1, index2)  Arguments index1 — Hexagon index number. Type: UInt64.index2 — Hexagon index number. Type: UInt64. Returned value 1 — Indexes are neighbours.0 — Indexes are not neighbours. Type: UInt8. Example Query: SELECT h3IndexesAreNeighbors(617420388351344639, 617420388352655359) AS n;  Result: ┌─n─┐ │ 1 │ └───┘  "},{"title":"h3ToChildren​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3tochildren","content":"Returns an array of child indexes for the given H3 index. Syntax h3ToChildren(index, resolution)  Arguments index — Hexagon index number. Type: UInt64.resolution — Index resolution. Range: [0, 15]. Type: UInt8. Returned values Array of the child H3-indexes. Type: Array(UInt64). Example Query: SELECT h3ToChildren(599405990164561919, 6) AS children;  Result: ┌─children───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ [603909588852408319,603909588986626047,603909589120843775,603909589255061503,603909589389279231,603909589523496959,603909589657714687] │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘  "},{"title":"h3ToParent​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3toparent","content":"Returns the parent (coarser) index containing the given H3 index. Syntax h3ToParent(index, resolution)  Arguments index — Hexagon index number. Type: UInt64.resolution — Index resolution. Range: [0, 15]. Type: UInt8. Returned value Parent H3 index. Type: UInt64. Example Query: SELECT h3ToParent(599405990164561919, 3) AS parent;  Result: ┌─────────────parent─┐ │ 590398848891879423 │ └────────────────────┘  "},{"title":"h3ToString​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3tostring","content":"Converts the H3Index representation of the index to the string representation. h3ToString(index)  Parameter index — Hexagon index number. Type: UInt64. Returned value String representation of the H3 index. Type: String. Example Query: SELECT h3ToString(617420388352917503) AS h3_string;  Result: ┌─h3_string───────┐ │ 89184926cdbffff │ └─────────────────┘  "},{"title":"stringToH3​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#stringtoh3","content":"Converts the string representation to the H3Index (UInt64) representation. Syntax stringToH3(index_str)  Parameter index_str — String representation of the H3 index. Type: String. Returned value Hexagon index number. Returns 0 on error. Type: UInt64. Example Query: SELECT stringToH3('89184926cc3ffff') AS index;  Result: ┌──────────────index─┐ │ 617420388351344639 │ └────────────────────┘  "},{"title":"h3GetResolution​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3getresolution","content":"Returns the resolution of the H3 index. Syntax h3GetResolution(index)  Parameter index — Hexagon index number. Type: UInt64. Returned value Index resolution. Range: [0, 15]. Type: UInt8. Example Query: SELECT h3GetResolution(617420388352917503) AS res;  Result: ┌─res─┐ │ 9 │ └─────┘  "},{"title":"h3IsResClassIII​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3isresclassIII","content":"Returns whether H3 index has a resolution with Class III orientation. Syntax h3IsResClassIII(index)  Parameter index — Hexagon index number. Type: UInt64. Returned value 1 — Index has a resolution with Class III orientation.0 — Index doesn't have a resolution with Class III orientation. Type: UInt8. Example Query: SELECT h3IsResClassIII(617420388352917503) AS res;  Result: ┌─res─┐ │ 1 │ └─────┘  "},{"title":"h3IsPentagon​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3ispentagon","content":"Returns whether this H3 index represents a pentagonal cell. Syntax h3IsPentagon(index)  Parameter index — Hexagon index number. Type: UInt64. Returned value 1 — Index represents a pentagonal cell.0 — Index doesn't represent a pentagonal cell. Type: UInt8. Example Query: SELECT h3IsPentagon(644721767722457330) AS pentagon;  Result: ┌─pentagon─┐ │ 0 │ └──────────┘  "},{"title":"h3GetFaces​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3getfaces","content":"Returns icosahedron faces intersected by a given H3 index. Syntax h3GetFaces(index)  Parameter index — Hexagon index number. Type: UInt64. Returned values Array containing icosahedron faces intersected by a given H3 index. Type: Array(UInt64). Example Query: SELECT h3GetFaces(599686042433355775) AS faces;  Result: ┌─faces─┐ │ [7] │ └───────┘  "},{"title":"h3CellAreaM2​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3cellaream2","content":"Returns the exact area of a specific cell in square meters corresponding to the given input H3 index. Syntax h3CellAreaM2(index)  Parameter index — Hexagon index number. Type: UInt64. Returned value Cell area in square meters. Type: Float64. Example Query: SELECT h3CellAreaM2(579205133326352383) AS area;  Result: ┌───────────────area─┐ │ 4106166334463.9233 │ └────────────────────┘  "},{"title":"h3CellAreaRads2​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3cellarearads2","content":"Returns the exact area of a specific cell in square radians corresponding to the given input H3 index. Syntax h3CellAreaRads2(index)  Parameter index — Hexagon index number. Type: UInt64. Returned value Cell area in square radians. Type: Float64. Example Query: SELECT h3CellAreaRads2(579205133326352383) AS area;  Result: ┌────────────────area─┐ │ 0.10116268528089567 │ └─────────────────────┘  "},{"title":"h3ToCenterChild​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3tocenterchild","content":"Returns the center child (finer) H3 index contained by given H3 at the given resolution. Syntax h3ToCenterChild(index, resolution)  Parameter index — Hexagon index number. Type: UInt64.resolution — Index resolution. Range: [0, 15]. Type: UInt8. Returned values H3 index of the center child contained by given H3 at the given resolution. Type: UInt64. Example Query: SELECT h3ToCenterChild(577023702256844799,1) AS centerToChild;  Result: ┌──────centerToChild─┐ │ 581496515558637567 │ └────────────────────┘  "},{"title":"h3ExactEdgeLengthM​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3exactedgelengthm","content":"Returns the exact edge length of the unidirectional edge represented by the input h3 index in meters. Syntax h3ExactEdgeLengthM(index)  Parameter index — Hexagon index number. Type: UInt64. Returned value Exact edge length in meters. Type: Float64. Example Query: SELECT h3ExactEdgeLengthM(1310277011704381439) AS exactEdgeLengthM;;  Result: ┌───exactEdgeLengthM─┐ │ 195449.63163407316 │ └────────────────────┘  "},{"title":"h3ExactEdgeLengthKm​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3exactedgelengthkm","content":"Returns the exact edge length of the unidirectional edge represented by the input h3 index in kilometers. Syntax h3ExactEdgeLengthKm(index)  Parameter index — Hexagon index number. Type: UInt64. Returned value Exact edge length in kilometers. Type: Float64. Example Query: SELECT h3ExactEdgeLengthKm(1310277011704381439) AS exactEdgeLengthKm;;  Result: ┌──exactEdgeLengthKm─┐ │ 195.44963163407317 │ └────────────────────┘  "},{"title":"h3ExactEdgeLengthRads​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3exactedgelengthrads","content":"Returns the exact edge length of the unidirectional edge represented by the input h3 index in radians. Syntax h3ExactEdgeLengthRads(index)  Parameter index — Hexagon index number. Type: UInt64. Returned value Exact edge length in radians. Type: Float64. Example Query: SELECT h3ExactEdgeLengthRads(1310277011704381439) AS exactEdgeLengthRads;;  Result: ┌──exactEdgeLengthRads─┐ │ 0.030677980118976447 │ └──────────────────────┘  "},{"title":"h3NumHexagons​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"docs/en/sql-reference/functions/geo/h3#h3numhexagons","content":"Returns the number of unique H3 indices at the given resolution. Syntax h3NumHexagons(resolution)  Parameter resolution — Index resolution. Range: [0, 15]. Type: UInt8. Returned value Number of H3 indices. Type: Int64. Example Query: SELECT h3NumHexagons(3) AS numHexagons;  Result: ┌─numHexagons─┐ │ 41162 │ └─────────────┘  Original article "},{"title":"Geo Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/geo/","content":"Geo Functions Original article","keywords":""},{"title":"Introspection Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/introspection","content":"","keywords":""},{"title":"addressToLine​","type":1,"pageTitle":"Introspection Functions","url":"docs/en/sql-reference/functions/introspection#addresstoline","content":"Converts virtual memory address inside ClickHouse server process to the filename and the line number in ClickHouse source code. If you use official ClickHouse packages, you need to install the clickhouse-common-static-dbg package. Syntax addressToLine(address_of_binary_instruction)  Arguments address_of_binary_instruction (UInt64) — Address of instruction in a running process. Returned value Source code filename and the line number in this file delimited by colon. For example, `/build/obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:199`, where `199` is a line number. Name of a binary, if the function couldn’t find the debug information. Empty string, if the address is not valid. Type: String. Example Enabling introspection functions: SET allow_introspection_functions=1;  Selecting the first string from the trace_log system table: SELECT * FROM system.trace_log LIMIT 1 \\G;  Row 1: ────── event_date: 2019-11-19 event_time: 2019-11-19 18:57:23 revision: 54429 timer_type: Real thread_number: 48 query_id: 421b6855-1858-45a5-8f37-f383409d6d72 trace: [140658411141617,94784174532828,94784076370703,94784076372094,94784076361020,94784175007680,140658411116251,140658403895439]  The trace field contains the stack trace at the moment of sampling. Getting the source code filename and the line number for a single address: SELECT addressToLine(94784076370703) \\G;  Row 1: ────── addressToLine(94784076370703): /build/obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:199  Applying the function to the whole stack trace: SELECT arrayStringConcat(arrayMap(x -&gt; addressToLine(x), trace), '\\n') AS trace_source_code_lines FROM system.trace_log LIMIT 1 \\G  The arrayMap function allows to process each individual element of the trace array by the addressToLine function. The result of this processing you see in the trace_source_code_lines column of output. Row 1: ────── trace_source_code_lines: /lib/x86_64-linux-gnu/libpthread-2.27.so /usr/lib/debug/usr/bin/clickhouse /build/obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:199 /build/obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:155 /usr/include/c++/9/bits/atomic_base.h:551 /usr/lib/debug/usr/bin/clickhouse /lib/x86_64-linux-gnu/libpthread-2.27.so /build/glibc-OTsEL5/glibc-2.27/misc/../sysdeps/unix/sysv/linux/x86_64/clone.S:97  "},{"title":"addressToLineWithInlines​","type":1,"pageTitle":"Introspection Functions","url":"docs/en/sql-reference/functions/introspection#addresstolinewithinlines","content":"Similar to addressToLine, but it will return an Array with all inline functions, and will be much slower as a price. If you use official ClickHouse packages, you need to install the clickhouse-common-static-dbg package. Syntax addressToLineWithInlines(address_of_binary_instruction)  Arguments address_of_binary_instruction (UInt64) — Address of instruction in a running process. Returned value Array which first element is source code filename and the line number in this file delimited by colon. And from second element, inline functions' source code filename and line number and function name are listed. Array with single element which is name of a binary, if the function couldn’t find the debug information. Empty array, if the address is not valid. Type: Array(String). Example Enabling introspection functions: SET allow_introspection_functions=1;  Applying the function to address. SELECT addressToLineWithInlines(531055181::UInt64);  ┌─addressToLineWithInlines(CAST('531055181', 'UInt64'))────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ ['./src/Functions/addressToLineWithInlines.cpp:98','./build_normal_debug/./src/Functions/addressToLineWithInlines.cpp:176:DB::(anonymous namespace)::FunctionAddressToLineWithInlines::implCached(unsigned long) const'] │ └──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘  Applying the function to the whole stack trace: SELECT ta, addressToLineWithInlines(arrayJoin(trace) as ta) FROM system.trace_log WHERE query_id = '5e173544-2020-45de-b645-5deebe2aae54';  The arrayJoin functions will split array to rows. ┌────────ta─┬─addressToLineWithInlines(arrayJoin(trace))───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ 365497529 │ ['./build_normal_debug/./contrib/libcxx/include/string_view:252'] │ │ 365593602 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:191'] │ │ 365593866 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:0'] │ │ 365592528 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:0'] │ │ 365591003 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:477'] │ │ 365590479 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:442'] │ │ 365590600 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:457'] │ │ 365598941 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:0'] │ │ 365607098 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:0'] │ │ 365590571 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:451'] │ │ 365598941 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:0'] │ │ 365607098 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:0'] │ │ 365590571 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:451'] │ │ 365598941 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:0'] │ │ 365607098 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:0'] │ │ 365590571 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:451'] │ │ 365598941 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:0'] │ │ 365597289 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:807'] │ │ 365599840 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:1118'] │ │ 531058145 │ ['./build_normal_debug/./src/Functions/addressToLineWithInlines.cpp:152'] │ │ 531055181 │ ['./src/Functions/addressToLineWithInlines.cpp:98','./build_normal_debug/./src/Functions/addressToLineWithInlines.cpp:176:DB::(anonymous namespace)::FunctionAddressToLineWithInlines::implCached(unsigned long) const'] │ │ 422333613 │ ['./build_normal_debug/./src/Functions/IFunctionAdaptors.h:21'] │ │ 586866022 │ ['./build_normal_debug/./src/Functions/IFunction.cpp:216'] │ │ 586869053 │ ['./build_normal_debug/./src/Functions/IFunction.cpp:264'] │ │ 586873237 │ ['./build_normal_debug/./src/Functions/IFunction.cpp:334'] │ │ 597901620 │ ['./build_normal_debug/./src/Interpreters/ExpressionActions.cpp:601'] │ │ 597898534 │ ['./build_normal_debug/./src/Interpreters/ExpressionActions.cpp:718'] │ │ 630442912 │ ['./build_normal_debug/./src/Processors/Transforms/ExpressionTransform.cpp:23'] │ │ 546354050 │ ['./build_normal_debug/./src/Processors/ISimpleTransform.h:38'] │ │ 626026993 │ ['./build_normal_debug/./src/Processors/ISimpleTransform.cpp:89'] │ │ 626294022 │ ['./build_normal_debug/./src/Processors/Executors/ExecutionThreadContext.cpp:45'] │ │ 626293730 │ ['./build_normal_debug/./src/Processors/Executors/ExecutionThreadContext.cpp:63'] │ │ 626169525 │ ['./build_normal_debug/./src/Processors/Executors/PipelineExecutor.cpp:213'] │ │ 626170308 │ ['./build_normal_debug/./src/Processors/Executors/PipelineExecutor.cpp:178'] │ │ 626166348 │ ['./build_normal_debug/./src/Processors/Executors/PipelineExecutor.cpp:329'] │ │ 626163461 │ ['./build_normal_debug/./src/Processors/Executors/PipelineExecutor.cpp:84'] │ │ 626323536 │ ['./build_normal_debug/./src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:85'] │ │ 626323277 │ ['./build_normal_debug/./src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:112'] │ │ 626323133 │ ['./build_normal_debug/./contrib/libcxx/include/type_traits:3682'] │ │ 626323041 │ ['./build_normal_debug/./contrib/libcxx/include/tuple:1415'] │ └───────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘  "},{"title":"addressToSymbol​","type":1,"pageTitle":"Introspection Functions","url":"docs/en/sql-reference/functions/introspection#addresstosymbol","content":"Converts virtual memory address inside ClickHouse server process to the symbol from ClickHouse object files. Syntax addressToSymbol(address_of_binary_instruction)  Arguments address_of_binary_instruction (UInt64) — Address of instruction in a running process. Returned value Symbol from ClickHouse object files.Empty string, if the address is not valid. Type: String. Example Enabling introspection functions: SET allow_introspection_functions=1;  Selecting the first string from the trace_log system table: SELECT * FROM system.trace_log LIMIT 1 \\G;  Row 1: ────── event_date: 2019-11-20 event_time: 2019-11-20 16:57:59 revision: 54429 timer_type: Real thread_number: 48 query_id: 724028bf-f550-45aa-910d-2af6212b94ac trace: [94138803686098,94138815010911,94138815096522,94138815101224,94138815102091,94138814222988,94138806823642,94138814457211,94138806823642,94138814457211,94138806823642,94138806795179,94138806796144,94138753770094,94138753771646,94138753760572,94138852407232,140399185266395,140399178045583]  The trace field contains the stack trace at the moment of sampling. Getting a symbol for a single address: SELECT addressToSymbol(94138803686098) \\G;  Row 1: ────── addressToSymbol(94138803686098): _ZNK2DB24IAggregateFunctionHelperINS_20AggregateFunctionSumImmNS_24AggregateFunctionSumDataImEEEEE19addBatchSinglePlaceEmPcPPKNS_7IColumnEPNS_5ArenaE  Applying the function to the whole stack trace: SELECT arrayStringConcat(arrayMap(x -&gt; addressToSymbol(x), trace), '\\n') AS trace_symbols FROM system.trace_log LIMIT 1 \\G  The arrayMap function allows to process each individual element of the trace array by the addressToSymbols function. The result of this processing you see in the trace_symbols column of output. Row 1: ────── trace_symbols: _ZNK2DB24IAggregateFunctionHelperINS_20AggregateFunctionSumImmNS_24AggregateFunctionSumDataImEEEEE19addBatchSinglePlaceEmPcPPKNS_7IColumnEPNS_5ArenaE _ZNK2DB10Aggregator21executeWithoutKeyImplERPcmPNS0_28AggregateFunctionInstructionEPNS_5ArenaE _ZN2DB10Aggregator14executeOnBlockESt6vectorIN3COWINS_7IColumnEE13immutable_ptrIS3_EESaIS6_EEmRNS_22AggregatedDataVariantsERS1_IPKS3_SaISC_EERS1_ISE_SaISE_EERb _ZN2DB10Aggregator14executeOnBlockERKNS_5BlockERNS_22AggregatedDataVariantsERSt6vectorIPKNS_7IColumnESaIS9_EERS6_ISB_SaISB_EERb _ZN2DB10Aggregator7executeERKSt10shared_ptrINS_17IBlockInputStreamEERNS_22AggregatedDataVariantsE _ZN2DB27AggregatingBlockInputStream8readImplEv _ZN2DB17IBlockInputStream4readEv _ZN2DB26ExpressionBlockInputStream8readImplEv _ZN2DB17IBlockInputStream4readEv _ZN2DB26ExpressionBlockInputStream8readImplEv _ZN2DB17IBlockInputStream4readEv _ZN2DB28AsynchronousBlockInputStream9calculateEv _ZNSt17_Function_handlerIFvvEZN2DB28AsynchronousBlockInputStream4nextEvEUlvE_E9_M_invokeERKSt9_Any_data _ZN14ThreadPoolImplI20ThreadFromGlobalPoolE6workerESt14_List_iteratorIS0_E _ZZN20ThreadFromGlobalPoolC4IZN14ThreadPoolImplIS_E12scheduleImplIvEET_St8functionIFvvEEiSt8optionalImEEUlvE1_JEEEOS4_DpOT0_ENKUlvE_clEv _ZN14ThreadPoolImplISt6threadE6workerESt14_List_iteratorIS0_E execute_native_thread_routine start_thread clone  "},{"title":"demangle​","type":1,"pageTitle":"Introspection Functions","url":"docs/en/sql-reference/functions/introspection#demangle","content":"Converts a symbol that you can get using the addressToSymbol function to the C++ function name. Syntax demangle(symbol)  Arguments symbol (String) — Symbol from an object file. Returned value Name of the C++ function.Empty string if a symbol is not valid. Type: String. Example Enabling introspection functions: SET allow_introspection_functions=1;  Selecting the first string from the trace_log system table: SELECT * FROM system.trace_log LIMIT 1 \\G;  Row 1: ────── event_date: 2019-11-20 event_time: 2019-11-20 16:57:59 revision: 54429 timer_type: Real thread_number: 48 query_id: 724028bf-f550-45aa-910d-2af6212b94ac trace: [94138803686098,94138815010911,94138815096522,94138815101224,94138815102091,94138814222988,94138806823642,94138814457211,94138806823642,94138814457211,94138806823642,94138806795179,94138806796144,94138753770094,94138753771646,94138753760572,94138852407232,140399185266395,140399178045583]  The trace field contains the stack trace at the moment of sampling. Getting a function name for a single address: SELECT demangle(addressToSymbol(94138803686098)) \\G;  Row 1: ────── demangle(addressToSymbol(94138803686098)): DB::IAggregateFunctionHelper&lt;DB::AggregateFunctionSum&lt;unsigned long, unsigned long, DB::AggregateFunctionSumData&lt;unsigned long&gt; &gt; &gt;::addBatchSinglePlace(unsigned long, char*, DB::IColumn const**, DB::Arena*) const  Applying the function to the whole stack trace: SELECT arrayStringConcat(arrayMap(x -&gt; demangle(addressToSymbol(x)), trace), '\\n') AS trace_functions FROM system.trace_log LIMIT 1 \\G  The arrayMap function allows to process each individual element of the trace array by the demangle function. The result of this processing you see in the trace_functions column of output. Row 1: ────── trace_functions: DB::IAggregateFunctionHelper&lt;DB::AggregateFunctionSum&lt;unsigned long, unsigned long, DB::AggregateFunctionSumData&lt;unsigned long&gt; &gt; &gt;::addBatchSinglePlace(unsigned long, char*, DB::IColumn const**, DB::Arena*) const DB::Aggregator::executeWithoutKeyImpl(char*&amp;, unsigned long, DB::Aggregator::AggregateFunctionInstruction*, DB::Arena*) const DB::Aggregator::executeOnBlock(std::vector&lt;COW&lt;DB::IColumn&gt;::immutable_ptr&lt;DB::IColumn&gt;, std::allocator&lt;COW&lt;DB::IColumn&gt;::immutable_ptr&lt;DB::IColumn&gt; &gt; &gt;, unsigned long, DB::AggregatedDataVariants&amp;, std::vector&lt;DB::IColumn const*, std::allocator&lt;DB::IColumn const*&gt; &gt;&amp;, std::vector&lt;std::vector&lt;DB::IColumn const*, std::allocator&lt;DB::IColumn const*&gt; &gt;, std::allocator&lt;std::vector&lt;DB::IColumn const*, std::allocator&lt;DB::IColumn const*&gt; &gt; &gt; &gt;&amp;, bool&amp;) DB::Aggregator::executeOnBlock(DB::Block const&amp;, DB::AggregatedDataVariants&amp;, std::vector&lt;DB::IColumn const*, std::allocator&lt;DB::IColumn const*&gt; &gt;&amp;, std::vector&lt;std::vector&lt;DB::IColumn const*, std::allocator&lt;DB::IColumn const*&gt; &gt;, std::allocator&lt;std::vector&lt;DB::IColumn const*, std::allocator&lt;DB::IColumn const*&gt; &gt; &gt; &gt;&amp;, bool&amp;) DB::Aggregator::execute(std::shared_ptr&lt;DB::IBlockInputStream&gt; const&amp;, DB::AggregatedDataVariants&amp;) DB::AggregatingBlockInputStream::readImpl() DB::IBlockInputStream::read() DB::ExpressionBlockInputStream::readImpl() DB::IBlockInputStream::read() DB::ExpressionBlockInputStream::readImpl() DB::IBlockInputStream::read() DB::AsynchronousBlockInputStream::calculate() std::_Function_handler&lt;void (), DB::AsynchronousBlockInputStream::next()::{lambda()#1}&gt;::_M_invoke(std::_Any_data const&amp;) ThreadPoolImpl&lt;ThreadFromGlobalPool&gt;::worker(std::_List_iterator&lt;ThreadFromGlobalPool&gt;) ThreadFromGlobalPool::ThreadFromGlobalPool&lt;ThreadPoolImpl&lt;ThreadFromGlobalPool&gt;::scheduleImpl&lt;void&gt;(std::function&lt;void ()&gt;, int, std::optional&lt;unsigned long&gt;)::{lambda()#3}&gt;(ThreadPoolImpl&lt;ThreadFromGlobalPool&gt;::scheduleImpl&lt;void&gt;(std::function&lt;void ()&gt;, int, std::optional&lt;unsigned long&gt;)::{lambda()#3}&amp;&amp;)::{lambda()#1}::operator()() const ThreadPoolImpl&lt;std::thread&gt;::worker(std::_List_iterator&lt;std::thread&gt;) execute_native_thread_routine start_thread clone  "},{"title":"tid​","type":1,"pageTitle":"Introspection Functions","url":"docs/en/sql-reference/functions/introspection#tid","content":"Returns id of the thread, in which current Block is processed. Syntax tid()  Returned value Current thread id. Uint64. Example Query: SELECT tid();  Result: ┌─tid()─┐ │ 3878 │ └───────┘  "},{"title":"logTrace​","type":1,"pageTitle":"Introspection Functions","url":"docs/en/sql-reference/functions/introspection#logtrace","content":"Emits trace log message to server log for each Block. Syntax logTrace('message')  Arguments message — Message that is emitted to server log. String. Returned value Always returns 0. Example Query: SELECT logTrace('logTrace message');  Result: ┌─logTrace('logTrace message')─┐ │ 0 │ └──────────────────────────────┘  "},{"title":"Array Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/array-functions","content":"","keywords":""},{"title":"empty​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#function-empty","content":"Checks whether the input array is empty. Syntax empty([x])  An array is considered empty if it does not contain any elements. note Can be optimized by enabling the optimize_functions_to_subcolumns setting. With optimize_functions_to_subcolumns = 1 the function reads only size0 subcolumn instead of reading and processing the whole array column. The query SELECT empty(arr) FROM TABLE; transforms to SELECT arr.size0 = 0 FROM TABLE;. The function also works for strings or UUID. Arguments [x] — Input array. Array. Returned value Returns 1 for an empty array or 0 for a non-empty array. Type: UInt8. Example Query: SELECT empty([]);  Result: ┌─empty(array())─┐ │ 1 │ └────────────────┘  "},{"title":"notEmpty​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#function-notempty","content":"Checks whether the input array is non-empty. Syntax notEmpty([x])  An array is considered non-empty if it contains at least one element. note Can be optimized by enabling the optimize_functions_to_subcolumns setting. With optimize_functions_to_subcolumns = 1 the function reads only size0 subcolumn instead of reading and processing the whole array column. The query SELECT notEmpty(arr) FROM table transforms to SELECT arr.size0 != 0 FROM TABLE. The function also works for strings or UUID. Arguments [x] — Input array. Array. Returned value Returns 1 for a non-empty array or 0 for an empty array. Type: UInt8. Example Query: SELECT notEmpty([1,2]);  Result: ┌─notEmpty([1, 2])─┐ │ 1 │ └──────────────────┘  "},{"title":"length​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#array_functions-length","content":"Returns the number of items in the array. The result type is UInt64. The function also works for strings. Can be optimized by enabling the optimize_functions_to_subcolumns setting. With optimize_functions_to_subcolumns = 1 the function reads only size0 subcolumn instead of reading and processing the whole array column. The query SELECT length(arr) FROM table transforms to SELECT arr.size0 FROM TABLE. "},{"title":"emptyArrayUInt8, emptyArrayUInt16, emptyArrayUInt32, emptyArrayUInt64​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#emptyarrayuint8-emptyarrayuint16-emptyarrayuint32-emptyarrayuint64","content":""},{"title":"emptyArrayInt8, emptyArrayInt16, emptyArrayInt32, emptyArrayInt64​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#emptyarrayint8-emptyarrayint16-emptyarrayint32-emptyarrayint64","content":""},{"title":"emptyArrayFloat32, emptyArrayFloat64​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#emptyarrayfloat32-emptyarrayfloat64","content":""},{"title":"emptyArrayDate, emptyArrayDateTime​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#emptyarraydate-emptyarraydatetime","content":""},{"title":"emptyArrayString​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#emptyarraystring","content":"Accepts zero arguments and returns an empty array of the appropriate type. "},{"title":"emptyArrayToSingle​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#emptyarraytosingle","content":"Accepts an empty array and returns a one-element array that is equal to the default value. "},{"title":"range(end), range([start, ] end [, step])​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#range","content":"Returns an array of UInt numbers from start to end - 1 by step. Syntax range([start, ] end [, step])  Arguments start — The first element of the array. Optional, required if step is used. Default value: 0. UIntend — The number before which the array is constructed. Required. UIntstep — Determines the incremental step between each element in the array. Optional. Default value: 1. UInt Returned value Array of UInt numbers from start to end - 1 by step. Implementation details All arguments must be positive values: start, end, step are UInt data types, as well as elements of the returned array.An exception is thrown if query results in arrays with a total length of more than number of elements specified by the function_range_max_elements_in_block setting. Examples Query: SELECT range(5), range(1, 5), range(1, 5, 2);  Result: ┌─range(5)────┬─range(1, 5)─┬─range(1, 5, 2)─┐ │ [0,1,2,3,4] │ [1,2,3,4] │ [1,3] │ └─────────────┴─────────────┴────────────────┘  "},{"title":"array(x1, …), operator [x1, …]​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arrayx1-operator-x1","content":"Creates an array from the function arguments. The arguments must be constants and have types that have the smallest common type. At least one argument must be passed, because otherwise it isn’t clear which type of array to create. That is, you can’t use this function to create an empty array (to do that, use the ‘emptyArray*’ function described above). Returns an ‘Array(T)’ type result, where ‘T’ is the smallest common type out of the passed arguments. "},{"title":"arrayConcat​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arrayconcat","content":"Combines arrays passed as arguments. arrayConcat(arrays)  Arguments arrays – Arbitrary number of arguments of Array type.Example SELECT arrayConcat([1, 2], [3, 4], [5, 6]) AS res  ┌─res───────────┐ │ [1,2,3,4,5,6] │ └───────────────┘  "},{"title":"arrayElement(arr, n), operator arr[n]​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arrayelementarr-n-operator-arrn","content":"Get the element with the index n from the array arr. n must be any integer type. Indexes in an array begin from one. Negative indexes are supported. In this case, it selects the corresponding element numbered from the end. For example, arr[-1] is the last item in the array. If the index falls outside of the bounds of an array, it returns some default value (0 for numbers, an empty string for strings, etc.), except for the case with a non-constant array and a constant index 0 (in this case there will be an error Array indices are 1-based). "},{"title":"has(arr, elem)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#hasarr-elem","content":"Checks whether the ‘arr’ array has the ‘elem’ element. Returns 0 if the element is not in the array, or 1 if it is. NULL is processed as a value. SELECT has([1, 2, NULL], NULL)  ┌─has([1, 2, NULL], NULL)─┐ │ 1 │ └─────────────────────────┘  "},{"title":"hasAll​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#hasall","content":"Checks whether one array is a subset of another. hasAll(set, subset)  Arguments set – Array of any type with a set of elements.subset – Array of any type with elements that should be tested to be a subset of set. Return values 1, if set contains all of the elements from subset.0, otherwise. Peculiar properties An empty array is a subset of any array.Null processed as a value.Order of values in both of arrays does not matter. Examples SELECT hasAll([], []) returns 1. SELECT hasAll([1, Null], [Null]) returns 1. SELECT hasAll([1.0, 2, 3, 4], [1, 3]) returns 1. SELECT hasAll(['a', 'b'], ['a']) returns 1. SELECT hasAll([1], ['a']) returns 0. SELECT hasAll([[1, 2], [3, 4]], [[1, 2], [3, 5]]) returns 0. "},{"title":"hasAny​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#hasany","content":"Checks whether two arrays have intersection by some elements. hasAny(array1, array2)  Arguments array1 – Array of any type with a set of elements.array2 – Array of any type with a set of elements. Return values 1, if array1 and array2 have one similar element at least.0, otherwise. Peculiar properties Null processed as a value.Order of values in both of arrays does not matter. Examples SELECT hasAny([1], []) returns 0. SELECT hasAny([Null], [Null, 1]) returns 1. SELECT hasAny([-128, 1., 512], [1]) returns 1. SELECT hasAny([[1, 2], [3, 4]], ['a', 'c']) returns 0. SELECT hasAll([[1, 2], [3, 4]], [[1, 2], [1, 2]]) returns 1. "},{"title":"hasSubstr​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#hassubstr","content":"Checks whether all the elements of array2 appear in array1 in the same exact order. Therefore, the function will return 1, if and only if array1 = prefix + array2 + suffix. hasSubstr(array1, array2)  In other words, the functions will check whether all the elements of array2 are contained in array1 like the hasAll function. In addition, it will check that the elements are observed in the same order in both array1 and array2. For Example: hasSubstr([1,2,3,4], [2,3]) returns 1. However, hasSubstr([1,2,3,4], [3,2]) will return 0.hasSubstr([1,2,3,4], [1,2,3]) returns 1. However, hasSubstr([1,2,3,4], [1,2,4]) will return 0. Arguments array1 – Array of any type with a set of elements.array2 – Array of any type with a set of elements. Return values 1, if array1 contains array2.0, otherwise. Peculiar properties The function will return 1 if array2 is empty.Null processed as a value. In other words hasSubstr([1, 2, NULL, 3, 4], [2,3]) will return 0. However, hasSubstr([1, 2, NULL, 3, 4], [2,NULL,3]) will return 1Order of values in both of arrays does matter. Examples SELECT hasSubstr([], []) returns 1. SELECT hasSubstr([1, Null], [Null]) returns 1. SELECT hasSubstr([1.0, 2, 3, 4], [1, 3]) returns 0. SELECT hasSubstr(['a', 'b'], ['a']) returns 1. SELECT hasSubstr(['a', 'b' , 'c'], ['a', 'b']) returns 1. SELECT hasSubstr(['a', 'b' , 'c'], ['a', 'c']) returns 0. SELECT hasSubstr([[1, 2], [3, 4], [5, 6]], [[1, 2], [3, 4]]) returns 1. "},{"title":"indexOf(arr, x)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#indexofarr-x","content":"Returns the index of the first ‘x’ element (starting from 1) if it is in the array, or 0 if it is not. Example: SELECT indexOf([1, 3, NULL, NULL], NULL)  ┌─indexOf([1, 3, NULL, NULL], NULL)─┐ │ 3 │ └───────────────────────────────────┘  Elements set to NULL are handled as normal values. "},{"title":"arrayCount([func,] arr1, …)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#array-count","content":"Returns the number of elements in the arr array for which func returns something other than 0. If ‘func’ is not specified, it returns the number of non-zero elements in the array. Note that the arrayCount is a higher-order function. You can pass a lambda function to it as the first argument. "},{"title":"countEqual(arr, x)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#countequalarr-x","content":"Returns the number of elements in the array equal to x. Equivalent to arrayCount (elem -&gt; elem = x, arr). NULL elements are handled as separate values. Example: SELECT countEqual([1, 2, NULL, NULL], NULL)  ┌─countEqual([1, 2, NULL, NULL], NULL)─┐ │ 2 │ └──────────────────────────────────────┘  "},{"title":"arrayEnumerate(arr)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#array_functions-arrayenumerate","content":"Returns the array [1, 2, 3, …, length (arr) ] This function is normally used with ARRAY JOIN. It allows counting something just once for each array after applying ARRAY JOIN. Example: SELECT count() AS Reaches, countIf(num = 1) AS Hits FROM test.hits ARRAY JOIN GoalsReached, arrayEnumerate(GoalsReached) AS num WHERE CounterID = 160656 LIMIT 10  ┌─Reaches─┬──Hits─┐ │ 95606 │ 31406 │ └─────────┴───────┘  In this example, Reaches is the number of conversions (the strings received after applying ARRAY JOIN), and Hits is the number of pageviews (strings before ARRAY JOIN). In this particular case, you can get the same result in an easier way: SELECT sum(length(GoalsReached)) AS Reaches, count() AS Hits FROM test.hits WHERE (CounterID = 160656) AND notEmpty(GoalsReached)  ┌─Reaches─┬──Hits─┐ │ 95606 │ 31406 │ └─────────┴───────┘  This function can also be used in higher-order functions. For example, you can use it to get array indexes for elements that match a condition. "},{"title":"arrayEnumerateUniq(arr, …)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arrayenumerateuniqarr","content":"Returns an array the same size as the source array, indicating for each element what its position is among elements with the same value. For example: arrayEnumerateUniq([10, 20, 10, 30]) = [1, 1, 2, 1]. This function is useful when using ARRAY JOIN and aggregation of array elements. Example: SELECT Goals.ID AS GoalID, sum(Sign) AS Reaches, sumIf(Sign, num = 1) AS Visits FROM test.visits ARRAY JOIN Goals, arrayEnumerateUniq(Goals.ID) AS num WHERE CounterID = 160656 GROUP BY GoalID ORDER BY Reaches DESC LIMIT 10  ┌──GoalID─┬─Reaches─┬─Visits─┐ │ 53225 │ 3214 │ 1097 │ │ 2825062 │ 3188 │ 1097 │ │ 56600 │ 2803 │ 488 │ │ 1989037 │ 2401 │ 365 │ │ 2830064 │ 2396 │ 910 │ │ 1113562 │ 2372 │ 373 │ │ 3270895 │ 2262 │ 812 │ │ 1084657 │ 2262 │ 345 │ │ 56599 │ 2260 │ 799 │ │ 3271094 │ 2256 │ 812 │ └─────────┴─────────┴────────┘  In this example, each goal ID has a calculation of the number of conversions (each element in the Goals nested data structure is a goal that was reached, which we refer to as a conversion) and the number of sessions. Without ARRAY JOIN, we would have counted the number of sessions as sum(Sign). But in this particular case, the rows were multiplied by the nested Goals structure, so in order to count each session one time after this, we apply a condition to the value of the arrayEnumerateUniq(Goals.ID) function. The arrayEnumerateUniq function can take multiple arrays of the same size as arguments. In this case, uniqueness is considered for tuples of elements in the same positions in all the arrays. SELECT arrayEnumerateUniq([1, 1, 1, 2, 2, 2], [1, 1, 2, 1, 1, 2]) AS res  ┌─res───────────┐ │ [1,2,1,1,2,1] │ └───────────────┘  This is necessary when using ARRAY JOIN with a nested data structure and further aggregation across multiple elements in this structure. "},{"title":"arrayPopBack​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arraypopback","content":"Removes the last item from the array. arrayPopBack(array)  Arguments array – Array. Example SELECT arrayPopBack([1, 2, 3]) AS res;  ┌─res───┐ │ [1,2] │ └───────┘  "},{"title":"arrayPopFront​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arraypopfront","content":"Removes the first item from the array. arrayPopFront(array)  Arguments array – Array. Example SELECT arrayPopFront([1, 2, 3]) AS res;  ┌─res───┐ │ [2,3] │ └───────┘  "},{"title":"arrayPushBack​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arraypushback","content":"Adds one item to the end of the array. arrayPushBack(array, single_value)  Arguments array – Array.single_value – A single value. Only numbers can be added to an array with numbers, and only strings can be added to an array of strings. When adding numbers, ClickHouse automatically sets the single_value type for the data type of the array. For more information about the types of data in ClickHouse, see “Data types”. Can be NULL. The function adds a NULL element to an array, and the type of array elements converts to Nullable. Example SELECT arrayPushBack(['a'], 'b') AS res;  ┌─res───────┐ │ ['a','b'] │ └───────────┘  "},{"title":"arrayPushFront​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arraypushfront","content":"Adds one element to the beginning of the array. arrayPushFront(array, single_value)  Arguments array – Array.single_value – A single value. Only numbers can be added to an array with numbers, and only strings can be added to an array of strings. When adding numbers, ClickHouse automatically sets the single_value type for the data type of the array. For more information about the types of data in ClickHouse, see “Data types”. Can be NULL. The function adds a NULL element to an array, and the type of array elements converts to Nullable. Example SELECT arrayPushFront(['b'], 'a') AS res;  ┌─res───────┐ │ ['a','b'] │ └───────────┘  "},{"title":"arrayResize​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arrayresize","content":"Changes the length of the array. arrayResize(array, size[, extender])  Arguments: array — Array.size — Required length of the array. If size is less than the original size of the array, the array is truncated from the right. If size is larger than the initial size of the array, the array is extended to the right with extender values or default values for the data type of the array items.extender — Value for extending an array. Can be NULL. Returned value: An array of length size. Examples of calls SELECT arrayResize([1], 3);  ┌─arrayResize([1], 3)─┐ │ [1,0,0] │ └─────────────────────┘  SELECT arrayResize([1], 3, NULL);  ┌─arrayResize([1], 3, NULL)─┐ │ [1,NULL,NULL] │ └───────────────────────────┘  "},{"title":"arraySlice​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arrayslice","content":"Returns a slice of the array. arraySlice(array, offset[, length])  Arguments array – Array of data.offset – Indent from the edge of the array. A positive value indicates an offset on the left, and a negative value is an indent on the right. Numbering of the array items begins with 1.length – The length of the required slice. If you specify a negative value, the function returns an open slice [offset, array_length - length). If you omit the value, the function returns the slice [offset, the_end_of_array]. Example SELECT arraySlice([1, 2, NULL, 4, 5], 2, 3) AS res;  ┌─res────────┐ │ [2,NULL,4] │ └────────────┘  Array elements set to NULL are handled as normal values. "},{"title":"arraySort([func,] arr, …)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#array_functions-sort","content":"Sorts the elements of the arr array in ascending order. If the func function is specified, sorting order is determined by the result of the func function applied to the elements of the array. If func accepts multiple arguments, the arraySort function is passed several arrays that the arguments of func will correspond to. Detailed examples are shown at the end of arraySort description. Example of integer values sorting: SELECT arraySort([1, 3, 3, 0]);  ┌─arraySort([1, 3, 3, 0])─┐ │ [0,1,3,3] │ └─────────────────────────┘  Example of string values sorting: SELECT arraySort(['hello', 'world', '!']);  ┌─arraySort(['hello', 'world', '!'])─┐ │ ['!','hello','world'] │ └────────────────────────────────────┘  Consider the following sorting order for the NULL, NaN and Inf values: SELECT arraySort([1, nan, 2, NULL, 3, nan, -4, NULL, inf, -inf]);  ┌─arraySort([1, nan, 2, NULL, 3, nan, -4, NULL, inf, -inf])─┐ │ [-inf,-4,1,2,3,inf,nan,nan,NULL,NULL] │ └───────────────────────────────────────────────────────────┘  -Inf values are first in the array.NULL values are last in the array.NaN values are right before NULL.Inf values are right before NaN. Note that arraySort is a higher-order function. You can pass a lambda function to it as the first argument. In this case, sorting order is determined by the result of the lambda function applied to the elements of the array. Let’s consider the following example: SELECT arraySort((x) -&gt; -x, [1, 2, 3]) as res;  ┌─res─────┐ │ [3,2,1] │ └─────────┘  For each element of the source array, the lambda function returns the sorting key, that is, [1 –&gt; -1, 2 –&gt; -2, 3 –&gt; -3]. Since the arraySort function sorts the keys in ascending order, the result is [3, 2, 1]. Thus, the (x) –&gt; -x lambda function sets the descending order in a sorting. The lambda function can accept multiple arguments. In this case, you need to pass the arraySort function several arrays of identical length that the arguments of lambda function will correspond to. The resulting array will consist of elements from the first input array; elements from the next input array(s) specify the sorting keys. For example: SELECT arraySort((x, y) -&gt; y, ['hello', 'world'], [2, 1]) as res;  ┌─res────────────────┐ │ ['world', 'hello'] │ └────────────────────┘  Here, the elements that are passed in the second array ([2, 1]) define a sorting key for the corresponding element from the source array ([‘hello’, ‘world’]), that is, [‘hello’ –&gt; 2, ‘world’ –&gt; 1]. Since the lambda function does not use x, actual values of the source array do not affect the order in the result. So, ‘hello’ will be the second element in the result, and ‘world’ will be the first. Other examples are shown below. SELECT arraySort((x, y) -&gt; y, [0, 1, 2], ['c', 'b', 'a']) as res;  ┌─res─────┐ │ [2,1,0] │ └─────────┘  SELECT arraySort((x, y) -&gt; -y, [0, 1, 2], [1, 2, 3]) as res;  ┌─res─────┐ │ [2,1,0] │ └─────────┘  note To improve sorting efficiency, the Schwartzian transform is used. "},{"title":"arrayReverseSort([func,] arr, …)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#array_functions-reverse-sort","content":"Sorts the elements of the arr array in descending order. If the func function is specified, arr is sorted according to the result of the func function applied to the elements of the array, and then the sorted array is reversed. If func accepts multiple arguments, the arrayReverseSort function is passed several arrays that the arguments of func will correspond to. Detailed examples are shown at the end of arrayReverseSort description. Example of integer values sorting: SELECT arrayReverseSort([1, 3, 3, 0]);  ┌─arrayReverseSort([1, 3, 3, 0])─┐ │ [3,3,1,0] │ └────────────────────────────────┘  Example of string values sorting: SELECT arrayReverseSort(['hello', 'world', '!']);  ┌─arrayReverseSort(['hello', 'world', '!'])─┐ │ ['world','hello','!'] │ └───────────────────────────────────────────┘  Consider the following sorting order for the NULL, NaN and Inf values: SELECT arrayReverseSort([1, nan, 2, NULL, 3, nan, -4, NULL, inf, -inf]) as res;  ┌─res───────────────────────────────────┐ │ [inf,3,2,1,-4,-inf,nan,nan,NULL,NULL] │ └───────────────────────────────────────┘  Inf values are first in the array.NULL values are last in the array.NaN values are right before NULL.-Inf values are right before NaN. Note that the arrayReverseSort is a higher-order function. You can pass a lambda function to it as the first argument. Example is shown below. SELECT arrayReverseSort((x) -&gt; -x, [1, 2, 3]) as res;  ┌─res─────┐ │ [1,2,3] │ └─────────┘  The array is sorted in the following way: At first, the source array ([1, 2, 3]) is sorted according to the result of the lambda function applied to the elements of the array. The result is an array [3, 2, 1].Array that is obtained on the previous step, is reversed. So, the final result is [1, 2, 3]. The lambda function can accept multiple arguments. In this case, you need to pass the arrayReverseSort function several arrays of identical length that the arguments of lambda function will correspond to. The resulting array will consist of elements from the first input array; elements from the next input array(s) specify the sorting keys. For example: SELECT arrayReverseSort((x, y) -&gt; y, ['hello', 'world'], [2, 1]) as res;  ┌─res───────────────┐ │ ['hello','world'] │ └───────────────────┘  In this example, the array is sorted in the following way: At first, the source array ([‘hello’, ‘world’]) is sorted according to the result of the lambda function applied to the elements of the arrays. The elements that are passed in the second array ([2, 1]), define the sorting keys for corresponding elements from the source array. The result is an array [‘world’, ‘hello’].Array that was sorted on the previous step, is reversed. So, the final result is [‘hello’, ‘world’]. Other examples are shown below. SELECT arrayReverseSort((x, y) -&gt; y, [4, 3, 5], ['a', 'b', 'c']) AS res;  ┌─res─────┐ │ [5,3,4] │ └─────────┘  SELECT arrayReverseSort((x, y) -&gt; -y, [4, 3, 5], [1, 2, 3]) AS res;  ┌─res─────┐ │ [4,3,5] │ └─────────┘  "},{"title":"arrayUniq(arr, …)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arrayuniqarr","content":"If one argument is passed, it counts the number of different elements in the array. If multiple arguments are passed, it counts the number of different tuples of elements at corresponding positions in multiple arrays. If you want to get a list of unique items in an array, you can use arrayReduce(‘groupUniqArray’, arr). "},{"title":"arrayJoin(arr)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#array-functions-join","content":"A special function. See the section “ArrayJoin function”. "},{"title":"arrayDifference​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arraydifference","content":"Calculates the difference between adjacent array elements. Returns an array where the first element will be 0, the second is the difference between a[1] - a[0], etc. The type of elements in the resulting array is determined by the type inference rules for subtraction (e.g. UInt8 - UInt8 = Int16). Syntax arrayDifference(array)  Arguments array – Array. Returned values Returns an array of differences between adjacent elements. Type: UInt*, Int*, Float*. Example Query: SELECT arrayDifference([1, 2, 3, 4]);  Result: ┌─arrayDifference([1, 2, 3, 4])─┐ │ [0,1,1,1] │ └───────────────────────────────┘  Example of the overflow due to result type Int64: Query: SELECT arrayDifference([0, 10000000000000000000]);  Result: ┌─arrayDifference([0, 10000000000000000000])─┐ │ [0,-8446744073709551616] │ └────────────────────────────────────────────┘  "},{"title":"arrayDistinct​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arraydistinct","content":"Takes an array, returns an array containing the distinct elements only. Syntax arrayDistinct(array)  Arguments array – Array. Returned values Returns an array containing the distinct elements. Example Query: SELECT arrayDistinct([1, 2, 2, 3, 1]);  Result: ┌─arrayDistinct([1, 2, 2, 3, 1])─┐ │ [1,2,3] │ └────────────────────────────────┘  "},{"title":"arrayEnumerateDense(arr)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#array_functions-arrayenumeratedense","content":"Returns an array of the same size as the source array, indicating where each element first appears in the source array. Example: SELECT arrayEnumerateDense([10, 20, 10, 30])  ┌─arrayEnumerateDense([10, 20, 10, 30])─┐ │ [1,2,1,3] │ └───────────────────────────────────────┘  "},{"title":"arrayIntersect(arr)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#array-functions-arrayintersect","content":"Takes multiple arrays, returns an array with elements that are present in all source arrays. Example: SELECT arrayIntersect([1, 2], [1, 3], [2, 3]) AS no_intersect, arrayIntersect([1, 2], [1, 3], [1, 4]) AS intersect  ┌─no_intersect─┬─intersect─┐ │ [] │ [1] │ └──────────────┴───────────┘  "},{"title":"arrayReduce​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arrayreduce","content":"Applies an aggregate function to array elements and returns its result. The name of the aggregation function is passed as a string in single quotes 'max', 'sum'. When using parametric aggregate functions, the parameter is indicated after the function name in parentheses 'uniqUpTo(6)'. Syntax arrayReduce(agg_func, arr1, arr2, ..., arrN)  Arguments agg_func — The name of an aggregate function which should be a constant string.arr — Any number of array type columns as the parameters of the aggregation function. Returned value Example Query: SELECT arrayReduce('max', [1, 2, 3]);  Result: ┌─arrayReduce('max', [1, 2, 3])─┐ │ 3 │ └───────────────────────────────┘  If an aggregate function takes multiple arguments, then this function must be applied to multiple arrays of the same size. Query: SELECT arrayReduce('maxIf', [3, 5], [1, 0]);  Result: ┌─arrayReduce('maxIf', [3, 5], [1, 0])─┐ │ 3 │ └──────────────────────────────────────┘  Example with a parametric aggregate function: Query: SELECT arrayReduce('uniqUpTo(3)', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]);  Result: ┌─arrayReduce('uniqUpTo(3)', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])─┐ │ 4 │ └─────────────────────────────────────────────────────────────┘  "},{"title":"arrayReduceInRanges​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arrayreduceinranges","content":"Applies an aggregate function to array elements in given ranges and returns an array containing the result corresponding to each range. The function will return the same result as multiple arrayReduce(agg_func, arraySlice(arr1, index, length), ...). Syntax arrayReduceInRanges(agg_func, ranges, arr1, arr2, ..., arrN)  Arguments agg_func — The name of an aggregate function which should be a constant string.ranges — The ranges to aggretate which should be an array of tuples which containing the index and the length of each range.arr — Any number of Array type columns as the parameters of the aggregation function. Returned value Array containing results of the aggregate function over specified ranges. Type: Array. Example Query: SELECT arrayReduceInRanges( 'sum', [(1, 5), (2, 3), (3, 4), (4, 4)], [1000000, 200000, 30000, 4000, 500, 60, 7] ) AS res  Result: ┌─res─────────────────────────┐ │ [1234500,234000,34560,4567] │ └─────────────────────────────┘  "},{"title":"arrayReverse(arr)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arrayreverse","content":"Returns an array of the same size as the original array containing the elements in reverse order. Example: SELECT arrayReverse([1, 2, 3])  ┌─arrayReverse([1, 2, 3])─┐ │ [3,2,1] │ └─────────────────────────┘  "},{"title":"reverse(arr)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#array-functions-reverse","content":"Synonym for “arrayReverse” "},{"title":"arrayFlatten​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arrayflatten","content":"Converts an array of arrays to a flat array. Function: Applies to any depth of nested arrays.Does not change arrays that are already flat. The flattened array contains all the elements from all source arrays. Syntax flatten(array_of_arrays)  Alias: flatten. Arguments array_of_arrays — Array of arrays. For example, [[1,2,3], [4,5]]. Examples SELECT flatten([[[1]], [[2], [3]]]);  ┌─flatten(array(array([1]), array([2], [3])))─┐ │ [1,2,3] │ └─────────────────────────────────────────────┘  "},{"title":"arrayCompact​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arraycompact","content":"Removes consecutive duplicate elements from an array. The order of result values is determined by the order in the source array. Syntax arrayCompact(arr)  Arguments arr — The array to inspect. Returned value The array without duplicate. Type: Array. Example Query: SELECT arrayCompact([1, 1, nan, nan, 2, 3, 3, 3]);  Result: ┌─arrayCompact([1, 1, nan, nan, 2, 3, 3, 3])─┐ │ [1,nan,nan,2,3] │ └────────────────────────────────────────────┘  "},{"title":"arrayZip​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arrayzip","content":"Combines multiple arrays into a single array. The resulting array contains the corresponding elements of the source arrays grouped into tuples in the listed order of arguments. Syntax arrayZip(arr1, arr2, ..., arrN)  Arguments arrN — Array. The function can take any number of arrays of different types. All the input arrays must be of equal size. Returned value Array with elements from the source arrays grouped into tuples. Data types in the tuple are the same as types of the input arrays and in the same order as arrays are passed. Type: Array. Example Query: SELECT arrayZip(['a', 'b', 'c'], [5, 2, 1]);  Result: ┌─arrayZip(['a', 'b', 'c'], [5, 2, 1])─┐ │ [('a',5),('b',2),('c',1)] │ └──────────────────────────────────────┘  "},{"title":"arrayAUC​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arrayauc","content":"Calculate AUC (Area Under the Curve, which is a concept in machine learning, see more details: https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve). Syntax arrayAUC(arr_scores, arr_labels)  Arguments arr_scores — scores prediction model gives.arr_labels — labels of samples, usually 1 for positive sample and 0 for negtive sample. Returned value Returns AUC value with type Float64. Example Query: select arrayAUC([0.1, 0.4, 0.35, 0.8], [0, 0, 1, 1]);  Result: ┌─arrayAUC([0.1, 0.4, 0.35, 0.8], [0, 0, 1, 1])─┐ │ 0.75 │ └───────────────────────────────────────────────┘  "},{"title":"arrayMap(func, arr1, …)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#array-map","content":"Returns an array obtained from the original application of the func function to each element in the arr array. Examples: SELECT arrayMap(x -&gt; (x + 2), [1, 2, 3]) as res;  ┌─res─────┐ │ [3,4,5] │ └─────────┘  The following example shows how to create a tuple of elements from different arrays: SELECT arrayMap((x, y) -&gt; (x, y), [1, 2, 3], [4, 5, 6]) AS res  ┌─res─────────────────┐ │ [(1,4),(2,5),(3,6)] │ └─────────────────────┘  Note that the arrayMap is a higher-order function. You must pass a lambda function to it as the first argument, and it can’t be omitted. "},{"title":"arrayFilter(func, arr1, …)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#array-filter","content":"Returns an array containing only the elements in arr1 for which func returns something other than 0. Examples: SELECT arrayFilter(x -&gt; x LIKE '%World%', ['Hello', 'abc World']) AS res  ┌─res───────────┐ │ ['abc World'] │ └───────────────┘  SELECT arrayFilter( (i, x) -&gt; x LIKE '%World%', arrayEnumerate(arr), ['Hello', 'abc World'] AS arr) AS res  ┌─res─┐ │ [2] │ └─────┘  Note that the arrayFilter is a higher-order function. You must pass a lambda function to it as the first argument, and it can’t be omitted. "},{"title":"arrayFill(func, arr1, …)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#array-fill","content":"Scan through arr1 from the first element to the last element and replace arr1[i] by arr1[i - 1] if func returns 0. The first element of arr1 will not be replaced. Examples: SELECT arrayFill(x -&gt; not isNull(x), [1, null, 3, 11, 12, null, null, 5, 6, 14, null, null]) AS res  ┌─res──────────────────────────────┐ │ [1,1,3,11,12,12,12,5,6,14,14,14] │ └──────────────────────────────────┘  Note that the arrayFill is a higher-order function. You must pass a lambda function to it as the first argument, and it can’t be omitted. "},{"title":"arrayReverseFill(func, arr1, …)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#array-reverse-fill","content":"Scan through arr1 from the last element to the first element and replace arr1[i] by arr1[i + 1] if func returns 0. The last element of arr1 will not be replaced. Examples: SELECT arrayReverseFill(x -&gt; not isNull(x), [1, null, 3, 11, 12, null, null, 5, 6, 14, null, null]) AS res  ┌─res────────────────────────────────┐ │ [1,3,3,11,12,5,5,5,6,14,NULL,NULL] │ └────────────────────────────────────┘  Note that the arrayReverseFill is a higher-order function. You must pass a lambda function to it as the first argument, and it can’t be omitted. "},{"title":"arraySplit(func, arr1, …)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#array-split","content":"Split arr1 into multiple arrays. When func returns something other than 0, the array will be split on the left hand side of the element. The array will not be split before the first element. Examples: SELECT arraySplit((x, y) -&gt; y, [1, 2, 3, 4, 5], [1, 0, 0, 1, 0]) AS res  ┌─res─────────────┐ │ [[1,2,3],[4,5]] │ └─────────────────┘  Note that the arraySplit is a higher-order function. You must pass a lambda function to it as the first argument, and it can’t be omitted. "},{"title":"arrayReverseSplit(func, arr1, …)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#array-reverse-split","content":"Split arr1 into multiple arrays. When func returns something other than 0, the array will be split on the right hand side of the element. The array will not be split after the last element. Examples: SELECT arrayReverseSplit((x, y) -&gt; y, [1, 2, 3, 4, 5], [1, 0, 0, 1, 0]) AS res  ┌─res───────────────┐ │ [[1],[2,3,4],[5]] │ └───────────────────┘  Note that the arrayReverseSplit is a higher-order function. You must pass a lambda function to it as the first argument, and it can’t be omitted. "},{"title":"arrayExists([func,] arr1, …)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arrayexistsfunc-arr1","content":"Returns 1 if there is at least one element in arr for which func returns something other than 0. Otherwise, it returns 0. Note that the arrayExists is a higher-order function. You can pass a lambda function to it as the first argument. "},{"title":"arrayAll([func,] arr1, …)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arrayallfunc-arr1","content":"Returns 1 if func returns something other than 0 for all the elements in arr. Otherwise, it returns 0. Note that the arrayAll is a higher-order function. You can pass a lambda function to it as the first argument. "},{"title":"arrayFirst(func, arr1, …)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#array-first","content":"Returns the first element in the arr1 array for which func returns something other than 0. Note that the arrayFirst is a higher-order function. You must pass a lambda function to it as the first argument, and it can’t be omitted. "},{"title":"arrayLast(func, arr1, …)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#array-last","content":"Returns the last element in the arr1 array for which func returns something other than 0. Note that the arrayLast is a higher-order function. You must pass a lambda function to it as the first argument, and it can’t be omitted. "},{"title":"arrayFirstIndex(func, arr1, …)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#array-first-index","content":"Returns the index of the first element in the arr1 array for which func returns something other than 0. Note that the arrayFirstIndex is a higher-order function. You must pass a lambda function to it as the first argument, and it can’t be omitted. "},{"title":"arrayLastIndex(func, arr1, …)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#array-last-index","content":"Returns the index of the last element in the arr1 array for which func returns something other than 0. Note that the arrayLastIndex is a higher-order function. You must pass a lambda function to it as the first argument, and it can’t be omitted. "},{"title":"arrayMin​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#array-min","content":"Returns the minimum of elements in the source array. If the func function is specified, returns the mininum of elements converted by this function. Note that the arrayMin is a higher-order function. You can pass a lambda function to it as the first argument. Syntax arrayMin([func,] arr)  Arguments func — Function. Expression.arr — Array. Array. Returned value The minimum of function values (or the array minimum). Type: if func is specified, matches func return value type, else matches the array elements type. Examples Query: SELECT arrayMin([1, 2, 4]) AS res;  Result: ┌─res─┐ │ 1 │ └─────┘  Query: SELECT arrayMin(x -&gt; (-x), [1, 2, 4]) AS res;  Result: ┌─res─┐ │ -4 │ └─────┘  "},{"title":"arrayMax​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#array-max","content":"Returns the maximum of elements in the source array. If the func function is specified, returns the maximum of elements converted by this function. Note that the arrayMax is a higher-order function. You can pass a lambda function to it as the first argument. Syntax arrayMax([func,] arr)  Arguments func — Function. Expression.arr — Array. Array. Returned value The maximum of function values (or the array maximum). Type: if func is specified, matches func return value type, else matches the array elements type. Examples Query: SELECT arrayMax([1, 2, 4]) AS res;  Result: ┌─res─┐ │ 4 │ └─────┘  Query: SELECT arrayMax(x -&gt; (-x), [1, 2, 4]) AS res;  Result: ┌─res─┐ │ -1 │ └─────┘  "},{"title":"arraySum​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#array-sum","content":"Returns the sum of elements in the source array. If the func function is specified, returns the sum of elements converted by this function. Note that the arraySum is a higher-order function. You can pass a lambda function to it as the first argument. Syntax arraySum([func,] arr)  Arguments func — Function. Expression.arr — Array. Array. Returned value The sum of the function values (or the array sum). Type: for decimal numbers in source array (or for converted values, if func is specified) — Decimal128, for floating point numbers — Float64, for numeric unsigned — UInt64, and for numeric signed — Int64. Examples Query: SELECT arraySum([2, 3]) AS res;  Result: ┌─res─┐ │ 5 │ └─────┘  Query: SELECT arraySum(x -&gt; x*x, [2, 3]) AS res;  Result: ┌─res─┐ │ 13 │ └─────┘  "},{"title":"arrayAvg​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#array-avg","content":"Returns the average of elements in the source array. If the func function is specified, returns the average of elements converted by this function. Note that the arrayAvg is a higher-order function. You can pass a lambda function to it as the first argument. Syntax arrayAvg([func,] arr)  Arguments func — Function. Expression.arr — Array. Array. Returned value The average of function values (or the array average). Type: Float64. Examples Query: SELECT arrayAvg([1, 2, 4]) AS res;  Result: ┌────────────────res─┐ │ 2.3333333333333335 │ └────────────────────┘  Query: SELECT arrayAvg(x -&gt; (x * x), [2, 4]) AS res;  Result: ┌─res─┐ │ 10 │ └─────┘  "},{"title":"arrayCumSum([func,] arr1, …)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arraycumsumfunc-arr1","content":"Returns an array of partial sums of elements in the source array (a running sum). If the func function is specified, then the values of the array elements are converted by this function before summing. Example: SELECT arrayCumSum([1, 1, 1, 1]) AS res  ┌─res──────────┐ │ [1, 2, 3, 4] │ └──────────────┘  Note that the arrayCumSum is a higher-order function. You can pass a lambda function to it as the first argument. "},{"title":"arrayCumSumNonNegative(arr)​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arraycumsumnonnegativearr","content":"Same as arrayCumSum, returns an array of partial sums of elements in the source array (a running sum). Different arrayCumSum, when then returned value contains a value less than zero, the value is replace with zero and the subsequent calculation is performed with zero parameters. For example: SELECT arrayCumSumNonNegative([1, 1, -4, 1]) AS res  ┌─res───────┐ │ [1,2,0,1] │ └───────────┘  Note that the arraySumNonNegative is a higher-order function. You can pass a lambda function to it as the first argument. "},{"title":"arrayProduct​","type":1,"pageTitle":"Array Functions","url":"docs/en/sql-reference/functions/array-functions#arrayproduct","content":"Multiplies elements of an array. Syntax arrayProduct(arr)  Arguments arr — Array of numeric values. Returned value A product of array's elements. Type: Float64. Examples Query: SELECT arrayProduct([1,2,3,4,5,6]) as res;  Result: ┌─res───┐ │ 720 │ └───────┘  Query: SELECT arrayProduct([toDecimal64(1,8), toDecimal64(2,8), toDecimal64(3,8)]) as res, toTypeName(res);  Return value type is always Float64. Result: ┌─res─┬─toTypeName(arrayProduct(array(toDecimal64(1, 8), toDecimal64(2, 8), toDecimal64(3, 8))))─┐ │ 6 │ Float64 │ └─────┴──────────────────────────────────────────────────────────────────────────────────────────┘  "},{"title":"Hash Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/hash-functions","content":"","keywords":""},{"title":"halfMD5​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#hash-functions-halfmd5","content":"Interprets all the input parameters as strings and calculates the MD5 hash value for each of them. Then combines hashes, takes the first 8 bytes of the hash of the resulting string, and interprets them as UInt64 in big-endian byte order. halfMD5(par1, ...)  The function is relatively slow (5 million short strings per second per processor core). Consider using the sipHash64 function instead. Arguments The function takes a variable number of input parameters. Arguments can be any of the supported data types. Returned Value A UInt64 data type hash value. Example SELECT halfMD5(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS halfMD5hash, toTypeName(halfMD5hash) AS type;  ┌────────halfMD5hash─┬─type───┐ │ 186182704141653334 │ UInt64 │ └────────────────────┴────────┘  "},{"title":"MD4​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#hash_functions-md4","content":"Calculates the MD4 from a string and returns the resulting set of bytes as FixedString(16). "},{"title":"MD5​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#hash_functions-md5","content":"Calculates the MD5 from a string and returns the resulting set of bytes as FixedString(16). If you do not need MD5 in particular, but you need a decent cryptographic 128-bit hash, use the ‘sipHash128’ function instead. If you want to get the same result as output by the md5sum utility, use lower(hex(MD5(s))). "},{"title":"sipHash64​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#hash_functions-siphash64","content":"Produces a 64-bit SipHash hash value. sipHash64(par1,...)  This is a cryptographic hash function. It works at least three times faster than the MD5 function. Function interprets all the input parameters as strings and calculates the hash value for each of them. Then combines hashes by the following algorithm: After hashing all the input parameters, the function gets the array of hashes.Function takes the first and the second elements and calculates a hash for the array of them.Then the function takes the hash value, calculated at the previous step, and the third element of the initial hash array, and calculates a hash for the array of them.The previous step is repeated for all the remaining elements of the initial hash array. Arguments The function takes a variable number of input parameters. Arguments can be any of the supported data types. Returned Value A UInt64 data type hash value. Example SELECT sipHash64(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS SipHash, toTypeName(SipHash) AS type;  ┌──────────────SipHash─┬─type───┐ │ 13726873534472839665 │ UInt64 │ └──────────────────────┴────────┘  "},{"title":"sipHash128​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#hash_functions-siphash128","content":"Produces a 128-bit SipHash hash value. Differs from sipHash64 in that the final xor-folding state is done up to 128 bits. Syntax sipHash128(par1,...)  Arguments The function takes a variable number of input parameters. Arguments can be any of the supported data types. Returned value A 128-bit SipHash hash value. Type: FixedString(16). Example Query: SELECT hex(sipHash128('foo', '\\x01', 3));  Result: ┌─hex(sipHash128('foo', '', 3))────┐ │ 9DE516A64A414D4B1B609415E4523F24 │ └──────────────────────────────────┘  "},{"title":"cityHash64​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#cityhash64","content":"Produces a 64-bit CityHash hash value. cityHash64(par1,...)  This is a fast non-cryptographic hash function. It uses the CityHash algorithm for string parameters and implementation-specific fast non-cryptographic hash function for parameters with other data types. The function uses the CityHash combinator to get the final results. Arguments The function takes a variable number of input parameters. Arguments can be any of the supported data types. Returned Value A UInt64 data type hash value. Examples Call example: SELECT cityHash64(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS CityHash, toTypeName(CityHash) AS type;  ┌─────────────CityHash─┬─type───┐ │ 12072650598913549138 │ UInt64 │ └──────────────────────┴────────┘  The following example shows how to compute the checksum of the entire table with accuracy up to the row order: SELECT groupBitXor(cityHash64(*)) FROM table  "},{"title":"intHash32​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#inthash32","content":"Calculates a 32-bit hash code from any type of integer. This is a relatively fast non-cryptographic hash function of average quality for numbers. "},{"title":"intHash64​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#inthash64","content":"Calculates a 64-bit hash code from any type of integer. It works faster than intHash32. Average quality. "},{"title":"SHA1, SHA224, SHA256, SHA512​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#sha","content":"Calculates SHA-1, SHA-224, SHA-256, SHA-512 hash from a string and returns the resulting set of bytes as FixedString. Syntax SHA1('s') ... SHA512('s')  The function works fairly slowly (SHA-1 processes about 5 million short strings per second per processor core, while SHA-224 and SHA-256 process about 2.2 million). We recommend using this function only in cases when you need a specific hash function and you can’t select it. Even in these cases, we recommend applying the function offline and pre-calculating values when inserting them into the table, instead of applying it in SELECT queries. Arguments s — Input string for SHA hash calculation. String. Returned value SHA hash as a hex-unencoded FixedString. SHA-1 returns as FixedString(20), SHA-224 as FixedString(28), SHA-256 — FixedString(32), SHA-512 — FixedString(64). Type: FixedString. Example Use the hex function to represent the result as a hex-encoded string. Query: SELECT hex(SHA1('abc'));  Result: ┌─hex(SHA1('abc'))─────────────────────────┐ │ A9993E364706816ABA3E25717850C26C9CD0D89D │ └──────────────────────────────────────────┘  "},{"title":"URLHash(url[, N])​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#urlhashurl-n","content":"A fast, decent-quality non-cryptographic hash function for a string obtained from a URL using some type of normalization.URLHash(s) – Calculates a hash from a string without one of the trailing symbols /,? or # at the end, if present.URLHash(s, N) – Calculates a hash from a string up to the N level in the URL hierarchy, without one of the trailing symbols /,? or # at the end, if present. Levels are the same as in URLHierarchy. "},{"title":"farmFingerprint64​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#farmfingerprint64","content":""},{"title":"farmHash64​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#farmhash64","content":"Produces a 64-bit FarmHash or Fingerprint value. farmFingerprint64 is preferred for a stable and portable value. farmFingerprint64(par1, ...) farmHash64(par1, ...)  These functions use the Fingerprint64 and Hash64 methods respectively from all available methods. Arguments The function takes a variable number of input parameters. Arguments can be any of the supported data types. Returned Value A UInt64 data type hash value. Example SELECT farmHash64(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS FarmHash, toTypeName(FarmHash) AS type;  ┌─────────────FarmHash─┬─type───┐ │ 17790458267262532859 │ UInt64 │ └──────────────────────┴────────┘  "},{"title":"javaHash​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#hash_functions-javahash","content":"Calculates JavaHash from a string. This hash function is neither fast nor having a good quality. The only reason to use it is when this algorithm is already used in another system and you have to calculate exactly the same result. Syntax SELECT javaHash('')  Returned value A Int32 data type hash value. Example Query: SELECT javaHash('Hello, world!');  Result: ┌─javaHash('Hello, world!')─┐ │ -1880044555 │ └───────────────────────────┘  "},{"title":"javaHashUTF16LE​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#javahashutf16le","content":"Calculates JavaHash from a string, assuming it contains bytes representing a string in UTF-16LE encoding. Syntax javaHashUTF16LE(stringUtf16le)  Arguments stringUtf16le — a string in UTF-16LE encoding. Returned value A Int32 data type hash value. Example Correct query with UTF-16LE encoded string. Query: SELECT javaHashUTF16LE(convertCharset('test', 'utf-8', 'utf-16le'));  Result: ┌─javaHashUTF16LE(convertCharset('test', 'utf-8', 'utf-16le'))─┐ │ 3556498 │ └──────────────────────────────────────────────────────────────┘  "},{"title":"hiveHash​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#hash-functions-hivehash","content":"Calculates HiveHash from a string. SELECT hiveHash('')  This is just JavaHash with zeroed out sign bit. This function is used in Apache Hive for versions before 3.0. This hash function is neither fast nor having a good quality. The only reason to use it is when this algorithm is already used in another system and you have to calculate exactly the same result. Returned value A Int32 data type hash value. Type: hiveHash. Example Query: SELECT hiveHash('Hello, world!');  Result: ┌─hiveHash('Hello, world!')─┐ │ 267439093 │ └───────────────────────────┘  "},{"title":"metroHash64​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#metrohash64","content":"Produces a 64-bit MetroHash hash value. metroHash64(par1, ...)  Arguments The function takes a variable number of input parameters. Arguments can be any of the supported data types. Returned Value A UInt64 data type hash value. Example SELECT metroHash64(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS MetroHash, toTypeName(MetroHash) AS type;  ┌────────────MetroHash─┬─type───┐ │ 14235658766382344533 │ UInt64 │ └──────────────────────┴────────┘  "},{"title":"jumpConsistentHash​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#jumpconsistenthash","content":"Calculates JumpConsistentHash form a UInt64. Accepts two arguments: a UInt64-type key and the number of buckets. Returns Int32. For more information, see the link: JumpConsistentHash "},{"title":"murmurHash2_32, murmurHash2_64​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#murmurhash2-32-murmurhash2-64","content":"Produces a MurmurHash2 hash value. murmurHash2_32(par1, ...) murmurHash2_64(par1, ...)  Arguments Both functions take a variable number of input parameters. Arguments can be any of the supported data types. Returned Value The murmurHash2_32 function returns hash value having the UInt32 data type.The murmurHash2_64 function returns hash value having the UInt64 data type. Example SELECT murmurHash2_64(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS MurmurHash2, toTypeName(MurmurHash2) AS type;  ┌──────────MurmurHash2─┬─type───┐ │ 11832096901709403633 │ UInt64 │ └──────────────────────┴────────┘  "},{"title":"gccMurmurHash​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#gccmurmurhash","content":"Calculates a 64-bit MurmurHash2 hash value using the same hash seed as gcc. It is portable between CLang and GCC builds. Syntax gccMurmurHash(par1, ...)  Arguments par1, ... — A variable number of parameters that can be any of the supported data types. Returned value Calculated hash value. Type: UInt64. Example Query: SELECT gccMurmurHash(1, 2, 3) AS res1, gccMurmurHash(('a', [1, 2, 3], 4, (4, ['foo', 'bar'], 1, (1, 2)))) AS res2  Result: ┌─────────────────res1─┬────────────────res2─┐ │ 12384823029245979431 │ 1188926775431157506 │ └──────────────────────┴─────────────────────┘  "},{"title":"murmurHash3_32, murmurHash3_64​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#murmurhash3-32-murmurhash3-64","content":"Produces a MurmurHash3 hash value. murmurHash3_32(par1, ...) murmurHash3_64(par1, ...)  Arguments Both functions take a variable number of input parameters. Arguments can be any of the supported data types. Returned Value The murmurHash3_32 function returns a UInt32 data type hash value.The murmurHash3_64 function returns a UInt64 data type hash value. Example SELECT murmurHash3_32(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS MurmurHash3, toTypeName(MurmurHash3) AS type;  ┌─MurmurHash3─┬─type───┐ │ 2152717 │ UInt32 │ └─────────────┴────────┘  "},{"title":"murmurHash3_128​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#murmurhash3-128","content":"Produces a 128-bit MurmurHash3 hash value. Syntax murmurHash3_128(expr)  Arguments expr — A list of expressions. String. Returned value A 128-bit MurmurHash3 hash value. Type: FixedString(16). Example Query: SELECT hex(murmurHash3_128('foo', 'foo', 'foo'));  Result: ┌─hex(murmurHash3_128('foo', 'foo', 'foo'))─┐ │ F8F7AD9B6CD4CF117A71E277E2EC2931 │ └───────────────────────────────────────────┘  "},{"title":"xxHash32, xxHash64​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#hash-functions-xxhash32","content":"Calculates xxHash from a string. It is proposed in two flavors, 32 and 64 bits. SELECT xxHash32('') OR SELECT xxHash64('')  Returned value A Uint32 or Uint64 data type hash value. Type: xxHash. Example Query: SELECT xxHash32('Hello, world!');  Result: ┌─xxHash32('Hello, world!')─┐ │ 834093149 │ └───────────────────────────┘  See Also xxHash. "},{"title":"ngramSimHash​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#ngramsimhash","content":"Splits a ASCII string into n-grams of ngramsize symbols and returns the n-gram simhash. Is case sensitive. Can be used for detection of semi-duplicate strings with bitHammingDistance. The smaller is the Hamming Distance of the calculated simhashes of two strings, the more likely these strings are the same. Syntax ngramSimHash(string[, ngramsize])  Arguments string — String. String.ngramsize — The size of an n-gram. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8. Returned value Hash value. Type: UInt64. Example Query: SELECT ngramSimHash('ClickHouse') AS Hash;  Result: ┌───────Hash─┐ │ 1627567969 │ └────────────┘  "},{"title":"ngramSimHashCaseInsensitive​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#ngramsimhashcaseinsensitive","content":"Splits a ASCII string into n-grams of ngramsize symbols and returns the n-gram simhash. Is case insensitive. Can be used for detection of semi-duplicate strings with bitHammingDistance. The smaller is the Hamming Distance of the calculated simhashes of two strings, the more likely these strings are the same. Syntax ngramSimHashCaseInsensitive(string[, ngramsize])  Arguments string — String. String.ngramsize — The size of an n-gram. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8. Returned value Hash value. Type: UInt64. Example Query: SELECT ngramSimHashCaseInsensitive('ClickHouse') AS Hash;  Result: ┌──────Hash─┐ │ 562180645 │ └───────────┘  "},{"title":"ngramSimHashUTF8​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#ngramsimhashutf8","content":"Splits a UTF-8 string into n-grams of ngramsize symbols and returns the n-gram simhash. Is case sensitive. Can be used for detection of semi-duplicate strings with bitHammingDistance. The smaller is the Hamming Distance of the calculated simhashes of two strings, the more likely these strings are the same. Syntax ngramSimHashUTF8(string[, ngramsize])  Arguments string — String. String.ngramsize — The size of an n-gram. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8. Returned value Hash value. Type: UInt64. Example Query: SELECT ngramSimHashUTF8('ClickHouse') AS Hash;  Result: ┌───────Hash─┐ │ 1628157797 │ └────────────┘  "},{"title":"ngramSimHashCaseInsensitiveUTF8​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#ngramsimhashcaseinsensitiveutf8","content":"Splits a UTF-8 string into n-grams of ngramsize symbols and returns the n-gram simhash. Is case insensitive. Can be used for detection of semi-duplicate strings with bitHammingDistance. The smaller is the Hamming Distance of the calculated simhashes of two strings, the more likely these strings are the same. Syntax ngramSimHashCaseInsensitiveUTF8(string[, ngramsize])  Arguments string — String. String.ngramsize — The size of an n-gram. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8. Returned value Hash value. Type: UInt64. Example Query: SELECT ngramSimHashCaseInsensitiveUTF8('ClickHouse') AS Hash;  Result: ┌───────Hash─┐ │ 1636742693 │ └────────────┘  "},{"title":"wordShingleSimHash​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#wordshinglesimhash","content":"Splits a ASCII string into parts (shingles) of shinglesize words and returns the word shingle simhash. Is case sensitive. Can be used for detection of semi-duplicate strings with bitHammingDistance. The smaller is the Hamming Distance of the calculated simhashes of two strings, the more likely these strings are the same. Syntax wordShingleSimHash(string[, shinglesize])  Arguments string — String. String.shinglesize — The size of a word shingle. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8. Returned value Hash value. Type: UInt64. Example Query: SELECT wordShingleSimHash('ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).') AS Hash;  Result: ┌───────Hash─┐ │ 2328277067 │ └────────────┘  "},{"title":"wordShingleSimHashCaseInsensitive​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#wordshinglesimhashcaseinsensitive","content":"Splits a ASCII string into parts (shingles) of shinglesize words and returns the word shingle simhash. Is case insensitive. Can be used for detection of semi-duplicate strings with bitHammingDistance. The smaller is the Hamming Distance of the calculated simhashes of two strings, the more likely these strings are the same. Syntax wordShingleSimHashCaseInsensitive(string[, shinglesize])  Arguments string — String. String.shinglesize — The size of a word shingle. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8. Returned value Hash value. Type: UInt64. Example Query: SELECT wordShingleSimHashCaseInsensitive('ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).') AS Hash;  Result: ┌───────Hash─┐ │ 2194812424 │ └────────────┘  "},{"title":"wordShingleSimHashUTF8​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#wordshinglesimhashutf8","content":"Splits a UTF-8 string into parts (shingles) of shinglesize words and returns the word shingle simhash. Is case sensitive. Can be used for detection of semi-duplicate strings with bitHammingDistance. The smaller is the Hamming Distance of the calculated simhashes of two strings, the more likely these strings are the same. Syntax wordShingleSimHashUTF8(string[, shinglesize])  Arguments string — String. String.shinglesize — The size of a word shingle. Optinal. Possible values: any number from 1 to 25. Default value: 3. UInt8. Returned value Hash value. Type: UInt64. Example Query: SELECT wordShingleSimHashUTF8('ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).') AS Hash;  Result: ┌───────Hash─┐ │ 2328277067 │ └────────────┘  "},{"title":"wordShingleSimHashCaseInsensitiveUTF8​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#wordshinglesimhashcaseinsensitiveutf8","content":"Splits a UTF-8 string into parts (shingles) of shinglesize words and returns the word shingle simhash. Is case insensitive. Can be used for detection of semi-duplicate strings with bitHammingDistance. The smaller is the Hamming Distance of the calculated simhashes of two strings, the more likely these strings are the same. Syntax wordShingleSimHashCaseInsensitiveUTF8(string[, shinglesize])  Arguments string — String. String.shinglesize — The size of a word shingle. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8. Returned value Hash value. Type: UInt64. Example Query: SELECT wordShingleSimHashCaseInsensitiveUTF8('ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).') AS Hash;  Result: ┌───────Hash─┐ │ 2194812424 │ └────────────┘  "},{"title":"ngramMinHash​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#ngramminhash","content":"Splits a ASCII string into n-grams of ngramsize symbols and calculates hash values for each n-gram. Uses hashnum minimum hashes to calculate the minimum hash and hashnum maximum hashes to calculate the maximum hash. Returns a tuple with these hashes. Is case sensitive. Can be used for detection of semi-duplicate strings with tupleHammingDistance. For two strings: if one of the returned hashes is the same for both strings, we think that those strings are the same. Syntax ngramMinHash(string[, ngramsize, hashnum])  Arguments string — String. String.ngramsize — The size of an n-gram. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two hashes — the minimum and the maximum. Type: Tuple(UInt64, UInt64). Example Query: SELECT ngramMinHash('ClickHouse') AS Tuple;  Result: ┌─Tuple──────────────────────────────────────┐ │ (18333312859352735453,9054248444481805918) │ └────────────────────────────────────────────┘  "},{"title":"ngramMinHashCaseInsensitive​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#ngramminhashcaseinsensitive","content":"Splits a ASCII string into n-grams of ngramsize symbols and calculates hash values for each n-gram. Uses hashnum minimum hashes to calculate the minimum hash and hashnum maximum hashes to calculate the maximum hash. Returns a tuple with these hashes. Is case insensitive. Can be used for detection of semi-duplicate strings with tupleHammingDistance. For two strings: if one of the returned hashes is the same for both strings, we think that those strings are the same. Syntax ngramMinHashCaseInsensitive(string[, ngramsize, hashnum])  Arguments string — String. String.ngramsize — The size of an n-gram. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two hashes — the minimum and the maximum. Type: Tuple(UInt64, UInt64). Example Query: SELECT ngramMinHashCaseInsensitive('ClickHouse') AS Tuple;  Result: ┌─Tuple──────────────────────────────────────┐ │ (2106263556442004574,13203602793651726206) │ └────────────────────────────────────────────┘  "},{"title":"ngramMinHashUTF8​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#ngramminhashutf8","content":"Splits a UTF-8 string into n-grams of ngramsize symbols and calculates hash values for each n-gram. Uses hashnum minimum hashes to calculate the minimum hash and hashnum maximum hashes to calculate the maximum hash. Returns a tuple with these hashes. Is case sensitive. Can be used for detection of semi-duplicate strings with tupleHammingDistance. For two strings: if one of the returned hashes is the same for both strings, we think that those strings are the same. Syntax ngramMinHashUTF8(string[, ngramsize, hashnum])  Arguments string — String. String.ngramsize — The size of an n-gram. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two hashes — the minimum and the maximum. Type: Tuple(UInt64, UInt64). Example Query: SELECT ngramMinHashUTF8('ClickHouse') AS Tuple;  Result: ┌─Tuple──────────────────────────────────────┐ │ (18333312859352735453,6742163577938632877) │ └────────────────────────────────────────────┘  "},{"title":"ngramMinHashCaseInsensitiveUTF8​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#ngramminhashcaseinsensitiveutf8","content":"Splits a UTF-8 string into n-grams of ngramsize symbols and calculates hash values for each n-gram. Uses hashnum minimum hashes to calculate the minimum hash and hashnum maximum hashes to calculate the maximum hash. Returns a tuple with these hashes. Is case insensitive. Can be used for detection of semi-duplicate strings with tupleHammingDistance. For two strings: if one of the returned hashes is the same for both strings, we think that those strings are the same. Syntax ngramMinHashCaseInsensitiveUTF8(string [, ngramsize, hashnum])  Arguments string — String. String.ngramsize — The size of an n-gram. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two hashes — the minimum and the maximum. Type: Tuple(UInt64, UInt64). Example Query: SELECT ngramMinHashCaseInsensitiveUTF8('ClickHouse') AS Tuple;  Result: ┌─Tuple───────────────────────────────────────┐ │ (12493625717655877135,13203602793651726206) │ └─────────────────────────────────────────────┘  "},{"title":"ngramMinHashArg​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#ngramminhasharg","content":"Splits a ASCII string into n-grams of ngramsize symbols and returns the n-grams with minimum and maximum hashes, calculated by the ngramMinHash function with the same input. Is case sensitive. Syntax ngramMinHashArg(string[, ngramsize, hashnum])  Arguments string — String. String.ngramsize — The size of an n-gram. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two tuples with hashnum n-grams each. Type: Tuple(Tuple(String), Tuple(String)). Example Query: SELECT ngramMinHashArg('ClickHouse') AS Tuple;  Result: ┌─Tuple─────────────────────────────────────────────────────────────────────────┐ │ (('ous','ick','lic','Hou','kHo','use'),('Hou','lic','ick','ous','ckH','Cli')) │ └───────────────────────────────────────────────────────────────────────────────┘  "},{"title":"ngramMinHashArgCaseInsensitive​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#ngramminhashargcaseinsensitive","content":"Splits a ASCII string into n-grams of ngramsize symbols and returns the n-grams with minimum and maximum hashes, calculated by the ngramMinHashCaseInsensitive function with the same input. Is case insensitive. Syntax ngramMinHashArgCaseInsensitive(string[, ngramsize, hashnum])  Arguments string — String. String.ngramsize — The size of an n-gram. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two tuples with hashnum n-grams each. Type: Tuple(Tuple(String), Tuple(String)). Example Query: SELECT ngramMinHashArgCaseInsensitive('ClickHouse') AS Tuple;  Result: ┌─Tuple─────────────────────────────────────────────────────────────────────────┐ │ (('ous','ick','lic','kHo','use','Cli'),('kHo','lic','ick','ous','ckH','Hou')) │ └───────────────────────────────────────────────────────────────────────────────┘  "},{"title":"ngramMinHashArgUTF8​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#ngramminhashargutf8","content":"Splits a UTF-8 string into n-grams of ngramsize symbols and returns the n-grams with minimum and maximum hashes, calculated by the ngramMinHashUTF8 function with the same input. Is case sensitive. Syntax ngramMinHashArgUTF8(string[, ngramsize, hashnum])  Arguments string — String. String.ngramsize — The size of an n-gram. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two tuples with hashnum n-grams each. Type: Tuple(Tuple(String), Tuple(String)). Example Query: SELECT ngramMinHashArgUTF8('ClickHouse') AS Tuple;  Result: ┌─Tuple─────────────────────────────────────────────────────────────────────────┐ │ (('ous','ick','lic','Hou','kHo','use'),('kHo','Hou','lic','ick','ous','ckH')) │ └───────────────────────────────────────────────────────────────────────────────┘  "},{"title":"ngramMinHashArgCaseInsensitiveUTF8​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#ngramminhashargcaseinsensitiveutf8","content":"Splits a UTF-8 string into n-grams of ngramsize symbols and returns the n-grams with minimum and maximum hashes, calculated by the ngramMinHashCaseInsensitiveUTF8 function with the same input. Is case insensitive. Syntax ngramMinHashArgCaseInsensitiveUTF8(string[, ngramsize, hashnum])  Arguments string — String. String.ngramsize — The size of an n-gram. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two tuples with hashnum n-grams each. Type: Tuple(Tuple(String), Tuple(String)). Example Query: SELECT ngramMinHashArgCaseInsensitiveUTF8('ClickHouse') AS Tuple;  Result: ┌─Tuple─────────────────────────────────────────────────────────────────────────┐ │ (('ckH','ous','ick','lic','kHo','use'),('kHo','lic','ick','ous','ckH','Hou')) │ └───────────────────────────────────────────────────────────────────────────────┘  "},{"title":"wordShingleMinHash​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#wordshingleminhash","content":"Splits a ASCII string into parts (shingles) of shinglesize words and calculates hash values for each word shingle. Uses hashnum minimum hashes to calculate the minimum hash and hashnum maximum hashes to calculate the maximum hash. Returns a tuple with these hashes. Is case sensitive. Can be used for detection of semi-duplicate strings with tupleHammingDistance. For two strings: if one of the returned hashes is the same for both strings, we think that those strings are the same. Syntax wordShingleMinHash(string[, shinglesize, hashnum])  Arguments string — String. String.shinglesize — The size of a word shingle. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two hashes — the minimum and the maximum. Type: Tuple(UInt64, UInt64). Example Query: SELECT wordShingleMinHash('ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).') AS Tuple;  Result: ┌─Tuple──────────────────────────────────────┐ │ (16452112859864147620,5844417301642981317) │ └────────────────────────────────────────────┘  "},{"title":"wordShingleMinHashCaseInsensitive​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#wordshingleminhashcaseinsensitive","content":"Splits a ASCII string into parts (shingles) of shinglesize words and calculates hash values for each word shingle. Uses hashnum minimum hashes to calculate the minimum hash and hashnum maximum hashes to calculate the maximum hash. Returns a tuple with these hashes. Is case insensitive. Can be used for detection of semi-duplicate strings with tupleHammingDistance. For two strings: if one of the returned hashes is the same for both strings, we think that those strings are the same. Syntax wordShingleMinHashCaseInsensitive(string[, shinglesize, hashnum])  Arguments string — String. String.shinglesize — The size of a word shingle. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two hashes — the minimum and the maximum. Type: Tuple(UInt64, UInt64). Example Query: SELECT wordShingleMinHashCaseInsensitive('ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).') AS Tuple;  Result: ┌─Tuple─────────────────────────────────────┐ │ (3065874883688416519,1634050779997673240) │ └───────────────────────────────────────────┘  "},{"title":"wordShingleMinHashUTF8​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#wordshingleminhashutf8","content":"Splits a UTF-8 string into parts (shingles) of shinglesize words and calculates hash values for each word shingle. Uses hashnum minimum hashes to calculate the minimum hash and hashnum maximum hashes to calculate the maximum hash. Returns a tuple with these hashes. Is case sensitive. Can be used for detection of semi-duplicate strings with tupleHammingDistance. For two strings: if one of the returned hashes is the same for both strings, we think that those strings are the same. Syntax wordShingleMinHashUTF8(string[, shinglesize, hashnum])  Arguments string — String. String.shinglesize — The size of a word shingle. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two hashes — the minimum and the maximum. Type: Tuple(UInt64, UInt64). Example Query: SELECT wordShingleMinHashUTF8('ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).') AS Tuple;  Result: ┌─Tuple──────────────────────────────────────┐ │ (16452112859864147620,5844417301642981317) │ └────────────────────────────────────────────┘  "},{"title":"wordShingleMinHashCaseInsensitiveUTF8​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#wordshingleminhashcaseinsensitiveutf8","content":"Splits a UTF-8 string into parts (shingles) of shinglesize words and calculates hash values for each word shingle. Uses hashnum minimum hashes to calculate the minimum hash and hashnum maximum hashes to calculate the maximum hash. Returns a tuple with these hashes. Is case insensitive. Can be used for detection of semi-duplicate strings with tupleHammingDistance. For two strings: if one of the returned hashes is the same for both strings, we think that those strings are the same. Syntax wordShingleMinHashCaseInsensitiveUTF8(string[, shinglesize, hashnum])  Arguments string — String. String.shinglesize — The size of a word shingle. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two hashes — the minimum and the maximum. Type: Tuple(UInt64, UInt64). Example Query: SELECT wordShingleMinHashCaseInsensitiveUTF8('ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).') AS Tuple;  Result: ┌─Tuple─────────────────────────────────────┐ │ (3065874883688416519,1634050779997673240) │ └───────────────────────────────────────────┘  "},{"title":"wordShingleMinHashArg​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#wordshingleminhasharg","content":"Splits a ASCII string into parts (shingles) of shinglesize words each and returns the shingles with minimum and maximum word hashes, calculated by the wordshingleMinHash function with the same input. Is case sensitive. Syntax wordShingleMinHashArg(string[, shinglesize, hashnum])  Arguments string — String. String.shinglesize — The size of a word shingle. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two tuples with hashnum word shingles each. Type: Tuple(Tuple(String), Tuple(String)). Example Query: SELECT wordShingleMinHashArg('ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).', 1, 3) AS Tuple;  Result: ┌─Tuple─────────────────────────────────────────────────────────────────┐ │ (('OLAP','database','analytical'),('online','oriented','processing')) │ └───────────────────────────────────────────────────────────────────────┘  "},{"title":"wordShingleMinHashArgCaseInsensitive​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#wordshingleminhashargcaseinsensitive","content":"Splits a ASCII string into parts (shingles) of shinglesize words each and returns the shingles with minimum and maximum word hashes, calculated by the wordShingleMinHashCaseInsensitive function with the same input. Is case insensitive. Syntax wordShingleMinHashArgCaseInsensitive(string[, shinglesize, hashnum])  Arguments string — String. String.shinglesize — The size of a word shingle. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two tuples with hashnum word shingles each. Type: Tuple(Tuple(String), Tuple(String)). Example Query: SELECT wordShingleMinHashArgCaseInsensitive('ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).', 1, 3) AS Tuple;  Result: ┌─Tuple──────────────────────────────────────────────────────────────────┐ │ (('queries','database','analytical'),('oriented','processing','DBMS')) │ └────────────────────────────────────────────────────────────────────────┘  "},{"title":"wordShingleMinHashArgUTF8​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#wordshingleminhashargutf8","content":"Splits a UTF-8 string into parts (shingles) of shinglesize words each and returns the shingles with minimum and maximum word hashes, calculated by the wordShingleMinHashUTF8 function with the same input. Is case sensitive. Syntax wordShingleMinHashArgUTF8(string[, shinglesize, hashnum])  Arguments string — String. String.shinglesize — The size of a word shingle. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two tuples with hashnum word shingles each. Type: Tuple(Tuple(String), Tuple(String)). Example Query: SELECT wordShingleMinHashArgUTF8('ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).', 1, 3) AS Tuple;  Result: ┌─Tuple─────────────────────────────────────────────────────────────────┐ │ (('OLAP','database','analytical'),('online','oriented','processing')) │ └───────────────────────────────────────────────────────────────────────┘  "},{"title":"wordShingleMinHashArgCaseInsensitiveUTF8​","type":1,"pageTitle":"Hash Functions","url":"docs/en/sql-reference/functions/hash-functions#wordshingleminhashargcaseinsensitiveutf8","content":"Splits a UTF-8 string into parts (shingles) of shinglesize words each and returns the shingles with minimum and maximum word hashes, calculated by the wordShingleMinHashCaseInsensitiveUTF8 function with the same input. Is case insensitive. Syntax wordShingleMinHashArgCaseInsensitiveUTF8(string[, shinglesize, hashnum])  Arguments string — String. String.shinglesize — The size of a word shingle. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two tuples with hashnum word shingles each. Type: Tuple(Tuple(String), Tuple(String)). Example Query: SELECT wordShingleMinHashArgCaseInsensitiveUTF8('ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).', 1, 3) AS Tuple;  Result: ┌─Tuple──────────────────────────────────────────────────────────────────┐ │ (('queries','database','analytical'),('oriented','processing','DBMS')) │ └────────────────────────────────────────────────────────────────────────┘  "},{"title":"Logical Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/logical-functions","content":"","keywords":""},{"title":"and​","type":1,"pageTitle":"Logical Functions","url":"docs/en/sql-reference/functions/logical-functions#logical-and-function","content":"Calculates the result of the logical conjunction between two or more values. Corresponds to Logical AND Operator. Syntax and(val1, val2...)  You can use the short_circuit_function_evaluation setting to calculate the and function according to a short scheme. If this setting is enabled, vali is evaluated only on rows where (val1 AND val2 AND ... AND val{i-1}) is true. For example, an exception about division by zero is not thrown when executing the query SELECT and(number = 2, intDiv(1, number)) FROM numbers(10). Arguments val1, val2, ... — List of at least two values. Int, UInt, Float or Nullable. Returned value 0, if there is at least one zero value argument.NULL, if there are no zero values arguments and there is at least one NULL argument.1, otherwise. Type: UInt8 or Nullable(UInt8). Example Query: SELECT and(0, 1, -2);  Result: ┌─and(0, 1, -2)─┐ │ 0 │ └───────────────┘  With NULL: SELECT and(NULL, 1, 10, -2);  Result: ┌─and(NULL, 1, 10, -2)─┐ │ ᴺᵁᴸᴸ │ └──────────────────────┘  "},{"title":"or​","type":1,"pageTitle":"Logical Functions","url":"docs/en/sql-reference/functions/logical-functions#logical-or-function","content":"Calculates the result of the logical disjunction between two or more values. Corresponds to Logical OR Operator. Syntax or(val1, val2...)  You can use the short_circuit_function_evaluation setting to calculate the or function according to a short scheme. If this setting is enabled, vali is evaluated only on rows where ((NOT val1) AND (NOT val2) AND ... AND (NOT val{i-1})) is true. For example, an exception about division by zero is not thrown when executing the query SELECT or(number = 0, intDiv(1, number) != 0) FROM numbers(10). Arguments val1, val2, ... — List of at least two values. Int, UInt, Float or Nullable. Returned value 1, if there is at least one non-zero value.0, if there are only zero values.NULL, if there are only zero values and NULL. Type: UInt8 or Nullable(UInt8). Example Query: SELECT or(1, 0, 0, 2, NULL);  Result: ┌─or(1, 0, 0, 2, NULL)─┐ │ 1 │ └──────────────────────┘  With NULL: SELECT or(0, NULL);  Result: ┌─or(0, NULL)─┐ │ ᴺᵁᴸᴸ │ └─────────────┘  "},{"title":"not​","type":1,"pageTitle":"Logical Functions","url":"docs/en/sql-reference/functions/logical-functions#logical-not-function","content":"Calculates the result of the logical negation of the value. Corresponds to Logical Negation Operator. Syntax not(val);  Arguments val — The value. Int, UInt, Float or Nullable. Returned value 1, if the val is 0.0, if the val is a non-zero value.NULL, if the val is a NULL value. Type: UInt8 or Nullable(UInt8). Example Query: SELECT NOT(1);  Result: ┌─not(1)─┐ │ 0 │ └────────┘  "},{"title":"xor​","type":1,"pageTitle":"Logical Functions","url":"docs/en/sql-reference/functions/logical-functions#logical-xor-function","content":"Calculates the result of the logical exclusive disjunction between two or more values. For more than two values the function works as if it calculates XOR of the first two values and then uses the result with the next value to calculate XOR and so on. Syntax xor(val1, val2...)  Arguments val1, val2, ... — List of at least two values. Int, UInt, Float or Nullable. Returned value 1, for two values: if one of the values is zero and other is not.0, for two values: if both values are zero or non-zero at the same time.NULL, if there is at least one NULL value. Type: UInt8 or Nullable(UInt8). Example Query: SELECT xor(0, 1, 1);  Result: ┌─xor(0, 1, 1)─┐ │ 0 │ └──────────────┘  "},{"title":"Functions for Working with Nullable Values","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/functions-for-nulls","content":"","keywords":""},{"title":"isNull​","type":1,"pageTitle":"Functions for Working with Nullable Values","url":"docs/en/sql-reference/functions/functions-for-nulls#isnull","content":"Checks whether the argument is NULL. isNull(x)  Alias: ISNULL. Arguments x — A value with a non-compound data type. Returned value 1 if x is NULL.0 if x is not NULL. Example Input table ┌─x─┬────y─┐ │ 1 │ ᴺᵁᴸᴸ │ │ 2 │ 3 │ └───┴──────┘  Query SELECT x FROM t_null WHERE isNull(y);  ┌─x─┐ │ 1 │ └───┘  "},{"title":"isNotNull​","type":1,"pageTitle":"Functions for Working with Nullable Values","url":"docs/en/sql-reference/functions/functions-for-nulls#isnotnull","content":"Checks whether the argument is NULL. isNotNull(x)  Arguments: x — A value with a non-compound data type. Returned value 0 if x is NULL.1 if x is not NULL. Example Input table ┌─x─┬────y─┐ │ 1 │ ᴺᵁᴸᴸ │ │ 2 │ 3 │ └───┴──────┘  Query SELECT x FROM t_null WHERE isNotNull(y);  ┌─x─┐ │ 2 │ └───┘  "},{"title":"coalesce​","type":1,"pageTitle":"Functions for Working with Nullable Values","url":"docs/en/sql-reference/functions/functions-for-nulls#coalesce","content":"Checks from left to right whether NULL arguments were passed and returns the first non-NULL argument. coalesce(x,...)  Arguments: Any number of parameters of a non-compound type. All parameters must be compatible by data type. Returned values The first non-NULL argument.NULL, if all arguments are NULL. Example Consider a list of contacts that may specify multiple ways to contact a customer. ┌─name─────┬─mail─┬─phone─────┬──icq─┐ │ client 1 │ ᴺᵁᴸᴸ │ 123-45-67 │ 123 │ │ client 2 │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ └──────────┴──────┴───────────┴──────┘  The mail and phone fields are of type String, but the icq field is UInt32, so it needs to be converted to String. Get the first available contact method for the customer from the contact list: SELECT name, coalesce(mail, phone, CAST(icq,'Nullable(String)')) FROM aBook;  ┌─name─────┬─coalesce(mail, phone, CAST(icq, 'Nullable(String)'))─┐ │ client 1 │ 123-45-67 │ │ client 2 │ ᴺᵁᴸᴸ │ └──────────┴──────────────────────────────────────────────────────┘  "},{"title":"ifNull​","type":1,"pageTitle":"Functions for Working with Nullable Values","url":"docs/en/sql-reference/functions/functions-for-nulls#ifnull","content":"Returns an alternative value if the main argument is NULL. ifNull(x,alt)  Arguments: x — The value to check for NULL.alt — The value that the function returns if x is NULL. Returned values The value x, if x is not NULL.The value alt, if x is NULL. Example SELECT ifNull('a', 'b');  ┌─ifNull('a', 'b')─┐ │ a │ └──────────────────┘  SELECT ifNull(NULL, 'b');  ┌─ifNull(NULL, 'b')─┐ │ b │ └───────────────────┘  "},{"title":"nullIf​","type":1,"pageTitle":"Functions for Working with Nullable Values","url":"docs/en/sql-reference/functions/functions-for-nulls#nullif","content":"Returns NULL if the arguments are equal. nullIf(x, y)  Arguments: x, y — Values for comparison. They must be compatible types, or ClickHouse will generate an exception. Returned values NULL, if the arguments are equal.The x value, if the arguments are not equal. Example SELECT nullIf(1, 1);  ┌─nullIf(1, 1)─┐ │ ᴺᵁᴸᴸ │ └──────────────┘  SELECT nullIf(1, 2);  ┌─nullIf(1, 2)─┐ │ 1 │ └──────────────┘  "},{"title":"assumeNotNull​","type":1,"pageTitle":"Functions for Working with Nullable Values","url":"docs/en/sql-reference/functions/functions-for-nulls#assumenotnull","content":"Results in an equivalent non-Nullable value for a Nullable type. In case the original value is NULL the result is undetermined. See also ifNull and coalesce functions. assumeNotNull(x)  Arguments: x — The original value. Returned values The original value from the non-Nullable type, if it is not NULL.Implementation specific result if the original value was NULL. Example Consider the t_null table. SHOW CREATE TABLE t_null;  ┌─statement─────────────────────────────────────────────────────────────────┐ │ CREATE TABLE default.t_null ( x Int8, y Nullable(Int8)) ENGINE = TinyLog │ └───────────────────────────────────────────────────────────────────────────┘  ┌─x─┬────y─┐ │ 1 │ ᴺᵁᴸᴸ │ │ 2 │ 3 │ └───┴──────┘  Apply the assumeNotNull function to the y column. SELECT assumeNotNull(y) FROM t_null;  ┌─assumeNotNull(y)─┐ │ 0 │ │ 3 │ └──────────────────┘  SELECT toTypeName(assumeNotNull(y)) FROM t_null;  ┌─toTypeName(assumeNotNull(y))─┐ │ Int8 │ │ Int8 │ └──────────────────────────────┘  "},{"title":"toNullable​","type":1,"pageTitle":"Functions for Working with Nullable Values","url":"docs/en/sql-reference/functions/functions-for-nulls#tonullable","content":"Converts the argument type to Nullable. toNullable(x)  Arguments: x — The value of any non-compound type. Returned value The input value with a Nullable type. Example SELECT toTypeName(10);  ┌─toTypeName(10)─┐ │ UInt8 │ └────────────────┘  SELECT toTypeName(toNullable(10));  ┌─toTypeName(toNullable(10))─┐ │ Nullable(UInt8) │ └────────────────────────────┘  "},{"title":"Machine Learning Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/machine-learning-functions","content":"","keywords":""},{"title":"evalMLMethod​","type":1,"pageTitle":"Machine Learning Functions","url":"docs/en/sql-reference/functions/machine-learning-functions#machine_learning_methods-evalmlmethod","content":"Prediction using fitted regression models uses evalMLMethod function. See link in linearRegression. "},{"title":"stochasticLinearRegression​","type":1,"pageTitle":"Machine Learning Functions","url":"docs/en/sql-reference/functions/machine-learning-functions#stochastic-linear-regression","content":"The stochasticLinearRegression aggregate function implements stochastic gradient descent method using linear model and MSE loss function. Uses evalMLMethod to predict on new data. "},{"title":"stochasticLogisticRegression​","type":1,"pageTitle":"Machine Learning Functions","url":"docs/en/sql-reference/functions/machine-learning-functions#stochastic-logistic-regression","content":"The stochasticLogisticRegression aggregate function implements stochastic gradient descent method for binary classification problem. Uses evalMLMethod to predict on new data. "},{"title":"Mathematical Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/math-functions","content":"","keywords":""},{"title":"e()​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#e","content":"Returns a Float64 number that is close to the number e. "},{"title":"pi()​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#pi","content":"Returns a Float64 number that is close to the number π. "},{"title":"exp(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#expx","content":"Accepts a numeric argument and returns a Float64 number close to the exponent of the argument. "},{"title":"log(x), ln(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#logx-lnx","content":"Accepts a numeric argument and returns a Float64 number close to the natural logarithm of the argument. "},{"title":"exp2(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#exp2x","content":"Accepts a numeric argument and returns a Float64 number close to 2 to the power of x. "},{"title":"log2(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#log2x","content":"Accepts a numeric argument and returns a Float64 number close to the binary logarithm of the argument. "},{"title":"exp10(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#exp10x","content":"Accepts a numeric argument and returns a Float64 number close to 10 to the power of x. "},{"title":"log10(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#log10x","content":"Accepts a numeric argument and returns a Float64 number close to the decimal logarithm of the argument. "},{"title":"sqrt(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#sqrtx","content":"Accepts a numeric argument and returns a Float64 number close to the square root of the argument. "},{"title":"cbrt(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#cbrtx","content":"Accepts a numeric argument and returns a Float64 number close to the cubic root of the argument. "},{"title":"erf(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#erfx","content":"If ‘x’ is non-negative, then erf(x / σ√2) is the probability that a random variable having a normal distribution with standard deviation ‘σ’ takes the value that is separated from the expected value by more than ‘x’. Example (three sigma rule): SELECT erf(3 / sqrt(2));  ┌─erf(divide(3, sqrt(2)))─┐ │ 0.9973002039367398 │ └─────────────────────────┘  "},{"title":"erfc(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#erfcx","content":"Accepts a numeric argument and returns a Float64 number close to 1 - erf(x), but without loss of precision for large ‘x’ values. "},{"title":"lgamma(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#lgammax","content":"The logarithm of the gamma function. "},{"title":"tgamma(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#tgammax","content":"Gamma function. "},{"title":"sin(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#sinx","content":"The sine. "},{"title":"cos(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#cosx","content":"The cosine. "},{"title":"tan(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#tanx","content":"The tangent. "},{"title":"asin(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#asinx","content":"The arc sine. "},{"title":"acos(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#acosx","content":"The arc cosine. "},{"title":"atan(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#atanx","content":"The arc tangent. "},{"title":"pow(x, y), power(x, y)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#powx-y-powerx-y","content":"Takes two numeric arguments x and y. Returns a Float64 number close to x to the power of y. "},{"title":"intExp2​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#intexp2","content":"Accepts a numeric argument and returns a UInt64 number close to 2 to the power of x. "},{"title":"intExp10​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#intexp10","content":"Accepts a numeric argument and returns a UInt64 number close to 10 to the power of x. "},{"title":"cosh(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#coshx","content":"Hyperbolic cosine. Syntax cosh(x)  Arguments x — The angle, in radians. Values from the interval: -∞ &lt; x &lt; +∞. Float64. Returned value Values from the interval: 1 &lt;= cosh(x) &lt; +∞. Type: Float64. Example Query: SELECT cosh(0);  Result: ┌─cosh(0)──┐ │ 1 │ └──────────┘  "},{"title":"acosh(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#acoshx","content":"Inverse hyperbolic cosine. Syntax acosh(x)  Arguments x — Hyperbolic cosine of angle. Values from the interval: 1 &lt;= x &lt; +∞. Float64. Returned value The angle, in radians. Values from the interval: 0 &lt;= acosh(x) &lt; +∞. Type: Float64. Example Query: SELECT acosh(1);  Result: ┌─acosh(1)─┐ │ 0 │ └──────────┘  See Also cosh(x) "},{"title":"sinh(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#sinhx","content":"Hyperbolic sine. Syntax sinh(x)  Arguments x — The angle, in radians. Values from the interval: -∞ &lt; x &lt; +∞. Float64. Returned value Values from the interval: -∞ &lt; sinh(x) &lt; +∞. Type: Float64. Example Query: SELECT sinh(0);  Result: ┌─sinh(0)──┐ │ 0 │ └──────────┘  "},{"title":"asinh(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#asinhx","content":"Inverse hyperbolic sine. Syntax asinh(x)  Arguments x — Hyperbolic sine of angle. Values from the interval: -∞ &lt; x &lt; +∞. Float64. Returned value The angle, in radians. Values from the interval: -∞ &lt; asinh(x) &lt; +∞. Type: Float64. Example Query: SELECT asinh(0);  Result: ┌─asinh(0)─┐ │ 0 │ └──────────┘  See Also sinh(x) "},{"title":"atanh(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#atanhx","content":"Inverse hyperbolic tangent. Syntax atanh(x)  Arguments x — Hyperbolic tangent of angle. Values from the interval: –1 &lt; x &lt; 1. Float64. Returned value The angle, in radians. Values from the interval: -∞ &lt; atanh(x) &lt; +∞. Type: Float64. Example Query: SELECT atanh(0);  Result: ┌─atanh(0)─┐ │ 0 │ └──────────┘  "},{"title":"atan2(y, x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#atan2yx","content":"The function calculates the angle in the Euclidean plane, given in radians, between the positive x axis and the ray to the point (x, y) ≠ (0, 0). Syntax atan2(y, x)  Arguments y — y-coordinate of the point through which the ray passes. Float64.x — x-coordinate of the point through which the ray passes. Float64. Returned value The angle θ such that −π &lt; θ ≤ π, in radians. Type: Float64. Example Query: SELECT atan2(1, 1);  Result: ┌────────atan2(1, 1)─┐ │ 0.7853981633974483 │ └────────────────────┘  "},{"title":"hypot(x, y)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#hypotxy","content":"Calculates the length of the hypotenuse of a right-angle triangle. The function avoids problems that occur when squaring very large or very small numbers. Syntax hypot(x, y)  Arguments x — The first cathetus of a right-angle triangle. Float64.y — The second cathetus of a right-angle triangle. Float64. Returned value The length of the hypotenuse of a right-angle triangle. Type: Float64. Example Query: SELECT hypot(1, 1);  Result: ┌────────hypot(1, 1)─┐ │ 1.4142135623730951 │ └────────────────────┘  "},{"title":"log1p(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#log1px","content":"Calculates log(1+x). The function log1p(x) is more accurate than log(1+x) for small values of x. Syntax log1p(x)  Arguments x — Values from the interval: -1 &lt; x &lt; +∞. Float64. Returned value Values from the interval: -∞ &lt; log1p(x) &lt; +∞. Type: Float64. Example Query: SELECT log1p(0);  Result: ┌─log1p(0)─┐ │ 0 │ └──────────┘  See Also log(x) "},{"title":"sign(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#signx","content":"Returns the sign of a real number. Syntax sign(x)  Arguments x — Values from -∞ to +∞. Support all numeric types in ClickHouse. Returned value -1 for x &lt; 00 for x = 01 for x &gt; 0 Examples Sign for the zero value: SELECT sign(0);  Result: ┌─sign(0)─┐ │ 0 │ └─────────┘  Sign for the positive value: SELECT sign(1);  Result: ┌─sign(1)─┐ │ 1 │ └─────────┘  Sign for the negative value: SELECT sign(-1);  Result: ┌─sign(-1)─┐ │ -1 │ └──────────┘  "},{"title":"degrees(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#degreesx","content":"Converts the input value in radians to degrees. Syntax degrees(x)  Arguments x — Input in radians. Float64. Returned value Value in degrees. Type: Float64. Example Query: SELECT degrees(3.141592653589793);  Result: ┌─degrees(3.141592653589793)─┐ │ 180 │ └────────────────────────────┘  "},{"title":"radians(x)​","type":1,"pageTitle":"Mathematical Functions","url":"docs/en/sql-reference/functions/math-functions#radiansx","content":"Converts the input value in degrees to radians. Syntax radians(x)  Arguments x — Input in degrees. Float64. Returned value Value in radians. Type: Float64. Example Query: SELECT radians(180);  Result: ┌──────radians(180)─┐ │ 3.141592653589793 │ └───────────────────┘  "},{"title":"Functions for Generating Pseudo-Random Numbers","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/random-functions","content":"","keywords":""},{"title":"rand, rand32​","type":1,"pageTitle":"Functions for Generating Pseudo-Random Numbers","url":"docs/en/sql-reference/functions/random-functions#rand","content":"Returns a pseudo-random UInt32 number, evenly distributed among all UInt32-type numbers. Uses a linear congruential generator. "},{"title":"rand64​","type":1,"pageTitle":"Functions for Generating Pseudo-Random Numbers","url":"docs/en/sql-reference/functions/random-functions#rand64","content":"Returns a pseudo-random UInt64 number, evenly distributed among all UInt64-type numbers. Uses a linear congruential generator. "},{"title":"randConstant​","type":1,"pageTitle":"Functions for Generating Pseudo-Random Numbers","url":"docs/en/sql-reference/functions/random-functions#randconstant","content":"Produces a constant column with a random value. Syntax randConstant([x])  Arguments x — Expression resulting in any of the supported data types. The resulting value is discarded, but the expression itself if used for bypassing common subexpression elimination if the function is called multiple times in one query. Optional parameter. Returned value Pseudo-random number. Type: UInt32. Example Query: SELECT rand(), rand(1), rand(number), randConstant(), randConstant(1), randConstant(number) FROM numbers(3)  Result: ┌─────rand()─┬────rand(1)─┬─rand(number)─┬─randConstant()─┬─randConstant(1)─┬─randConstant(number)─┐ │ 3047369878 │ 4132449925 │ 4044508545 │ 2740811946 │ 4229401477 │ 1924032898 │ │ 2938880146 │ 1267722397 │ 4154983056 │ 2740811946 │ 4229401477 │ 1924032898 │ │ 956619638 │ 4238287282 │ 1104342490 │ 2740811946 │ 4229401477 │ 1924032898 │ └────────────┴────────────┴──────────────┴────────────────┴─────────────────┴──────────────────────┘  Random Functions for Working with Strings "},{"title":"randomString​","type":1,"pageTitle":"Functions for Generating Pseudo-Random Numbers","url":"docs/en/sql-reference/functions/random-functions#random-string","content":""},{"title":"randomFixedString​","type":1,"pageTitle":"Functions for Generating Pseudo-Random Numbers","url":"docs/en/sql-reference/functions/random-functions#random-fixed-string","content":""},{"title":"randomPrintableASCII​","type":1,"pageTitle":"Functions for Generating Pseudo-Random Numbers","url":"docs/en/sql-reference/functions/random-functions#random-printable-ascii","content":""},{"title":"randomStringUTF8​","type":1,"pageTitle":"Functions for Generating Pseudo-Random Numbers","url":"docs/en/sql-reference/functions/random-functions#random-string-utf8","content":""},{"title":"fuzzBits​","type":1,"pageTitle":"Functions for Generating Pseudo-Random Numbers","url":"docs/en/sql-reference/functions/random-functions#fuzzbits","content":"Syntax fuzzBits([s], [prob])  Inverts bits of s, each with probability prob. Arguments s - String or FixedStringprob - constant Float32/64 Returned valueFuzzed string with same as s type. Example SELECT fuzzBits(materialize('abacaba'), 0.1) FROM numbers(3)  ``` text ┌─fuzzBits(materialize(‘abacaba’), 0.1)─┐ │ abaaaja │ │ a*cjab+ │ │ aeca2A │ └───────────────────────────────────────┘ "},{"title":"Functions for Searching and Replacing in Strings","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/string-replace-functions","content":"","keywords":""},{"title":"replaceOne(haystack, pattern, replacement)​","type":1,"pageTitle":"Functions for Searching and Replacing in Strings","url":"docs/en/sql-reference/functions/string-replace-functions#replaceonehaystack-pattern-replacement","content":"Replaces the first occurrence, if it exists, of the ‘pattern’ substring in ‘haystack’ with the ‘replacement’ substring. Hereafter, ‘pattern’ and ‘replacement’ must be constants. "},{"title":"replaceAll(haystack, pattern, replacement), replace(haystack, pattern, replacement)​","type":1,"pageTitle":"Functions for Searching and Replacing in Strings","url":"docs/en/sql-reference/functions/string-replace-functions#replaceallhaystack-pattern-replacement-replacehaystack-pattern-replacement","content":"Replaces all occurrences of the ‘pattern’ substring in ‘haystack’ with the ‘replacement’ substring. "},{"title":"replaceRegexpOne(haystack, pattern, replacement)​","type":1,"pageTitle":"Functions for Searching and Replacing in Strings","url":"docs/en/sql-reference/functions/string-replace-functions#replaceregexponehaystack-pattern-replacement","content":"Replacement using the ‘pattern’ regular expression. A re2 regular expression. Replaces only the first occurrence, if it exists. A pattern can be specified as ‘replacement’. This pattern can include substitutions \\0-\\9. The substitution \\0 includes the entire regular expression. Substitutions \\1-\\9 correspond to the subpattern numbers.To use the \\ character in a template, escape it using \\. Also keep in mind that a string literal requires an extra escape. Example 1. Converting the date to American format: SELECT DISTINCT EventDate, replaceRegexpOne(toString(EventDate), '(\\\\d{4})-(\\\\d{2})-(\\\\d{2})', '\\\\2/\\\\3/\\\\1') AS res FROM test.hits LIMIT 7 FORMAT TabSeparated  2014-03-17 03/17/2014 2014-03-18 03/18/2014 2014-03-19 03/19/2014 2014-03-20 03/20/2014 2014-03-21 03/21/2014 2014-03-22 03/22/2014 2014-03-23 03/23/2014  Example 2. Copying a string ten times: SELECT replaceRegexpOne('Hello, World!', '.*', '\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0') AS res  ┌─res────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World! │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘  "},{"title":"replaceRegexpAll(haystack, pattern, replacement)​","type":1,"pageTitle":"Functions for Searching and Replacing in Strings","url":"docs/en/sql-reference/functions/string-replace-functions#replaceregexpallhaystack-pattern-replacement","content":"This does the same thing, but replaces all the occurrences. Example: SELECT replaceRegexpAll('Hello, World!', '.', '\\\\0\\\\0') AS res  ┌─res────────────────────────┐ │ HHeelllloo,, WWoorrlldd!! │ └────────────────────────────┘  As an exception, if a regular expression worked on an empty substring, the replacement is not made more than once. Example: SELECT replaceRegexpAll('Hello, World!', '^', 'here: ') AS res  ┌─res─────────────────┐ │ here: Hello, World! │ └─────────────────────┘  "},{"title":"regexpQuoteMeta(s)​","type":1,"pageTitle":"Functions for Searching and Replacing in Strings","url":"docs/en/sql-reference/functions/string-replace-functions#regexpquotemetas","content":"The function adds a backslash before some predefined characters in the string. Predefined characters: \\0, \\\\, |, (, ), ^, $, ., [, ], ?, *, +, {, :, -. This implementation slightly differs from re2::RE2::QuoteMeta. It escapes zero byte as \\0 instead of \\x00 and it escapes only required characters. For more information, see the link: RE2 "},{"title":"Functions for Working with JSON","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/json-functions","content":"","keywords":""},{"title":"visitParamHas(params, name)​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#visitparamhasparams-name","content":"Checks whether there is a field with the name name. Alias: simpleJSONHas. "},{"title":"visitParamExtractUInt(params, name)​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#visitparamextractuintparams-name","content":"Parses UInt64 from the value of the field named name. If this is a string field, it tries to parse a number from the beginning of the string. If the field does not exist, or it exists but does not contain a number, it returns 0. Alias: simpleJSONExtractUInt. "},{"title":"visitParamExtractInt(params, name)​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#visitparamextractintparams-name","content":"The same as for Int64. Alias: simpleJSONExtractInt. "},{"title":"visitParamExtractFloat(params, name)​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#visitparamextractfloatparams-name","content":"The same as for Float64. Alias: simpleJSONExtractFloat. "},{"title":"visitParamExtractBool(params, name)​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#visitparamextractboolparams-name","content":"Parses a true/false value. The result is UInt8. Alias: simpleJSONExtractBool. "},{"title":"visitParamExtractRaw(params, name)​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#visitparamextractrawparams-name","content":"Returns the value of a field, including separators. Alias: simpleJSONExtractRaw. Examples: visitParamExtractRaw('{&quot;abc&quot;:&quot;\\\\n\\\\u0000&quot;}', 'abc') = '&quot;\\\\n\\\\u0000&quot;'; visitParamExtractRaw('{&quot;abc&quot;:{&quot;def&quot;:[1,2,3]}}', 'abc') = '{&quot;def&quot;:[1,2,3]}';  "},{"title":"visitParamExtractString(params, name)​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#visitparamextractstringparams-name","content":"Parses the string in double quotes. The value is unescaped. If unescaping failed, it returns an empty string. Alias: simpleJSONExtractString. Examples: visitParamExtractString('{&quot;abc&quot;:&quot;\\\\n\\\\u0000&quot;}', 'abc') = '\\n\\0'; visitParamExtractString('{&quot;abc&quot;:&quot;\\\\u263a&quot;}', 'abc') = '☺'; visitParamExtractString('{&quot;abc&quot;:&quot;\\\\u263&quot;}', 'abc') = ''; visitParamExtractString('{&quot;abc&quot;:&quot;hello}', 'abc') = '';  There is currently no support for code points in the format \\uXXXX\\uYYYY that are not from the basic multilingual plane (they are converted to CESU-8 instead of UTF-8). The following functions are based on simdjson designed for more complex JSON parsing requirements. The assumption 2 mentioned above still applies. "},{"title":"isValidJSON(json)​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#isvalidjsonjson","content":"Checks that passed string is a valid json. Examples: SELECT isValidJSON('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}') = 1 SELECT isValidJSON('not a json') = 0  "},{"title":"JSONHas(json[, indices_or_keys]…)​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#jsonhasjson-indices-or-keys","content":"If the value exists in the JSON document, 1 will be returned. If the value does not exist, 0 will be returned. Examples: SELECT JSONHas('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'b') = 1 SELECT JSONHas('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'b', 4) = 0  indices_or_keys is a list of zero or more arguments each of them can be either string or integer. String = access object member by key.Positive integer = access the n-th member/key from the beginning.Negative integer = access the n-th member/key from the end. Minimum index of the element is 1. Thus the element 0 does not exist. You may use integers to access both JSON arrays and JSON objects. So, for example: SELECT JSONExtractKey('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 1) = 'a' SELECT JSONExtractKey('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 2) = 'b' SELECT JSONExtractKey('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', -1) = 'b' SELECT JSONExtractKey('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', -2) = 'a' SELECT JSONExtractString('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 1) = 'hello'  "},{"title":"JSONLength(json[, indices_or_keys]…)​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#jsonlengthjson-indices-or-keys","content":"Return the length of a JSON array or a JSON object. If the value does not exist or has a wrong type, 0 will be returned. Examples: SELECT JSONLength('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'b') = 3 SELECT JSONLength('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}') = 2  "},{"title":"JSONType(json[, indices_or_keys]…)​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#jsontypejson-indices-or-keys","content":"Return the type of a JSON value. If the value does not exist, Null will be returned. Examples: SELECT JSONType('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}') = 'Object' SELECT JSONType('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'a') = 'String' SELECT JSONType('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'b') = 'Array'  "},{"title":"JSONExtractUInt(json[, indices_or_keys]…)​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#jsonextractuintjson-indices-or-keys","content":""},{"title":"JSONExtractInt(json[, indices_or_keys]…)​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#jsonextractintjson-indices-or-keys","content":""},{"title":"JSONExtractFloat(json[, indices_or_keys]…)​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#jsonextractfloatjson-indices-or-keys","content":""},{"title":"JSONExtractBool(json[, indices_or_keys]…)​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#jsonextractbooljson-indices-or-keys","content":"Parses a JSON and extract a value. These functions are similar to visitParam functions. If the value does not exist or has a wrong type, 0 will be returned. Examples: SELECT JSONExtractInt('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'b', 1) = -100 SELECT JSONExtractFloat('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'b', 2) = 200.0 SELECT JSONExtractUInt('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'b', -1) = 300  "},{"title":"JSONExtractString(json[, indices_or_keys]…)​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#jsonextractstringjson-indices-or-keys","content":"Parses a JSON and extract a string. This function is similar to visitParamExtractString functions. If the value does not exist or has a wrong type, an empty string will be returned. The value is unescaped. If unescaping failed, it returns an empty string. Examples: SELECT JSONExtractString('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'a') = 'hello' SELECT JSONExtractString('{&quot;abc&quot;:&quot;\\\\n\\\\u0000&quot;}', 'abc') = '\\n\\0' SELECT JSONExtractString('{&quot;abc&quot;:&quot;\\\\u263a&quot;}', 'abc') = '☺' SELECT JSONExtractString('{&quot;abc&quot;:&quot;\\\\u263&quot;}', 'abc') = '' SELECT JSONExtractString('{&quot;abc&quot;:&quot;hello}', 'abc') = ''  "},{"title":"JSONExtract(json[, indices_or_keys…], Return_type)​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#jsonextractjson-indices-or-keys-return-type","content":"Parses a JSON and extract a value of the given ClickHouse data type. This is a generalization of the previous JSONExtract&lt;type&gt; functions. This meansJSONExtract(..., 'String') returns exactly the same as JSONExtractString(),JSONExtract(..., 'Float64') returns exactly the same as JSONExtractFloat(). Examples: SELECT JSONExtract('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'Tuple(String, Array(Float64))') = ('hello',[-100,200,300]) SELECT JSONExtract('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'Tuple(b Array(Float64), a String)') = ([-100,200,300],'hello') SELECT JSONExtract('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'b', 'Array(Nullable(Int8))') = [-100, NULL, NULL] SELECT JSONExtract('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'b', 4, 'Nullable(Int64)') = NULL SELECT JSONExtract('{&quot;passed&quot;: true}', 'passed', 'UInt8') = 1 SELECT JSONExtract('{&quot;day&quot;: &quot;Thursday&quot;}', 'day', 'Enum8(\\'Sunday\\' = 0, \\'Monday\\' = 1, \\'Tuesday\\' = 2, \\'Wednesday\\' = 3, \\'Thursday\\' = 4, \\'Friday\\' = 5, \\'Saturday\\' = 6)') = 'Thursday' SELECT JSONExtract('{&quot;day&quot;: 5}', 'day', 'Enum8(\\'Sunday\\' = 0, \\'Monday\\' = 1, \\'Tuesday\\' = 2, \\'Wednesday\\' = 3, \\'Thursday\\' = 4, \\'Friday\\' = 5, \\'Saturday\\' = 6)') = 'Friday'  "},{"title":"JSONExtractKeysAndValues(json[, indices_or_keys…], Value_type)​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#jsonextractkeysandvaluesjson-indices-or-keys-value-type","content":"Parses key-value pairs from a JSON where the values are of the given ClickHouse data type. Example: SELECT JSONExtractKeysAndValues('{&quot;x&quot;: {&quot;a&quot;: 5, &quot;b&quot;: 7, &quot;c&quot;: 11}}', 'x', 'Int8') = [('a',5),('b',7),('c',11)];  "},{"title":"JSONExtractKeys​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#jsonextractkeysjson-indices-or-keys","content":"Parses a JSON string and extracts the keys. Syntax JSONExtractKeys(json[, a, b, c...])  Arguments json — String with valid JSON.a, b, c... — Comma-separated indices or keys that specify the path to the inner field in a nested JSON object. Each argument can be either a String to get the field by the key or an Integer to get the N-th field (indexed from 1, negative integers count from the end). If not set, the whole JSON is parsed as the top-level object. Optional parameter. Returned value Array with the keys of the JSON. Type: Array(String). Example Query: SELECT JSONExtractKeys('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}');  Result: text ┌─JSONExtractKeys('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}')─┐ │ ['a','b'] │ └────────────────────────────────────────────────────────────┘  "},{"title":"JSONExtractRaw(json[, indices_or_keys]…)​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#jsonextractrawjson-indices-or-keys","content":"Returns a part of JSON as unparsed string. If the part does not exist or has a wrong type, an empty string will be returned. Example: SELECT JSONExtractRaw('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'b') = '[-100, 200.0, 300]';  "},{"title":"JSONExtractArrayRaw(json[, indices_or_keys…])​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#jsonextractarrayrawjson-indices-or-keys","content":"Returns an array with elements of JSON array, each represented as unparsed string. If the part does not exist or isn’t array, an empty array will be returned. Example: SELECT JSONExtractArrayRaw('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, &quot;hello&quot;]}', 'b') = ['-100', '200.0', '&quot;hello&quot;'];  "},{"title":"JSONExtractKeysAndValuesRaw​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#json-extract-keys-and-values-raw","content":"Extracts raw data from a JSON object. Syntax JSONExtractKeysAndValuesRaw(json[, p, a, t, h])  Arguments json — String with valid JSON.p, a, t, h — Comma-separated indices or keys that specify the path to the inner field in a nested JSON object. Each argument can be either a string to get the field by the key or an integer to get the N-th field (indexed from 1, negative integers count from the end). If not set, the whole JSON is parsed as the top-level object. Optional parameter. Returned values Array with ('key', 'value') tuples. Both tuple members are strings.Empty array if the requested object does not exist, or input JSON is invalid. Type: Array(Tuple(String, String). Examples Query: SELECT JSONExtractKeysAndValuesRaw('{&quot;a&quot;: [-100, 200.0], &quot;b&quot;:{&quot;c&quot;: {&quot;d&quot;: &quot;hello&quot;, &quot;f&quot;: &quot;world&quot;}}}');  Result: ┌─JSONExtractKeysAndValuesRaw('{&quot;a&quot;: [-100, 200.0], &quot;b&quot;:{&quot;c&quot;: {&quot;d&quot;: &quot;hello&quot;, &quot;f&quot;: &quot;world&quot;}}}')─┐ │ [('a','[-100,200]'),('b','{&quot;c&quot;:{&quot;d&quot;:&quot;hello&quot;,&quot;f&quot;:&quot;world&quot;}}')] │ └──────────────────────────────────────────────────────────────────────────────────────────────┘  Query: SELECT JSONExtractKeysAndValuesRaw('{&quot;a&quot;: [-100, 200.0], &quot;b&quot;:{&quot;c&quot;: {&quot;d&quot;: &quot;hello&quot;, &quot;f&quot;: &quot;world&quot;}}}', 'b');  Result: ┌─JSONExtractKeysAndValuesRaw('{&quot;a&quot;: [-100, 200.0], &quot;b&quot;:{&quot;c&quot;: {&quot;d&quot;: &quot;hello&quot;, &quot;f&quot;: &quot;world&quot;}}}', 'b')─┐ │ [('c','{&quot;d&quot;:&quot;hello&quot;,&quot;f&quot;:&quot;world&quot;}')] │ └───────────────────────────────────────────────────────────────────────────────────────────────────┘  Query: SELECT JSONExtractKeysAndValuesRaw('{&quot;a&quot;: [-100, 200.0], &quot;b&quot;:{&quot;c&quot;: {&quot;d&quot;: &quot;hello&quot;, &quot;f&quot;: &quot;world&quot;}}}', -1, 'c');  Result: ┌─JSONExtractKeysAndValuesRaw('{&quot;a&quot;: [-100, 200.0], &quot;b&quot;:{&quot;c&quot;: {&quot;d&quot;: &quot;hello&quot;, &quot;f&quot;: &quot;world&quot;}}}', -1, 'c')─┐ │ [('d','&quot;hello&quot;'),('f','&quot;world&quot;')] │ └───────────────────────────────────────────────────────────────────────────────────────────────────────┘  "},{"title":"JSON_EXISTS(json, path)​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#json-exists","content":"If the value exists in the JSON document, 1 will be returned. If the value does not exist, 0 will be returned. Examples: SELECT JSON_EXISTS('{&quot;hello&quot;:1}', '$.hello'); SELECT JSON_EXISTS('{&quot;hello&quot;:{&quot;world&quot;:1}}', '$.hello.world'); SELECT JSON_EXISTS('{&quot;hello&quot;:[&quot;world&quot;]}', '$.hello[*]'); SELECT JSON_EXISTS('{&quot;hello&quot;:[&quot;world&quot;]}', '$.hello[0]');  note Before version 21.11 the order of arguments was wrong, i.e. JSON_EXISTS(path, json) "},{"title":"JSON_QUERY(json, path)​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#json-query","content":"Parses a JSON and extract a value as JSON array or JSON object. If the value does not exist, an empty string will be returned. Example: SELECT JSON_QUERY('{&quot;hello&quot;:&quot;world&quot;}', '$.hello'); SELECT JSON_QUERY('{&quot;array&quot;:[[0, 1, 2, 3, 4, 5], [0, -1, -2, -3, -4, -5]]}', '$.array[*][0 to 2, 4]'); SELECT JSON_QUERY('{&quot;hello&quot;:2}', '$.hello'); SELECT toTypeName(JSON_QUERY('{&quot;hello&quot;:2}', '$.hello'));  Result: [&quot;world&quot;] [0, 1, 4, 0, -1, -4] [2] String  note Before version 21.11 the order of arguments was wrong, i.e. JSON_QUERY(path, json) "},{"title":"JSON_VALUE(json, path)​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#json-value","content":"Parses a JSON and extract a value as JSON scalar. If the value does not exist, an empty string will be returned. Example: SELECT JSON_VALUE('{&quot;hello&quot;:&quot;world&quot;}', '$.hello'); SELECT JSON_VALUE('{&quot;array&quot;:[[0, 1, 2, 3, 4, 5], [0, -1, -2, -3, -4, -5]]}', '$.array[*][0 to 2, 4]'); SELECT JSON_VALUE('{&quot;hello&quot;:2}', '$.hello'); SELECT toTypeName(JSON_VALUE('{&quot;hello&quot;:2}', '$.hello'));  Result: &quot;world&quot; 0 2 String  note Before version 21.11 the order of arguments was wrong, i.e. JSON_VALUE(path, json) "},{"title":"toJSONString​","type":1,"pageTitle":"Functions for Working with JSON","url":"docs/en/sql-reference/functions/json-functions#tojsonstring","content":"Serializes a value to its JSON representation. Various data types and nested structures are supported. 64-bit integers or bigger (like UInt64 or Int128) are enclosed in quotes by default. output_format_json_quote_64bit_integers controls this behavior. Special values NaN and inf are replaced with null. Enable output_format_json_quote_denormals setting to show them. When serializing an Enum value, the function outputs its name. Syntax toJSONString(value)  Arguments value — Value to serialize. Value may be of any data type. Returned value JSON representation of the value. Type: String. Example The first example shows serialization of a Map. The second example shows some special values wrapped into a Tuple. Query: SELECT toJSONString(map('key1', 1, 'key2', 2)); SELECT toJSONString(tuple(1.25, NULL, NaN, +inf, -inf, [])) SETTINGS output_format_json_quote_denormals = 1;  Result: {&quot;key1&quot;:1,&quot;key2&quot;:2} [1.25,null,&quot;nan&quot;,&quot;inf&quot;,&quot;-inf&quot;,[]]  See Also output_format_json_quote_64bit_integersoutput_format_json_quote_denormals "},{"title":"[experimental] Natural Language Processing functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/nlp-functions","content":"","keywords":""},{"title":"stem​","type":1,"pageTitle":"[experimental] Natural Language Processing functions","url":"docs/en/sql-reference/functions/nlp-functions#stem","content":"Performs stemming on a given word. Syntax stem('language', word)  Arguments language — Language which rules will be applied. Must be in lowercase. String.word — word that needs to be stemmed. Must be in lowercase. String. Examples Query: SELECT arrayMap(x -&gt; stem('en', x), ['I', 'think', 'it', 'is', 'a', 'blessing', 'in', 'disguise']) as res;  Result: ┌─res────────────────────────────────────────────────┐ │ ['I','think','it','is','a','bless','in','disguis'] │ └────────────────────────────────────────────────────┘  "},{"title":"lemmatize​","type":1,"pageTitle":"[experimental] Natural Language Processing functions","url":"docs/en/sql-reference/functions/nlp-functions#lemmatize","content":"Performs lemmatization on a given word. Needs dictionaries to operate, which can be obtained here. Syntax lemmatize('language', word)  Arguments language — Language which rules will be applied. String.word — Word that needs to be lemmatized. Must be lowercase. String. Examples Query: SELECT lemmatize('en', 'wolves');  Result: ┌─lemmatize(&quot;wolves&quot;)─┐ │ &quot;wolf&quot; │ └─────────────────────┘  Configuration: &lt;lemmatizers&gt; &lt;lemmatizer&gt; &lt;lang&gt;en&lt;/lang&gt; &lt;path&gt;en.bin&lt;/path&gt; &lt;/lemmatizer&gt; &lt;/lemmatizers&gt;  "},{"title":"synonyms​","type":1,"pageTitle":"[experimental] Natural Language Processing functions","url":"docs/en/sql-reference/functions/nlp-functions#synonyms","content":"Finds synonyms to a given word. There are two types of synonym extensions: plain and wordnet. With the plain extension type we need to provide a path to a simple text file, where each line corresponds to a certain synonym set. Words in this line must be separated with space or tab characters. With the wordnet extension type we need to provide a path to a directory with WordNet thesaurus in it. Thesaurus must contain a WordNet sense index. Syntax synonyms('extension_name', word)  Arguments extension_name — Name of the extension in which search will be performed. String.word — Word that will be searched in extension. String. Examples Query: SELECT synonyms('list', 'important');  Result: ┌─synonyms('list', 'important')────────────┐ │ ['important','big','critical','crucial'] │ └──────────────────────────────────────────┘  Configuration: &lt;synonyms_extensions&gt; &lt;extension&gt; &lt;name&gt;en&lt;/name&gt; &lt;type&gt;plain&lt;/type&gt; &lt;path&gt;en.txt&lt;/path&gt; &lt;/extension&gt; &lt;extension&gt; &lt;name&gt;en&lt;/name&gt; &lt;type&gt;wordnet&lt;/type&gt; &lt;path&gt;en/&lt;/path&gt; &lt;/extension&gt; &lt;/synonyms_extensions&gt;  "},{"title":"Functions for Working with IPv4 and IPv6 Addresses","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/ip-address-functions","content":"","keywords":""},{"title":"IPv4NumToString(num)​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"docs/en/sql-reference/functions/ip-address-functions#ipv4numtostringnum","content":"Takes a UInt32 number. Interprets it as an IPv4 address in big endian. Returns a string containing the corresponding IPv4 address in the format A.B.C.d (dot-separated numbers in decimal form). Alias: INET_NTOA. "},{"title":"IPv4StringToNum(s)​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"docs/en/sql-reference/functions/ip-address-functions#ipv4stringtonums","content":"The reverse function of IPv4NumToString. If the IPv4 address has an invalid format, it returns 0. Alias: INET_ATON. "},{"title":"IPv4NumToStringClassC(num)​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"docs/en/sql-reference/functions/ip-address-functions#ipv4numtostringclasscnum","content":"Similar to IPv4NumToString, but using xxx instead of the last octet. Example: SELECT IPv4NumToStringClassC(ClientIP) AS k, count() AS c FROM test.hits GROUP BY k ORDER BY c DESC LIMIT 10  ┌─k──────────────┬─────c─┐ │ 83.149.9.xxx │ 26238 │ │ 217.118.81.xxx │ 26074 │ │ 213.87.129.xxx │ 25481 │ │ 83.149.8.xxx │ 24984 │ │ 217.118.83.xxx │ 22797 │ │ 78.25.120.xxx │ 22354 │ │ 213.87.131.xxx │ 21285 │ │ 78.25.121.xxx │ 20887 │ │ 188.162.65.xxx │ 19694 │ │ 83.149.48.xxx │ 17406 │ └────────────────┴───────┘  Since using ‘xxx’ is highly unusual, this may be changed in the future. We recommend that you do not rely on the exact format of this fragment. "},{"title":"IPv6NumToString(x)​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"docs/en/sql-reference/functions/ip-address-functions#ipv6numtostringx","content":"Accepts a FixedString(16) value containing the IPv6 address in binary format. Returns a string containing this address in text format. IPv6-mapped IPv4 addresses are output in the format ::ffff:111.222.33.44. Alias: INET6_NTOA. Examples: SELECT IPv6NumToString(toFixedString(unhex('2A0206B8000000000000000000000011'), 16)) AS addr;  ┌─addr─────────┐ │ 2a02:6b8::11 │ └──────────────┘  SELECT IPv6NumToString(ClientIP6 AS k), count() AS c FROM hits_all WHERE EventDate = today() AND substring(ClientIP6, 1, 12) != unhex('00000000000000000000FFFF') GROUP BY k ORDER BY c DESC LIMIT 10  ┌─IPv6NumToString(ClientIP6)──────────────┬─────c─┐ │ 2a02:2168:aaa:bbbb::2 │ 24695 │ │ 2a02:2698:abcd:abcd:abcd:abcd:8888:5555 │ 22408 │ │ 2a02:6b8:0:fff::ff │ 16389 │ │ 2a01:4f8:111:6666::2 │ 16016 │ │ 2a02:2168:888:222::1 │ 15896 │ │ 2a01:7e00::ffff:ffff:ffff:222 │ 14774 │ │ 2a02:8109:eee:ee:eeee:eeee:eeee:eeee │ 14443 │ │ 2a02:810b:8888:888:8888:8888:8888:8888 │ 14345 │ │ 2a02:6b8:0:444:4444:4444:4444:4444 │ 14279 │ │ 2a01:7e00::ffff:ffff:ffff:ffff │ 13880 │ └─────────────────────────────────────────┴───────┘  SELECT IPv6NumToString(ClientIP6 AS k), count() AS c FROM hits_all WHERE EventDate = today() GROUP BY k ORDER BY c DESC LIMIT 10  ┌─IPv6NumToString(ClientIP6)─┬──────c─┐ │ ::ffff:94.26.111.111 │ 747440 │ │ ::ffff:37.143.222.4 │ 529483 │ │ ::ffff:5.166.111.99 │ 317707 │ │ ::ffff:46.38.11.77 │ 263086 │ │ ::ffff:79.105.111.111 │ 186611 │ │ ::ffff:93.92.111.88 │ 176773 │ │ ::ffff:84.53.111.33 │ 158709 │ │ ::ffff:217.118.11.22 │ 154004 │ │ ::ffff:217.118.11.33 │ 148449 │ │ ::ffff:217.118.11.44 │ 148243 │ └────────────────────────────┴────────┘  "},{"title":"IPv6StringToNum​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"docs/en/sql-reference/functions/ip-address-functions#ipv6stringtonums","content":"The reverse function of IPv6NumToString. If the IPv6 address has an invalid format, it returns a string of null bytes. If the input string contains a valid IPv4 address, returns its IPv6 equivalent. HEX can be uppercase or lowercase. Alias: INET6_ATON. Syntax IPv6StringToNum(string)  Argument string — IP address. String. Returned value IPv6 address in binary format. Type: FixedString(16). Example Query: SELECT addr, cutIPv6(IPv6StringToNum(addr), 0, 0) FROM (SELECT ['notaddress', '127.0.0.1', '1111::ffff'] AS addr) ARRAY JOIN addr;  Result: ┌─addr───────┬─cutIPv6(IPv6StringToNum(addr), 0, 0)─┐ │ notaddress │ :: │ │ 127.0.0.1 │ ::ffff:127.0.0.1 │ │ 1111::ffff │ 1111::ffff │ └────────────┴──────────────────────────────────────┘  See Also cutIPv6. "},{"title":"IPv4ToIPv6(x)​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"docs/en/sql-reference/functions/ip-address-functions#ipv4toipv6x","content":"Takes a UInt32 number. Interprets it as an IPv4 address in big endian. Returns a FixedString(16) value containing the IPv6 address in binary format. Examples: SELECT IPv6NumToString(IPv4ToIPv6(IPv4StringToNum('192.168.0.1'))) AS addr;  ┌─addr───────────────┐ │ ::ffff:192.168.0.1 │ └────────────────────┘  "},{"title":"cutIPv6(x, bytesToCutForIPv6, bytesToCutForIPv4)​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"docs/en/sql-reference/functions/ip-address-functions#cutipv6x-bytestocutforipv6-bytestocutforipv4","content":"Accepts a FixedString(16) value containing the IPv6 address in binary format. Returns a string containing the address of the specified number of bytes removed in text format. For example: WITH IPv6StringToNum('2001:0DB8:AC10:FE01:FEED:BABE:CAFE:F00D') AS ipv6, IPv4ToIPv6(IPv4StringToNum('192.168.0.1')) AS ipv4 SELECT cutIPv6(ipv6, 2, 0), cutIPv6(ipv4, 0, 2)  ┌─cutIPv6(ipv6, 2, 0)─────────────────┬─cutIPv6(ipv4, 0, 2)─┐ │ 2001:db8:ac10:fe01:feed:babe:cafe:0 │ ::ffff:192.168.0.0 │ └─────────────────────────────────────┴─────────────────────┘  "},{"title":"IPv4CIDRToRange(ipv4, Cidr),​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"docs/en/sql-reference/functions/ip-address-functions#ipv4cidrtorangeipv4-cidr","content":"Accepts an IPv4 and an UInt8 value containing the CIDR. Return a tuple with two IPv4 containing the lower range and the higher range of the subnet. SELECT IPv4CIDRToRange(toIPv4('192.168.5.2'), 16);  ┌─IPv4CIDRToRange(toIPv4('192.168.5.2'), 16)─┐ │ ('192.168.0.0','192.168.255.255') │ └────────────────────────────────────────────┘  "},{"title":"IPv6CIDRToRange(ipv6, Cidr),​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"docs/en/sql-reference/functions/ip-address-functions#ipv6cidrtorangeipv6-cidr","content":"Accepts an IPv6 and an UInt8 value containing the CIDR. Return a tuple with two IPv6 containing the lower range and the higher range of the subnet. SELECT IPv6CIDRToRange(toIPv6('2001:0db8:0000:85a3:0000:0000:ac1f:8001'), 32);  ┌─IPv6CIDRToRange(toIPv6('2001:0db8:0000:85a3:0000:0000:ac1f:8001'), 32)─┐ │ ('2001:db8::','2001:db8:ffff:ffff:ffff:ffff:ffff:ffff') │ └────────────────────────────────────────────────────────────────────────┘  "},{"title":"toIPv4(string)​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"docs/en/sql-reference/functions/ip-address-functions#toipv4string","content":"An alias to IPv4StringToNum() that takes a string form of IPv4 address and returns value of IPv4 type, which is binary equal to value returned by IPv4StringToNum(). WITH '171.225.130.45' as IPv4_string SELECT toTypeName(IPv4StringToNum(IPv4_string)), toTypeName(toIPv4(IPv4_string))  ┌─toTypeName(IPv4StringToNum(IPv4_string))─┬─toTypeName(toIPv4(IPv4_string))─┐ │ UInt32 │ IPv4 │ └──────────────────────────────────────────┴─────────────────────────────────┘  WITH '171.225.130.45' as IPv4_string SELECT hex(IPv4StringToNum(IPv4_string)), hex(toIPv4(IPv4_string))  ┌─hex(IPv4StringToNum(IPv4_string))─┬─hex(toIPv4(IPv4_string))─┐ │ ABE1822D │ ABE1822D │ └───────────────────────────────────┴──────────────────────────┘  "},{"title":"toIPv6​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"docs/en/sql-reference/functions/ip-address-functions#toipv6string","content":"Converts a string form of IPv6 address to IPv6 type. If the IPv6 address has an invalid format, returns an empty value. Similar to IPv6StringToNum function, which converts IPv6 address to binary format. If the input string contains a valid IPv4 address, then the IPv6 equivalent of the IPv4 address is returned. Syntax toIPv6(string)  Argument string — IP address. String Returned value IP address. Type: IPv6. Examples Query: WITH '2001:438:ffff::407d:1bc1' AS IPv6_string SELECT hex(IPv6StringToNum(IPv6_string)), hex(toIPv6(IPv6_string));  Result: ┌─hex(IPv6StringToNum(IPv6_string))─┬─hex(toIPv6(IPv6_string))─────────┐ │ 20010438FFFF000000000000407D1BC1 │ 20010438FFFF000000000000407D1BC1 │ └───────────────────────────────────┴──────────────────────────────────┘  Query: SELECT toIPv6('127.0.0.1');  Result: ┌─toIPv6('127.0.0.1')─┐ │ ::ffff:127.0.0.1 │ └─────────────────────┘  "},{"title":"isIPv4String​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"docs/en/sql-reference/functions/ip-address-functions#isipv4string","content":"Determines whether the input string is an IPv4 address or not. If string is IPv6 address returns 0. Syntax isIPv4String(string)  Arguments string — IP address. String. Returned value 1 if string is IPv4 address, 0 otherwise. Type: UInt8. Examples Query: SELECT addr, isIPv4String(addr) FROM ( SELECT ['0.0.0.0', '127.0.0.1', '::ffff:127.0.0.1'] AS addr ) ARRAY JOIN addr;  Result: ┌─addr─────────────┬─isIPv4String(addr)─┐ │ 0.0.0.0 │ 1 │ │ 127.0.0.1 │ 1 │ │ ::ffff:127.0.0.1 │ 0 │ └──────────────────┴────────────────────┘  "},{"title":"isIPv6String​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"docs/en/sql-reference/functions/ip-address-functions#isipv6string","content":"Determines whether the input string is an IPv6 address or not. If string is IPv4 address returns 0. Syntax isIPv6String(string)  Arguments string — IP address. String. Returned value 1 if string is IPv6 address, 0 otherwise. Type: UInt8. Examples Query: SELECT addr, isIPv6String(addr) FROM ( SELECT ['::', '1111::ffff', '::ffff:127.0.0.1', '127.0.0.1'] AS addr ) ARRAY JOIN addr;  Result: ┌─addr─────────────┬─isIPv6String(addr)─┐ │ :: │ 1 │ │ 1111::ffff │ 1 │ │ ::ffff:127.0.0.1 │ 1 │ │ 127.0.0.1 │ 0 │ └──────────────────┴────────────────────┘  "},{"title":"isIPAddressInRange​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"docs/en/sql-reference/functions/ip-address-functions#isipaddressinrange","content":"Determines if an IP address is contained in a network represented in the CIDR notation. Returns 1 if true, or 0 otherwise. Syntax isIPAddressInRange(address, prefix)  This function accepts both IPv4 and IPv6 addresses (and networks) represented as strings. It returns 0 if the IP version of the address and the CIDR don't match. Arguments address — An IPv4 or IPv6 address. String.prefix — An IPv4 or IPv6 network prefix in CIDR. String. Returned value 1 or 0. Type: UInt8. Example Query: SELECT isIPAddressInRange('127.0.0.1', '127.0.0.0/8');  Result: ┌─isIPAddressInRange('127.0.0.1', '127.0.0.0/8')─┐ │ 1 │ └────────────────────────────────────────────────┘  Query: SELECT isIPAddressInRange('127.0.0.1', 'ffff::/16');  Result: ┌─isIPAddressInRange('127.0.0.1', 'ffff::/16')─┐ │ 0 │ └──────────────────────────────────────────────┘  "},{"title":"Time Window Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/time-window-functions","content":"","keywords":""},{"title":"tumble​","type":1,"pageTitle":"Time Window Functions","url":"docs/en/sql-reference/functions/time-window-functions#time-window-functions-tumble","content":"A tumbling time window assigns records to non-overlapping, continuous windows with a fixed duration (interval). tumble(time_attr, interval [, timezone])  Arguments time_attr - Date and time. DateTime data type.interval - Window interval in Interval data type.timezone — Timezone name (optional).  Returned values The inclusive lower and exclusive upper bound of the corresponding tumbling window. Type: Tuple(DateTime, DateTime) Example Query: SELECT tumble(now(), toIntervalDay('1'))  Result: ┌─tumble(now(), toIntervalDay('1'))─────────────┐ │ ['2020-01-01 00:00:00','2020-01-02 00:00:00'] │ └───────────────────────────────────────────────┘  "},{"title":"hop​","type":1,"pageTitle":"Time Window Functions","url":"docs/en/sql-reference/functions/time-window-functions#time-window-functions-hop","content":"A hopping time window has a fixed duration (window_interval) and hops by a specified hop interval (hop_interval). If the hop_interval is smaller than the window_interval, hopping windows are overlapping. Thus, records can be assigned to multiple windows. hop(time_attr, hop_interval, window_interval [, timezone])  Arguments time_attr - Date and time. DateTime data type.hop_interval - Hop interval in Interval data type. Should be a positive number.window_interval - Window interval in Interval data type. Should be a positive number.timezone — Timezone name (optional).  Returned values The inclusive lower and exclusive upper bound of the corresponding hopping window. Since one record can be assigned to multiple hop windows, the function only returns the bound of the first window when hop function is used without WINDOW VIEW. Type: Tuple(DateTime, DateTime) Example Query: SELECT hop(now(), INTERVAL '1' SECOND, INTERVAL '2' SECOND)  Result: ┌─hop(now(), toIntervalSecond('1'), toIntervalSecond('2'))──┐ │ ('2020-01-14 16:58:22','2020-01-14 16:58:24') │ └───────────────────────────────────────────────────────────┘  "},{"title":"tumbleStart​","type":1,"pageTitle":"Time Window Functions","url":"docs/en/sql-reference/functions/time-window-functions#time-window-functions-tumblestart","content":"Returns the inclusive lower bound of the corresponding tumbling window. tumbleStart(time_attr, interval [, timezone]);  "},{"title":"tumbleEnd​","type":1,"pageTitle":"Time Window Functions","url":"docs/en/sql-reference/functions/time-window-functions#time-window-functions-tumbleend","content":"Returns the exclusive upper bound of the corresponding tumbling window. tumbleEnd(time_attr, interval [, timezone]);  "},{"title":"hopStart​","type":1,"pageTitle":"Time Window Functions","url":"docs/en/sql-reference/functions/time-window-functions#time-window-functions-hopstart","content":"Returns the inclusive lower bound of the corresponding hopping window. hopStart(time_attr, hop_interval, window_interval [, timezone]);  "},{"title":"hopEnd​","type":1,"pageTitle":"Time Window Functions","url":"docs/en/sql-reference/functions/time-window-functions#time-window-functions-hopend","content":"Returns the exclusive upper bound of the corresponding hopping window. hopEnd(time_attr, hop_interval, window_interval [, timezone]);  "},{"title":"Rounding Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/rounding-functions","content":"","keywords":""},{"title":"floor(x[, N])​","type":1,"pageTitle":"Rounding Functions","url":"docs/en/sql-reference/functions/rounding-functions#floorx-n","content":"Returns the largest round number that is less than or equal to x. A round number is a multiple of 1/10N, or the nearest number of the appropriate data type if 1 / 10N isn’t exact. ‘N’ is an integer constant, optional parameter. By default it is zero, which means to round to an integer. ‘N’ may be negative. Examples: floor(123.45, 1) = 123.4, floor(123.45, -1) = 120. x is any numeric type. The result is a number of the same type. For integer arguments, it makes sense to round with a negative N value (for non-negative N, the function does not do anything). If rounding causes overflow (for example, floor(-128, -1)), an implementation-specific result is returned. "},{"title":"ceil(x[, N]), ceiling(x[, N])​","type":1,"pageTitle":"Rounding Functions","url":"docs/en/sql-reference/functions/rounding-functions#ceilx-n-ceilingx-n","content":"Returns the smallest round number that is greater than or equal to x. In every other way, it is the same as the floor function (see above). "},{"title":"trunc(x[, N]), truncate(x[, N])​","type":1,"pageTitle":"Rounding Functions","url":"docs/en/sql-reference/functions/rounding-functions#truncx-n-truncatex-n","content":"Returns the round number with largest absolute value that has an absolute value less than or equal to x‘s. In every other way, it is the same as the ’floor’ function (see above). "},{"title":"round(x[, N])​","type":1,"pageTitle":"Rounding Functions","url":"docs/en/sql-reference/functions/rounding-functions#rounding_functions-round","content":"Rounds a value to a specified number of decimal places. The function returns the nearest number of the specified order. In case when given number has equal distance to surrounding numbers, the function uses banker’s rounding for float number types and rounds away from zero for the other number types (Decimal). round(expression [, decimal_places])  Arguments expression — A number to be rounded. Can be any expression returning the numeric data type.decimal-places — An integer value. If decimal-places &gt; 0 then the function rounds the value to the right of the decimal point.If decimal-places &lt; 0 then the function rounds the value to the left of the decimal point.If decimal-places = 0 then the function rounds the value to integer. In this case the argument can be omitted. Returned value: The rounded number of the same type as the input number. "},{"title":"Examples​","type":1,"pageTitle":"Rounding Functions","url":"docs/en/sql-reference/functions/rounding-functions#examples","content":"Example of use with Float SELECT number / 2 AS x, round(x) FROM system.numbers LIMIT 3  ┌───x─┬─round(divide(number, 2))─┐ │ 0 │ 0 │ │ 0.5 │ 0 │ │ 1 │ 1 │ └─────┴──────────────────────────┘  Example of use with Decimal SELECT cast(number / 2 AS Decimal(10,4)) AS x, round(x) FROM system.numbers LIMIT 3  ┌──────x─┬─round(CAST(divide(number, 2), 'Decimal(10, 4)'))─┐ │ 0.0000 │ 0.0000 │ │ 0.5000 │ 1.0000 │ │ 1.0000 │ 1.0000 │ └────────┴──────────────────────────────────────────────────┘  Examples of rounding Rounding to the nearest number. round(3.2, 0) = 3 round(4.1267, 2) = 4.13 round(22,-1) = 20 round(467,-2) = 500 round(-467,-2) = -500  Banker’s rounding. round(3.5) = 4 round(4.5) = 4 round(3.55, 1) = 3.6 round(3.65, 1) = 3.6  See Also roundBankers "},{"title":"roundBankers​","type":1,"pageTitle":"Rounding Functions","url":"docs/en/sql-reference/functions/rounding-functions#roundbankers","content":"Rounds a number to a specified decimal position. If the rounding number is halfway between two numbers, the function uses banker’s rounding. Banker's rounding is a method of rounding fractional numbers. When the rounding number is halfway between two numbers, it's rounded to the nearest even digit at the specified decimal position. For example: 3.5 rounds up to 4, 2.5 rounds down to 2. It's the default rounding method for floating point numbers defined in [IEEE 754](https://en.wikipedia.org/wiki/IEEE_754#Roundings_to_nearest). The [round](#rounding_functions-round) function performs the same rounding for floating point numbers. The `roundBankers` function also rounds integers the same way, for example, `roundBankers(45, -1) = 40`. In other cases, the function rounds numbers to the nearest integer. Using banker’s rounding, you can reduce the effect that rounding numbers has on the results of summing or subtracting these numbers. For example, sum numbers 1.5, 2.5, 3.5, 4.5 with different rounding: No rounding: 1.5 + 2.5 + 3.5 + 4.5 = 12.Banker’s rounding: 2 + 2 + 4 + 4 = 12.Rounding to the nearest integer: 2 + 3 + 4 + 5 = 14. Syntax roundBankers(expression [, decimal_places])  Arguments expression — A number to be rounded. Can be any expression returning the numeric data type.decimal-places — Decimal places. An integer number. decimal-places &gt; 0 — The function rounds the number to the given position right of the decimal point. Example: roundBankers(3.55, 1) = 3.6.decimal-places &lt; 0 — The function rounds the number to the given position left of the decimal point. Example: roundBankers(24.55, -1) = 20.decimal-places = 0 — The function rounds the number to an integer. In this case the argument can be omitted. Example: roundBankers(2.5) = 2. Returned value A value rounded by the banker’s rounding method. "},{"title":"Examples​","type":1,"pageTitle":"Rounding Functions","url":"docs/en/sql-reference/functions/rounding-functions#examples-1","content":"Example of use Query:  SELECT number / 2 AS x, roundBankers(x, 0) AS b fROM system.numbers limit 10  Result: ┌───x─┬─b─┐ │ 0 │ 0 │ │ 0.5 │ 0 │ │ 1 │ 1 │ │ 1.5 │ 2 │ │ 2 │ 2 │ │ 2.5 │ 2 │ │ 3 │ 3 │ │ 3.5 │ 4 │ │ 4 │ 4 │ │ 4.5 │ 4 │ └─────┴───┘  Examples of Banker’s rounding roundBankers(0.4) = 0 roundBankers(-3.5) = -4 roundBankers(4.5) = 4 roundBankers(3.55, 1) = 3.6 roundBankers(3.65, 1) = 3.6 roundBankers(10.35, 1) = 10.4 roundBankers(10.755, 2) = 10.76  See Also round "},{"title":"roundToExp2(num)​","type":1,"pageTitle":"Rounding Functions","url":"docs/en/sql-reference/functions/rounding-functions#roundtoexp2num","content":"Accepts a number. If the number is less than one, it returns 0. Otherwise, it rounds the number down to the nearest (whole non-negative) degree of two. "},{"title":"roundDuration(num)​","type":1,"pageTitle":"Rounding Functions","url":"docs/en/sql-reference/functions/rounding-functions#rounddurationnum","content":"Accepts a number. If the number is less than one, it returns 0. Otherwise, it rounds the number down to numbers from the set: 1, 10, 30, 60, 120, 180, 240, 300, 600, 1200, 1800, 3600, 7200, 18000, 36000. "},{"title":"roundAge(num)​","type":1,"pageTitle":"Rounding Functions","url":"docs/en/sql-reference/functions/rounding-functions#roundagenum","content":"Accepts a number. If the number is less than 18, it returns 0. Otherwise, it rounds the number down to a number from the set: 18, 25, 35, 45, 55. "},{"title":"roundDown(num, arr)​","type":1,"pageTitle":"Rounding Functions","url":"docs/en/sql-reference/functions/rounding-functions#rounddownnum-arr","content":"Accepts a number and rounds it down to an element in the specified array. If the value is less than the lowest bound, the lowest bound is returned. "},{"title":"Functions for Splitting and Merging Strings and Arrays","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/splitting-merging-functions","content":"","keywords":""},{"title":"splitByChar(separator, s)​","type":1,"pageTitle":"Functions for Splitting and Merging Strings and Arrays","url":"docs/en/sql-reference/functions/splitting-merging-functions#splitbycharseparator-s","content":"Splits a string into substrings separated by a specified character. It uses a constant string separator which consisting of exactly one character. Returns an array of selected substrings. Empty substrings may be selected if the separator occurs at the beginning or end of the string, or if there are multiple consecutive separators. Syntax splitByChar(separator, s)  Arguments separator — The separator which should contain exactly one character. String.s — The string to split. String. Returned value(s) Returns an array of selected substrings. Empty substrings may be selected when: A separator occurs at the beginning or end of the string;There are multiple consecutive separators;The original string s is empty. Type: Array(String). Example SELECT splitByChar(',', '1,2,3,abcde');  ┌─splitByChar(',', '1,2,3,abcde')─┐ │ ['1','2','3','abcde'] │ └─────────────────────────────────┘  "},{"title":"splitByString(separator, s)​","type":1,"pageTitle":"Functions for Splitting and Merging Strings and Arrays","url":"docs/en/sql-reference/functions/splitting-merging-functions#splitbystringseparator-s","content":"Splits a string into substrings separated by a string. It uses a constant string separator of multiple characters as the separator. If the string separator is empty, it will split the string s into an array of single characters. Syntax splitByString(separator, s)  Arguments separator — The separator. String.s — The string to split. String. Returned value(s) Returns an array of selected substrings. Empty substrings may be selected when: Type: Array(String). A non-empty separator occurs at the beginning or end of the string;There are multiple consecutive non-empty separators;The original string s is empty while the separator is not empty. Example SELECT splitByString(', ', '1, 2 3, 4,5, abcde');  ┌─splitByString(', ', '1, 2 3, 4,5, abcde')─┐ │ ['1','2 3','4,5','abcde'] │ └───────────────────────────────────────────┘  SELECT splitByString('', 'abcde');  ┌─splitByString('', 'abcde')─┐ │ ['a','b','c','d','e'] │ └────────────────────────────┘  "},{"title":"splitByRegexp(regexp, s)​","type":1,"pageTitle":"Functions for Splitting and Merging Strings and Arrays","url":"docs/en/sql-reference/functions/splitting-merging-functions#splitbyregexpseparator-s","content":"Splits a string into substrings separated by a regular expression. It uses a regular expression string regexp as the separator. If the regexp is empty, it will split the string s into an array of single characters. If no match is found for this regular expression, the string s won't be split. Syntax splitByRegexp(regexp, s)  Arguments regexp — Regular expression. Constant. String or FixedString.s — The string to split. String. Returned value(s) Returns an array of selected substrings. Empty substrings may be selected when: A non-empty regular expression match occurs at the beginning or end of the string;There are multiple consecutive non-empty regular expression matches;The original string s is empty while the regular expression is not empty. Type: Array(String). Example Query: SELECT splitByRegexp('\\\\d+', 'a12bc23de345f');  Result: ┌─splitByRegexp('\\\\d+', 'a12bc23de345f')─┐ │ ['a','bc','de','f'] │ └────────────────────────────────────────┘  Query: SELECT splitByRegexp('', 'abcde');  Result: ┌─splitByRegexp('', 'abcde')─┐ │ ['a','b','c','d','e'] │ └────────────────────────────┘  "},{"title":"splitByWhitespace(s)​","type":1,"pageTitle":"Functions for Splitting and Merging Strings and Arrays","url":"docs/en/sql-reference/functions/splitting-merging-functions#splitbywhitespaceseparator-s","content":"Splits a string into substrings separated by whitespace characters. Returns an array of selected substrings. Syntax splitByWhitespace(s)  Arguments s — The string to split. String. Returned value(s) Returns an array of selected substrings. Type: Array(String). Example SELECT splitByWhitespace(' 1! a, b. ');  ┌─splitByWhitespace(' 1! a, b. ')─┐ │ ['1!','a,','b.'] │ └─────────────────────────────────────┘  "},{"title":"splitByNonAlpha(s)​","type":1,"pageTitle":"Functions for Splitting and Merging Strings and Arrays","url":"docs/en/sql-reference/functions/splitting-merging-functions#splitbynonalphaseparator-s","content":"Splits a string into substrings separated by whitespace and punctuation characters. Returns an array of selected substrings. Syntax splitByNonAlpha(s)  Arguments s — The string to split. String. Returned value(s) Returns an array of selected substrings. Type: Array(String). Example SELECT splitByNonAlpha(' 1! a, b. ');  ┌─splitByNonAlpha(' 1! a, b. ')─┐ │ ['1','a','b'] │ └───────────────────────────────────┘  "},{"title":"arrayStringConcat(arr[, separator])​","type":1,"pageTitle":"Functions for Splitting and Merging Strings and Arrays","url":"docs/en/sql-reference/functions/splitting-merging-functions#arraystringconcatarr-separator","content":"Concatenates string representations of values listed in the array with the separator. separator is an optional parameter: a constant string, set to an empty string by default. Returns the string. "},{"title":"alphaTokens(s)​","type":1,"pageTitle":"Functions for Splitting and Merging Strings and Arrays","url":"docs/en/sql-reference/functions/splitting-merging-functions#alphatokenss","content":"Selects substrings of consecutive bytes from the ranges a-z and A-Z.Returns an array of substrings. Example SELECT alphaTokens('abca1abc');  ┌─alphaTokens('abca1abc')─┐ │ ['abca','abc'] │ └─────────────────────────┘  "},{"title":"extractAllGroups(text, regexp)​","type":1,"pageTitle":"Functions for Splitting and Merging Strings and Arrays","url":"docs/en/sql-reference/functions/splitting-merging-functions#extractallgroups","content":"Extracts all groups from non-overlapping substrings matched by a regular expression. Syntax extractAllGroups(text, regexp)  Arguments text — String or FixedString.regexp — Regular expression. Constant. String or FixedString. Returned values If the function finds at least one matching group, it returns Array(Array(String)) column, clustered by group_id (1 to N, where N is number of capturing groups in regexp). If there is no matching group, returns an empty array. Type: Array. Example Query: SELECT extractAllGroups('abc=123, 8=&quot;hkl&quot;', '(&quot;[^&quot;]+&quot;|\\\\w+)=(&quot;[^&quot;]+&quot;|\\\\w+)');  Result: ┌─extractAllGroups('abc=123, 8=&quot;hkl&quot;', '(&quot;[^&quot;]+&quot;|\\\\w+)=(&quot;[^&quot;]+&quot;|\\\\w+)')─┐ │ [['abc','123'],['8','&quot;hkl&quot;']] │ └───────────────────────────────────────────────────────────────────────┘  "},{"title":"ngrams​","type":1,"pageTitle":"Functions for Splitting and Merging Strings and Arrays","url":"docs/en/sql-reference/functions/splitting-merging-functions#ngrams","content":"Splits the UTF-8 string into n-grams of ngramsize symbols. Syntax ngrams(string, ngramsize)  Arguments string — String. String or FixedString.ngramsize — The size of an n-gram. UInt. Returned values Array with n-grams. Type: Array(String). Example Query: SELECT ngrams('ClickHouse', 3);  Result: ┌─ngrams('ClickHouse', 3)───────────────────────────┐ │ ['Cli','lic','ick','ckH','kHo','Hou','ous','use'] │ └───────────────────────────────────────────────────┘  "},{"title":"tokens​","type":1,"pageTitle":"Functions for Splitting and Merging Strings and Arrays","url":"docs/en/sql-reference/functions/splitting-merging-functions#tokens","content":"Splits a string into tokens using non-alphanumeric ASCII characters as separators. Arguments input_string — Any set of bytes represented as the String data type object. Returned value The resulting array of tokens from input string. Type: Array. Example Query: SELECT tokens('test1,;\\\\ test2,;\\\\ test3,;\\\\ test4') AS tokens;  Result: ┌─tokens────────────────────────────┐ │ ['test1','test2','test3','test4'] │ └───────────────────────────────────┘  "},{"title":"Functions for Searching in Strings","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/string-search-functions","content":"","keywords":""},{"title":"position(haystack, needle), locate(haystack, needle)​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#position","content":"Searches for the substring needle in the string haystack. Returns the position (in bytes) of the found substring in the string, starting from 1. For a case-insensitive search, use the function positionCaseInsensitive. Syntax position(haystack, needle[, start_pos])  position(needle IN haystack)  Alias: locate(haystack, needle[, start_pos]). note Syntax of position(needle IN haystack) provides SQL-compatibility, the function works the same way as to position(haystack, needle). Arguments haystack — String, in which substring will to be searched. String.needle — Substring to be searched. String.start_pos – Position of the first character in the string to start search. UInt. Optional. Returned values Starting position in bytes (counting from 1), if substring was found.0, if the substring was not found. Type: Integer. Examples The phrase “Hello, world!” contains a set of bytes representing a single-byte encoded text. The function returns some expected result: Query: SELECT position('Hello, world!', '!');  Result: ┌─position('Hello, world!', '!')─┐ │ 13 │ └────────────────────────────────┘  SELECT position('Hello, world!', 'o', 1), position('Hello, world!', 'o', 7)  ┌─position('Hello, world!', 'o', 1)─┬─position('Hello, world!', 'o', 7)─┐ │ 5 │ 9 │ └───────────────────────────────────┴───────────────────────────────────┘  The same phrase in Russian contains characters which can’t be represented using a single byte. The function returns some unexpected result (use positionUTF8 function for multi-byte encoded text): Query: SELECT position('Привет, мир!', '!');  Result: ┌─position('Привет, мир!', '!')─┐ │ 21 │ └───────────────────────────────┘  Examples for POSITION(needle IN haystack) syntax Query: SELECT 3 = position('c' IN 'abc');  Result: ┌─equals(3, position('abc', 'c'))─┐ │ 1 │ └─────────────────────────────────┘  Query: SELECT 6 = position('/' IN s) FROM (SELECT 'Hello/World' AS s);  Result: ┌─equals(6, position(s, '/'))─┐ │ 1 │ └─────────────────────────────┘  "},{"title":"positionCaseInsensitive​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#positioncaseinsensitive","content":"The same as position returns the position (in bytes) of the found substring in the string, starting from 1. Use the function for a case-insensitive search. Works under the assumption that the string contains a set of bytes representing a single-byte encoded text. If this assumption is not met and a character can’t be represented using a single byte, the function does not throw an exception and returns some unexpected result. If character can be represented using two bytes, it will use two bytes and so on. Syntax positionCaseInsensitive(haystack, needle[, start_pos])  Arguments haystack — String, in which substring will to be searched. String.needle — Substring to be searched. String.start_pos — Optional parameter, position of the first character in the string to start search. UInt. Returned values Starting position in bytes (counting from 1), if substring was found.0, if the substring was not found. Type: Integer. Example Query: SELECT positionCaseInsensitive('Hello, world!', 'hello');  Result: ┌─positionCaseInsensitive('Hello, world!', 'hello')─┐ │ 1 │ └───────────────────────────────────────────────────┘  "},{"title":"positionUTF8​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#positionutf8","content":"Returns the position (in Unicode points) of the found substring in the string, starting from 1. Works under the assumption that the string contains a set of bytes representing a UTF-8 encoded text. If this assumption is not met, the function does not throw an exception and returns some unexpected result. If character can be represented using two Unicode points, it will use two and so on. For a case-insensitive search, use the function positionCaseInsensitiveUTF8. Syntax positionUTF8(haystack, needle[, start_pos])  Arguments haystack — String, in which substring will to be searched. String.needle — Substring to be searched. String.start_pos — Optional parameter, position of the first character in the string to start search. UInt Returned values Starting position in Unicode points (counting from 1), if substring was found.0, if the substring was not found. Type: Integer. Examples The phrase “Hello, world!” in Russian contains a set of Unicode points representing a single-point encoded text. The function returns some expected result: Query: SELECT positionUTF8('Привет, мир!', '!');  Result: ┌─positionUTF8('Привет, мир!', '!')─┐ │ 12 │ └───────────────────────────────────┘  The phrase “Salut, étudiante!”, where character é can be represented using a one point (U+00E9) or two points (U+0065U+0301) the function can be returned some unexpected result: Query for the letter é, which is represented one Unicode point U+00E9: SELECT positionUTF8('Salut, étudiante!', '!');  Result: ┌─positionUTF8('Salut, étudiante!', '!')─┐ │ 17 │ └────────────────────────────────────────┘  Query for the letter é, which is represented two Unicode points U+0065U+0301: SELECT positionUTF8('Salut, étudiante!', '!');  Result: ┌─positionUTF8('Salut, étudiante!', '!')─┐ │ 18 │ └────────────────────────────────────────┘  "},{"title":"positionCaseInsensitiveUTF8​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#positioncaseinsensitiveutf8","content":"The same as positionUTF8, but is case-insensitive. Returns the position (in Unicode points) of the found substring in the string, starting from 1. Works under the assumption that the string contains a set of bytes representing a UTF-8 encoded text. If this assumption is not met, the function does not throw an exception and returns some unexpected result. If character can be represented using two Unicode points, it will use two and so on. Syntax positionCaseInsensitiveUTF8(haystack, needle[, start_pos])  Arguments haystack — String, in which substring will to be searched. String.needle — Substring to be searched. String.start_pos — Optional parameter, position of the first character in the string to start search. UInt Returned value Starting position in Unicode points (counting from 1), if substring was found.0, if the substring was not found. Type: Integer. Example Query: SELECT positionCaseInsensitiveUTF8('Привет, мир!', 'Мир');  Result: ┌─positionCaseInsensitiveUTF8('Привет, мир!', 'Мир')─┐ │ 9 │ └────────────────────────────────────────────────────┘  "},{"title":"multiSearchAllPositions​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#multisearchallpositions","content":"The same as position but returns Array of positions (in bytes) of the found corresponding substrings in the string. Positions are indexed starting from 1. The search is performed on sequences of bytes without respect to string encoding and collation. For case-insensitive ASCII search, use the function multiSearchAllPositionsCaseInsensitive.For search in UTF-8, use the function multiSearchAllPositionsUTF8.For case-insensitive UTF-8 search, use the function multiSearchAllPositionsCaseInsensitiveUTF8. Syntax multiSearchAllPositions(haystack, [needle1, needle2, ..., needlen])  Arguments haystack — String, in which substring will to be searched. String.needle — Substring to be searched. String. Returned values Array of starting positions in bytes (counting from 1), if the corresponding substring was found and 0 if not found. Example Query: SELECT multiSearchAllPositions('Hello, World!', ['hello', '!', 'world']);  Result: ┌─multiSearchAllPositions('Hello, World!', ['hello', '!', 'world'])─┐ │ [0,13,0] │ └───────────────────────────────────────────────────────────────────┘  "},{"title":"multiSearchAllPositionsUTF8​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#multiSearchAllPositionsUTF8","content":"See multiSearchAllPositions. "},{"title":"multiSearchFirstPosition(haystack, [needle1, needle2, …, needlen])​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#multisearchfirstposition","content":"The same as position but returns the leftmost offset of the string haystack that is matched to some of the needles. For a case-insensitive search or/and in UTF-8 format use functions multiSearchFirstPositionCaseInsensitive, multiSearchFirstPositionUTF8, multiSearchFirstPositionCaseInsensitiveUTF8. "},{"title":"multiSearchFirstIndex(haystack, [needle1, needle2, …, needlen])​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#multisearchfirstindexhaystack-needle1-needle2-needlen","content":"Returns the index i (starting from 1) of the leftmost found needlei in the string haystack and 0 otherwise. For a case-insensitive search or/and in UTF-8 format use functions multiSearchFirstIndexCaseInsensitive, multiSearchFirstIndexUTF8, multiSearchFirstIndexCaseInsensitiveUTF8. "},{"title":"multiSearchAny(haystack, [needle1, needle2, …, needlen])​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#function-multisearchany","content":"Returns 1, if at least one string needlei matches the string haystack and 0 otherwise. For a case-insensitive search or/and in UTF-8 format use functions multiSearchAnyCaseInsensitive, multiSearchAnyUTF8, multiSearchAnyCaseInsensitiveUTF8. note In all multiSearch* functions the number of needles should be less than 28 because of implementation specification. "},{"title":"match(haystack, pattern)​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#matchhaystack-pattern","content":"Checks whether the string matches the pattern regular expression. A re2 regular expression. The syntax of the re2 regular expressions is more limited than the syntax of the Perl regular expressions. Returns 0 if it does not match, or 1 if it matches. The regular expression works with the string as if it is a set of bytes. The regular expression can’t contain null bytes. For patterns to search for substrings in a string, it is better to use LIKE or ‘position’, since they work much faster. "},{"title":"multiMatchAny(haystack, [pattern1, pattern2, …, patternn])​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#multimatchanyhaystack-pattern1-pattern2-patternn","content":"The same as match, but returns 0 if none of the regular expressions are matched and 1 if any of the patterns matches. It uses hyperscan library. For patterns to search substrings in a string, it is better to use multiSearchAny since it works much faster. note The length of any of the haystack string must be less than 232 bytes otherwise the exception is thrown. This restriction takes place because of hyperscan API. "},{"title":"multiMatchAnyIndex(haystack, [pattern1, pattern2, …, patternn])​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#multimatchanyindexhaystack-pattern1-pattern2-patternn","content":"The same as multiMatchAny, but returns any index that matches the haystack. "},{"title":"multiMatchAllIndices(haystack, [pattern1, pattern2, …, patternn])​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#multimatchallindiceshaystack-pattern1-pattern2-patternn","content":"The same as multiMatchAny, but returns the array of all indicies that match the haystack in any order. "},{"title":"multiFuzzyMatchAny(haystack, distance, [pattern1, pattern2, …, patternn])​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#multifuzzymatchanyhaystack-distance-pattern1-pattern2-patternn","content":"The same as multiMatchAny, but returns 1 if any pattern matches the haystack within a constant edit distance. This function relies on the experimental feature of hyperscan library, and can be slow for some corner cases. The performance depends on the edit distance value and patterns used, but it's always more expensive compared to a non-fuzzy variants. "},{"title":"multiFuzzyMatchAnyIndex(haystack, distance, [pattern1, pattern2, …, patternn])​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#multifuzzymatchanyindexhaystack-distance-pattern1-pattern2-patternn","content":"The same as multiFuzzyMatchAny, but returns any index that matches the haystack within a constant edit distance. "},{"title":"multiFuzzyMatchAllIndices(haystack, distance, [pattern1, pattern2, …, patternn])​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#multifuzzymatchallindiceshaystack-distance-pattern1-pattern2-patternn","content":"The same as multiFuzzyMatchAny, but returns the array of all indices in any order that match the haystack within a constant edit distance. note multiFuzzyMatch* functions do not support UTF-8 regular expressions, and such expressions are treated as bytes because of hyperscan restriction. note To turn off all functions that use hyperscan, use setting SET allow_hyperscan = 0;. "},{"title":"extract(haystack, pattern)​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#extracthaystack-pattern","content":"Extracts a fragment of a string using a regular expression. If ‘haystack’ does not match the ‘pattern’ regex, an empty string is returned. If the regex does not contain subpatterns, it takes the fragment that matches the entire regex. Otherwise, it takes the fragment that matches the first subpattern. "},{"title":"extractAll(haystack, pattern)​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#extractallhaystack-pattern","content":"Extracts all the fragments of a string using a regular expression. If ‘haystack’ does not match the ‘pattern’ regex, an empty string is returned. Returns an array of strings consisting of all matches to the regex. In general, the behavior is the same as the ‘extract’ function (it takes the first subpattern, or the entire expression if there isn’t a subpattern). "},{"title":"extractAllGroupsHorizontal​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#extractallgroups-horizontal","content":"Matches all groups of the haystack string using the pattern regular expression. Returns an array of arrays, where the first array includes all fragments matching the first group, the second array - matching the second group, etc. note extractAllGroupsHorizontal function is slower than extractAllGroupsVertical. Syntax extractAllGroupsHorizontal(haystack, pattern)  Arguments haystack — Input string. Type: String.pattern — Regular expression with re2 syntax. Must contain groups, each group enclosed in parentheses. If pattern contains no groups, an exception is thrown. Type: String. Returned value Type: Array. If haystack does not match the pattern regex, an array of empty arrays is returned. Example Query: SELECT extractAllGroupsHorizontal('abc=111, def=222, ghi=333', '(&quot;[^&quot;]+&quot;|\\\\w+)=(&quot;[^&quot;]+&quot;|\\\\w+)');  Result: ┌─extractAllGroupsHorizontal('abc=111, def=222, ghi=333', '(&quot;[^&quot;]+&quot;|\\\\w+)=(&quot;[^&quot;]+&quot;|\\\\w+)')─┐ │ [['abc','def','ghi'],['111','222','333']] │ └──────────────────────────────────────────────────────────────────────────────────────────┘  See Also extractAllGroupsVertical "},{"title":"extractAllGroupsVertical​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#extractallgroups-vertical","content":"Matches all groups of the haystack string using the pattern regular expression. Returns an array of arrays, where each array includes matching fragments from every group. Fragments are grouped in order of appearance in the haystack. Syntax extractAllGroupsVertical(haystack, pattern)  Arguments haystack — Input string. Type: String.pattern — Regular expression with re2 syntax. Must contain groups, each group enclosed in parentheses. If pattern contains no groups, an exception is thrown. Type: String. Returned value Type: Array. If haystack does not match the pattern regex, an empty array is returned. Example Query: SELECT extractAllGroupsVertical('abc=111, def=222, ghi=333', '(&quot;[^&quot;]+&quot;|\\\\w+)=(&quot;[^&quot;]+&quot;|\\\\w+)');  Result: ┌─extractAllGroupsVertical('abc=111, def=222, ghi=333', '(&quot;[^&quot;]+&quot;|\\\\w+)=(&quot;[^&quot;]+&quot;|\\\\w+)')─┐ │ [['abc','111'],['def','222'],['ghi','333']] │ └────────────────────────────────────────────────────────────────────────────────────────┘  See Also extractAllGroupsHorizontal "},{"title":"like(haystack, pattern), haystack LIKE pattern operator​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#function-like","content":"Checks whether a string matches a simple regular expression. The regular expression can contain the metasymbols % and _. % indicates any quantity of any bytes (including zero characters). _ indicates any one byte. Use the backslash (\\) for escaping metasymbols. See the note on escaping in the description of the ‘match’ function. For regular expressions like %needle%, the code is more optimal and works as fast as the position function. For other regular expressions, the code is the same as for the ‘match’ function. "},{"title":"notLike(haystack, pattern), haystack NOT LIKE pattern operator​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#function-notlike","content":"The same thing as ‘like’, but negative. "},{"title":"ilike​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#ilike","content":"Case insensitive variant of like function. You can use ILIKE operator instead of the ilike function. Syntax ilike(haystack, pattern)  Arguments haystack — Input string. String.pattern — If pattern does not contain percent signs or underscores, then the pattern only represents the string itself. An underscore (_) in pattern stands for (matches) any single character. A percent sign (%) matches any sequence of zero or more characters. Some pattern examples: 'abc' ILIKE 'abc' true 'abc' ILIKE 'a%' true 'abc' ILIKE '_b_' true 'abc' ILIKE 'c' false  Returned values True, if the string matches pattern.False, if the string does not match pattern. Example Input table: ┌─id─┬─name─────┬─days─┐ │ 1 │ January │ 31 │ │ 2 │ February │ 29 │ │ 3 │ March │ 31 │ │ 4 │ April │ 30 │ └────┴──────────┴──────┘  Query: SELECT * FROM Months WHERE ilike(name, '%j%');  Result: ┌─id─┬─name────┬─days─┐ │ 1 │ January │ 31 │ └────┴─────────┴──────┘  See Also like  "},{"title":"ngramDistance(haystack, needle)​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#ngramdistancehaystack-needle","content":"Calculates the 4-gram distance between haystack and needle: counts the symmetric difference between two multisets of 4-grams and normalizes it by the sum of their cardinalities. Returns float number from 0 to 1 – the closer to zero, the more strings are similar to each other. If the constant needle or haystack is more than 32Kb, throws an exception. If some of the non-constant haystack or needle strings are more than 32Kb, the distance is always one. For case-insensitive search or/and in UTF-8 format use functions ngramDistanceCaseInsensitive, ngramDistanceUTF8, ngramDistanceCaseInsensitiveUTF8. "},{"title":"ngramSearch(haystack, needle)​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#ngramsearchhaystack-needle","content":"Same as ngramDistance but calculates the non-symmetric difference between needle and haystack – the number of n-grams from needle minus the common number of n-grams normalized by the number of needle n-grams. The closer to one, the more likely needle is in the haystack. Can be useful for fuzzy string search. For case-insensitive search or/and in UTF-8 format use functions ngramSearchCaseInsensitive, ngramSearchUTF8, ngramSearchCaseInsensitiveUTF8. note For UTF-8 case we use 3-gram distance. All these are not perfectly fair n-gram distances. We use 2-byte hashes to hash n-grams and then calculate the (non-)symmetric difference between these hash tables – collisions may occur. With UTF-8 case-insensitive format we do not use fair tolower function – we zero the 5-th bit (starting from zero) of each codepoint byte and first bit of zeroth byte if bytes more than one – this works for Latin and mostly for all Cyrillic letters. "},{"title":"countSubstrings​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#countSubstrings","content":"Returns the number of substring occurrences. For a case-insensitive search, use countSubstringsCaseInsensitive or countSubstringsCaseInsensitiveUTF8 functions. Syntax countSubstrings(haystack, needle[, start_pos])  Arguments haystack — The string to search in. String.needle — The substring to search for. String.start_pos – Position of the first character in the string to start search. Optional. UInt. Returned values Number of occurrences. Type: UInt64. Examples Query: SELECT countSubstrings('foobar.com', '.');  Result: ┌─countSubstrings('foobar.com', '.')─┐ │ 1 │ └────────────────────────────────────┘  Query: SELECT countSubstrings('aaaa', 'aa');  Result: ┌─countSubstrings('aaaa', 'aa')─┐ │ 2 │ └───────────────────────────────┘  Query: SELECT countSubstrings('abc___abc', 'abc', 4);  Result: ┌─countSubstrings('abc___abc', 'abc', 4)─┐ │ 1 │ └────────────────────────────────────────┘  "},{"title":"countSubstringsCaseInsensitive​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#countSubstringsCaseInsensitive","content":"Returns the number of substring occurrences case-insensitive. Syntax countSubstringsCaseInsensitive(haystack, needle[, start_pos])  Arguments haystack — The string to search in. String.needle — The substring to search for. String.start_pos — Position of the first character in the string to start search. Optional. UInt. Returned values Number of occurrences. Type: UInt64. Examples Query: SELECT countSubstringsCaseInsensitive('aba', 'B');  Result: ┌─countSubstringsCaseInsensitive('aba', 'B')─┐ │ 1 │ └────────────────────────────────────────────┘  Query: SELECT countSubstringsCaseInsensitive('foobar.com', 'CoM');  Result: ┌─countSubstringsCaseInsensitive('foobar.com', 'CoM')─┐ │ 1 │ └─────────────────────────────────────────────────────┘  Query: SELECT countSubstringsCaseInsensitive('abC___abC', 'aBc', 2);  Result: ┌─countSubstringsCaseInsensitive('abC___abC', 'aBc', 2)─┐ │ 1 │ └───────────────────────────────────────────────────────┘  "},{"title":"countSubstringsCaseInsensitiveUTF8​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#countSubstringsCaseInsensitiveUTF8","content":"Returns the number of substring occurrences in UTF-8 case-insensitive. Syntax SELECT countSubstringsCaseInsensitiveUTF8(haystack, needle[, start_pos])  Arguments haystack — The string to search in. String.needle — The substring to search for. String.start_pos — Position of the first character in the string to start search. Optional. UInt. Returned values Number of occurrences. Type: UInt64. Examples Query: SELECT countSubstringsCaseInsensitiveUTF8('абв', 'A');  Result: ┌─countSubstringsCaseInsensitiveUTF8('абв', 'A')─┐ │ 1 │ └────────────────────────────────────────────────┘  Query: SELECT countSubstringsCaseInsensitiveUTF8('аБв__АбВ__абв', 'Абв');  Result: ┌─countSubstringsCaseInsensitiveUTF8('аБв__АбВ__абв', 'Абв')─┐ │ 3 │ └────────────────────────────────────────────────────────────┘  "},{"title":"countMatches(haystack, pattern)​","type":1,"pageTitle":"Functions for Searching in Strings","url":"docs/en/sql-reference/functions/string-search-functions#countmatcheshaystack-pattern","content":"Returns the number of regular expression matches for a pattern in a haystack. Syntax countMatches(haystack, pattern)  Arguments haystack — The string to search in. String.pattern — The regular expression with re2 syntax. String. Returned value The number of matches. Type: UInt64. Examples Query: SELECT countMatches('foobar.com', 'o+');  Result: ┌─countMatches('foobar.com', 'o+')─┐ │ 2 │ └──────────────────────────────────┘  Query: SELECT countMatches('aaaa', 'aa');  Result: ┌─countMatches('aaaa', 'aa')────┐ │ 2 │ └───────────────────────────────┘  "},{"title":"Functions for maps","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/tuple-map-functions","content":"","keywords":""},{"title":"map​","type":1,"pageTitle":"Functions for maps","url":"docs/en/sql-reference/functions/tuple-map-functions#function-map","content":"Arranges key:value pairs into Map(key, value) data type. Syntax map(key1, value1[, key2, value2, ...])  Arguments key — The key part of the pair. String or Integer.value — The value part of the pair. String, Integer or Array. Returned value Data structure as key:value pairs. Type: Map(key, value). Examples Query: SELECT map('key1', number, 'key2', number * 2) FROM numbers(3);  Result: ┌─map('key1', number, 'key2', multiply(number, 2))─┐ │ {'key1':0,'key2':0} │ │ {'key1':1,'key2':2} │ │ {'key1':2,'key2':4} │ └──────────────────────────────────────────────────┘  Query: CREATE TABLE table_map (a Map(String, UInt64)) ENGINE = MergeTree() ORDER BY a; INSERT INTO table_map SELECT map('key1', number, 'key2', number * 2) FROM numbers(3); SELECT a['key2'] FROM table_map;  Result: ┌─arrayElement(a, 'key2')─┐ │ 0 │ │ 2 │ │ 4 │ └─────────────────────────┘  See Also Map(key, value) data type "},{"title":"mapAdd​","type":1,"pageTitle":"Functions for maps","url":"docs/en/sql-reference/functions/tuple-map-functions#function-mapadd","content":"Collect all the keys and sum corresponding values. Syntax mapAdd(arg1, arg2 [, ...])  Arguments Arguments are maps or tuples of two arrays, where items in the first array represent keys, and the second array contains values for the each key. All key arrays should have same type, and all value arrays should contain items which are promoted to the one type (Int64, UInt64 or Float64). The common promoted type is used as a type for the result array. Returned value Depending on the arguments returns one map or tuple, where the first array contains the sorted keys and the second array contains values. Example Query with a tuple: SELECT mapAdd(([toUInt8(1), 2], [1, 1]), ([toUInt8(1), 2], [1, 1])) as res, toTypeName(res) as type;  Result: ┌─res───────────┬─type───────────────────────────────┐ │ ([1,2],[2,2]) │ Tuple(Array(UInt8), Array(UInt64)) │ └───────────────┴────────────────────────────────────┘  Query with Map type: SELECT mapAdd(map(1,1), map(1,1));  Result: ┌─mapAdd(map(1, 1), map(1, 1))─┐ │ {1:2} │ └──────────────────────────────┘  "},{"title":"mapSubtract​","type":1,"pageTitle":"Functions for maps","url":"docs/en/sql-reference/functions/tuple-map-functions#function-mapsubtract","content":"Collect all the keys and subtract corresponding values. Syntax mapSubtract(Tuple(Array, Array), Tuple(Array, Array) [, ...])  Arguments Arguments are maps or tuples of two arrays, where items in the first array represent keys, and the second array contains values for the each key. All key arrays should have same type, and all value arrays should contain items which are promote to the one type (Int64, UInt64 or Float64). The common promoted type is used as a type for the result array. Returned value Depending on the arguments returns one map or tuple, where the first array contains the sorted keys and the second array contains values. Example Query with a tuple map: SELECT mapSubtract(([toUInt8(1), 2], [toInt32(1), 1]), ([toUInt8(1), 2], [toInt32(2), 1])) as res, toTypeName(res) as type;  Result: ┌─res────────────┬─type──────────────────────────────┐ │ ([1,2],[-1,0]) │ Tuple(Array(UInt8), Array(Int64)) │ └────────────────┴───────────────────────────────────┘  Query with Map type: SELECT mapSubtract(map(1,1), map(1,1));  Result: ┌─mapSubtract(map(1, 1), map(1, 1))─┐ │ {1:0} │ └───────────────────────────────────┘  "},{"title":"mapPopulateSeries​","type":1,"pageTitle":"Functions for maps","url":"docs/en/sql-reference/functions/tuple-map-functions#function-mappopulateseries","content":"Fills missing keys in the maps (key and value array pair), where keys are integers. Also, it supports specifying the max key, which is used to extend the keys array. Syntax mapPopulateSeries(keys, values[, max]) mapPopulateSeries(map[, max])  Generates a map (a tuple with two arrays or a value of Map type, depending on the arguments), where keys are a series of numbers, from minimum to maximum keys (or max argument if it specified) taken from the map with a step size of one, and corresponding values. If the value is not specified for the key, then it uses the default value in the resulting map. For repeated keys, only the first value (in order of appearing) gets associated with the key. For array arguments the number of elements in keys and values must be the same for each row. Arguments Arguments are maps or two arrays, where the first array represent keys, and the second array contains values for the each key. Mapped arrays: keys — Array of keys. Array(Int).values — Array of values. Array(Int).max — Maximum key value. Optional. Int8, Int16, Int32, Int64, Int128, Int256. or map — Map with integer keys. Map. Returned value Depending on the arguments returns a map or a tuple of two arrays: keys in sorted order, and values the corresponding keys. Example Query with mapped arrays: SELECT mapPopulateSeries([1,2,4], [11,22,44], 5) AS res, toTypeName(res) AS type;  Result: ┌─res──────────────────────────┬─type──────────────────────────────┐ │ ([1,2,3,4,5],[11,22,0,44,0]) │ Tuple(Array(UInt8), Array(UInt8)) │ └──────────────────────────────┴───────────────────────────────────┘  Query with Map type: SELECT mapPopulateSeries(map(1, 10, 5, 20), 6);  Result: ┌─mapPopulateSeries(map(1, 10, 5, 20), 6)─┐ │ {1:10,2:0,3:0,4:0,5:20,6:0} │ └─────────────────────────────────────────┘  "},{"title":"mapContains​","type":1,"pageTitle":"Functions for maps","url":"docs/en/sql-reference/functions/tuple-map-functions#mapcontains","content":"Determines whether the map contains the key parameter. Syntax mapContains(map, key)  Parameters map — Map. Map.key — Key. Type matches the type of keys of map parameter. Returned value 1 if map contains key, 0 if not. Type: UInt8. Example Query: CREATE TABLE test (a Map(String,String)) ENGINE = Memory; INSERT INTO test VALUES ({'name':'eleven','age':'11'}), ({'number':'twelve','position':'6.0'}); SELECT mapContains(a, 'name') FROM test;  Result: ┌─mapContains(a, 'name')─┐ │ 1 │ │ 0 │ └────────────────────────┘  "},{"title":"mapKeys​","type":1,"pageTitle":"Functions for maps","url":"docs/en/sql-reference/functions/tuple-map-functions#mapkeys","content":"Returns all keys from the map parameter. Can be optimized by enabling the optimize_functions_to_subcolumns setting. With optimize_functions_to_subcolumns = 1 the function reads only keys subcolumn instead of reading and processing the whole column data. The query SELECT mapKeys(m) FROM table transforms to SELECT m.keys FROM table. Syntax mapKeys(map)  Parameters map — Map. Map. Returned value Array containing all keys from the map. Type: Array. Example Query: CREATE TABLE test (a Map(String,String)) ENGINE = Memory; INSERT INTO test VALUES ({'name':'eleven','age':'11'}), ({'number':'twelve','position':'6.0'}); SELECT mapKeys(a) FROM test;  Result: ┌─mapKeys(a)────────────┐ │ ['name','age'] │ │ ['number','position'] │ └───────────────────────┘  "},{"title":"mapValues​","type":1,"pageTitle":"Functions for maps","url":"docs/en/sql-reference/functions/tuple-map-functions#mapvalues","content":"Returns all values from the map parameter. Can be optimized by enabling the optimize_functions_to_subcolumns setting. With optimize_functions_to_subcolumns = 1 the function reads only values subcolumn instead of reading and processing the whole column data. The query SELECT mapValues(m) FROM table transforms to SELECT m.values FROM table. Syntax mapValues(map)  Parameters map — Map. Map. Returned value Array containing all the values from map. Type: Array. Example Query: CREATE TABLE test (a Map(String,String)) ENGINE = Memory; INSERT INTO test VALUES ({'name':'eleven','age':'11'}), ({'number':'twelve','position':'6.0'}); SELECT mapValues(a) FROM test;  Result: ┌─mapValues(a)─────┐ │ ['eleven','11'] │ │ ['twelve','6.0'] │ └──────────────────┘  "},{"title":"mapContainsKeyLike​","type":1,"pageTitle":"Functions for maps","url":"docs/en/sql-reference/functions/tuple-map-functions#mapContainsKeyLike","content":"Syntax mapContainsKeyLike(map, pattern)  Parameters map — Map. Map. pattern - String pattern to match.  Returned value 1 if map contains key like specified pattern, 0 if not.  Example Query: CREATE TABLE test (a Map(String,String)) ENGINE = Memory; INSERT INTO test VALUES ({'abc':'abc','def':'def'}), ({'hij':'hij','klm':'klm'}); SELECT mapContainsKeyLike(a, 'a%') FROM test;  Result: ┌─mapContainsKeyLike(a, 'a%')─┐ │ 1 │ │ 0 │ └─────────────────────────────┘  "},{"title":"mapExtractKeyLike​","type":1,"pageTitle":"Functions for maps","url":"docs/en/sql-reference/functions/tuple-map-functions#mapExtractKeyLike","content":"Syntax mapExtractKeyLike(map, pattern)  Parameters map — Map. Map. pattern - String pattern to match.  Returned value A map contained elements the key of which matchs the specified pattern. If there are no elements matched the pattern, it will return an empty map. Example Query: CREATE TABLE test (a Map(String,String)) ENGINE = Memory; INSERT INTO test VALUES ({'abc':'abc','def':'def'}), ({'hij':'hij','klm':'klm'}); SELECT mapExtractKeyLike(a, 'a%') FROM test;  Result: ┌─mapExtractKeyLike(a, 'a%')─┐ │ {'abc':'abc'} │ │ {} │ └────────────────────────────┘  Original article "},{"title":"Functions for Working with Strings","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/string-functions","content":"","keywords":""},{"title":"empty​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#empty","content":"Checks whether the input string is empty. Syntax empty(x)  A string is considered non-empty if it contains at least one byte, even if this is a space or a null byte. The function also works for arrays or UUID. Arguments x — Input value. String. Returned value Returns 1 for an empty string or 0 for a non-empty string. Type: UInt8. Example Query: SELECT empty('');  Result: ┌─empty('')─┐ │ 1 │ └───────────┘  "},{"title":"notEmpty​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#notempty","content":"Checks whether the input string is non-empty. Syntax notEmpty(x)  A string is considered non-empty if it contains at least one byte, even if this is a space or a null byte. The function also works for arrays or UUID. Arguments x — Input value. String. Returned value Returns 1 for a non-empty string or 0 for an empty string string. Type: UInt8. Example Query: SELECT notEmpty('text');  Result: ┌─notEmpty('text')─┐ │ 1 │ └──────────────────┘  "},{"title":"length​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#length","content":"Returns the length of a string in bytes (not in characters, and not in code points). The result type is UInt64. The function also works for arrays. "},{"title":"lengthUTF8​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#lengthutf8","content":"Returns the length of a string in Unicode code points (not in characters), assuming that the string contains a set of bytes that make up UTF-8 encoded text. If this assumption is not met, it returns some result (it does not throw an exception). The result type is UInt64. "},{"title":"char_length, CHAR_LENGTH​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#char-length","content":"Returns the length of a string in Unicode code points (not in characters), assuming that the string contains a set of bytes that make up UTF-8 encoded text. If this assumption is not met, it returns some result (it does not throw an exception). The result type is UInt64. "},{"title":"character_length, CHARACTER_LENGTH​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#character-length","content":"Returns the length of a string in Unicode code points (not in characters), assuming that the string contains a set of bytes that make up UTF-8 encoded text. If this assumption is not met, it returns some result (it does not throw an exception). The result type is UInt64. "},{"title":"leftPad​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#leftpad","content":"Pads the current string from the left with spaces or a specified string (multiple times, if needed) until the resulting string reaches the given length. Similarly to the MySQL LPAD function. Syntax leftPad('string', 'length'[, 'pad_string'])  Arguments string — Input string that needs to be padded. String.length — The length of the resulting string. UInt. If the value is less than the input string length, then the input string is returned as-is.pad_string — The string to pad the input string with. String. Optional. If not specified, then the input string is padded with spaces. Returned value The resulting string of the given length. Type: String. Example Query: SELECT leftPad('abc', 7, '*'), leftPad('def', 7);  Result: ┌─leftPad('abc', 7, '*')─┬─leftPad('def', 7)─┐ │ ****abc │ def │ └────────────────────────┴───────────────────┘  "},{"title":"leftPadUTF8​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#leftpadutf8","content":"Pads the current string from the left with spaces or a specified string (multiple times, if needed) until the resulting string reaches the given length. Similarly to the MySQL LPAD function. While in the leftPad function the length is measured in bytes, here in the leftPadUTF8 function it is measured in code points. Syntax leftPadUTF8('string','length'[, 'pad_string'])  Arguments string — Input string that needs to be padded. String.length — The length of the resulting string. UInt. If the value is less than the input string length, then the input string is returned as-is.pad_string — The string to pad the input string with. String. Optional. If not specified, then the input string is padded with spaces. Returned value The resulting string of the given length. Type: String. Example Query: SELECT leftPadUTF8('абвг', 7, '*'), leftPadUTF8('дежз', 7);  Result: ┌─leftPadUTF8('абвг', 7, '*')─┬─leftPadUTF8('дежз', 7)─┐ │ ***абвг │ дежз │ └─────────────────────────────┴────────────────────────┘  "},{"title":"rightPad​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#rightpad","content":"Pads the current string from the right with spaces or a specified string (multiple times, if needed) until the resulting string reaches the given length. Similarly to the MySQL RPAD function. Syntax rightPad('string', 'length'[, 'pad_string'])  Arguments string — Input string that needs to be padded. String.length — The length of the resulting string. UInt. If the value is less than the input string length, then the input string is returned as-is.pad_string — The string to pad the input string with. String. Optional. If not specified, then the input string is padded with spaces. Returned value The resulting string of the given length. Type: String. Example Query: SELECT rightPad('abc', 7, '*'), rightPad('abc', 7);  Result: ┌─rightPad('abc', 7, '*')─┬─rightPad('abc', 7)─┐ │ abc**** │ abc │ └─────────────────────────┴────────────────────┘  "},{"title":"rightPadUTF8​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#rightpadutf8","content":"Pads the current string from the right with spaces or a specified string (multiple times, if needed) until the resulting string reaches the given length. Similarly to the MySQL RPAD function. While in the rightPad function the length is measured in bytes, here in the rightPadUTF8 function it is measured in code points. Syntax rightPadUTF8('string','length'[, 'pad_string'])  Arguments string — Input string that needs to be padded. String.length — The length of the resulting string. UInt. If the value is less than the input string length, then the input string is returned as-is.pad_string — The string to pad the input string with. String. Optional. If not specified, then the input string is padded with spaces. Returned value The resulting string of the given length. Type: String. Example Query: SELECT rightPadUTF8('абвг', 7, '*'), rightPadUTF8('абвг', 7);  Result: ┌─rightPadUTF8('абвг', 7, '*')─┬─rightPadUTF8('абвг', 7)─┐ │ абвг*** │ абвг │ └──────────────────────────────┴─────────────────────────┘  "},{"title":"lower, lcase​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#lower","content":"Converts ASCII Latin symbols in a string to lowercase. "},{"title":"upper, ucase​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#upper","content":"Converts ASCII Latin symbols in a string to uppercase. "},{"title":"lowerUTF8​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#lowerutf8","content":"Converts a string to lowercase, assuming the string contains a set of bytes that make up a UTF-8 encoded text. It does not detect the language. So for Turkish the result might not be exactly correct. If the length of the UTF-8 byte sequence is different for upper and lower case of a code point, the result may be incorrect for this code point. If the string contains a set of bytes that is not UTF-8, then the behavior is undefined. "},{"title":"upperUTF8​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#upperutf8","content":"Converts a string to uppercase, assuming the string contains a set of bytes that make up a UTF-8 encoded text. It does not detect the language. So for Turkish the result might not be exactly correct. If the length of the UTF-8 byte sequence is different for upper and lower case of a code point, the result may be incorrect for this code point. If the string contains a set of bytes that is not UTF-8, then the behavior is undefined. "},{"title":"isValidUTF8​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#isvalidutf8","content":"Returns 1, if the set of bytes is valid UTF-8 encoded, otherwise 0. "},{"title":"toValidUTF8​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#tovalidutf8","content":"Replaces invalid UTF-8 characters by the � (U+FFFD) character. All running in a row invalid characters are collapsed into the one replacement character. toValidUTF8(input_string)  Arguments input_string — Any set of bytes represented as the String data type object. Returned value: Valid UTF-8 string. Example SELECT toValidUTF8('\\x61\\xF0\\x80\\x80\\x80b');  ┌─toValidUTF8('a����b')─┐ │ a�b │ └───────────────────────┘  "},{"title":"repeat​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#repeat","content":"Repeats a string as many times as specified and concatenates the replicated values as a single string. Alias: REPEAT. Syntax repeat(s, n)  Arguments s — The string to repeat. String.n — The number of times to repeat the string. UInt. Returned value The single string, which contains the string s repeated n times. If n \\&lt; 1, the function returns empty string. Type: String. Example Query: SELECT repeat('abc', 10);  Result: ┌─repeat('abc', 10)──────────────┐ │ abcabcabcabcabcabcabcabcabcabc │ └────────────────────────────────┘  "},{"title":"reverse​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#reverse","content":"Reverses the string (as a sequence of bytes). "},{"title":"reverseUTF8​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#reverseutf8","content":"Reverses a sequence of Unicode code points, assuming that the string contains a set of bytes representing a UTF-8 text. Otherwise, it does something else (it does not throw an exception). "},{"title":"format(pattern, s0, s1, …)​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#format","content":"Formatting constant pattern with the string listed in the arguments. pattern is a simplified Python format pattern. Format string contains “replacement fields” surrounded by curly braces {}. Anything that is not contained in braces is considered literal text, which is copied unchanged to the output. If you need to include a brace character in the literal text, it can be escaped by doubling: {{ '{{' }} and {{ '}}' }}. Field names can be numbers (starting from zero) or empty (then they are treated as consequence numbers). SELECT format('{1} {0} {1}', 'World', 'Hello')  ┌─format('{1} {0} {1}', 'World', 'Hello')─┐ │ Hello World Hello │ └─────────────────────────────────────────┘  SELECT format('{} {}', 'Hello', 'World')  ┌─format('{} {}', 'Hello', 'World')─┐ │ Hello World │ └───────────────────────────────────┘  "},{"title":"concat​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#concat","content":"Concatenates the strings listed in the arguments, without a separator. Syntax concat(s1, s2, ...)  Arguments Values of type String or FixedString. Returned values Returns the String that results from concatenating the arguments. If any of argument values is NULL, concat returns NULL. Example Query: SELECT concat('Hello, ', 'World!');  Result: ┌─concat('Hello, ', 'World!')─┐ │ Hello, World! │ └─────────────────────────────┘  "},{"title":"concatAssumeInjective​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#concatassumeinjective","content":"Same as concat, the difference is that you need to ensure that concat(s1, s2, ...) → sn is injective, it will be used for optimization of GROUP BY. The function is named “injective” if it always returns different result for different values of arguments. In other words: different arguments never yield identical result. Syntax concatAssumeInjective(s1, s2, ...)  Arguments Values of type String or FixedString. Returned values Returns the String that results from concatenating the arguments. If any of argument values is NULL, concatAssumeInjective returns NULL. Example Input table: CREATE TABLE key_val(`key1` String, `key2` String, `value` UInt32) ENGINE = TinyLog; INSERT INTO key_val VALUES ('Hello, ','World',1), ('Hello, ','World',2), ('Hello, ','World!',3), ('Hello',', World!',2); SELECT * from key_val;  ┌─key1────┬─key2─────┬─value─┐ │ Hello, │ World │ 1 │ │ Hello, │ World │ 2 │ │ Hello, │ World! │ 3 │ │ Hello │ , World! │ 2 │ └─────────┴──────────┴───────┘  Query: SELECT concat(key1, key2), sum(value) FROM key_val GROUP BY concatAssumeInjective(key1, key2);  Result: ┌─concat(key1, key2)─┬─sum(value)─┐ │ Hello, World! │ 3 │ │ Hello, World! │ 2 │ │ Hello, World │ 3 │ └────────────────────┴────────────┘  "},{"title":"substring(s, offset, length), mid(s, offset, length), substr(s, offset, length)​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#substring","content":"Returns a substring starting with the byte from the ‘offset’ index that is ‘length’ bytes long. Character indexing starts from one (as in standard SQL). The ‘offset’ and ‘length’ arguments must be constants. "},{"title":"substringUTF8(s, offset, length)​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#substringutf8","content":"The same as ‘substring’, but for Unicode code points. Works under the assumption that the string contains a set of bytes representing a UTF-8 encoded text. If this assumption is not met, it returns some result (it does not throw an exception). "},{"title":"appendTrailingCharIfAbsent(s, c)​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#appendtrailingcharifabsent","content":"If the ‘s’ string is non-empty and does not contain the ‘c’ character at the end, it appends the ‘c’ character to the end. "},{"title":"convertCharset(s, from, to)​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#convertcharset","content":"Returns the string ‘s’ that was converted from the encoding in ‘from’ to the encoding in ‘to’. "},{"title":"base64Encode(s)​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#base64encode","content":"Encodes ‘s’ string into base64 Alias: TO_BASE64. "},{"title":"base64Decode(s)​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#base64decode","content":"Decode base64-encoded string ‘s’ into original string. In case of failure raises an exception. Alias: FROM_BASE64. "},{"title":"tryBase64Decode(s)​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#trybase64decode","content":"Similar to base64Decode, but in case of error an empty string would be returned. "},{"title":"endsWith(s, suffix)​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#endswith","content":"Returns whether to end with the specified suffix. Returns 1 if the string ends with the specified suffix, otherwise it returns 0. "},{"title":"startsWith(str, prefix)​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#startswith","content":"Returns 1 whether string starts with the specified prefix, otherwise it returns 0. SELECT startsWith('Spider-Man', 'Spi');  Returned values 1, if the string starts with the specified prefix.0, if the string does not start with the specified prefix. Example Query: SELECT startsWith('Hello, world!', 'He');  Result: ┌─startsWith('Hello, world!', 'He')─┐ │ 1 │ └───────────────────────────────────┘  "},{"title":"trim​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#trim","content":"Removes all specified characters from the start or end of a string. By default removes all consecutive occurrences of common whitespace (ASCII character 32) from both ends of a string. Syntax trim([[LEADING|TRAILING|BOTH] trim_character FROM] input_string)  Arguments trim_character — Specified characters for trim. String.input_string — String for trim. String. Returned value A string without leading and (or) trailing specified characters. Type: String. Example Query: SELECT trim(BOTH ' ()' FROM '( Hello, world! )');  Result: ┌─trim(BOTH ' ()' FROM '( Hello, world! )')─┐ │ Hello, world! │ └───────────────────────────────────────────────┘  "},{"title":"trimLeft​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#trimleft","content":"Removes all consecutive occurrences of common whitespace (ASCII character 32) from the beginning of a string. It does not remove other kinds of whitespace characters (tab, no-break space, etc.). Syntax trimLeft(input_string)  Alias: ltrim(input_string). Arguments input_string — string to trim. String. Returned value A string without leading common whitespaces. Type: String. Example Query: SELECT trimLeft(' Hello, world! ');  Result: ┌─trimLeft(' Hello, world! ')─┐ │ Hello, world! │ └─────────────────────────────────────┘  "},{"title":"trimRight​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#trimright","content":"Removes all consecutive occurrences of common whitespace (ASCII character 32) from the end of a string. It does not remove other kinds of whitespace characters (tab, no-break space, etc.). Syntax trimRight(input_string)  Alias: rtrim(input_string). Arguments input_string — string to trim. String. Returned value A string without trailing common whitespaces. Type: String. Example Query: SELECT trimRight(' Hello, world! ');  Result: ┌─trimRight(' Hello, world! ')─┐ │ Hello, world! │ └──────────────────────────────────────┘  "},{"title":"trimBoth​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#trimboth","content":"Removes all consecutive occurrences of common whitespace (ASCII character 32) from both ends of a string. It does not remove other kinds of whitespace characters (tab, no-break space, etc.). Syntax trimBoth(input_string)  Alias: trim(input_string). Arguments input_string — string to trim. String. Returned value A string without leading and trailing common whitespaces. Type: String. Example Query: SELECT trimBoth(' Hello, world! ');  Result: ┌─trimBoth(' Hello, world! ')─┐ │ Hello, world! │ └─────────────────────────────────────┘  "},{"title":"CRC32(s)​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#crc32","content":"Returns the CRC32 checksum of a string, using CRC-32-IEEE 802.3 polynomial and initial value 0xffffffff (zlib implementation). The result type is UInt32. "},{"title":"CRC32IEEE(s)​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#crc32ieee","content":"Returns the CRC32 checksum of a string, using CRC-32-IEEE 802.3 polynomial. The result type is UInt32. "},{"title":"CRC64(s)​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#crc64","content":"Returns the CRC64 checksum of a string, using CRC-64-ECMA polynomial. The result type is UInt64. "},{"title":"normalizeQuery​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#normalized-query","content":"Replaces literals, sequences of literals and complex aliases with placeholders. Syntax normalizeQuery(x)  Arguments x — Sequence of characters. String. Returned value Sequence of characters with placeholders. Type: String. Example Query: SELECT normalizeQuery('[1, 2, 3, x]') AS query;  Result: ┌─query────┐ │ [?.., x] │ └──────────┘  "},{"title":"normalizedQueryHash​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#normalized-query-hash","content":"Returns identical 64bit hash values without the values of literals for similar queries. It helps to analyze query log. Syntax normalizedQueryHash(x)  Arguments x — Sequence of characters. String. Returned value Hash value. Type: UInt64. Example Query: SELECT normalizedQueryHash('SELECT 1 AS `xyz`') != normalizedQueryHash('SELECT 1 AS `abc`') AS res;  Result: ┌─res─┐ │ 1 │ └─────┘  "},{"title":"normalizeUTF8NFC​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#normalizeutf8nfc","content":"Converts a string to NFC normalized form, assuming the string contains a set of bytes that make up a UTF-8 encoded text. Syntax normalizeUTF8NFC(words)  Arguments words — Input string that contains UTF-8 encoded text. String. Returned value String transformed to NFC normalization form. Type: String. Example Query: SELECT length('â'), normalizeUTF8NFC('â') AS nfc, length(nfc) AS nfc_len;  Result: ┌─length('â')─┬─nfc─┬─nfc_len─┐ │ 2 │ â │ 2 │ └─────────────┴─────┴─────────┘  "},{"title":"normalizeUTF8NFD​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#normalizeutf8nfd","content":"Converts a string to NFD normalized form, assuming the string contains a set of bytes that make up a UTF-8 encoded text. Syntax normalizeUTF8NFD(words)  Arguments words — Input string that contains UTF-8 encoded text. String. Returned value String transformed to NFD normalization form. Type: String. Example Query: SELECT length('â'), normalizeUTF8NFD('â') AS nfd, length(nfd) AS nfd_len;  Result: ┌─length('â')─┬─nfd─┬─nfd_len─┐ │ 2 │ â │ 3 │ └─────────────┴─────┴─────────┘  "},{"title":"normalizeUTF8NFKC​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#normalizeutf8nfkc","content":"Converts a string to NFKC normalized form, assuming the string contains a set of bytes that make up a UTF-8 encoded text. Syntax normalizeUTF8NFKC(words)  Arguments words — Input string that contains UTF-8 encoded text. String. Returned value String transformed to NFKC normalization form. Type: String. Example Query: SELECT length('â'), normalizeUTF8NFKC('â') AS nfkc, length(nfkc) AS nfkc_len;  Result: ┌─length('â')─┬─nfkc─┬─nfkc_len─┐ │ 2 │ â │ 2 │ └─────────────┴──────┴──────────┘  "},{"title":"normalizeUTF8NFKD​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#normalizeutf8nfkd","content":"Converts a string to NFKD normalized form, assuming the string contains a set of bytes that make up a UTF-8 encoded text. Syntax normalizeUTF8NFKD(words)  Arguments words — Input string that contains UTF-8 encoded text. String. Returned value String transformed to NFKD normalization form. Type: String. Example Query: SELECT length('â'), normalizeUTF8NFKD('â') AS nfkd, length(nfkd) AS nfkd_len;  Result: ┌─length('â')─┬─nfkd─┬─nfkd_len─┐ │ 2 │ â │ 3 │ └─────────────┴──────┴──────────┘  "},{"title":"encodeXMLComponent​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#encode-xml-component","content":"Escapes characters to place string into XML text node or attribute. The following five XML predefined entities will be replaced: &lt;, &amp;, &gt;, &quot;, '. Syntax encodeXMLComponent(x)  Arguments x — The sequence of characters. String. Returned value The sequence of characters with escape characters. Type: String. Example Query: SELECT encodeXMLComponent('Hello, &quot;world&quot;!'); SELECT encodeXMLComponent('&lt;123&gt;'); SELECT encodeXMLComponent('&amp;clickhouse'); SELECT encodeXMLComponent('\\'foo\\'');  Result: Hello, &amp;quot;world&amp;quot;! &amp;lt;123&amp;gt; &amp;amp;clickhouse &amp;apos;foo&amp;apos;  "},{"title":"decodeXMLComponent​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#decode-xml-component","content":"Replaces XML predefined entities with characters. Predefined entities are &amp;quot; &amp;amp; &amp;apos; &amp;gt; &amp;lt;This function also replaces numeric character references with Unicode characters. Both decimal (like &amp;#10003;) and hexadecimal (&amp;#x2713;) forms are supported. Syntax decodeXMLComponent(x)  Arguments x — A sequence of characters. String. Returned value The sequence of characters after replacement. Type: String. Example Query: SELECT decodeXMLComponent('&amp;apos;foo&amp;apos;'); SELECT decodeXMLComponent('&amp;lt; &amp;#x3A3; &amp;gt;');  Result: 'foo' &lt; Σ &gt;  See Also List of XML and HTML character entity references "},{"title":"extractTextFromHTML​","type":1,"pageTitle":"Functions for Working with Strings","url":"docs/en/sql-reference/functions/string-functions#extracttextfromhtml","content":"A function to extract text from HTML or XHTML. It does not necessarily 100% conform to any of the HTML, XML or XHTML standards, but the implementation is reasonably accurate and it is fast. The rules are the following: Comments are skipped. Example: &lt;!-- test --&gt;. Comment must end with --&gt;. Nested comments are not possible. Note: constructions like &lt;!--&gt; and &lt;!---&gt; are not valid comments in HTML but they are skipped by other rules.CDATA is pasted verbatim. Note: CDATA is XML/XHTML specific. But it is processed for &quot;best-effort&quot; approach.script and style elements are removed with all their content. Note: it is assumed that closing tag cannot appear inside content. For example, in JS string literal has to be escaped like &quot;&lt;\\/script&gt;&quot;. Note: comments and CDATA are possible inside script or style - then closing tags are not searched inside CDATA. Example: &lt;script&gt;&lt;![CDATA[&lt;/script&gt;]]&gt;&lt;/script&gt;. But they are still searched inside comments. Sometimes it becomes complicated: &lt;script&gt;var x = &quot;&lt;!--&quot;; &lt;/script&gt; var y = &quot;--&gt;&quot;; alert(x + y);&lt;/script&gt;Note: script and style can be the names of XML namespaces - then they are not treated like usual script or style elements. Example: &lt;script:a&gt;Hello&lt;/script:a&gt;. Note: whitespaces are possible after closing tag name: &lt;/script &gt; but not before: &lt; / script&gt;.Other tags or tag-like elements are skipped without inner content. Example: &lt;a&gt;.&lt;/a&gt;Note: it is expected that this HTML is illegal: &lt;a test=&quot;&gt;&quot;&gt;&lt;/a&gt;Note: it also skips something like tags: &lt;&gt;, &lt;!&gt;, etc. Note: tag without end is skipped to the end of input: &lt;hello HTML and XML entities are not decoded. They must be processed by separate function.Whitespaces in the text are collapsed or inserted by specific rules. Whitespaces at the beginning and at the end are removed.Consecutive whitespaces are collapsed.But if the text is separated by other elements and there is no whitespace, it is inserted.It may cause unnatural examples: Hello&lt;b&gt;world&lt;/b&gt;, Hello&lt;!-- --&gt;world - there is no whitespace in HTML, but the function inserts it. Also consider: Hello&lt;p&gt;world&lt;/p&gt;, Hello&lt;br&gt;world. This behavior is reasonable for data analysis, e.g. to convert HTML to a bag of words. Also note that correct handling of whitespaces requires the support of &lt;pre&gt;&lt;/pre&gt; and CSS display and white-space properties. Syntax extractTextFromHTML(x)  Arguments x — input text. String. Returned value Extracted text. Type: String. Example The first example contains several tags and a comment and also shows whitespace processing. The second example shows CDATA and script tag processing. In the third example text is extracted from the full HTML response received by the url function. Query: SELECT extractTextFromHTML(' &lt;p&gt; A text &lt;i&gt;with&lt;/i&gt;&lt;b&gt;tags&lt;/b&gt;. &lt;!-- comments --&gt; &lt;/p&gt; '); SELECT extractTextFromHTML('&lt;![CDATA[The content within &lt;b&gt;CDATA&lt;/b&gt;]]&gt; &lt;script&gt;alert(&quot;Script&quot;);&lt;/script&gt;'); SELECT extractTextFromHTML(html) FROM url('http://www.donothingfor2minutes.com/', RawBLOB, 'html String');  Result: A text with tags . The content within &lt;b&gt;CDATA&lt;/b&gt; Do Nothing for 2 Minutes 2:00 &amp;nbsp;  "},{"title":"Functions for Working with Embedded Dictionaries","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/ym-dict-functions","content":"","keywords":""},{"title":"Multiple Geobases​","type":1,"pageTitle":"Functions for Working with Embedded Dictionaries","url":"docs/en/sql-reference/functions/ym-dict-functions#multiple-geobases","content":"ClickHouse supports working with multiple alternative geobases (regional hierarchies) simultaneously, in order to support various perspectives on which countries certain regions belong to. The ‘clickhouse-server’ config specifies the file with the regional hierarchy::&lt;path_to_regions_hierarchy_file&gt;/opt/geo/regions_hierarchy.txt&lt;/path_to_regions_hierarchy_file&gt; Besides this file, it also searches for files nearby that have the _ symbol and any suffix appended to the name (before the file extension). For example, it will also find the file /opt/geo/regions_hierarchy_ua.txt, if present. ua is called the dictionary key. For a dictionary without a suffix, the key is an empty string. All the dictionaries are re-loaded in runtime (once every certain number of seconds, as defined in the builtin_dictionaries_reload_interval config parameter, or once an hour by default). However, the list of available dictionaries is defined one time, when the server starts. All functions for working with regions have an optional argument at the end – the dictionary key. It is referred to as the geobase. Example: regionToCountry(RegionID) – Uses the default dictionary: /opt/geo/regions_hierarchy.txt regionToCountry(RegionID, '') – Uses the default dictionary: /opt/geo/regions_hierarchy.txt regionToCountry(RegionID, 'ua') – Uses the dictionary for the 'ua' key: /opt/geo/regions_hierarchy_ua.txt  "},{"title":"regionToCity(id[, geobase])​","type":1,"pageTitle":"Functions for Working with Embedded Dictionaries","url":"docs/en/sql-reference/functions/ym-dict-functions#regiontocityid-geobase","content":"Accepts a UInt32 number – the region ID from the geobase. If this region is a city or part of a city, it returns the region ID for the appropriate city. Otherwise, returns 0. "},{"title":"regionToArea(id[, geobase])​","type":1,"pageTitle":"Functions for Working with Embedded Dictionaries","url":"docs/en/sql-reference/functions/ym-dict-functions#regiontoareaid-geobase","content":"Converts a region to an area (type 5 in the geobase). In every other way, this function is the same as ‘regionToCity’. SELECT DISTINCT regionToName(regionToArea(toUInt32(number), 'ua')) FROM system.numbers LIMIT 15  ┌─regionToName(regionToArea(toUInt32(number), \\'ua\\'))─┐ │ │ │ Moscow and Moscow region │ │ St. Petersburg and Leningrad region │ │ Belgorod region │ │ Ivanovsk region │ │ Kaluga region │ │ Kostroma region │ │ Kursk region │ │ Lipetsk region │ │ Orlov region │ │ Ryazan region │ │ Smolensk region │ │ Tambov region │ │ Tver region │ │ Tula region │ └──────────────────────────────────────────────────────┘  "},{"title":"regionToDistrict(id[, geobase])​","type":1,"pageTitle":"Functions for Working with Embedded Dictionaries","url":"docs/en/sql-reference/functions/ym-dict-functions#regiontodistrictid-geobase","content":"Converts a region to a federal district (type 4 in the geobase). In every other way, this function is the same as ‘regionToCity’. SELECT DISTINCT regionToName(regionToDistrict(toUInt32(number), 'ua')) FROM system.numbers LIMIT 15  ┌─regionToName(regionToDistrict(toUInt32(number), \\'ua\\'))─┐ │ │ │ Central federal district │ │ Northwest federal district │ │ South federal district │ │ North Caucases federal district │ │ Privolga federal district │ │ Ural federal district │ │ Siberian federal district │ │ Far East federal district │ │ Scotland │ │ Faroe Islands │ │ Flemish region │ │ Brussels capital region │ │ Wallonia │ │ Federation of Bosnia and Herzegovina │ └──────────────────────────────────────────────────────────┘  "},{"title":"regionToCountry(id[, geobase])​","type":1,"pageTitle":"Functions for Working with Embedded Dictionaries","url":"docs/en/sql-reference/functions/ym-dict-functions#regiontocountryid-geobase","content":"Converts a region to a country. In every other way, this function is the same as ‘regionToCity’. Example: regionToCountry(toUInt32(213)) = 225 converts Moscow (213) to Russia (225). "},{"title":"regionToContinent(id[, geobase])​","type":1,"pageTitle":"Functions for Working with Embedded Dictionaries","url":"docs/en/sql-reference/functions/ym-dict-functions#regiontocontinentid-geobase","content":"Converts a region to a continent. In every other way, this function is the same as ‘regionToCity’. Example: regionToContinent(toUInt32(213)) = 10001 converts Moscow (213) to Eurasia (10001). "},{"title":"regionToTopContinent (#regiontotopcontinent)​","type":1,"pageTitle":"Functions for Working with Embedded Dictionaries","url":"docs/en/sql-reference/functions/ym-dict-functions#regiontotopcontinent-regiontotopcontinent","content":"Finds the highest continent in the hierarchy for the region. Syntax regionToTopContinent(id[, geobase])  Arguments id — Region ID from the geobase. UInt32.geobase — Dictionary key. See Multiple Geobases. String. Optional. Returned value Identifier of the top level continent (the latter when you climb the hierarchy of regions).0, if there is none. Type: UInt32. "},{"title":"regionToPopulation(id[, geobase])​","type":1,"pageTitle":"Functions for Working with Embedded Dictionaries","url":"docs/en/sql-reference/functions/ym-dict-functions#regiontopopulationid-geobase","content":"Gets the population for a region. The population can be recorded in files with the geobase. See the section “External dictionaries”. If the population is not recorded for the region, it returns 0. In the geobase, the population might be recorded for child regions, but not for parent regions. "},{"title":"regionIn(lhs, rhs[, geobase])​","type":1,"pageTitle":"Functions for Working with Embedded Dictionaries","url":"docs/en/sql-reference/functions/ym-dict-functions#regioninlhs-rhs-geobase","content":"Checks whether a ‘lhs’ region belongs to a ‘rhs’ region. Returns a UInt8 number equal to 1 if it belongs, or 0 if it does not belong. The relationship is reflexive – any region also belongs to itself. "},{"title":"regionHierarchy(id[, geobase])​","type":1,"pageTitle":"Functions for Working with Embedded Dictionaries","url":"docs/en/sql-reference/functions/ym-dict-functions#regionhierarchyid-geobase","content":"Accepts a UInt32 number – the region ID from the geobase. Returns an array of region IDs consisting of the passed region and all parents along the chain. Example: regionHierarchy(toUInt32(213)) = [213,1,3,225,10001,10000]. "},{"title":"regionToName(id[, lang])​","type":1,"pageTitle":"Functions for Working with Embedded Dictionaries","url":"docs/en/sql-reference/functions/ym-dict-functions#regiontonameid-lang","content":"Accepts a UInt32 number – the region ID from the geobase. A string with the name of the language can be passed as a second argument. Supported languages are: ru, en, ua, uk, by, kz, tr. If the second argument is omitted, the language ‘ru’ is used. If the language is not supported, an exception is thrown. Returns a string – the name of the region in the corresponding language. If the region with the specified ID does not exist, an empty string is returned. ua and uk both mean Ukrainian. "},{"title":"EXISTS","type":0,"sectionRef":"#","url":"docs/en/sql-reference/operators/exists","content":"EXISTS The EXISTS operator checks how many records are in the result of a subquery. If it is empty, then the operator returns 0. Otherwise, it returns 1. EXISTS can be used in a WHERE clause. warning References to main query tables and columns are not supported in a subquery. Syntax WHERE EXISTS(subquery) Example Query with a subquery returning several rows: SELECT count() FROM numbers(10) WHERE EXISTS(SELECT number FROM numbers(10) WHERE number &gt; 8); Result: ┌─count()─┐ │ 10 │ └─────────┘ Query with a subquery that returns an empty result: SELECT count() FROM numbers(10) WHERE EXISTS(SELECT number FROM numbers(10) WHERE number &gt; 11); Result: ┌─count()─┐ │ 0 │ └─────────┘ ","keywords":""},{"title":"ClickHouse SQL Statements","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/","content":"ClickHouse SQL Statements Statements represent various kinds of action you can perform using SQL queries. Each kind of statement has it’s own syntax and usage details that are described separately: SELECTINSERT INTOCREATEALTERSYSTEMSHOWGRANTREVOKEATTACHCHECK TABLEDESCRIBE TABLEDETACHDROPEXISTSKILLOPTIMIZERENAMESETSET ROLETRUNCATEUSEEXPLAIN","keywords":""},{"title":"Operators","type":0,"sectionRef":"#","url":"docs/en/sql-reference/operators/","content":"","keywords":""},{"title":"Access Operators​","type":1,"pageTitle":"Operators","url":"docs/en/sql-reference/operators/#access-operators","content":"a[N] – Access to an element of an array. The arrayElement(a, N) function. a.N – Access to a tuple element. The tupleElement(a, N) function. "},{"title":"Numeric Negation Operator​","type":1,"pageTitle":"Operators","url":"docs/en/sql-reference/operators/#numeric-negation-operator","content":"-a – The negate (a) function. For tuple negation: tupleNegate. "},{"title":"Multiplication and Division Operators​","type":1,"pageTitle":"Operators","url":"docs/en/sql-reference/operators/#multiplication-and-division-operators","content":"a * b – The multiply (a, b) function. For multiplying tuple by number: tupleMultiplyByNumber, for scalar profuct: dotProduct. a / b – The divide(a, b) function. For dividing tuple by number: tupleDivideByNumber. a % b – The modulo(a, b) function. "},{"title":"Addition and Subtraction Operators​","type":1,"pageTitle":"Operators","url":"docs/en/sql-reference/operators/#addition-and-subtraction-operators","content":"a + b – The plus(a, b) function. For tuple addiction: tuplePlus. a - b – The minus(a, b) function. For tuple subtraction: tupleMinus. "},{"title":"Comparison Operators​","type":1,"pageTitle":"Operators","url":"docs/en/sql-reference/operators/#comparison-operators","content":"a = b – The equals(a, b) function. a == b – The equals(a, b) function. a != b – The notEquals(a, b) function. a &lt;&gt; b – The notEquals(a, b) function. a &lt;= b – The lessOrEquals(a, b) function. a &gt;= b – The greaterOrEquals(a, b) function. a &lt; b – The less(a, b) function. a &gt; b – The greater(a, b) function. a LIKE s – The like(a, b) function. a NOT LIKE s – The notLike(a, b) function. a ILIKE s – The ilike(a, b) function. a BETWEEN b AND c – The same as a &gt;= b AND a &lt;= c. a NOT BETWEEN b AND c – The same as a &lt; b OR a &gt; c. "},{"title":"Operators for Working with Data Sets​","type":1,"pageTitle":"Operators","url":"docs/en/sql-reference/operators/#operators-for-working-with-data-sets","content":"See IN operators and EXISTS operator. a IN ... – The in(a, b) function. a NOT IN ... – The notIn(a, b) function. a GLOBAL IN ... – The globalIn(a, b) function. a GLOBAL NOT IN ... – The globalNotIn(a, b) function. a = ANY (subquery) – The in(a, subquery) function. a != ANY (subquery) – The same as a NOT IN (SELECT singleValueOrNull(*) FROM subquery). a = ALL (subquery) – The same as a IN (SELECT singleValueOrNull(*) FROM subquery). a != ALL (subquery) – The notIn(a, subquery) function. Examples Query with ALL: SELECT number AS a FROM numbers(10) WHERE a &gt; ALL (SELECT number FROM numbers(3, 3));  Result: ┌─a─┐ │ 6 │ │ 7 │ │ 8 │ │ 9 │ └───┘  Query with ANY: SELECT number AS a FROM numbers(10) WHERE a &gt; ANY (SELECT number FROM numbers(3, 3));  Result: ┌─a─┐ │ 4 │ │ 5 │ │ 6 │ │ 7 │ │ 8 │ │ 9 │ └───┘  "},{"title":"Operators for Working with Dates and Times​","type":1,"pageTitle":"Operators","url":"docs/en/sql-reference/operators/#operators-datetime","content":""},{"title":"EXTRACT​","type":1,"pageTitle":"Operators","url":"docs/en/sql-reference/operators/#operator-extract","content":"EXTRACT(part FROM date);  Extract parts from a given date. For example, you can retrieve a month from a given date, or a second from a time. The part parameter specifies which part of the date to retrieve. The following values are available: DAY — The day of the month. Possible values: 1–31.MONTH — The number of a month. Possible values: 1–12.YEAR — The year.SECOND — The second. Possible values: 0–59.MINUTE — The minute. Possible values: 0–59.HOUR — The hour. Possible values: 0–23. The part parameter is case-insensitive. The date parameter specifies the date or the time to process. Either Date or DateTime type is supported. Examples: SELECT EXTRACT(DAY FROM toDate('2017-06-15')); SELECT EXTRACT(MONTH FROM toDate('2017-06-15')); SELECT EXTRACT(YEAR FROM toDate('2017-06-15'));  In the following example we create a table and insert into it a value with the DateTime type. CREATE TABLE test.Orders ( OrderId UInt64, OrderName String, OrderDate DateTime ) ENGINE = Log;  INSERT INTO test.Orders VALUES (1, 'Jarlsberg Cheese', toDateTime('2008-10-11 13:23:44'));  SELECT toYear(OrderDate) AS OrderYear, toMonth(OrderDate) AS OrderMonth, toDayOfMonth(OrderDate) AS OrderDay, toHour(OrderDate) AS OrderHour, toMinute(OrderDate) AS OrderMinute, toSecond(OrderDate) AS OrderSecond FROM test.Orders;  ┌─OrderYear─┬─OrderMonth─┬─OrderDay─┬─OrderHour─┬─OrderMinute─┬─OrderSecond─┐ │ 2008 │ 10 │ 11 │ 13 │ 23 │ 44 │ └───────────┴────────────┴──────────┴───────────┴─────────────┴─────────────┘  You can see more examples in tests. "},{"title":"INTERVAL​","type":1,"pageTitle":"Operators","url":"docs/en/sql-reference/operators/#operator-interval","content":"Creates an Interval-type value that should be used in arithmetical operations with Date and DateTime-type values. Types of intervals: SECONDMINUTEHOURDAYWEEKMONTHQUARTERYEAR You can also use a string literal when setting the INTERVAL value. For example, INTERVAL 1 HOUR is identical to the INTERVAL '1 hour' or INTERVAL '1' hour. warning Intervals with different types can’t be combined. You can’t use expressions like INTERVAL 4 DAY 1 HOUR. Specify intervals in units that are smaller or equal to the smallest unit of the interval, for example, INTERVAL 25 HOUR. You can use consecutive operations, like in the example below. Examples: SELECT now() AS current_date_time, current_date_time + INTERVAL 4 DAY + INTERVAL 3 HOUR;  ┌───current_date_time─┬─plus(plus(now(), toIntervalDay(4)), toIntervalHour(3))─┐ │ 2020-11-03 22:09:50 │ 2020-11-08 01:09:50 │ └─────────────────────┴────────────────────────────────────────────────────────┘  SELECT now() AS current_date_time, current_date_time + INTERVAL '4 day' + INTERVAL '3 hour';  ┌───current_date_time─┬─plus(plus(now(), toIntervalDay(4)), toIntervalHour(3))─┐ │ 2020-11-03 22:12:10 │ 2020-11-08 01:12:10 │ └─────────────────────┴────────────────────────────────────────────────────────┘  SELECT now() AS current_date_time, current_date_time + INTERVAL '4' day + INTERVAL '3' hour;  ┌───current_date_time─┬─plus(plus(now(), toIntervalDay('4')), toIntervalHour('3'))─┐ │ 2020-11-03 22:33:19 │ 2020-11-08 01:33:19 │ └─────────────────────┴────────────────────────────────────────────────────────────┘  You can work with dates without using INTERVAL, just by adding or subtracting seconds, minutes, and hours. For example, an interval of one day can be set by adding 60*60*24. note The INTERVAL syntax or addDays function are always preferred. Simple addition or subtraction (syntax like now() + ...) doesn't consider time settings. For example, daylight saving time. Examples: SELECT toDateTime('2014-10-26 00:00:00', 'Asia/Istanbul') AS time, time + 60 * 60 * 24 AS time_plus_24_hours, time + toIntervalDay(1) AS time_plus_1_day;  ┌────────────────time─┬──time_plus_24_hours─┬─────time_plus_1_day─┐ │ 2014-10-26 00:00:00 │ 2014-10-26 23:00:00 │ 2014-10-27 00:00:00 │ └─────────────────────┴─────────────────────┴─────────────────────┘  See Also Interval data typetoInterval type conversion functions "},{"title":"Logical AND Operator​","type":1,"pageTitle":"Operators","url":"docs/en/sql-reference/operators/#logical-and-operator","content":"Syntax SELECT a AND b — calculates logical conjunction of a and b with the function and. "},{"title":"Logical OR Operator​","type":1,"pageTitle":"Operators","url":"docs/en/sql-reference/operators/#logical-or-operator","content":"Syntax SELECT a OR b — calculates logical disjunction of a and b with the function or. "},{"title":"Logical Negation Operator​","type":1,"pageTitle":"Operators","url":"docs/en/sql-reference/operators/#logical-negation-operator","content":"Syntax SELECT NOT a — calculates logical negation of a with the function not. "},{"title":"Conditional Operator​","type":1,"pageTitle":"Operators","url":"docs/en/sql-reference/operators/#conditional-operator","content":"a ? b : c – The if(a, b, c) function. Note: The conditional operator calculates the values of b and c, then checks whether condition a is met, and then returns the corresponding value. If b or C is an arrayJoin() function, each row will be replicated regardless of the “a” condition. "},{"title":"Conditional Expression​","type":1,"pageTitle":"Operators","url":"docs/en/sql-reference/operators/#operator_case","content":"CASE [x] WHEN a THEN b [WHEN ... THEN ...] [ELSE c] END  If x is specified, then transform(x, [a, ...], [b, ...], c) function is used. Otherwise – multiIf(a, b, ..., c). If there is no ELSE c clause in the expression, the default value is NULL. The transform function does not work with NULL. "},{"title":"Concatenation Operator​","type":1,"pageTitle":"Operators","url":"docs/en/sql-reference/operators/#concatenation-operator","content":"s1 || s2 – The concat(s1, s2) function. "},{"title":"Lambda Creation Operator​","type":1,"pageTitle":"Operators","url":"docs/en/sql-reference/operators/#lambda-creation-operator","content":"x -&gt; expr – The lambda(x, expr) function. The following operators do not have a priority since they are brackets: "},{"title":"Array Creation Operator​","type":1,"pageTitle":"Operators","url":"docs/en/sql-reference/operators/#array-creation-operator","content":"[x1, ...] – The array(x1, ...) function. "},{"title":"Tuple Creation Operator​","type":1,"pageTitle":"Operators","url":"docs/en/sql-reference/operators/#tuple-creation-operator","content":"(x1, x2, ...) – The tuple(x2, x2, ...) function. "},{"title":"Associativity​","type":1,"pageTitle":"Operators","url":"docs/en/sql-reference/operators/#associativity","content":"All binary operators have left associativity. For example, 1 + 2 + 3 is transformed to plus(plus(1, 2), 3). Sometimes this does not work the way you expect. For example, SELECT 4 &gt; 2 &gt; 3 will result in 0. For efficiency, the and and or functions accept any number of arguments. The corresponding chains of AND and OR operators are transformed into a single call of these functions. "},{"title":"Checking for NULL​","type":1,"pageTitle":"Operators","url":"docs/en/sql-reference/operators/#checking-for-null","content":"ClickHouse supports the IS NULL and IS NOT NULL operators. "},{"title":"IS NULL​","type":1,"pageTitle":"Operators","url":"docs/en/sql-reference/operators/#operator-is-null","content":"For Nullable type values, the IS NULL operator returns: 1, if the value is NULL.0 otherwise. For other values, the IS NULL operator always returns 0. Can be optimized by enabling the optimize_functions_to_subcolumns setting. With optimize_functions_to_subcolumns = 1 the function reads only null subcolumn instead of reading and processing the whole column data. The query SELECT n IS NULL FROM table transforms to SELECT n.null FROM TABLE. SELECT x+100 FROM t_null WHERE y IS NULL  ┌─plus(x, 100)─┐ │ 101 │ └──────────────┘  "},{"title":"IS NOT NULL​","type":1,"pageTitle":"Operators","url":"docs/en/sql-reference/operators/#is-not-null","content":"For Nullable type values, the IS NOT NULL operator returns: 0, if the value is NULL.1 otherwise. For other values, the IS NOT NULL operator always returns 1. SELECT * FROM t_null WHERE y IS NOT NULL  ┌─x─┬─y─┐ │ 2 │ 3 │ └───┴───┘  Can be optimized by enabling the optimize_functions_to_subcolumns setting. With optimize_functions_to_subcolumns = 1 the function reads only null subcolumn instead of reading and processing the whole column data. The query SELECT n IS NOT NULL FROM table transforms to SELECT NOT n.null FROM TABLE. "},{"title":"ALTER","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/alter/","content":"","keywords":""},{"title":"Mutations​","type":1,"pageTitle":"ALTER","url":"docs/en/sql-reference/statements/alter/#mutations","content":"ALTER queries that are intended to manipulate table data are implemented with a mechanism called “mutations”, most notably ALTER TABLE … DELETE and ALTER TABLE … UPDATE. They are asynchronous background processes similar to merges in MergeTree tables that to produce new “mutated” versions of parts. For *MergeTree tables mutations execute by rewriting whole data parts. There is no atomicity - parts are substituted for mutated parts as soon as they are ready and a SELECT query that started executing during a mutation will see data from parts that have already been mutated along with data from parts that have not been mutated yet. Mutations are totally ordered by their creation order and are applied to each part in that order. Mutations are also partially ordered with INSERT INTO queries: data that was inserted into the table before the mutation was submitted will be mutated and data that was inserted after that will not be mutated. Note that mutations do not block inserts in any way. A mutation query returns immediately after the mutation entry is added (in case of replicated tables to ZooKeeper, for non-replicated tables - to the filesystem). The mutation itself executes asynchronously using the system profile settings. To track the progress of mutations you can use the system.mutations table. A mutation that was successfully submitted will continue to execute even if ClickHouse servers are restarted. There is no way to roll back the mutation once it is submitted, but if the mutation is stuck for some reason it can be cancelled with the KILL MUTATION query. Entries for finished mutations are not deleted right away (the number of preserved entries is determined by the finished_mutations_to_keep storage engine parameter). Older mutation entries are deleted. "},{"title":"Synchronicity of ALTER Queries​","type":1,"pageTitle":"ALTER","url":"docs/en/sql-reference/statements/alter/#synchronicity-of-alter-queries","content":"For non-replicated tables, all ALTER queries are performed synchronously. For replicated tables, the query just adds instructions for the appropriate actions to ZooKeeper, and the actions themselves are performed as soon as possible. However, the query can wait for these actions to be completed on all the replicas. For all ALTER queries, you can use the replication_alter_partitions_sync setting to set up waiting. You can specify how long (in seconds) to wait for inactive replicas to execute all ALTER queries with the replication_wait_for_inactive_replica_timeout setting. note For all ALTER queries, if replication_alter_partitions_sync = 2 and some replicas are not active for more than the time, specified in the replication_wait_for_inactive_replica_timeout setting, then an exception UNFINISHED is thrown. For ALTER TABLE ... UPDATE|DELETE queries the synchronicity is defined by the mutations_sync setting. "},{"title":"IN Operators","type":0,"sectionRef":"#","url":"docs/en/sql-reference/operators/in","content":"","keywords":""},{"title":"NULL Processing​","type":1,"pageTitle":"IN Operators","url":"docs/en/sql-reference/operators/in#in-null-processing","content":"During request processing, the IN operator assumes that the result of an operation with NULL always equals 0, regardless of whether NULL is on the right or left side of the operator. NULL values are not included in any dataset, do not correspond to each other and cannot be compared if transform_null_in = 0. Here is an example with the t_null table: ┌─x─┬────y─┐ │ 1 │ ᴺᵁᴸᴸ │ │ 2 │ 3 │ └───┴──────┘  Running the query SELECT x FROM t_null WHERE y IN (NULL,3) gives you the following result: ┌─x─┐ │ 2 │ └───┘  You can see that the row in which y = NULL is thrown out of the query results. This is because ClickHouse can’t decide whether NULL is included in the (NULL,3) set, returns 0 as the result of the operation, and SELECT excludes this row from the final output. SELECT y IN (NULL, 3) FROM t_null  ┌─in(y, tuple(NULL, 3))─┐ │ 0 │ │ 1 │ └───────────────────────┘  "},{"title":"Distributed Subqueries​","type":1,"pageTitle":"IN Operators","url":"docs/en/sql-reference/operators/in#select-distributed-subqueries","content":"There are two options for IN-s with subqueries (similar to JOINs): normal IN / JOIN and GLOBAL IN / GLOBAL JOIN. They differ in how they are run for distributed query processing. note Remember that the algorithms described below may work differently depending on the settings distributed_product_mode setting. When using the regular IN, the query is sent to remote servers, and each of them runs the subqueries in the IN or JOIN clause. When using GLOBAL IN / GLOBAL JOINs, first all the subqueries are run for GLOBAL IN / GLOBAL JOINs, and the results are collected in temporary tables. Then the temporary tables are sent to each remote server, where the queries are run using this temporary data. For a non-distributed query, use the regular IN / JOIN. Be careful when using subqueries in the IN / JOIN clauses for distributed query processing. Let’s look at some examples. Assume that each server in the cluster has a normal local_table. Each server also has a distributed_table table with the Distributed type, which looks at all the servers in the cluster. For a query to the distributed_table, the query will be sent to all the remote servers and run on them using the local_table. For example, the query SELECT uniq(UserID) FROM distributed_table  will be sent to all remote servers as SELECT uniq(UserID) FROM local_table  and run on each of them in parallel, until it reaches the stage where intermediate results can be combined. Then the intermediate results will be returned to the requestor server and merged on it, and the final result will be sent to the client. Now let’s examine a query with IN: SELECT uniq(UserID) FROM distributed_table WHERE CounterID = 101500 AND UserID IN (SELECT UserID FROM local_table WHERE CounterID = 34)  Calculation of the intersection of audiences of two sites. This query will be sent to all remote servers as SELECT uniq(UserID) FROM local_table WHERE CounterID = 101500 AND UserID IN (SELECT UserID FROM local_table WHERE CounterID = 34)  In other words, the data set in the IN clause will be collected on each server independently, only across the data that is stored locally on each of the servers. This will work correctly and optimally if you are prepared for this case and have spread data across the cluster servers such that the data for a single UserID resides entirely on a single server. In this case, all the necessary data will be available locally on each server. Otherwise, the result will be inaccurate. We refer to this variation of the query as “local IN”. To correct how the query works when data is spread randomly across the cluster servers, you could specify distributed_table inside a subquery. The query would look like this: SELECT uniq(UserID) FROM distributed_table WHERE CounterID = 101500 AND UserID IN (SELECT UserID FROM distributed_table WHERE CounterID = 34)  This query will be sent to all remote servers as SELECT uniq(UserID) FROM local_table WHERE CounterID = 101500 AND UserID IN (SELECT UserID FROM distributed_table WHERE CounterID = 34)  The subquery will begin running on each remote server. Since the subquery uses a distributed table, the subquery that is on each remote server will be resent to every remote server as SELECT UserID FROM local_table WHERE CounterID = 34  For example, if you have a cluster of 100 servers, executing the entire query will require 10,000 elementary requests, which is generally considered unacceptable. In such cases, you should always use GLOBAL IN instead of IN. Let’s look at how it works for the query SELECT uniq(UserID) FROM distributed_table WHERE CounterID = 101500 AND UserID GLOBAL IN (SELECT UserID FROM distributed_table WHERE CounterID = 34)  The requestor server will run the subquery SELECT UserID FROM distributed_table WHERE CounterID = 34  and the result will be put in a temporary table in RAM. Then the request will be sent to each remote server as SELECT uniq(UserID) FROM local_table WHERE CounterID = 101500 AND UserID GLOBAL IN _data1  and the temporary table _data1 will be sent to every remote server with the query (the name of the temporary table is implementation-defined). This is more optimal than using the normal IN. However, keep the following points in mind: When creating a temporary table, data is not made unique. To reduce the volume of data transmitted over the network, specify DISTINCT in the subquery. (You do not need to do this for a normal IN.)The temporary table will be sent to all the remote servers. Transmission does not account for network topology. For example, if 10 remote servers reside in a datacenter that is very remote in relation to the requestor server, the data will be sent 10 times over the channel to the remote datacenter. Try to avoid large data sets when using GLOBAL IN.When transmitting data to remote servers, restrictions on network bandwidth are not configurable. You might overload the network.Try to distribute data across servers so that you do not need to use GLOBAL IN on a regular basis.If you need to use GLOBAL IN often, plan the location of the ClickHouse cluster so that a single group of replicas resides in no more than one data center with a fast network between them, so that a query can be processed entirely within a single data center. It also makes sense to specify a local table in the GLOBAL IN clause, in case this local table is only available on the requestor server and you want to use data from it on remote servers. "},{"title":"Distributed Subqueries and max_rows_in_set​","type":1,"pageTitle":"IN Operators","url":"docs/en/sql-reference/operators/in#distributed-subqueries-and-max_rows_in_set","content":"You can use max_rows_in_set and max_bytes_in_set to control how much data is tranferred during distributed queries. This is specially important if the global in query returns a large amount of data. Consider the following sql - select * from table1 where col1 global in (select col1 from table2 where &lt;some_predicate&gt;)  If some_predicate is not selective enough, it will return large amount of data and cause performance issues. In such cases, it is wise to limit the data transfer over the network. Also, note that set_overflow_mode is set to throw (by default) meaning that an exception is raised when these thresholds are met. "},{"title":"Distributed Subqueries and max_parallel_replicas​","type":1,"pageTitle":"IN Operators","url":"docs/en/sql-reference/operators/in#max_parallel_replica-subqueries","content":"When max_parallel_replicas is greater than 1, distributed queries are further transformed. For example, the following: SELECT CounterID, count() FROM distributed_table_1 WHERE UserID IN (SELECT UserID FROM local_table_2 WHERE CounterID &lt; 100) SETTINGS max_parallel_replicas=3  is transformed on each server into SELECT CounterID, count() FROM local_table_1 WHERE UserID IN (SELECT UserID FROM local_table_2 WHERE CounterID &lt; 100) SETTINGS parallel_replicas_count=3, parallel_replicas_offset=M  where M is between 1 and 3 depending on which replica the local query is executing on. These settings affect every MergeTree-family table in the query and have the same effect as applying SAMPLE 1/3 OFFSET (M-1)/3 on each table. Therefore adding the max_parallel_replicas setting will only produce correct results if both tables have the same replication scheme and are sampled by UserID or a subkey of it. In particular, if local_table_2 does not have a sampling key, incorrect results will be produced. The same rule applies to JOIN. One workaround if local_table_2 does not meet the requirements, is to use GLOBAL IN or GLOBAL JOIN. "},{"title":"ALTER TABLE … MODIFY COMMENT","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/alter/comment","content":"ALTER TABLE … MODIFY COMMENT Adds, modifies, or removes comment to the table, regardless if it was set before or not. Comment change is reflected in both system.tables and SHOW CREATE TABLE query. Syntax ALTER TABLE [db].name [ON CLUSTER cluster] MODIFY COMMENT 'Comment' Examples Creating a table with comment (for more information, see the [COMMENT] clause(../../../sql-reference/statements/create/table.md#comment-table)): CREATE TABLE table_with_comment ( `k` UInt64, `s` String ) ENGINE = Memory() COMMENT 'The temporary table'; Modifying the table comment: ALTER TABLE table_with_comment MODIFY COMMENT 'new comment on a table'; SELECT comment FROM system.tables WHERE database = currentDatabase() AND name = 'table_with_comment'; Output of a new comment: ┌─comment────────────────┐ │ new comment on a table │ └────────────────────────┘ Removing the table comment: ALTER TABLE table_with_comment MODIFY COMMENT ''; SELECT comment FROM system.tables WHERE database = currentDatabase() AND name = 'table_with_comment'; Output of a removed comment: ┌─comment─┐ │ │ └─────────┘ ","keywords":""},{"title":"ALTER TABLE … DELETE Statement","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/alter/delete","content":"ALTER TABLE … DELETE Statement ALTER TABLE [db.]table [ON CLUSTER cluster] DELETE WHERE filter_expr Deletes data matching the specified filtering expression. Implemented as a mutation. note The ALTER TABLE prefix makes this syntax different from most other systems supporting SQL. It is intended to signify that unlike similar queries in OLTP databases this is a heavy operation not designed for frequent use. The filter_expr must be of type UInt8. The query deletes rows in the table for which this expression takes a non-zero value. One query can contain several commands separated by commas. The synchronicity of the query processing is defined by the mutations_sync setting. By default, it is asynchronous. See also MutationsSynchronicity of ALTER Queriesmutations_sync setting","keywords":""},{"title":"Functions for Working with UUID","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/uuid-functions","content":"","keywords":""},{"title":"generateUUIDv4​","type":1,"pageTitle":"Functions for Working with UUID","url":"docs/en/sql-reference/functions/uuid-functions#uuid-function-generate","content":"Generates the UUID of version 4. generateUUIDv4()  Returned value The UUID type value. Usage example This example demonstrates creating a table with the UUID type column and inserting a value into the table. CREATE TABLE t_uuid (x UUID) ENGINE=TinyLog INSERT INTO t_uuid SELECT generateUUIDv4() SELECT * FROM t_uuid  ┌────────────────────────────────────x─┐ │ f4bf890f-f9dc-4332-ad5c-0c18e73f28e9 │ └──────────────────────────────────────┘  "},{"title":"empty​","type":1,"pageTitle":"Functions for Working with UUID","url":"docs/en/sql-reference/functions/uuid-functions#empty","content":"Checks whether the input UUID is empty. Syntax empty(UUID)  The UUID is considered empty if it contains all zeros (zero UUID). The function also works for arrays or strings. Arguments x — Input UUID. UUID. Returned value Returns 1 for an empty UUID or 0 for a non-empty UUID.  Type: UInt8. Example To generate the UUID value, ClickHouse provides the generateUUIDv4 function. Query: SELECT empty(generateUUIDv4());  Result: ┌─empty(generateUUIDv4())─┐ │ 0 │ └─────────────────────────┘  "},{"title":"notEmpty​","type":1,"pageTitle":"Functions for Working with UUID","url":"docs/en/sql-reference/functions/uuid-functions#notempty","content":"Checks whether the input UUID is non-empty. Syntax notEmpty(UUID)  The UUID is considered empty if it contains all zeros (zero UUID). The function also works for arrays or strings. Arguments x — Input UUID. UUID. Returned value Returns 1 for a non-empty UUID or 0 for an empty UUID.  Type: UInt8. Example To generate the UUID value, ClickHouse provides the generateUUIDv4 function. Query: SELECT notEmpty(generateUUIDv4());  Result: ┌─notEmpty(generateUUIDv4())─┐ │ 1 │ └────────────────────────────┘  "},{"title":"toUUID (x)​","type":1,"pageTitle":"Functions for Working with UUID","url":"docs/en/sql-reference/functions/uuid-functions#touuid-x","content":"Converts String type value to UUID type. toUUID(String)  Returned value The UUID type value. Usage example SELECT toUUID('61f0c404-5cb3-11e7-907b-a6006ad3dba0') AS uuid  ┌─────────────────────────────────uuid─┐ │ 61f0c404-5cb3-11e7-907b-a6006ad3dba0 │ └──────────────────────────────────────┘  "},{"title":"toUUIDOrNull (x)​","type":1,"pageTitle":"Functions for Working with UUID","url":"docs/en/sql-reference/functions/uuid-functions#touuidornull-x","content":"It takes an argument of type String and tries to parse it into UUID. If failed, returns NULL. toUUIDOrNull(String)  Returned value The Nullable(UUID) type value. Usage example SELECT toUUIDOrNull('61f0c404-5cb3-11e7-907b-a6006ad3dba0T') AS uuid  ┌─uuid─┐ │ ᴺᵁᴸᴸ │ └──────┘  "},{"title":"toUUIDOrZero (x)​","type":1,"pageTitle":"Functions for Working with UUID","url":"docs/en/sql-reference/functions/uuid-functions#touuidorzero-x","content":"It takes an argument of type String and tries to parse it into UUID. If failed, returns zero UUID. toUUIDOrZero(String)  Returned value The UUID type value. Usage example SELECT toUUIDOrZero('61f0c404-5cb3-11e7-907b-a6006ad3dba0T') AS uuid  ┌─────────────────────────────────uuid─┐ │ 00000000-0000-0000-0000-000000000000 │ └──────────────────────────────────────┘  "},{"title":"UUIDStringToNum​","type":1,"pageTitle":"Functions for Working with UUID","url":"docs/en/sql-reference/functions/uuid-functions#uuidstringtonum","content":"Accepts a string containing 36 characters in the format xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx, and returns it as a set of bytes in a FixedString(16). UUIDStringToNum(String)  Returned value FixedString(16) Usage examples SELECT '612f3c40-5d3b-217e-707b-6a546a3d7b29' AS uuid, UUIDStringToNum(uuid) AS bytes  ┌─uuid─────────────────────────────────┬─bytes────────────┐ │ 612f3c40-5d3b-217e-707b-6a546a3d7b29 │ a/&lt;@];!~p{jTj={) │ └──────────────────────────────────────┴──────────────────┘  "},{"title":"UUIDNumToString​","type":1,"pageTitle":"Functions for Working with UUID","url":"docs/en/sql-reference/functions/uuid-functions#uuidnumtostring","content":"Accepts a FixedString(16) value, and returns a string containing 36 characters in text format. UUIDNumToString(FixedString(16))  Returned value String. Usage example SELECT 'a/&lt;@];!~p{jTj={)' AS bytes, UUIDNumToString(toFixedString(bytes, 16)) AS uuid  ┌─bytes────────────┬─uuid─────────────────────────────────┐ │ a/&lt;@];!~p{jTj={) │ 612f3c40-5d3b-217e-707b-6a546a3d7b29 │ └──────────────────┴──────────────────────────────────────┘  "},{"title":"serverUUID()​","type":1,"pageTitle":"Functions for Working with UUID","url":"docs/en/sql-reference/functions/uuid-functions#server-uuid","content":"Returns the random and unique UUID, which is generated when the server is first started and stored forever. The result writes to the file uuid created in the ClickHouse server directory /var/lib/clickhouse/. Syntax serverUUID()  Returned value The UUID of the server.  Type: UUID. "},{"title":"See Also​","type":1,"pageTitle":"Functions for Working with UUID","url":"docs/en/sql-reference/functions/uuid-functions#see-also","content":"dictGetUUID "},{"title":"Manipulating Data Skipping Indices","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/alter/index/","content":"Manipulating Data Skipping Indices The following operations are available: ALTER TABLE [db].name ADD INDEX name expression TYPE type GRANULARITY value [FIRST|AFTER name] - Adds index description to tables metadata. ALTER TABLE [db].name DROP INDEX name - Removes index description from tables metadata and deletes index files from disk. ALTER TABLE [db.]table MATERIALIZE INDEX name IN PARTITION partition_name - The query rebuilds the secondary index name in the partition partition_name. Implemented as a mutation. To rebuild index over the whole data in the table you need to remove IN PARTITION from query. The first two commands are lightweight in a sense that they only change metadata or remove files. Also, they are replicated, syncing indices metadata via ZooKeeper. note Index manipulation is supported only for tables with *MergeTree engine (including replicated variants).","keywords":""},{"title":"Manipulating Constraints","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/alter/constraint","content":"Manipulating Constraints Constraints could be added or deleted using following syntax: ALTER TABLE [db].name ADD CONSTRAINT constraint_name CHECK expression; ALTER TABLE [db].name DROP CONSTRAINT constraint_name; See more on constraints. Queries will add or remove metadata about constraints from table so they are processed immediately. warning Constraint check will not be executed on existing data if it was added. All changes on replicated tables are broadcasted to ZooKeeper and will be applied on other replicas as well.","keywords":""},{"title":"Manipulating Key Expressions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/alter/order-by","content":"Manipulating Key Expressions ALTER TABLE [db].name [ON CLUSTER cluster] MODIFY ORDER BY new_expression The command changes the sorting key of the table to new_expression (an expression or a tuple of expressions). Primary key remains the same. The command is lightweight in a sense that it only changes metadata. To keep the property that data part rows are ordered by the sorting key expression you cannot add expressions containing existing columns to the sorting key (only columns added by the ADD COLUMN command in the same ALTER query, without default column value). note It only works for tables in the MergeTree family (including replicated tables).","keywords":""},{"title":"Functions for Working with Tuples","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/tuple-functions","content":"","keywords":""},{"title":"tuple​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#tuple","content":"A function that allows grouping multiple columns. For columns with the types T1, T2, …, it returns a Tuple(T1, T2, …) type tuple containing these columns. There is no cost to execute the function. Tuples are normally used as intermediate values for an argument of IN operators, or for creating a list of formal parameters of lambda functions. Tuples can’t be written to a table. The function implements the operator (x, y, …). Syntax tuple(x, y, …)  "},{"title":"tupleElement​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#tupleelement","content":"A function that allows getting a column from a tuple. ‘N’ is the column index, starting from 1. ‘N’ must be a constant. ‘N’ must be a strict postive integer no greater than the size of the tuple. There is no cost to execute the function. The function implements the operator x.N. Syntax tupleElement(tuple, n)  "},{"title":"untuple​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#untuple","content":"Performs syntactic substitution of tuple elements in the call location. Syntax untuple(x)  You can use the EXCEPT expression to skip columns as a result of the query. Arguments x — A tuple function, column, or tuple of elements. Tuple. Returned value None. Examples Input table: ┌─key─┬─v1─┬─v2─┬─v3─┬─v4─┬─v5─┬─v6────────┐ │ 1 │ 10 │ 20 │ 40 │ 30 │ 15 │ (33,'ab') │ │ 2 │ 25 │ 65 │ 70 │ 40 │ 6 │ (44,'cd') │ │ 3 │ 57 │ 30 │ 20 │ 10 │ 5 │ (55,'ef') │ │ 4 │ 55 │ 12 │ 7 │ 80 │ 90 │ (66,'gh') │ │ 5 │ 30 │ 50 │ 70 │ 25 │ 55 │ (77,'kl') │ └─────┴────┴────┴────┴────┴────┴───────────┘  Example of using a Tuple-type column as the untuple function parameter: Query: SELECT untuple(v6) FROM kv;  Result: ┌─_ut_1─┬─_ut_2─┐ │ 33 │ ab │ │ 44 │ cd │ │ 55 │ ef │ │ 66 │ gh │ │ 77 │ kl │ └───────┴───────┘  Note: the names are implementation specific and are subject to change. You should not assume specific names of the columns after application of the untuple. Example of using an EXCEPT expression: Query: SELECT untuple((* EXCEPT (v2, v3),)) FROM kv;  Result: ┌─key─┬─v1─┬─v4─┬─v5─┬─v6────────┐ │ 1 │ 10 │ 30 │ 15 │ (33,'ab') │ │ 2 │ 25 │ 40 │ 6 │ (44,'cd') │ │ 3 │ 57 │ 10 │ 5 │ (55,'ef') │ │ 4 │ 55 │ 80 │ 90 │ (66,'gh') │ │ 5 │ 30 │ 25 │ 55 │ (77,'kl') │ └─────┴────┴────┴────┴───────────┘  See Also Tuple "},{"title":"tupleHammingDistance​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#tuplehammingdistance","content":"Returns the Hamming Distance between two tuples of the same size. Syntax tupleHammingDistance(tuple1, tuple2)  Arguments tuple1 — First tuple. Tuple.tuple2 — Second tuple. Tuple. Tuples should have the same type of the elements. Returned value The Hamming distance. Type: The result type is calculed the same way it is for Arithmetic functions, based on the number of elements in the input tuples. SELECT toTypeName(tupleHammingDistance(tuple(0), tuple(0))) AS t1, toTypeName(tupleHammingDistance((0, 0), (0, 0))) AS t2, toTypeName(tupleHammingDistance((0, 0, 0), (0, 0, 0))) AS t3, toTypeName(tupleHammingDistance((0, 0, 0, 0), (0, 0, 0, 0))) AS t4, toTypeName(tupleHammingDistance((0, 0, 0, 0, 0), (0, 0, 0, 0, 0))) AS t5  ┌─t1────┬─t2─────┬─t3─────┬─t4─────┬─t5─────┐ │ UInt8 │ UInt16 │ UInt32 │ UInt64 │ UInt64 │ └───────┴────────┴────────┴────────┴────────┘  Examples Query: SELECT tupleHammingDistance((1, 2, 3), (3, 2, 1)) AS HammingDistance;  Result: ┌─HammingDistance─┐ │ 2 │ └─────────────────┘  Can be used with MinHash functions for detection of semi-duplicate strings: SELECT tupleHammingDistance(wordShingleMinHash(string), wordShingleMinHashCaseInsensitive(string)) as HammingDistance FROM (SELECT 'ClickHouse is a column-oriented database management system for online analytical processing of queries.' AS string);  Result: ┌─HammingDistance─┐ │ 2 │ └─────────────────┘  "},{"title":"tupleToNameValuePairs​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#tupletonamevaluepairs","content":"Turns a named tuple into an array of (name, value) pairs. For a Tuple(a T, b T, ..., c T) returns Array(Tuple(String, T), ...)in which the Strings represents the named fields of the tuple and T are the values associated with those names. All values in the tuple should be of the same type. Syntax tupleToNameValuePairs(tuple)  Arguments tuple — Named tuple. Tuple with any types of values. Returned value An array with (name, value) pairs. Type: Array(Tuple(String, ...)). Example Query: CREATE TABLE tupletest (`col` Tuple(user_ID UInt64, session_ID UInt64) ENGINE = Memory; INSERT INTO tupletest VALUES (tuple( 100, 2502)), (tuple(1,100)); SELECT tupleToNameValuePairs(col) FROM tupletest;  Result: ┌─tupleToNameValuePairs(col)────────────┐ │ [('user_ID',100),('session_ID',2502)] │ │ [('user_ID',1),('session_ID',100)] │ └───────────────────────────────────────┘  It is possible to transform colums to rows using this function: CREATE TABLE tupletest (`col` Tuple(CPU Float64, Memory Float64, Disk Float64)) ENGINE = Memory; INSERT INTO tupletest VALUES(tuple(3.3, 5.5, 6.6)); SELECT arrayJoin(tupleToNameValuePairs(col))FROM tupletest;  Result: ┌─arrayJoin(tupleToNameValuePairs(col))─┐ │ ('CPU',3.3) │ │ ('Memory',5.5) │ │ ('Disk',6.6) │ └───────────────────────────────────────┘  If you pass a simple tuple to the function, ClickHouse uses the indexes of the values as their names: SELECT tupleToNameValuePairs(tuple(3, 2, 1));  Result: ┌─tupleToNameValuePairs(tuple(3, 2, 1))─┐ │ [('1',3),('2',2),('3',1)] │ └───────────────────────────────────────┘  "},{"title":"tuplePlus​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#tupleplus","content":"Calculates the sum of corresponding values of two tuples of the same size. Syntax tuplePlus(tuple1, tuple2)  Alias: vectorSum. Arguments tuple1 — First tuple. Tuple.tuple2 — Second tuple. Tuple. Returned value Tuple with the sum. Type: Tuple. Example Query: SELECT tuplePlus((1, 2), (2, 3));  Result: ┌─tuplePlus((1, 2), (2, 3))─┐ │ (3,5) │ └───────────────────────────┘  "},{"title":"tupleMinus​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#tupleminus","content":"Calculates the subtraction of corresponding values of two tuples of the same size. Syntax tupleMinus(tuple1, tuple2)  Alias: vectorDifference. Arguments tuple1 — First tuple. Tuple.tuple2 — Second tuple. Tuple. Returned value Tuple with the result of subtraction. Type: Tuple. Example Query: SELECT tupleMinus((1, 2), (2, 3));  Result: ┌─tupleMinus((1, 2), (2, 3))─┐ │ (-1,-1) │ └────────────────────────────┘  "},{"title":"tupleMultiply​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#tuplemultiply","content":"Calculates the multiplication of corresponding values of two tuples of the same size. Syntax tupleMultiply(tuple1, tuple2)  Arguments tuple1 — First tuple. Tuple.tuple2 — Second tuple. Tuple. Returned value Tuple with the multiplication. Type: Tuple. Example Query: SELECT tupleMultiply((1, 2), (2, 3));  Result: ┌─tupleMultiply((1, 2), (2, 3))─┐ │ (2,6) │ └───────────────────────────────┘  "},{"title":"tupleDivide​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#tupledivide","content":"Calculates the division of corresponding values of two tuples of the same size. Note that division by zero will return inf. Syntax tupleDivide(tuple1, tuple2)  Arguments tuple1 — First tuple. Tuple.tuple2 — Second tuple. Tuple. Returned value Tuple with the result of division. Type: Tuple. Example Query: SELECT tupleDivide((1, 2), (2, 3));  Result: ┌─tupleDivide((1, 2), (2, 3))─┐ │ (0.5,0.6666666666666666) │ └─────────────────────────────┘  "},{"title":"tupleNegate​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#tuplenegate","content":"Calculates the negation of the tuple values. Syntax tupleNegate(tuple)  Arguments tuple — Tuple. Returned value Tuple with the result of negation. Type: Tuple. Example Query: SELECT tupleNegate((1, 2));  Result: ┌─tupleNegate((1, 2))─┐ │ (-1,-2) │ └─────────────────────┘  "},{"title":"tupleMultiplyByNumber​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#tuplemultiplybynumber","content":"Returns a tuple with all values multiplied by a number. Syntax tupleMultiplyByNumber(tuple, number)  Arguments tuple — Tuple.number — Multiplier. Int/UInt, Float or Decimal. Returned value Tuple with multiplied values. Type: Tuple. Example Query: SELECT tupleMultiplyByNumber((1, 2), -2.1);  Result: ┌─tupleMultiplyByNumber((1, 2), -2.1)─┐ │ (-2.1,-4.2) │ └─────────────────────────────────────┘  "},{"title":"tupleDivideByNumber​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#tupledividebynumber","content":"Returns a tuple with all values divided by a number. Note that division by zero will return inf. Syntax tupleDivideByNumber(tuple, number)  Arguments tuple — Tuple.number — Divider. Int/UInt, Float or Decimal. Returned value Tuple with divided values. Type: Tuple. Example Query: SELECT tupleDivideByNumber((1, 2), 0.5);  Result: ┌─tupleDivideByNumber((1, 2), 0.5)─┐ │ (2,4) │ └──────────────────────────────────┘  "},{"title":"dotProduct​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#dotproduct","content":"Calculates the scalar product of two tuples of the same size. Syntax dotProduct(tuple1, tuple2)  Alias: scalarProduct. Arguments tuple1 — First tuple. Tuple.tuple2 — Second tuple. Tuple. Returned value Scalar product. Type: Int/UInt, Float or Decimal. Example Query: SELECT dotProduct((1, 2), (2, 3));  Result: ┌─dotProduct((1, 2), (2, 3))─┐ │ 8 │ └────────────────────────────┘  "},{"title":"L1Norm​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#l1norm","content":"Calculates the sum of absolute values of a tuple. Syntax L1Norm(tuple)  Alias: normL1. Arguments tuple — Tuple. Returned value L1-norm or taxicab geometry distance. Type: UInt, Float or Decimal. Example Query: SELECT L1Norm((1, 2));  Result: ┌─L1Norm((1, 2))─┐ │ 3 │ └────────────────┘  "},{"title":"L2Norm​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#l2norm","content":"Calculates the square root of the sum of the squares of the tuple values. Syntax L2Norm(tuple)  Alias: normL2. Arguments tuple — Tuple. Returned value L2-norm or Euclidean distance. Type: Float. Example Query: SELECT L2Norm((1, 2));  Result: ┌───L2Norm((1, 2))─┐ │ 2.23606797749979 │ └──────────────────┘  "},{"title":"LinfNorm​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#linfnorm","content":"Calculates the maximum of absolute values of a tuple. Syntax LinfNorm(tuple)  Alias: normLinf. Arguments tuple — Tuple. Returned value Linf-norm or the maximum absolute value. Type: Float. Example Query: SELECT LinfNorm((1, -2));  Result: ┌─LinfNorm((1, -2))─┐ │ 2 │ └───────────────────┘  "},{"title":"LpNorm​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#lpnorm","content":"Calculates the root of p-th power of the sum of the absolute values of a tuple in the power of p. Syntax LpNorm(tuple, p)  Alias: normLp. Arguments tuple — Tuple.p — The power. Possible values: real number in [1; inf). UInt or Float. Returned value Lp-norm Type: Float. Example Query: SELECT LpNorm((1, -2), 2);  Result: ┌─LpNorm((1, -2), 2)─┐ │ 2.23606797749979 │ └────────────────────┘  "},{"title":"L1Distance​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#l1distance","content":"Calculates the distance between two points (the values of the tuples are the coordinates) in L1 space (1-norm (taxicab geometry distance)). Syntax L1Distance(tuple1, tuple2)  Alias: distanceL1. Arguments tuple1 — First tuple. Tuple.tuple1 — Second tuple. Tuple. Returned value 1-norm distance. Type: Float. Example Query: SELECT L1Distance((1, 2), (2, 3));  Result: ┌─L1Distance((1, 2), (2, 3))─┐ │ 2 │ └────────────────────────────┘  "},{"title":"L2Distance​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#l2distance","content":"Calculates the distance between two points (the values of the tuples are the coordinates) in Euclidean space (Euclidean distance). Syntax L2Distance(tuple1, tuple2)  Alias: distanceL2. Arguments tuple1 — First tuple. Tuple.tuple1 — Second tuple. Tuple. Returned value 2-norm distance. Type: Float. Example Query: SELECT L2Distance((1, 2), (2, 3));  Result: ┌─L2Distance((1, 2), (2, 3))─┐ │ 1.4142135623730951 │ └────────────────────────────┘  "},{"title":"LinfDistance​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#linfdistance","content":"Calculates the distance between two points (the values of the tuples are the coordinates) in L_{inf} space (maximum norm). Syntax LinfDistance(tuple1, tuple2)  Alias: distanceLinf. Arguments tuple1 — First tuple. Tuple.tuple1 — Second tuple. Tuple. Returned value Infinity-norm distance. Type: Float. Example Query: SELECT LinfDistance((1, 2), (2, 3));  Result: ┌─LinfDistance((1, 2), (2, 3))─┐ │ 1 │ └──────────────────────────────┘  "},{"title":"LpDistance​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#lpdistance","content":"Calculates the distance between two points (the values of the tuples are the coordinates) in Lp space (p-norm distance). Syntax LpDistance(tuple1, tuple2, p)  Alias: distanceLp. Arguments tuple1 — First tuple. Tuple.tuple1 — Second tuple. Tuple.p — The power. Possible values: real number from [1; inf). UInt or Float. Returned value p-norm distance. Type: Float. Example Query: SELECT LpDistance((1, 2), (2, 3), 3);  Result: ┌─LpDistance((1, 2), (2, 3), 3)─┐ │ 1.2599210498948732 │ └───────────────────────────────┘  "},{"title":"L1Normalize​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#l1normalize","content":"Calculates the unit vector of a given vector (the values of the tuple are the coordinates) in L1 space (taxicab geometry). Syntax L1Normalize(tuple)  Alias: normalizeL1. Arguments tuple — Tuple. Returned value Unit vector. Type: Tuple of Float. Example Query: SELECT L1Normalize((1, 2));  Result: ┌─L1Normalize((1, 2))─────────────────────┐ │ (0.3333333333333333,0.6666666666666666) │ └─────────────────────────────────────────┘  "},{"title":"L2Normalize​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#l2normalize","content":"Calculates the unit vector of a given vector (the values of the tuple are the coordinates) in Euclidean space (using Euclidean distance). Syntax L2Normalize(tuple)  Alias: normalizeL1. Arguments tuple — Tuple. Returned value Unit vector. Type: Tuple of Float. Example Query: SELECT L2Normalize((3, 4));  Result: ┌─L2Normalize((3, 4))─┐ │ (0.6,0.8) │ └─────────────────────┘  "},{"title":"LinfNormalize​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#linfnormalize","content":"Calculates the unit vector of a given vector (the values of the tuple are the coordinates) in L_{inf} space (using maximum norm). Syntax LinfNormalize(tuple)  Alias: normalizeLinf . Arguments tuple — Tuple. Returned value Unit vector. Type: Tuple of Float. Example Query: SELECT LinfNormalize((3, 4));  Result: ┌─LinfNormalize((3, 4))─┐ │ (0.75,1) │ └───────────────────────┘  "},{"title":"LpNormalize​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#lpnormalize","content":"Calculates the unit vector of a given vector (the values of the tuple are the coordinates) in Lp space (using p-norm). Syntax LpNormalize(tuple, p)  Alias: normalizeLp . Arguments tuple — Tuple.p — The power. Possible values: any number from [1;inf). UInt or Float. Returned value Unit vector. Type: Tuple of Float. Example Query: SELECT LpNormalize((3, 4),5);  Result: ┌─LpNormalize((3, 4), 5)──────────────────┐ │ (0.7187302630182624,0.9583070173576831) │ └─────────────────────────────────────────┘  "},{"title":"cosineDistance​","type":1,"pageTitle":"Functions for Working with Tuples","url":"docs/en/sql-reference/functions/tuple-functions#cosinedistance","content":"Calculates the cosine distance between two vectors (the values of the tuples are the coordinates). The less the returned value is, the more similar are the vectors. Syntax cosineDistance(tuple1, tuple2)  Arguments tuple1 — First tuple. Tuple.tuple2 — Second tuple. Tuple. Returned value Cosine of the angle between two vectors substracted from one. Type: Float. Example Query: SELECT cosineDistance((1, 2), (2, 3));  Result: ┌─cosineDistance((1, 2), (2, 3))─┐ │ 0.007722123286332261 │ └────────────────────────────────┘  "},{"title":"ALTER QUOTA","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/alter/quota","content":"ALTER QUOTA Changes quotas. Syntax: ALTER QUOTA [IF EXISTS] name [ON CLUSTER cluster_name] [RENAME TO new_name] [KEYED BY {user_name | ip_address | client_key | client_key,user_name | client_key,ip_address} | NOT KEYED] [FOR [RANDOMIZED] INTERVAL number {second | minute | hour | day | week | month | quarter | year} {MAX { {queries | query_selects | query_inserts | errors | result_rows | result_bytes | read_rows | read_bytes | execution_time} = number } [,...] | NO LIMITS | TRACKING ONLY} [,...]] [TO {role [,...] | ALL | ALL EXCEPT role [,...]}] Keys user_name, ip_address, client_key, client_key, user_name and client_key, ip_address correspond to the fields in the system.quotas table. Parameters queries, query_selects, 'query_inserts', errors, result_rows, result_bytes, read_rows, read_bytes, execution_time` correspond to the fields in the system.quotas_usage table. ON CLUSTER clause allows creating quotas on a cluster, see Distributed DDL. Examples Limit the maximum number of queries for the current user with 123 queries in 15 months constraint: ALTER QUOTA IF EXISTS qA FOR INTERVAL 15 month MAX queries = 123 TO CURRENT_USER; For the default user limit the maximum execution time with half a second in 30 minutes, and limit the maximum number of queries with 321 and the maximum number of errors with 10 in 5 quaters: ALTER QUOTA IF EXISTS qB FOR INTERVAL 30 minute MAX execution_time = 0.5, FOR INTERVAL 5 quarter MAX queries = 321, errors = 10 TO default; ","keywords":""},{"title":"ALTER ROW POLICY","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/alter/row-policy","content":"ALTER ROW POLICY Changes row policy. Syntax: ALTER [ROW] POLICY [IF EXISTS] name1 [ON CLUSTER cluster_name1] ON [database1.]table1 [RENAME TO new_name1] [, name2 [ON CLUSTER cluster_name2] ON [database2.]table2 [RENAME TO new_name2] ...] [AS {PERMISSIVE | RESTRICTIVE}] [FOR SELECT] [USING {condition | NONE}][,...] [TO {role [,...] | ALL | ALL EXCEPT role [,...]}] ","keywords":""},{"title":"Manipulating Sampling-Key Expressions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/alter/sample-by","content":"Manipulating Sampling-Key Expressions Syntax: ALTER TABLE [db].name [ON CLUSTER cluster] MODIFY SAMPLE BY new_expression The command changes the sampling key of the table to new_expression (an expression or a tuple of expressions). The command is lightweight in the sense that it only changes metadata. The primary key must contain the new sample key. note It only works for tables in the MergeTree family (including replicated tables).","keywords":""},{"title":"Functions for Working with URLs","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/url-functions","content":"","keywords":""},{"title":"Functions that Extract Parts of a URL​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#functions-that-extract-parts-of-a-url","content":"If the relevant part isn’t present in a URL, an empty string is returned. "},{"title":"protocol​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#protocol","content":"Extracts the protocol from a URL. Examples of typical returned values: http, https, ftp, mailto, tel, magnet… "},{"title":"domain​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#domain","content":"Extracts the hostname from a URL. domain(url)  Arguments url — URL. Type: String. The URL can be specified with or without a scheme. Examples: svn+ssh://some.svn-hosting.com:80/repo/trunk some.svn-hosting.com:80/repo/trunk https://clickhouse.com/time/  For these examples, the domain function returns the following results: some.svn-hosting.com some.svn-hosting.com clickhouse.com  Returned values Host name. If ClickHouse can parse the input string as a URL.Empty string. If ClickHouse can’t parse the input string as a URL. Type: String. Example SELECT domain('svn+ssh://some.svn-hosting.com:80/repo/trunk');  ┌─domain('svn+ssh://some.svn-hosting.com:80/repo/trunk')─┐ │ some.svn-hosting.com │ └────────────────────────────────────────────────────────┘  "},{"title":"domainWithoutWWW​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#domainwithoutwww","content":"Returns the domain and removes no more than one ‘www.’ from the beginning of it, if present. "},{"title":"topLevelDomain​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#topleveldomain","content":"Extracts the the top-level domain from a URL. topLevelDomain(url)  Arguments url — URL. Type: String. The URL can be specified with or without a scheme. Examples: svn+ssh://some.svn-hosting.com:80/repo/trunk some.svn-hosting.com:80/repo/trunk https://clickhouse.com/time/  Returned values Domain name. If ClickHouse can parse the input string as a URL.Empty string. If ClickHouse cannot parse the input string as a URL. Type: String. Example SELECT topLevelDomain('svn+ssh://www.some.svn-hosting.com:80/repo/trunk');  ┌─topLevelDomain('svn+ssh://www.some.svn-hosting.com:80/repo/trunk')─┐ │ com │ └────────────────────────────────────────────────────────────────────┘  "},{"title":"firstSignificantSubdomain​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#firstsignificantsubdomain","content":"Returns the “first significant subdomain”. The first significant subdomain is a second-level domain if it is ‘com’, ‘net’, ‘org’, or ‘co’. Otherwise, it is a third-level domain. For example, firstSignificantSubdomain (‘https://news.clickhouse.com/’) = ‘clickhouse’, firstSignificantSubdomain (‘https://news.clickhouse.com.tr/’) = ‘clickhouse’. The list of “insignificant” second-level domains and other implementation details may change in the future. "},{"title":"cutToFirstSignificantSubdomain​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#cuttofirstsignificantsubdomain","content":"Returns the part of the domain that includes top-level subdomains up to the “first significant subdomain” (see the explanation above). For example: cutToFirstSignificantSubdomain('https://news.clickhouse.com.tr/') = 'clickhouse.com.tr'.cutToFirstSignificantSubdomain('www.tr') = 'tr'.cutToFirstSignificantSubdomain('tr') = ''. "},{"title":"cutToFirstSignificantSubdomainWithWWW​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#cuttofirstsignificantsubdomainwithwww","content":"Returns the part of the domain that includes top-level subdomains up to the “first significant subdomain”, without stripping &quot;www&quot;. For example: cutToFirstSignificantSubdomain('https://news.clickhouse.com.tr/') = 'clickhouse.com.tr'.cutToFirstSignificantSubdomain('www.tr') = 'www.tr'.cutToFirstSignificantSubdomain('tr') = ''. "},{"title":"cutToFirstSignificantSubdomainCustom​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#cuttofirstsignificantsubdomaincustom","content":"Returns the part of the domain that includes top-level subdomains up to the first significant subdomain. Accepts custom TLD list name. Can be useful if you need fresh TLD list or you have custom. Configuration example: &lt;!-- &lt;top_level_domains_path&gt;/var/lib/clickhouse/top_level_domains/&lt;/top_level_domains_path&gt; --&gt; &lt;top_level_domains_lists&gt; &lt;!-- https://publicsuffix.org/list/public_suffix_list.dat --&gt; &lt;public_suffix_list&gt;public_suffix_list.dat&lt;/public_suffix_list&gt; &lt;!-- NOTE: path is under top_level_domains_path --&gt; &lt;/top_level_domains_lists&gt;  Syntax cutToFirstSignificantSubdomain(URL, TLD)  Parameters URL — URL. String.TLD — Custom TLD list name. String. Returned value Part of the domain that includes top-level subdomains up to the first significant subdomain. Type: String. Example Query: SELECT cutToFirstSignificantSubdomainCustom('bar.foo.there-is-no-such-domain', 'public_suffix_list');  Result: ┌─cutToFirstSignificantSubdomainCustom('bar.foo.there-is-no-such-domain', 'public_suffix_list')─┐ │ foo.there-is-no-such-domain │ └───────────────────────────────────────────────────────────────────────────────────────────────┘  See Also firstSignificantSubdomain. "},{"title":"cutToFirstSignificantSubdomainCustomWithWWW​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#cuttofirstsignificantsubdomaincustomwithwww","content":"Returns the part of the domain that includes top-level subdomains up to the first significant subdomain without stripping www. Accepts custom TLD list name. Can be useful if you need fresh TLD list or you have custom. Configuration example: &lt;!-- &lt;top_level_domains_path&gt;/var/lib/clickhouse/top_level_domains/&lt;/top_level_domains_path&gt; --&gt; &lt;top_level_domains_lists&gt; &lt;!-- https://publicsuffix.org/list/public_suffix_list.dat --&gt; &lt;public_suffix_list&gt;public_suffix_list.dat&lt;/public_suffix_list&gt; &lt;!-- NOTE: path is under top_level_domains_path --&gt; &lt;/top_level_domains_lists&gt;  Syntax cutToFirstSignificantSubdomainCustomWithWWW(URL, TLD)  Parameters URL — URL. String.TLD — Custom TLD list name. String. Returned value Part of the domain that includes top-level subdomains up to the first significant subdomain without stripping www. Type: String. Example Query: SELECT cutToFirstSignificantSubdomainCustomWithWWW('www.foo', 'public_suffix_list');  Result: ┌─cutToFirstSignificantSubdomainCustomWithWWW('www.foo', 'public_suffix_list')─┐ │ www.foo │ └──────────────────────────────────────────────────────────────────────────────┘  See Also firstSignificantSubdomain. "},{"title":"firstSignificantSubdomainCustom​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#firstsignificantsubdomaincustom","content":"Returns the first significant subdomain. Accepts customs TLD list name. Can be useful if you need fresh TLD list or you have custom. Configuration example: &lt;!-- &lt;top_level_domains_path&gt;/var/lib/clickhouse/top_level_domains/&lt;/top_level_domains_path&gt; --&gt; &lt;top_level_domains_lists&gt; &lt;!-- https://publicsuffix.org/list/public_suffix_list.dat --&gt; &lt;public_suffix_list&gt;public_suffix_list.dat&lt;/public_suffix_list&gt; &lt;!-- NOTE: path is under top_level_domains_path --&gt; &lt;/top_level_domains_lists&gt;  Syntax firstSignificantSubdomainCustom(URL, TLD)  Parameters URL — URL. String.TLD — Custom TLD list name. String. Returned value First significant subdomain. Type: String. Example Query: SELECT firstSignificantSubdomainCustom('bar.foo.there-is-no-such-domain', 'public_suffix_list');  Result: ┌─firstSignificantSubdomainCustom('bar.foo.there-is-no-such-domain', 'public_suffix_list')─┐ │ foo │ └──────────────────────────────────────────────────────────────────────────────────────────┘  See Also firstSignificantSubdomain. "},{"title":"port(URL[, default_port = 0])​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#port","content":"Returns the port or default_port if there is no port in the URL (or in case of validation error). "},{"title":"path​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#path","content":"Returns the path. Example: /top/news.html The path does not include the query string. "},{"title":"pathFull​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#pathfull","content":"The same as above, but including query string and fragment. Example: /top/news.html?page=2#comments "},{"title":"queryString​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#querystring","content":"Returns the query string. Example: page=1&amp;lr=213. query-string does not include the initial question mark, as well as # and everything after #. "},{"title":"fragment​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#fragment","content":"Returns the fragment identifier. fragment does not include the initial hash symbol. "},{"title":"queryStringAndFragment​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#querystringandfragment","content":"Returns the query string and fragment identifier. Example: page=1#29390. "},{"title":"extractURLParameter(URL, name)​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#extracturlparameterurl-name","content":"Returns the value of the ‘name’ parameter in the URL, if present. Otherwise, an empty string. If there are many parameters with this name, it returns the first occurrence. This function works under the assumption that the parameter name is encoded in the URL exactly the same way as in the passed argument. "},{"title":"extractURLParameters(URL)​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#extracturlparametersurl","content":"Returns an array of name=value strings corresponding to the URL parameters. The values are not decoded in any way. "},{"title":"extractURLParameterNames(URL)​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#extracturlparameternamesurl","content":"Returns an array of name strings corresponding to the names of URL parameters. The values are not decoded in any way. "},{"title":"URLHierarchy(URL)​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#urlhierarchyurl","content":"Returns an array containing the URL, truncated at the end by the symbols /,? in the path and query-string. Consecutive separator characters are counted as one. The cut is made in the position after all the consecutive separator characters. "},{"title":"URLPathHierarchy(URL)​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#urlpathhierarchyurl","content":"The same as above, but without the protocol and host in the result. The / element (root) is not included. URLPathHierarchy('https://example.com/browse/CONV-6788') = [ '/browse/', '/browse/CONV-6788' ]  "},{"title":"encodeURLComponent(URL)​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#encodeurlcomponenturl","content":"Returns the encoded URL. Example: SELECT encodeURLComponent('http://127.0.0.1:8123/?query=SELECT 1;') AS EncodedURL;  ┌─EncodedURL───────────────────────────────────────────────┐ │ http%3A%2F%2F127.0.0.1%3A8123%2F%3Fquery%3DSELECT%201%3B │ └──────────────────────────────────────────────────────────┘  "},{"title":"decodeURLComponent(URL)​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#decodeurlcomponenturl","content":"Returns the decoded URL. Example: SELECT decodeURLComponent('http://127.0.0.1:8123/?query=SELECT%201%3B') AS DecodedURL;  ┌─DecodedURL─────────────────────────────┐ │ http://127.0.0.1:8123/?query=SELECT 1; │ └────────────────────────────────────────┘  "},{"title":"encodeURLFormComponent(URL)​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#encodeurlformcomponenturl","content":"Returns the encoded URL. Follows rfc-1866, space( ) is encoded as plus(+). Example: SELECT encodeURLFormComponent('http://127.0.0.1:8123/?query=SELECT 1 2+3') AS EncodedURL;  ┌─EncodedURL────────────────────────────────────────────────┐ │ http%3A%2F%2F127.0.0.1%3A8123%2F%3Fquery%3DSELECT+1+2%2B3 │ └───────────────────────────────────────────────────────────┘  "},{"title":"decodeURLFormComponent(URL)​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#decodeurlformcomponenturl","content":"Returns the decoded URL. Follows rfc-1866, plain plus(+) is decoded as space( ). Example: SELECT decodeURLFormComponent('http://127.0.0.1:8123/?query=SELECT%201+2%2B3') AS DecodedURL;  ┌─DecodedURL────────────────────────────────┐ │ http://127.0.0.1:8123/?query=SELECT 1 2+3 │ └───────────────────────────────────────────┘  "},{"title":"netloc​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#netloc","content":"Extracts network locality (username:password@host:port) from a URL. Syntax netloc(URL)  Arguments url — URL. String. Returned value username:password@host:port. Type: String. Example Query: SELECT netloc('http://paul@www.example.com:80/');  Result: ┌─netloc('http://paul@www.example.com:80/')─┐ │ paul@www.example.com:80 │ └───────────────────────────────────────────┘  "},{"title":"Functions that Remove Part of a URL​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#functions-that-remove-part-of-a-url","content":"If the URL does not have anything similar, the URL remains unchanged. "},{"title":"cutWWW​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#cutwww","content":"Removes no more than one ‘www.’ from the beginning of the URL’s domain, if present. "},{"title":"cutQueryString​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#cutquerystring","content":"Removes query string. The question mark is also removed. "},{"title":"cutFragment​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#cutfragment","content":"Removes the fragment identifier. The number sign is also removed. "},{"title":"cutQueryStringAndFragment​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#cutquerystringandfragment","content":"Removes the query string and fragment identifier. The question mark and number sign are also removed. "},{"title":"cutURLParameter(URL, name)​","type":1,"pageTitle":"Functions for Working with URLs","url":"docs/en/sql-reference/functions/url-functions#cuturlparameterurl-name","content":"Removes the ‘name’ URL parameter, if present. This function works under the assumption that the parameter name is encoded in the URL exactly the same way as in the passed argument. "},{"title":"Table Settings Manipulations","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/alter/setting","content":"","keywords":""},{"title":"MODIFY SETTING​","type":1,"pageTitle":"Table Settings Manipulations","url":"docs/en/sql-reference/statements/alter/setting#alter_modify_setting","content":"Changes table settings. Syntax MODIFY SETTING setting_name=value [, ...]  Example CREATE TABLE example_table (id UInt32, data String) ENGINE=MergeTree() ORDER BY id; ALTER TABLE example_table MODIFY SETTING max_part_loading_threads=8, max_parts_in_total=50000;  "},{"title":"RESET SETTING​","type":1,"pageTitle":"Table Settings Manipulations","url":"docs/en/sql-reference/statements/alter/setting#alter_reset_setting","content":"Resets table settings to their default values. If a setting is in a default state, then no action is taken. Syntax RESET SETTING setting_name [, ...]  Example CREATE TABLE example_table (id UInt32, data String) ENGINE=MergeTree() ORDER BY id SETTINGS max_part_loading_threads=8; ALTER TABLE example_table RESET SETTING max_part_loading_threads;  See Also MergeTree settings "},{"title":"Manipulating Projections","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/alter/projection","content":"Manipulating Projections The following operations with projections are available: ALTER TABLE [db].name ADD PROJECTION name ( SELECT &lt;COLUMN LIST EXPR&gt; [GROUP BY] [ORDER BY] ) - Adds projection description to tables metadata. ALTER TABLE [db].name DROP PROJECTION name - Removes projection description from tables metadata and deletes projection files from disk. Implemented as a mutation. ALTER TABLE [db.]table MATERIALIZE PROJECTION name IN PARTITION partition_name - The query rebuilds the projection name in the partition partition_name. Implemented as a mutation. ALTER TABLE [db.]table CLEAR PROJECTION name IN PARTITION partition_name - Deletes projection files from disk without removing description. Implemented as a mutation. The commands ADD, DROP and CLEAR are lightweight in a sense that they only change metadata or remove files. Also, they are replicated, syncing projections metadata via ZooKeeper. note Projection manipulation is supported only for tables with *MergeTree engine (including replicated variants).","keywords":""},{"title":"Manipulations with Table TTL","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/alter/ttl","content":"","keywords":""},{"title":"MODIFY TTL​","type":1,"pageTitle":"Manipulations with Table TTL","url":"docs/en/sql-reference/statements/alter/ttl#modify-ttl","content":"You can change table TTL with a request of the following form: ALTER TABLE table_name MODIFY TTL ttl_expression;  "},{"title":"REMOVE TTL​","type":1,"pageTitle":"Manipulations with Table TTL","url":"docs/en/sql-reference/statements/alter/ttl#remove-ttl","content":"TTL-property can be removed from table with the following query: ALTER TABLE table_name REMOVE TTL  Example Consider the table with table TTL: CREATE TABLE table_with_ttl ( event_time DateTime, UserID UInt64, Comment String ) ENGINE MergeTree() ORDER BY tuple() TTL event_time + INTERVAL 3 MONTH; SETTINGS min_bytes_for_wide_part = 0; INSERT INTO table_with_ttl VALUES (now(), 1, 'username1'); INSERT INTO table_with_ttl VALUES (now() - INTERVAL 4 MONTH, 2, 'username2');  Run OPTIMIZE to force TTL cleanup: OPTIMIZE TABLE table_with_ttl FINAL; SELECT * FROM table_with_ttl FORMAT PrettyCompact;  Second row was deleted from table. ┌─────────event_time────┬──UserID─┬─────Comment──┐ │ 2020-12-11 12:44:57 │ 1 │ username1 │ └───────────────────────┴─────────┴──────────────┘  Now remove table TTL with the following query: ALTER TABLE table_with_ttl REMOVE TTL;  Re-insert the deleted row and force the TTL cleanup again with OPTIMIZE: INSERT INTO table_with_ttl VALUES (now() - INTERVAL 4 MONTH, 2, 'username2'); OPTIMIZE TABLE table_with_ttl FINAL; SELECT * FROM table_with_ttl FORMAT PrettyCompact;  The TTL is no longer there, so the second row is not deleted: ┌─────────event_time────┬──UserID─┬─────Comment──┐ │ 2020-12-11 12:44:57 │ 1 │ username1 │ │ 2020-08-11 12:44:57 │ 2 │ username2 │ └───────────────────────┴─────────┴──────────────┘  See Also More about the TTL-expression.Modify column with TTL. "},{"title":"settings-profile","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/alter/settings-profile","content":"","keywords":""},{"title":"ALTER SETTINGS PROFILE​","type":1,"pageTitle":"settings-profile","url":"docs/en/sql-reference/statements/alter/settings-profile#alter-settings-profile-statement","content":"Changes settings profiles. Syntax: ALTER SETTINGS PROFILE [IF EXISTS] TO name1 [ON CLUSTER cluster_name1] [RENAME TO new_name1] [, name2 [ON CLUSTER cluster_name2] [RENAME TO new_name2] ...] [SETTINGS variable [= value] [MIN [=] min_value] [MAX [=] max_value] [READONLY|WRITABLE] | INHERIT 'profile_name'] [,...]  "},{"title":"ALTER USER","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/alter/user","content":"","keywords":""},{"title":"GRANTEES Clause​","type":1,"pageTitle":"ALTER USER","url":"docs/en/sql-reference/statements/alter/user#grantees","content":"Specifies users or roles which are allowed to receive privileges from this user on the condition this user has also all required access granted with GRANT OPTION. Options of the GRANTEES clause: user — Specifies a user this user can grant privileges to.role — Specifies a role this user can grant privileges to.ANY — This user can grant privileges to anyone. It's the default setting.NONE — This user can grant privileges to none. You can exclude any user or role by using the EXCEPT expression. For example, ALTER USER user1 GRANTEES ANY EXCEPT user2. It means if user1 has some privileges granted with GRANT OPTION it will be able to grant those privileges to anyone except user2. "},{"title":"Examples​","type":1,"pageTitle":"ALTER USER","url":"docs/en/sql-reference/statements/alter/user#alter-user-examples","content":"Set assigned roles as default: ALTER USER user DEFAULT ROLE role1, role2  If roles aren’t previously assigned to a user, ClickHouse throws an exception. Set all the assigned roles to default: ALTER USER user DEFAULT ROLE ALL  If a role is assigned to a user in the future, it will become default automatically. Set all the assigned roles to default, excepting role1 and role2: ALTER USER user DEFAULT ROLE ALL EXCEPT role1, role2  Allows the user with john account to grant his privileges to the user with jack account: ALTER USER john GRANTEES jack;  "},{"title":"Other Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/other-functions","content":"","keywords":""},{"title":"hostName()​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#hostname","content":"Returns a string with the name of the host that this function was performed on. For distributed processing, this is the name of the remote server host, if the function is performed on a remote server. If it is executed in the context of a distributed table, then it generates a normal column with values relevant to each shard. Otherwise it produces a constant value. "},{"title":"getMacro​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#getmacro","content":"Gets a named value from the macros section of the server configuration. Syntax getMacro(name);  Arguments name — Name to retrieve from the macros section. String. Returned value Value of the specified macro. Type: String. Example The example macros section in the server configuration file: &lt;macros&gt; &lt;test&gt;Value&lt;/test&gt; &lt;/macros&gt;  Query: SELECT getMacro('test');  Result: ┌─getMacro('test')─┐ │ Value │ └──────────────────┘  An alternative way to get the same value: SELECT * FROM system.macros WHERE macro = 'test';  ┌─macro─┬─substitution─┐ │ test │ Value │ └───────┴──────────────┘  "},{"title":"FQDN​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#fqdn","content":"Returns the fully qualified domain name. Syntax fqdn();  This function is case-insensitive. Returned value String with the fully qualified domain name. Type: String. Example Query: SELECT FQDN();  Result: ┌─FQDN()──────────────────────────┐ │ clickhouse.ru-central1.internal │ └─────────────────────────────────┘  "},{"title":"basename​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#basename","content":"Extracts the trailing part of a string after the last slash or backslash. This function if often used to extract the filename from a path. basename( expr )  Arguments expr — Expression resulting in a String type value. All the backslashes must be escaped in the resulting value. Returned Value A string that contains: The trailing part of a string after the last slash or backslash. If the input string contains a path ending with slash or backslash, for example, `/` or `c:\\`, the function returns an empty string. The original string if there are no slashes or backslashes. Example SELECT 'some/long/path/to/file' AS a, basename(a)  ┌─a──────────────────────┬─basename('some\\\\long\\\\path\\\\to\\\\file')─┐ │ some\\long\\path\\to\\file │ file │ └────────────────────────┴────────────────────────────────────────┘  SELECT 'some\\\\long\\\\path\\\\to\\\\file' AS a, basename(a)  ┌─a──────────────────────┬─basename('some\\\\long\\\\path\\\\to\\\\file')─┐ │ some\\long\\path\\to\\file │ file │ └────────────────────────┴────────────────────────────────────────┘  SELECT 'some-file-name' AS a, basename(a)  ┌─a──────────────┬─basename('some-file-name')─┐ │ some-file-name │ some-file-name │ └────────────────┴────────────────────────────┘  "},{"title":"visibleWidth(x)​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#visiblewidthx","content":"Calculates the approximate width when outputting values to the console in text format (tab-separated). This function is used by the system for implementing Pretty formats. NULL is represented as a string corresponding to NULL in Pretty formats. SELECT visibleWidth(NULL)  ┌─visibleWidth(NULL)─┐ │ 4 │ └────────────────────┘  "},{"title":"toTypeName(x)​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#totypenamex","content":"Returns a string containing the type name of the passed argument. If NULL is passed to the function as input, then it returns the Nullable(Nothing) type, which corresponds to an internal NULL representation in ClickHouse. "},{"title":"blockSize()​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#function-blocksize","content":"Gets the size of the block. In ClickHouse, queries are always run on blocks (sets of column parts). This function allows getting the size of the block that you called it for. "},{"title":"byteSize​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#function-bytesize","content":"Returns estimation of uncompressed byte size of its arguments in memory. Syntax byteSize(argument [, ...])  Arguments argument — Value. Returned value Estimation of byte size of the arguments in memory. Type: UInt64. Examples For String arguments the funtion returns the string length + 9 (terminating zero + length). Query: SELECT byteSize('string');  Result: ┌─byteSize('string')─┐ │ 15 │ └────────────────────┘  Query: CREATE TABLE test ( `key` Int32, `u8` UInt8, `u16` UInt16, `u32` UInt32, `u64` UInt64, `i8` Int8, `i16` Int16, `i32` Int32, `i64` Int64, `f32` Float32, `f64` Float64 ) ENGINE = MergeTree ORDER BY key; INSERT INTO test VALUES(1, 8, 16, 32, 64, -8, -16, -32, -64, 32.32, 64.64); SELECT key, byteSize(u8) AS `byteSize(UInt8)`, byteSize(u16) AS `byteSize(UInt16)`, byteSize(u32) AS `byteSize(UInt32)`, byteSize(u64) AS `byteSize(UInt64)`, byteSize(i8) AS `byteSize(Int8)`, byteSize(i16) AS `byteSize(Int16)`, byteSize(i32) AS `byteSize(Int32)`, byteSize(i64) AS `byteSize(Int64)`, byteSize(f32) AS `byteSize(Float32)`, byteSize(f64) AS `byteSize(Float64)` FROM test ORDER BY key ASC FORMAT Vertical;  Result: Row 1: ────── key: 1 byteSize(UInt8): 1 byteSize(UInt16): 2 byteSize(UInt32): 4 byteSize(UInt64): 8 byteSize(Int8): 1 byteSize(Int16): 2 byteSize(Int32): 4 byteSize(Int64): 8 byteSize(Float32): 4 byteSize(Float64): 8  If the function takes multiple arguments, it returns their combined byte size. Query: SELECT byteSize(NULL, 1, 0.3, '');  Result: ┌─byteSize(NULL, 1, 0.3, '')─┐ │ 19 │ └────────────────────────────┘  "},{"title":"materialize(x)​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#materializex","content":"Turns a constant into a full column containing just one value. In ClickHouse, full columns and constants are represented differently in memory. Functions work differently for constant arguments and normal arguments (different code is executed), although the result is almost always the same. This function is for debugging this behavior. "},{"title":"ignore(…)​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#ignore","content":"Accepts any arguments, including NULL. Always returns 0. However, the argument is still evaluated. This can be used for benchmarks. "},{"title":"sleep(seconds)​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#sleepseconds","content":"Sleeps ‘seconds’ seconds on each data block. You can specify an integer or a floating-point number. "},{"title":"sleepEachRow(seconds)​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#sleepeachrowseconds","content":"Sleeps ‘seconds’ seconds on each row. You can specify an integer or a floating-point number. "},{"title":"currentDatabase()​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#currentdatabase","content":"Returns the name of the current database. You can use this function in table engine parameters in a CREATE TABLE query where you need to specify the database. "},{"title":"currentUser()​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#other-function-currentuser","content":"Returns the login of current user. Login of user, that initiated query, will be returned in case distibuted query. SELECT currentUser();  Alias: user(), USER(). Returned values Login of current user.Login of user that initiated query in case of disributed query. Type: String. Example Query: SELECT currentUser();  Result: ┌─currentUser()─┐ │ default │ └───────────────┘  "},{"title":"isConstant​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#is-constant","content":"Checks whether the argument is a constant expression. A constant expression means an expression whose resulting value is known at the query analysis (i.e. before execution). For example, expressions over literals are constant expressions. The function is intended for development, debugging and demonstration. Syntax isConstant(x)  Arguments x — Expression to check. Returned values 1 — x is constant.0 — x is non-constant. Type: UInt8. Examples Query: SELECT isConstant(x + 1) FROM (SELECT 43 AS x)  Result: ┌─isConstant(plus(x, 1))─┐ │ 1 │ └────────────────────────┘  Query: WITH 3.14 AS pi SELECT isConstant(cos(pi))  Result: ┌─isConstant(cos(pi))─┐ │ 1 │ └─────────────────────┘  Query: SELECT isConstant(number) FROM numbers(1)  Result: ┌─isConstant(number)─┐ │ 0 │ └────────────────────┘  "},{"title":"isFinite(x)​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#isfinitex","content":"Accepts Float32 and Float64 and returns UInt8 equal to 1 if the argument is not infinite and not a NaN, otherwise 0. "},{"title":"isInfinite(x)​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#isinfinitex","content":"Accepts Float32 and Float64 and returns UInt8 equal to 1 if the argument is infinite, otherwise 0. Note that 0 is returned for a NaN. "},{"title":"ifNotFinite​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#ifnotfinite","content":"Checks whether floating point value is finite. Syntax ifNotFinite(x,y)  Arguments x — Value to be checked for infinity. Type: Float*.y — Fallback value. Type: Float*. Returned value x if x is finite.y if x is not finite. Example Query: SELECT 1/0 as infimum, ifNotFinite(infimum,42)  Result: ┌─infimum─┬─ifNotFinite(divide(1, 0), 42)─┐ │ inf │ 42 │ └─────────┴───────────────────────────────┘  You can get similar result by using ternary operator: isFinite(x) ? x : y. "},{"title":"isNaN(x)​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#isnanx","content":"Accepts Float32 and Float64 and returns UInt8 equal to 1 if the argument is a NaN, otherwise 0. "},{"title":"hasColumnInTable([‘hostname’[, ‘username’[, ‘password’]],] ‘database’, ‘table’, ‘column’)​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#hascolumnintablehostname-username-password-database-table-column","content":"Accepts constant strings: database name, table name, and column name. Returns a UInt8 constant expression equal to 1 if there is a column, otherwise 0. If the hostname parameter is set, the test will run on a remote server. The function throws an exception if the table does not exist. For elements in a nested data structure, the function checks for the existence of a column. For the nested data structure itself, the function returns 0. "},{"title":"bar​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#function-bar","content":"Allows building a unicode-art diagram. bar(x, min, max, width) draws a band with a width proportional to (x - min) and equal to width characters when x = max. Arguments x — Size to display.min, max — Integer constants. The value must fit in Int64.width — Constant, positive integer, can be fractional. The band is drawn with accuracy to one eighth of a symbol. Example: SELECT toHour(EventTime) AS h, count() AS c, bar(c, 0, 600000, 20) AS bar FROM test.hits GROUP BY h ORDER BY h ASC  ┌──h─┬──────c─┬─bar────────────────┐ │ 0 │ 292907 │ █████████▋ │ │ 1 │ 180563 │ ██████ │ │ 2 │ 114861 │ ███▋ │ │ 3 │ 85069 │ ██▋ │ │ 4 │ 68543 │ ██▎ │ │ 5 │ 78116 │ ██▌ │ │ 6 │ 113474 │ ███▋ │ │ 7 │ 170678 │ █████▋ │ │ 8 │ 278380 │ █████████▎ │ │ 9 │ 391053 │ █████████████ │ │ 10 │ 457681 │ ███████████████▎ │ │ 11 │ 493667 │ ████████████████▍ │ │ 12 │ 509641 │ ████████████████▊ │ │ 13 │ 522947 │ █████████████████▍ │ │ 14 │ 539954 │ █████████████████▊ │ │ 15 │ 528460 │ █████████████████▌ │ │ 16 │ 539201 │ █████████████████▊ │ │ 17 │ 523539 │ █████████████████▍ │ │ 18 │ 506467 │ ████████████████▊ │ │ 19 │ 520915 │ █████████████████▎ │ │ 20 │ 521665 │ █████████████████▍ │ │ 21 │ 542078 │ ██████████████████ │ │ 22 │ 493642 │ ████████████████▍ │ │ 23 │ 400397 │ █████████████▎ │ └────┴────────┴────────────────────┘  "},{"title":"transform​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#transform","content":"Transforms a value according to the explicitly defined mapping of some elements to other ones. There are two variations of this function: "},{"title":"transform(x, array_from, array_to, default)​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#transformx-array-from-array-to-default","content":"x – What to transform. array_from – Constant array of values for converting. array_to – Constant array of values to convert the values in ‘from’ to. default – Which value to use if ‘x’ is not equal to any of the values in ‘from’. array_from and array_to – Arrays of the same size. Types: transform(T, Array(T), Array(U), U) -&gt; U T and U can be numeric, string, or Date or DateTime types. Where the same letter is indicated (T or U), for numeric types these might not be matching types, but types that have a common type. For example, the first argument can have the Int64 type, while the second has the Array(UInt16) type. If the ‘x’ value is equal to one of the elements in the ‘array_from’ array, it returns the existing element (that is numbered the same) from the ‘array_to’ array. Otherwise, it returns ‘default’. If there are multiple matching elements in ‘array_from’, it returns one of the matches. Example: SELECT transform(SearchEngineID, [2, 3], ['Yandex', 'Google'], 'Other') AS title, count() AS c FROM test.hits WHERE SearchEngineID != 0 GROUP BY title ORDER BY c DESC  ┌─title─────┬──────c─┐ │ Yandex │ 498635 │ │ Google │ 229872 │ │ Other │ 104472 │ └───────────┴────────┘  "},{"title":"transform(x, array_from, array_to)​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#transformx-array-from-array-to","content":"Differs from the first variation in that the ‘default’ argument is omitted. If the ‘x’ value is equal to one of the elements in the ‘array_from’ array, it returns the matching element (that is numbered the same) from the ‘array_to’ array. Otherwise, it returns ‘x’. Types: transform(T, Array(T), Array(T)) -&gt; T Example: SELECT transform(domain(Referer), ['yandex.ru', 'google.ru', 'vk.com'], ['www.yandex', 'example.com']) AS s, count() AS c FROM test.hits GROUP BY domain(Referer) ORDER BY count() DESC LIMIT 10  ┌─s──────────────┬───────c─┐ │ │ 2906259 │ │ www.yandex │ 867767 │ │ ███████.ru │ 313599 │ │ mail.yandex.ru │ 107147 │ │ ██████.ru │ 100355 │ │ █████████.ru │ 65040 │ │ news.yandex.ru │ 64515 │ │ ██████.net │ 59141 │ │ example.com │ 57316 │ └────────────────┴─────────┘  "},{"title":"formatReadableSize(x)​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#formatreadablesizex","content":"Accepts the size (number of bytes). Returns a rounded size with a suffix (KiB, MiB, etc.) as a string. Example: SELECT arrayJoin([1, 1024, 1024*1024, 192851925]) AS filesize_bytes, formatReadableSize(filesize_bytes) AS filesize  ┌─filesize_bytes─┬─filesize───┐ │ 1 │ 1.00 B │ │ 1024 │ 1.00 KiB │ │ 1048576 │ 1.00 MiB │ │ 192851925 │ 183.92 MiB │ └────────────────┴────────────┘  "},{"title":"formatReadableQuantity(x)​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#formatreadablequantityx","content":"Accepts the number. Returns a rounded number with a suffix (thousand, million, billion, etc.) as a string. It is useful for reading big numbers by human. Example: SELECT arrayJoin([1024, 1234 * 1000, (4567 * 1000) * 1000, 98765432101234]) AS number, formatReadableQuantity(number) AS number_for_humans  ┌─────────number─┬─number_for_humans─┐ │ 1024 │ 1.02 thousand │ │ 1234000 │ 1.23 million │ │ 4567000000 │ 4.57 billion │ │ 98765432101234 │ 98.77 trillion │ └────────────────┴───────────────────┘  "},{"title":"formatReadableTimeDelta​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#formatreadabletimedelta","content":"Accepts the time delta in seconds. Returns a time delta with (year, month, day, hour, minute, second) as a string. Syntax formatReadableTimeDelta(column[, maximum_unit])  Arguments column — A column with numeric time delta.maximum_unit — Optional. Maximum unit to show. Acceptable values seconds, minutes, hours, days, months, years. Example: SELECT arrayJoin([100, 12345, 432546534]) AS elapsed, formatReadableTimeDelta(elapsed) AS time_delta  ┌────elapsed─┬─time_delta ─────────────────────────────────────────────────────┐ │ 100 │ 1 minute and 40 seconds │ │ 12345 │ 3 hours, 25 minutes and 45 seconds │ │ 432546534 │ 13 years, 8 months, 17 days, 7 hours, 48 minutes and 54 seconds │ └────────────┴─────────────────────────────────────────────────────────────────┘  SELECT arrayJoin([100, 12345, 432546534]) AS elapsed, formatReadableTimeDelta(elapsed, 'minutes') AS time_delta  ┌────elapsed─┬─time_delta ─────────────────────────────────────────────────────┐ │ 100 │ 1 minute and 40 seconds │ │ 12345 │ 205 minutes and 45 seconds │ │ 432546534 │ 7209108 minutes and 54 seconds │ └────────────┴─────────────────────────────────────────────────────────────────┘  "},{"title":"least(a, b)​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#leasta-b","content":"Returns the smallest value from a and b. "},{"title":"greatest(a, b)​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#greatesta-b","content":"Returns the largest value of a and b. "},{"title":"uptime()​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#uptime","content":"Returns the server’s uptime in seconds. If it is executed in the context of a distributed table, then it generates a normal column with values relevant to each shard. Otherwise it produces a constant value. "},{"title":"version()​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#version","content":"Returns the version of the server as a string. If it is executed in the context of a distributed table, then it generates a normal column with values relevant to each shard. Otherwise it produces a constant value. "},{"title":"buildId()​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#buildid","content":"Returns the build ID generated by a compiler for the running ClickHouse server binary. If it is executed in the context of a distributed table, then it generates a normal column with values relevant to each shard. Otherwise it produces a constant value. "},{"title":"blockNumber​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#blocknumber","content":"Returns the sequence number of the data block where the row is located. "},{"title":"rowNumberInBlock​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#function-rownumberinblock","content":"Returns the ordinal number of the row in the data block. Different data blocks are always recalculated. "},{"title":"rowNumberInAllBlocks()​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#rownumberinallblocks","content":"Returns the ordinal number of the row in the data block. This function only considers the affected data blocks. "},{"title":"neighbor​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#neighbor","content":"The window function that provides access to a row at a specified offset which comes before or after the current row of a given column. Syntax neighbor(column, offset[, default_value])  The result of the function depends on the affected data blocks and the order of data in the block. warning It can reach the neighbor rows only inside the currently processed data block. The rows order used during the calculation of neighbor can differ from the order of rows returned to the user. To prevent that you can make a subquery with ORDER BY and call the function from outside the subquery. Arguments column — A column name or scalar expression.offset — The number of rows forwards or backwards from the current row of column. Int64.default_value — Optional. The value to be returned if offset goes beyond the scope of the block. Type of data blocks affected. Returned values Value for column in offset distance from current row if offset value is not outside block bounds.Default value for column if offset value is outside block bounds. If default_value is given, then it will be used. Type: type of data blocks affected or default value type. Example Query: SELECT number, neighbor(number, 2) FROM system.numbers LIMIT 10;  Result: ┌─number─┬─neighbor(number, 2)─┐ │ 0 │ 2 │ │ 1 │ 3 │ │ 2 │ 4 │ │ 3 │ 5 │ │ 4 │ 6 │ │ 5 │ 7 │ │ 6 │ 8 │ │ 7 │ 9 │ │ 8 │ 0 │ │ 9 │ 0 │ └────────┴─────────────────────┘  Query: SELECT number, neighbor(number, 2, 999) FROM system.numbers LIMIT 10;  Result: ┌─number─┬─neighbor(number, 2, 999)─┐ │ 0 │ 2 │ │ 1 │ 3 │ │ 2 │ 4 │ │ 3 │ 5 │ │ 4 │ 6 │ │ 5 │ 7 │ │ 6 │ 8 │ │ 7 │ 9 │ │ 8 │ 999 │ │ 9 │ 999 │ └────────┴──────────────────────────┘  This function can be used to compute year-over-year metric value: Query: WITH toDate('2018-01-01') AS start_date SELECT toStartOfMonth(start_date + (number * 32)) AS month, toInt32(month) % 100 AS money, neighbor(money, -12) AS prev_year, round(prev_year / money, 2) AS year_over_year FROM numbers(16)  Result: ┌──────month─┬─money─┬─prev_year─┬─year_over_year─┐ │ 2018-01-01 │ 32 │ 0 │ 0 │ │ 2018-02-01 │ 63 │ 0 │ 0 │ │ 2018-03-01 │ 91 │ 0 │ 0 │ │ 2018-04-01 │ 22 │ 0 │ 0 │ │ 2018-05-01 │ 52 │ 0 │ 0 │ │ 2018-06-01 │ 83 │ 0 │ 0 │ │ 2018-07-01 │ 13 │ 0 │ 0 │ │ 2018-08-01 │ 44 │ 0 │ 0 │ │ 2018-09-01 │ 75 │ 0 │ 0 │ │ 2018-10-01 │ 5 │ 0 │ 0 │ │ 2018-11-01 │ 36 │ 0 │ 0 │ │ 2018-12-01 │ 66 │ 0 │ 0 │ │ 2019-01-01 │ 97 │ 32 │ 0.33 │ │ 2019-02-01 │ 28 │ 63 │ 2.25 │ │ 2019-03-01 │ 56 │ 91 │ 1.62 │ │ 2019-04-01 │ 87 │ 22 │ 0.25 │ └────────────┴───────┴───────────┴────────────────┘  "},{"title":"runningDifference(x)​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#other_functions-runningdifference","content":"Calculates the difference between successive row values ​​in the data block. Returns 0 for the first row and the difference from the previous row for each subsequent row. warning It can reach the previous row only inside the currently processed data block. The result of the function depends on the affected data blocks and the order of data in the block. The rows order used during the calculation of runningDifference can differ from the order of rows returned to the user. To prevent that you can make a subquery with ORDER BY and call the function from outside the subquery. Example: SELECT EventID, EventTime, runningDifference(EventTime) AS delta FROM ( SELECT EventID, EventTime FROM events WHERE EventDate = '2016-11-24' ORDER BY EventTime ASC LIMIT 5 )  ┌─EventID─┬───────────EventTime─┬─delta─┐ │ 1106 │ 2016-11-24 00:00:04 │ 0 │ │ 1107 │ 2016-11-24 00:00:05 │ 1 │ │ 1108 │ 2016-11-24 00:00:05 │ 0 │ │ 1109 │ 2016-11-24 00:00:09 │ 4 │ │ 1110 │ 2016-11-24 00:00:10 │ 1 │ └─────────┴─────────────────────┴───────┘  Please note - block size affects the result. With each new block, the runningDifference state is reset. SELECT number, runningDifference(number + 1) AS diff FROM numbers(100000) WHERE diff != 1  ┌─number─┬─diff─┐ │ 0 │ 0 │ └────────┴──────┘ ┌─number─┬─diff─┐ │ 65536 │ 0 │ └────────┴──────┘  set max_block_size=100000 -- default value is 65536! SELECT number, runningDifference(number + 1) AS diff FROM numbers(100000) WHERE diff != 1  ┌─number─┬─diff─┐ │ 0 │ 0 │ └────────┴──────┘  "},{"title":"runningDifferenceStartingWithFirstValue​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#runningdifferencestartingwithfirstvalue","content":"Same as for runningDifference, the difference is the value of the first row, returned the value of the first row, and each subsequent row returns the difference from the previous row. "},{"title":"runningConcurrency​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#runningconcurrency","content":"Calculates the number of concurrent events. Each event has a start time and an end time. The start time is included in the event, while the end time is excluded. Columns with a start time and an end time must be of the same data type. The function calculates the total number of active (concurrent) events for each event start time. warning Events must be ordered by the start time in ascending order. If this requirement is violated the function raises an exception. Every data block is processed separately. If events from different data blocks overlap then they can not be processed correctly. Syntax runningConcurrency(start, end)  Arguments start — A column with the start time of events. Date, DateTime, or DateTime64.end — A column with the end time of events. Date, DateTime, or DateTime64. Returned values The number of concurrent events at each event start time. Type: UInt32 Example Consider the table: ┌──────start─┬────────end─┐ │ 2021-03-03 │ 2021-03-11 │ │ 2021-03-06 │ 2021-03-12 │ │ 2021-03-07 │ 2021-03-08 │ │ 2021-03-11 │ 2021-03-12 │ └────────────┴────────────┘  Query: SELECT start, runningConcurrency(start, end) FROM example_table;  Result: ┌──────start─┬─runningConcurrency(start, end)─┐ │ 2021-03-03 │ 1 │ │ 2021-03-06 │ 2 │ │ 2021-03-07 │ 3 │ │ 2021-03-11 │ 2 │ └────────────┴────────────────────────────────┘  "},{"title":"MACNumToString(num)​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#macnumtostringnum","content":"Accepts a UInt64 number. Interprets it as a MAC address in big endian. Returns a string containing the corresponding MAC address in the format AA:BB:CC:DD:EE:FF (colon-separated numbers in hexadecimal form). "},{"title":"MACStringToNum(s)​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#macstringtonums","content":"The inverse function of MACNumToString. If the MAC address has an invalid format, it returns 0. "},{"title":"MACStringToOUI(s)​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#macstringtoouis","content":"Accepts a MAC address in the format AA:BB:CC:DD:EE:FF (colon-separated numbers in hexadecimal form). Returns the first three octets as a UInt64 number. If the MAC address has an invalid format, it returns 0. "},{"title":"getSizeOfEnumType​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#getsizeofenumtype","content":"Returns the number of fields in Enum. getSizeOfEnumType(value)  Arguments: value — Value of type Enum. Returned values The number of fields with Enum input values.An exception is thrown if the type is not Enum. Example SELECT getSizeOfEnumType( CAST('a' AS Enum8('a' = 1, 'b' = 2) ) ) AS x  ┌─x─┐ │ 2 │ └───┘  "},{"title":"blockSerializedSize​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#blockserializedsize","content":"Returns size on disk (without taking into account compression). blockSerializedSize(value[, value[, ...]])  Arguments value — Any value. Returned values The number of bytes that will be written to disk for block of values (without compression). Example Query: SELECT blockSerializedSize(maxState(1)) as x  Result: ┌─x─┐ │ 2 │ └───┘  "},{"title":"toColumnTypeName​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#tocolumntypename","content":"Returns the name of the class that represents the data type of the column in RAM. toColumnTypeName(value)  Arguments: value — Any type of value. Returned values A string with the name of the class that is used for representing the value data type in RAM. Example of the difference betweentoTypeName ' and ' toColumnTypeName SELECT toTypeName(CAST('2018-01-01 01:02:03' AS DateTime))  ┌─toTypeName(CAST('2018-01-01 01:02:03', 'DateTime'))─┐ │ DateTime │ └─────────────────────────────────────────────────────┘  SELECT toColumnTypeName(CAST('2018-01-01 01:02:03' AS DateTime))  ┌─toColumnTypeName(CAST('2018-01-01 01:02:03', 'DateTime'))─┐ │ Const(UInt32) │ └───────────────────────────────────────────────────────────┘  The example shows that the DateTime data type is stored in memory as Const(UInt32). "},{"title":"dumpColumnStructure​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#dumpcolumnstructure","content":"Outputs a detailed description of data structures in RAM dumpColumnStructure(value)  Arguments: value — Any type of value. Returned values A string describing the structure that is used for representing the value data type in RAM. Example SELECT dumpColumnStructure(CAST('2018-01-01 01:02:03', 'DateTime'))  ┌─dumpColumnStructure(CAST('2018-01-01 01:02:03', 'DateTime'))─┐ │ DateTime, Const(size = 1, UInt32(size = 1)) │ └──────────────────────────────────────────────────────────────┘  "},{"title":"defaultValueOfArgumentType​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#defaultvalueofargumenttype","content":"Outputs the default value for the data type. Does not include default values for custom columns set by the user. defaultValueOfArgumentType(expression)  Arguments: expression — Arbitrary type of value or an expression that results in a value of an arbitrary type. Returned values 0 for numbers.Empty string for strings.ᴺᵁᴸᴸ for Nullable. Example SELECT defaultValueOfArgumentType( CAST(1 AS Int8) )  ┌─defaultValueOfArgumentType(CAST(1, 'Int8'))─┐ │ 0 │ └─────────────────────────────────────────────┘  SELECT defaultValueOfArgumentType( CAST(1 AS Nullable(Int8) ) )  ┌─defaultValueOfArgumentType(CAST(1, 'Nullable(Int8)'))─┐ │ ᴺᵁᴸᴸ │ └───────────────────────────────────────────────────────┘  "},{"title":"defaultValueOfTypeName​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#defaultvalueoftypename","content":"Outputs the default value for given type name. Does not include default values for custom columns set by the user. defaultValueOfTypeName(type)  Arguments: type — A string representing a type name. Returned values 0 for numbers.Empty string for strings.ᴺᵁᴸᴸ for Nullable. Example SELECT defaultValueOfTypeName('Int8')  ┌─defaultValueOfTypeName('Int8')─┐ │ 0 │ └────────────────────────────────┘  SELECT defaultValueOfTypeName('Nullable(Int8)')  ┌─defaultValueOfTypeName('Nullable(Int8)')─┐ │ ᴺᵁᴸᴸ │ └──────────────────────────────────────────┘  "},{"title":"indexHint​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#indexhint","content":"The function is intended for debugging and introspection purposes. The function ignores it's argument and always returns 1. Arguments are not even evaluated. But for the purpose of index analysis, the argument of this function is analyzed as if it was present directly without being wrapped inside indexHint function. This allows to select data in index ranges by the corresponding condition but without further filtering by this condition. The index in ClickHouse is sparse and using indexHint will yield more data than specifying the same condition directly. Syntax SELECT * FROM table WHERE indexHint(&lt;expression&gt;)  Returned value Type: Uint8. Example Here is the example of test data from the table ontime. Input table: SELECT count() FROM ontime  ┌─count()─┐ │ 4276457 │ └─────────┘  The table has indexes on the fields (FlightDate, (Year, FlightDate)). Create a query, where the index is not used. Query: SELECT FlightDate AS k, count() FROM ontime GROUP BY k ORDER BY k  ClickHouse processed the entire table (Processed 4.28 million rows). Result: ┌──────────k─┬─count()─┐ │ 2017-01-01 │ 13970 │ │ 2017-01-02 │ 15882 │ ........................ │ 2017-09-28 │ 16411 │ │ 2017-09-29 │ 16384 │ │ 2017-09-30 │ 12520 │ └────────────┴─────────┘  To apply the index, select a specific date. Query: SELECT FlightDate AS k, count() FROM ontime WHERE k = '2017-09-15' GROUP BY k ORDER BY k  By using the index, ClickHouse processed a significantly smaller number of rows (Processed 32.74 thousand rows). Result: ┌──────────k─┬─count()─┐ │ 2017-09-15 │ 16428 │ └────────────┴─────────┘  Now wrap the expression k = '2017-09-15' into indexHint function. Query: SELECT FlightDate AS k, count() FROM ontime WHERE indexHint(k = '2017-09-15') GROUP BY k ORDER BY k ASC  ClickHouse used the index in the same way as the previous time (Processed 32.74 thousand rows). The expression k = '2017-09-15' was not used when generating the result. In examle the indexHint function allows to see adjacent dates. Result: ┌──────────k─┬─count()─┐ │ 2017-09-14 │ 7071 │ │ 2017-09-15 │ 16428 │ │ 2017-09-16 │ 1077 │ │ 2017-09-30 │ 8167 │ └────────────┴─────────┘  "},{"title":"replicate​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#other-functions-replicate","content":"Creates an array with a single value. Used for internal implementation of arrayJoin. SELECT replicate(x, arr);  Arguments: arr — Original array. ClickHouse creates a new array of the same length as the original and fills it with the value x.x — The value that the resulting array will be filled with. Returned value An array filled with the value x. Type: Array. Example Query: SELECT replicate(1, ['a', 'b', 'c'])  Result: ┌─replicate(1, ['a', 'b', 'c'])─┐ │ [1,1,1] │ └───────────────────────────────┘  "},{"title":"filesystemAvailable​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#filesystemavailable","content":"Returns amount of remaining space on the filesystem where the files of the databases located. It is always smaller than total free space (filesystemFree) because some space is reserved for OS. Syntax filesystemAvailable()  Returned value The amount of remaining space available in bytes. Type: UInt64. Example Query: SELECT formatReadableSize(filesystemAvailable()) AS &quot;Available space&quot;, toTypeName(filesystemAvailable()) AS &quot;Type&quot;;  Result: ┌─Available space─┬─Type───┐ │ 30.75 GiB │ UInt64 │ └─────────────────┴────────┘  "},{"title":"filesystemFree​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#filesystemfree","content":"Returns total amount of the free space on the filesystem where the files of the databases located. See also filesystemAvailable Syntax filesystemFree()  Returned value Amount of free space in bytes. Type: UInt64. Example Query: SELECT formatReadableSize(filesystemFree()) AS &quot;Free space&quot;, toTypeName(filesystemFree()) AS &quot;Type&quot;;  Result: ┌─Free space─┬─Type───┐ │ 32.39 GiB │ UInt64 │ └────────────┴────────┘  "},{"title":"filesystemCapacity​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#filesystemcapacity","content":"Returns the capacity of the filesystem in bytes. For evaluation, the path to the data directory must be configured. Syntax filesystemCapacity()  Returned value Capacity information of the filesystem in bytes. Type: UInt64. Example Query: SELECT formatReadableSize(filesystemCapacity()) AS &quot;Capacity&quot;, toTypeName(filesystemCapacity()) AS &quot;Type&quot;  Result: ┌─Capacity──┬─Type───┐ │ 39.32 GiB │ UInt64 │ └───────────┴────────┘  "},{"title":"initializeAggregation​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#initializeaggregation","content":"Calculates result of aggregate function based on single value. It is intended to use this function to initialize aggregate functions with combinator -State. You can create states of aggregate functions and insert them to columns of type AggregateFunction or use initialized aggregates as default values. Syntax initializeAggregation (aggregate_function, arg1, arg2, ..., argN)  Arguments aggregate_function — Name of the aggregation function to initialize. String.arg — Arguments of aggregate function. Returned value(s) Result of aggregation for every row passed to the function. The return type is the same as the return type of function, that initializeAgregation takes as first argument. Example Query: SELECT uniqMerge(state) FROM (SELECT initializeAggregation('uniqState', number % 3) AS state FROM numbers(10000));  Result: ┌─uniqMerge(state)─┐ │ 3 │ └──────────────────┘  Query: SELECT finalizeAggregation(state), toTypeName(state) FROM (SELECT initializeAggregation('sumState', number % 3) AS state FROM numbers(5));  Result: ┌─finalizeAggregation(state)─┬─toTypeName(state)─────────────┐ │ 0 │ AggregateFunction(sum, UInt8) │ │ 1 │ AggregateFunction(sum, UInt8) │ │ 2 │ AggregateFunction(sum, UInt8) │ │ 0 │ AggregateFunction(sum, UInt8) │ │ 1 │ AggregateFunction(sum, UInt8) │ └────────────────────────────┴───────────────────────────────┘  Example with AggregatingMergeTree table engine and AggregateFunction column: CREATE TABLE metrics ( key UInt64, value AggregateFunction(sum, UInt64) DEFAULT initializeAggregation('sumState', toUInt64(0)) ) ENGINE = AggregatingMergeTree ORDER BY key  INSERT INTO metrics VALUES (0, initializeAggregation('sumState', toUInt64(42)))  See Also arrayReduce "},{"title":"finalizeAggregation​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#function-finalizeaggregation","content":"Takes state of aggregate function. Returns result of aggregation (or finalized state when using-State combinator). Syntax finalizeAggregation(state)  Arguments state — State of aggregation. AggregateFunction. Returned value(s) Value/values that was aggregated. Type: Value of any types that was aggregated. Examples Query: SELECT finalizeAggregation(( SELECT countState(number) FROM numbers(10)));  Result: ┌─finalizeAggregation(_subquery16)─┐ │ 10 │ └──────────────────────────────────┘  Query: SELECT finalizeAggregation(( SELECT sumState(number) FROM numbers(10)));  Result: ┌─finalizeAggregation(_subquery20)─┐ │ 45 │ └──────────────────────────────────┘  Note that NULL values are ignored. Query: SELECT finalizeAggregation(arrayReduce('anyState', [NULL, 2, 3]));  Result: ┌─finalizeAggregation(arrayReduce('anyState', [NULL, 2, 3]))─┐ │ 2 │ └────────────────────────────────────────────────────────────┘  Combined example: Query: WITH initializeAggregation('sumState', number) AS one_row_sum_state SELECT number, finalizeAggregation(one_row_sum_state) AS one_row_sum, runningAccumulate(one_row_sum_state) AS cumulative_sum FROM numbers(10);  Result: ┌─number─┬─one_row_sum─┬─cumulative_sum─┐ │ 0 │ 0 │ 0 │ │ 1 │ 1 │ 1 │ │ 2 │ 2 │ 3 │ │ 3 │ 3 │ 6 │ │ 4 │ 4 │ 10 │ │ 5 │ 5 │ 15 │ │ 6 │ 6 │ 21 │ │ 7 │ 7 │ 28 │ │ 8 │ 8 │ 36 │ │ 9 │ 9 │ 45 │ └────────┴─────────────┴────────────────┘  See Also arrayReduceinitializeAggregation "},{"title":"runningAccumulate​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#runningaccumulate","content":"Accumulates states of an aggregate function for each row of a data block. warning The state is reset for each new data block. Syntax runningAccumulate(agg_state[, grouping]);  Arguments agg_state — State of the aggregate function. AggregateFunction.grouping — Grouping key. Optional. The state of the function is reset if the grouping value is changed. It can be any of the supported data types for which the equality operator is defined. Returned value Each resulting row contains a result of the aggregate function, accumulated for all the input rows from 0 to the current position. runningAccumulate resets states for each new data block or when the grouping value changes. Type depends on the aggregate function used. Examples Consider how you can use runningAccumulate to find the cumulative sum of numbers without and with grouping. Query: SELECT k, runningAccumulate(sum_k) AS res FROM (SELECT number as k, sumState(k) AS sum_k FROM numbers(10) GROUP BY k ORDER BY k);  Result: ┌─k─┬─res─┐ │ 0 │ 0 │ │ 1 │ 1 │ │ 2 │ 3 │ │ 3 │ 6 │ │ 4 │ 10 │ │ 5 │ 15 │ │ 6 │ 21 │ │ 7 │ 28 │ │ 8 │ 36 │ │ 9 │ 45 │ └───┴─────┘  The subquery generates sumState for every number from 0 to 9. sumState returns the state of the sum function that contains the sum of a single number. The whole query does the following: For the first row, runningAccumulate takes sumState(0) and returns 0.For the second row, the function merges sumState(0) and sumState(1) resulting in sumState(0 + 1), and returns 1 as a result.For the third row, the function merges sumState(0 + 1) and sumState(2) resulting in sumState(0 + 1 + 2), and returns 3 as a result.The actions are repeated until the block ends. The following example shows the groupping parameter usage: Query: SELECT grouping, item, runningAccumulate(state, grouping) AS res FROM ( SELECT toInt8(number / 4) AS grouping, number AS item, sumState(number) AS state FROM numbers(15) GROUP BY item ORDER BY item ASC );  Result: ┌─grouping─┬─item─┬─res─┐ │ 0 │ 0 │ 0 │ │ 0 │ 1 │ 1 │ │ 0 │ 2 │ 3 │ │ 0 │ 3 │ 6 │ │ 1 │ 4 │ 4 │ │ 1 │ 5 │ 9 │ │ 1 │ 6 │ 15 │ │ 1 │ 7 │ 22 │ │ 2 │ 8 │ 8 │ │ 2 │ 9 │ 17 │ │ 2 │ 10 │ 27 │ │ 2 │ 11 │ 38 │ │ 3 │ 12 │ 12 │ │ 3 │ 13 │ 25 │ │ 3 │ 14 │ 39 │ └──────────┴──────┴─────┘  As you can see, runningAccumulate merges states for each group of rows separately. "},{"title":"joinGet​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#joinget","content":"The function lets you extract data from the table the same way as from a dictionary. Gets data from Join tables using the specified join key. Only supports tables created with the ENGINE = Join(ANY, LEFT, &lt;join_keys&gt;) statement. Syntax joinGet(join_storage_table_name, `value_column`, join_keys)  Arguments join_storage_table_name — an identifier indicates where search is performed. The identifier is searched in the default database (see parameter default_database in the config file). To override the default database, use the USE db_name or specify the database and the table through the separator db_name.db_table, see the example.value_column — name of the column of the table that contains required data.join_keys — list of keys. Returned value Returns list of values corresponded to list of keys. If certain does not exist in source table then 0 or null will be returned based on join_use_nulls setting. More info about join_use_nulls in Join operation. Example Input table: CREATE DATABASE db_test CREATE TABLE db_test.id_val(`id` UInt32, `val` UInt32) ENGINE = Join(ANY, LEFT, id) SETTINGS join_use_nulls = 1 INSERT INTO db_test.id_val VALUES (1,11)(2,12)(4,13)  ┌─id─┬─val─┐ │ 4 │ 13 │ │ 2 │ 12 │ │ 1 │ 11 │ └────┴─────┘  Query: SELECT joinGet(db_test.id_val,'val',toUInt32(number)) from numbers(4) SETTINGS join_use_nulls = 1  Result: ┌─joinGet(db_test.id_val, 'val', toUInt32(number))─┐ │ 0 │ │ 11 │ │ 12 │ │ 0 │ └──────────────────────────────────────────────────┘  "},{"title":"modelEvaluate(model_name, …)​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#function-modelevaluate","content":"Evaluate external model. Accepts a model name and model arguments. Returns Float64. "},{"title":"throwIf(x[, custom_message])​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#throwifx-custom-message","content":"Throw an exception if the argument is non zero. custom_message - is an optional parameter: a constant string, provides an error message SELECT throwIf(number = 3, 'Too many') FROM numbers(10);  ↙ Progress: 0.00 rows, 0.00 B (0.00 rows/s., 0.00 B/s.) Received exception from server (version 19.14.1): Code: 395. DB::Exception: Received from localhost:9000. DB::Exception: Too many.  "},{"title":"identity​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#identity","content":"Returns the same value that was used as its argument. Used for debugging and testing, allows to cancel using index, and get the query performance of a full scan. When query is analyzed for possible use of index, the analyzer does not look inside identity functions. Also constant folding is not applied too. Syntax identity(x)  Example Query: SELECT identity(42)  Result: ┌─identity(42)─┐ │ 42 │ └──────────────┘  "},{"title":"randomPrintableASCII​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#randomascii","content":"Generates a string with a random set of ASCII printable characters. Syntax randomPrintableASCII(length)  Arguments length — Resulting string length. Positive integer. If you pass `length &lt; 0`, behavior of the function is undefined.  Returned value String with a random set of ASCII printable characters. Type: String Example SELECT number, randomPrintableASCII(30) as str, length(str) FROM system.numbers LIMIT 3  ┌─number─┬─str────────────────────────────┬─length(randomPrintableASCII(30))─┐ │ 0 │ SuiCOSTvC0csfABSw=UcSzp2.`rv8x │ 30 │ │ 1 │ 1Ag NlJ &amp;RCN:*&gt;HVPG;PE-nO&quot;SUFD │ 30 │ │ 2 │ /&quot;+&lt;&quot;wUTh:=LjJ Vm!c&amp;hI*m#XTfzz │ 30 │ └────────┴────────────────────────────────┴──────────────────────────────────┘  "},{"title":"randomString​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#randomstring","content":"Generates a binary string of the specified length filled with random bytes (including zero bytes). Syntax randomString(length)  Arguments length — String length. Positive integer. Returned value String filled with random bytes. Type: String. Example Query: SELECT randomString(30) AS str, length(str) AS len FROM numbers(2) FORMAT Vertical;  Result: Row 1: ────── str: 3 G : pT ?w тi k aV f6 len: 30 Row 2: ────── str: 9 ,] ^ ) ]?? 8 len: 30  See Also generateRandomrandomPrintableASCII "},{"title":"randomFixedString​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#randomfixedstring","content":"Generates a binary string of the specified length filled with random bytes (including zero bytes). Syntax randomFixedString(length);  Arguments length — String length in bytes. UInt64. Returned value(s) String filled with random bytes. Type: FixedString. Example Query: SELECT randomFixedString(13) as rnd, toTypeName(rnd)  Result: ┌─rnd──────┬─toTypeName(randomFixedString(13))─┐ │ j▒h㋖HɨZ'▒ │ FixedString(13) │ └──────────┴───────────────────────────────────┘  "},{"title":"randomStringUTF8​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#randomstringutf8","content":"Generates a random string of a specified length. Result string contains valid UTF-8 code points. The value of code points may be outside of the range of assigned Unicode. Syntax randomStringUTF8(length);  Arguments length — Required length of the resulting string in code points. UInt64. Returned value(s) UTF-8 random string. Type: String. Example Query: SELECT randomStringUTF8(13)  Result: ┌─randomStringUTF8(13)─┐ │ 𘤗𙉝д兠庇󡅴󱱎󦐪􂕌𔊹𓰛 │ └──────────────────────┘  "},{"title":"getSetting​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#getSetting","content":"Returns the current value of a custom setting. Syntax getSetting('custom_setting');  Parameter custom_setting — The setting name. String. Returned value The setting current value. Example SET custom_a = 123; SELECT getSetting('custom_a');  Result 123  See Also Custom Settings "},{"title":"isDecimalOverflow​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#is-decimal-overflow","content":"Checks whether the Decimal value is out of its (or specified) precision. Syntax isDecimalOverflow(d, [p])  Arguments d — value. Decimal.p — precision. Optional. If omitted, the initial precision of the first argument is used. Using of this paratemer could be helpful for data extraction to another DBMS or file. UInt8. Returned values 1 — Decimal value has more digits then it's precision allow,0 — Decimal value satisfies the specified precision. Example Query: SELECT isDecimalOverflow(toDecimal32(1000000000, 0), 9), isDecimalOverflow(toDecimal32(1000000000, 0)), isDecimalOverflow(toDecimal32(-1000000000, 0), 9), isDecimalOverflow(toDecimal32(-1000000000, 0));  Result: 1 1 1 1  "},{"title":"countDigits​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#count-digits","content":"Returns number of decimal digits you need to represent the value. Syntax countDigits(x)  Arguments x — Int or Decimal value. Returned value Number of digits. Type: UInt8. note For Decimal values takes into account their scales: calculates result over underlying integer type which is (value * scale). For example: countDigits(42) = 2, countDigits(42.000) = 5, countDigits(0.04200) = 4. I.e. you may check decimal overflow for Decimal64 with countDecimal(x) &gt; 18. It's a slow variant of isDecimalOverflow. Example Query: SELECT countDigits(toDecimal32(1, 9)), countDigits(toDecimal32(-1, 9)), countDigits(toDecimal64(1, 18)), countDigits(toDecimal64(-1, 18)), countDigits(toDecimal128(1, 38)), countDigits(toDecimal128(-1, 38));  Result: 10 10 19 19 39 39  "},{"title":"errorCodeToName​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#error-code-to-name","content":"Returned value Variable name for the error code. Type: LowCardinality(String). Syntax errorCodeToName(1)  Result: UNSUPPORTED_METHOD  "},{"title":"tcpPort​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#tcpPort","content":"Returns native interface TCP port number listened by this server. If it is executed in the context of a distributed table, then it generates a normal column, otherwise it produces a constant value. Syntax tcpPort()  Arguments None. Returned value The TCP port number. Type: UInt16. Example Query: SELECT tcpPort();  Result: ┌─tcpPort()─┐ │ 9000 │ └───────────┘  See Also tcp_port "},{"title":"currentProfiles​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#current-profiles","content":"Returns a list of the current settings profiles for the current user. The command SET PROFILE could be used to change the current setting profile. If the command SET PROFILE was not used the function returns the profiles specified at the current user's definition (see CREATE USER). Syntax currentProfiles()  Returned value List of the current user settings profiles.  Type: Array(String). "},{"title":"enabledProfiles​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#enabled-profiles","content":"Returns settings profiles, assigned to the current user both explicitly and implicitly. Explicitly assigned profiles are the same as returned by the currentProfiles function. Implicitly assigned profiles include parent profiles of other assigned profiles, profiles assigned via granted roles, profiles assigned via their own settings, and the main default profile (see the default_profile section in the main server configuration file). Syntax enabledProfiles()  Returned value List of the enabled settings profiles.  Type: Array(String). "},{"title":"defaultProfiles​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#default-profiles","content":"Returns all the profiles specified at the current user's definition (see CREATE USER statement). Syntax defaultProfiles()  Returned value List of the default settings profiles.  Type: Array(String). "},{"title":"currentRoles​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#current-roles","content":"Returns the names of the roles which are current for the current user. The current roles can be changed by the SET ROLE statement. If the SET ROLE statement was not used, the function currentRoles returns the same as defaultRoles. Syntax currentRoles()  Returned value List of the current roles for the current user.  Type: Array(String). "},{"title":"enabledRoles​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#enabled-roles","content":"Returns the names of the current roles and the roles, granted to some of the current roles. Syntax enabledRoles()  Returned value List of the enabled roles for the current user.  Type: Array(String). "},{"title":"defaultRoles​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#default-roles","content":"Returns the names of the roles which are enabled by default for the current user when he logins. Initially these are all roles granted to the current user (see GRANT), but that can be changed with the SET DEFAULT ROLE statement. Syntax defaultRoles()  Returned value List of the default roles for the current user.  Type: Array(String). "},{"title":"getServerPort​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#getserverport","content":"Returns the number of the server port. When the port is not used by the server, throws an exception. Syntax getServerPort(port_name)  Arguments port_name — The name of the server port. String. Possible values: 'tcp_port''tcp_port_secure''http_port''https_port''interserver_http_port''interserver_https_port''mysql_port''postgresql_port''grpc_port''prometheus.port' Returned value The number of the server port. Type: UInt16. Example Query: SELECT getServerPort('tcp_port');  Result: ┌─getServerPort('tcp_port')─┐ │ 9000 │ └───────────────────────────┘  "},{"title":"queryID​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#query-id","content":"Returns the ID of the current query. Other parameters of a query can be extracted from the system.query_log table via query_id. In contrast to initialQueryID function, queryID can return different results on different shards (see example). Syntax queryID()  Returned value The ID of the current query. Type: String Example Query: CREATE TABLE tmp (str String) ENGINE = Log; INSERT INTO tmp (*) VALUES ('a'); SELECT count(DISTINCT t) FROM (SELECT queryID() AS t FROM remote('127.0.0.{1..3}', currentDatabase(), 'tmp') GROUP BY queryID());  Result: ┌─count()─┐ │ 3 │ └─────────┘  "},{"title":"initialQueryID​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#initial-query-id","content":"Returns the ID of the initial current query. Other parameters of a query can be extracted from the system.query_log table via initial_query_id. In contrast to queryID function, initialQueryID returns the same results on different shards (see example). Syntax initialQueryID()  Returned value The ID of the initial current query. Type: String Example Query: CREATE TABLE tmp (str String) ENGINE = Log; INSERT INTO tmp (*) VALUES ('a'); SELECT count(DISTINCT t) FROM (SELECT initialQueryID() AS t FROM remote('127.0.0.{1..3}', currentDatabase(), 'tmp') GROUP BY queryID());  Result: ┌─count()─┐ │ 1 │ └─────────┘  "},{"title":"shardNum​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#shard-num","content":"Returns the index of a shard which processes a part of data for a distributed query. Indices are started from 1. If a query is not distributed then constant value 0 is returned. Syntax shardNum()  Returned value Shard index or constant 0. Type: UInt32. Example In the following example a configuration with two shards is used. The query is executed on the system.one table on every shard. Query: CREATE TABLE shard_num_example (dummy UInt8) ENGINE=Distributed(test_cluster_two_shards_localhost, system, one, dummy); SELECT dummy, shardNum(), shardCount() FROM shard_num_example;  Result: ┌─dummy─┬─shardNum()─┬─shardCount()─┐ │ 0 │ 2 │ 2 │ │ 0 │ 1 │ 2 │ └───────┴────────────┴──────────────┘  See Also Distributed Table Engine "},{"title":"shardCount​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#shard-count","content":"Returns the total number of shards for a distributed query. If a query is not distributed then constant value 0 is returned. Syntax shardCount()  Returned value Total number of shards or 0. Type: UInt32. See Also shardNum() function example also contains shardCount() function call. "},{"title":"getOSKernelVersion​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#getoskernelversion","content":"Returns a string with the current OS kernel version. Syntax getOSKernelVersion()  Arguments None. Returned value The current OS kernel version. Type: String. Example Query: SELECT getOSKernelVersion();  Result: ┌─getOSKernelVersion()────┐ │ Linux 4.15.0-55-generic │ └─────────────────────────┘  "},{"title":"zookeeperSessionUptime​","type":1,"pageTitle":"Other Functions","url":"docs/en/sql-reference/functions/other-functions#zookeepersessionuptime","content":"Returns the uptime of the current ZooKeeper session in seconds. Syntax zookeeperSessionUptime()  Arguments None. Returned value Uptime of the current ZooKeeper session in seconds. Type: UInt32. Example Query: SELECT zookeeperSessionUptime();  Result: ┌─zookeeperSessionUptime()─┐ │ 286 │ └──────────────────────────┘  "},{"title":"Type Conversion Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/functions/type-conversion-functions","content":"","keywords":""},{"title":"Common Issues of Numeric Conversions​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#numeric-conversion-issues","content":"When you convert a value from one to another data type, you should remember that in common case, it is an unsafe operation that can lead to a data loss. A data loss can occur if you try to fit value from a larger data type to a smaller data type, or if you convert values between different data types. ClickHouse has the same behavior as C++ programs. "},{"title":"toInt(8|16|32|64|128|256)​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#toint8163264128256","content":"Converts an input value to the Int data type. This function family includes: toInt8(expr) — Results in the Int8 data type.toInt16(expr) — Results in the Int16 data type.toInt32(expr) — Results in the Int32 data type.toInt64(expr) — Results in the Int64 data type.toInt128(expr) — Results in the Int128 data type.toInt256(expr) — Results in the Int256 data type. Arguments expr — Expression returning a number or a string with the decimal representation of a number. Binary, octal, and hexadecimal representations of numbers are not supported. Leading zeroes are stripped. Returned value Integer value in the Int8, Int16, Int32, Int64, Int128 or Int256 data type. Functions use rounding towards zero, meaning they truncate fractional digits of numbers. The behavior of functions for the NaN and Inf arguments is undefined. Remember about numeric convertions issues, when using the functions. Example Query: SELECT toInt64(nan), toInt32(32), toInt16('16'), toInt8(8.8);  Result: ┌─────────toInt64(nan)─┬─toInt32(32)─┬─toInt16('16')─┬─toInt8(8.8)─┐ │ -9223372036854775808 │ 32 │ 16 │ 8 │ └──────────────────────┴─────────────┴───────────────┴─────────────┘  "},{"title":"toInt(8|16|32|64|128|256)OrZero​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#toint8163264orzero","content":"It takes an argument of type String and tries to parse it into Int (8 | 16 | 32 | 64 | 128 | 256). If failed, returns 0. Example Query: SELECT toInt64OrZero('123123'), toInt8OrZero('123qwe123');  Result: ┌─toInt64OrZero('123123')─┬─toInt8OrZero('123qwe123')─┐ │ 123123 │ 0 │ └─────────────────────────┴───────────────────────────┘  "},{"title":"toInt(8|16|32|64|128|256)OrNull​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#toint8163264128256ornull","content":"It takes an argument of type String and tries to parse it into Int (8 | 16 | 32 | 64 | 128 | 256). If failed, returns NULL. Example Query: SELECT toInt64OrNull('123123'), toInt8OrNull('123qwe123');  Result: ┌─toInt64OrNull('123123')─┬─toInt8OrNull('123qwe123')─┐ │ 123123 │ ᴺᵁᴸᴸ │ └─────────────────────────┴───────────────────────────┘  "},{"title":"toInt(8|16|32|64|128|256)OrDefault​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#toint8163264128256orDefault","content":"It takes an argument of type String and tries to parse it into Int (8 | 16 | 32 | 64 | 128 | 256). If failed, returns the default type value. Example Query: SELECT toInt64OrDefault('123123', cast('-1' as Int64)), toInt8OrDefault('123qwe123', cast('-1' as Int8));  Result: ┌─toInt64OrDefault('123123', CAST('-1', 'Int64'))─┬─toInt8OrDefault('123qwe123', CAST('-1', 'Int8'))─┐ │ 123123 │ -1 │ └─────────────────────────────────────────────────┴──────────────────────────────────────────────────┘  "},{"title":"toUInt(8|16|32|64|256)​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#touint8163264256","content":"Converts an input value to the UInt data type. This function family includes: toUInt8(expr) — Results in the UInt8 data type.toUInt16(expr) — Results in the UInt16 data type.toUInt32(expr) — Results in the UInt32 data type.toUInt64(expr) — Results in the UInt64 data type.toUInt256(expr) — Results in the UInt256 data type. Arguments expr — Expression returning a number or a string with the decimal representation of a number. Binary, octal, and hexadecimal representations of numbers are not supported. Leading zeroes are stripped. Returned value Integer value in the UInt8, UInt16, UInt32, UInt64 or UInt256 data type. Functions use rounding towards zero, meaning they truncate fractional digits of numbers. The behavior of functions for negative agruments and for the NaN and Inf arguments is undefined. If you pass a string with a negative number, for example '-32', ClickHouse raises an exception. Remember about numeric convertions issues, when using the functions. Example Query: SELECT toUInt64(nan), toUInt32(-32), toUInt16('16'), toUInt8(8.8);  Result: ┌───────toUInt64(nan)─┬─toUInt32(-32)─┬─toUInt16('16')─┬─toUInt8(8.8)─┐ │ 9223372036854775808 │ 4294967264 │ 16 │ 8 │ └─────────────────────┴───────────────┴────────────────┴──────────────┘  "},{"title":"toUInt(8|16|32|64|256)OrZero​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#touint8163264256orzero","content":""},{"title":"toUInt(8|16|32|64|256)OrNull​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#touint8163264256ornull","content":""},{"title":"toUInt(8|16|32|64|256)OrDefault​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#touint8163264256ordefault","content":""},{"title":"toFloat(32|64)​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#tofloat3264","content":""},{"title":"toFloat(32|64)OrZero​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#tofloat3264orzero","content":""},{"title":"toFloat(32|64)OrNull​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#tofloat3264ornull","content":""},{"title":"toFloat(32|64)OrDefault​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#tofloat3264ordefault","content":""},{"title":"toDate​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#todate","content":"Alias: DATE. "},{"title":"toDateOrZero​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#todateorzero","content":""},{"title":"toDateOrNull​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#todateornull","content":""},{"title":"toDateOrDefault​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#todateordefault","content":""},{"title":"toDateTime​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#todatetime","content":""},{"title":"toDateTimeOrZero​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#todatetimeorzero","content":""},{"title":"toDateTimeOrNull​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#todatetimeornull","content":""},{"title":"toDateTimeOrDefault​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#todatetimeordefault","content":""},{"title":"toDate32​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#todate32","content":"Converts the argument to the Date32 data type. If the value is outside the range returns the border values supported by Date32. If the argument has Date type, borders of Date are taken into account. Syntax toDate32(expr)  Arguments expr — The value. String, UInt32 or Date. Returned value A calendar date. Type: Date32. Example The value is within the range: SELECT toDate32('1955-01-01') AS value, toTypeName(value);  ┌──────value─┬─toTypeName(toDate32('1925-01-01'))─┐ │ 1955-01-01 │ Date32 │ └────────────┴────────────────────────────────────┘  The value is outside the range: SELECT toDate32('1924-01-01') AS value, toTypeName(value);  ┌──────value─┬─toTypeName(toDate32('1925-01-01'))─┐ │ 1925-01-01 │ Date32 │ └────────────┴────────────────────────────────────┘  With Date-type argument: SELECT toDate32(toDate('1924-01-01')) AS value, toTypeName(value);  ┌──────value─┬─toTypeName(toDate32(toDate('1924-01-01')))─┐ │ 1970-01-01 │ Date32 │ └────────────┴────────────────────────────────────────────┘  "},{"title":"toDate32OrZero​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#todate32-or-zero","content":"The same as toDate32 but returns the min value of Date32 if invalid argument is received. Example Query: SELECT toDate32OrZero('1924-01-01'), toDate32OrZero('');  Result: ┌─toDate32OrZero('1924-01-01')─┬─toDate32OrZero('')─┐ │ 1925-01-01 │ 1925-01-01 │ └──────────────────────────────┴────────────────────┘  "},{"title":"toDate32OrNull​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#todate32-or-null","content":"The same as toDate32 but returns NULL if invalid argument is received. Example Query: SELECT toDate32OrNull('1955-01-01'), toDate32OrNull('');  Result: ┌─toDate32OrNull('1955-01-01')─┬─toDate32OrNull('')─┐ │ 1955-01-01 │ ᴺᵁᴸᴸ │ └──────────────────────────────┴────────────────────┘  "},{"title":"toDate32OrDefault​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#todate32-or-default","content":"Converts the argument to the Date32 data type. If the value is outside the range returns the lower border value supported by Date32. If the argument has Date type, borders of Date are taken into account. Returns default value if invalid argument is received. Example Query: SELECT toDate32OrDefault('1930-01-01', toDate32('2020-01-01')), toDate32OrDefault('xx1930-01-01', toDate32('2020-01-01'));  Result: ┌─toDate32OrDefault('1930-01-01', toDate32('2020-01-01'))─┬─toDate32OrDefault('xx1930-01-01', toDate32('2020-01-01'))─┐ │ 1930-01-01 │ 2020-01-01 │ └─────────────────────────────────────────────────────────┴───────────────────────────────────────────────────────────┘  "},{"title":"toDecimal(32|64|128|256)​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#todecimal3264128256","content":"Converts value to the Decimal data type with precision of S. The value can be a number or a string. The S (scale) parameter specifies the number of decimal places. toDecimal32(value, S)toDecimal64(value, S)toDecimal128(value, S)toDecimal256(value, S) "},{"title":"toDecimal(32|64|128|256)OrNull​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#todecimal3264128256ornull","content":"Converts an input string to a Nullable(Decimal(P,S)) data type value. This family of functions include: toDecimal32OrNull(expr, S) — Results in Nullable(Decimal32(S)) data type.toDecimal64OrNull(expr, S) — Results in Nullable(Decimal64(S)) data type.toDecimal128OrNull(expr, S) — Results in Nullable(Decimal128(S)) data type.toDecimal256OrNull(expr, S) — Results in Nullable(Decimal256(S)) data type. These functions should be used instead of toDecimal*() functions, if you prefer to get a NULL value instead of an exception in the event of an input value parsing error. Arguments expr — Expression, returns a value in the String data type. ClickHouse expects the textual representation of the decimal number. For example, '1.111'.S — Scale, the number of decimal places in the resulting value. Returned value A value in the Nullable(Decimal(P,S)) data type. The value contains: Number with S decimal places, if ClickHouse interprets the input string as a number.NULL, if ClickHouse can’t interpret the input string as a number or if the input number contains more than S decimal places. Examples Query: SELECT toDecimal32OrNull(toString(-1.111), 5) AS val, toTypeName(val);  Result: ┌────val─┬─toTypeName(toDecimal32OrNull(toString(-1.111), 5))─┐ │ -1.111 │ Nullable(Decimal(9, 5)) │ └────────┴────────────────────────────────────────────────────┘  Query: SELECT toDecimal32OrNull(toString(-1.111), 2) AS val, toTypeName(val);  Result: ┌──val─┬─toTypeName(toDecimal32OrNull(toString(-1.111), 2))─┐ │ ᴺᵁᴸᴸ │ Nullable(Decimal(9, 2)) │ └──────┴────────────────────────────────────────────────────┘  "},{"title":"toDecimal(32|64|128|256)OrDefault​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#todecimal3264128256ordefault","content":"Converts an input string to a Decimal(P,S) data type value. This family of functions include: toDecimal32OrDefault(expr, S) — Results in Decimal32(S) data type.toDecimal64OrDefault(expr, S) — Results in Decimal64(S) data type.toDecimal128OrDefault(expr, S) — Results in Decimal128(S) data type.toDecimal256OrDefault(expr, S) — Results in Decimal256(S) data type. These functions should be used instead of toDecimal*() functions, if you prefer to get a default value instead of an exception in the event of an input value parsing error. Arguments expr — Expression, returns a value in the String data type. ClickHouse expects the textual representation of the decimal number. For example, '1.111'.S — Scale, the number of decimal places in the resulting value. Returned value A value in the Decimal(P,S) data type. The value contains: Number with S decimal places, if ClickHouse interprets the input string as a number.Default Decimal(P,S) data type value, if ClickHouse can’t interpret the input string as a number or if the input number contains more than S decimal places. Examples Query: SELECT toDecimal32OrDefault(toString(-1.111), 5) AS val, toTypeName(val);  Result: ┌────val─┬─toTypeName(toDecimal32OrDefault(toString(-1.111), 5))─┐ │ -1.111 │ Decimal(9, 5) │ └────────┴───────────────────────────────────────────────────────┘  Query: SELECT toDecimal32OrDefault(toString(-1.111), 2) AS val, toTypeName(val);  Result: ┌─val─┬─toTypeName(toDecimal32OrDefault(toString(-1.111), 2))─┐ │ 0 │ Decimal(9, 2) │ └─────┴───────────────────────────────────────────────────────┘  "},{"title":"toDecimal(32|64|128|256)OrZero​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#todecimal3264128256orzero","content":"Converts an input value to the Decimal(P,S) data type. This family of functions include: toDecimal32OrZero( expr, S) — Results in Decimal32(S) data type.toDecimal64OrZero( expr, S) — Results in Decimal64(S) data type.toDecimal128OrZero( expr, S) — Results in Decimal128(S) data type.toDecimal256OrZero( expr, S) — Results in Decimal256(S) data type. These functions should be used instead of toDecimal*() functions, if you prefer to get a 0 value instead of an exception in the event of an input value parsing error. Arguments expr — Expression, returns a value in the String data type. ClickHouse expects the textual representation of the decimal number. For example, '1.111'.S — Scale, the number of decimal places in the resulting value. Returned value A value in the Nullable(Decimal(P,S)) data type. The value contains: Number with S decimal places, if ClickHouse interprets the input string as a number.0 with S decimal places, if ClickHouse can’t interpret the input string as a number or if the input number contains more than S decimal places. Example Query: SELECT toDecimal32OrZero(toString(-1.111), 5) AS val, toTypeName(val);  Result: ┌────val─┬─toTypeName(toDecimal32OrZero(toString(-1.111), 5))─┐ │ -1.111 │ Decimal(9, 5) │ └────────┴────────────────────────────────────────────────────┘  Query: SELECT toDecimal32OrZero(toString(-1.111), 2) AS val, toTypeName(val);  Result: ┌──val─┬─toTypeName(toDecimal32OrZero(toString(-1.111), 2))─┐ │ 0.00 │ Decimal(9, 2) │ └──────┴────────────────────────────────────────────────────┘  "},{"title":"toString​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#tostring","content":"Functions for converting between numbers, strings (but not fixed strings), dates, and dates with times. All these functions accept one argument. When converting to or from a string, the value is formatted or parsed using the same rules as for the TabSeparated format (and almost all other text formats). If the string can’t be parsed, an exception is thrown and the request is canceled. When converting dates to numbers or vice versa, the date corresponds to the number of days since the beginning of the Unix epoch. When converting dates with times to numbers or vice versa, the date with time corresponds to the number of seconds since the beginning of the Unix epoch. The date and date-with-time formats for the toDate/toDateTime functions are defined as follows: YYYY-MM-DD YYYY-MM-DD hh:mm:ss  As an exception, if converting from UInt32, Int32, UInt64, or Int64 numeric types to Date, and if the number is greater than or equal to 65536, the number is interpreted as a Unix timestamp (and not as the number of days) and is rounded to the date. This allows support for the common occurrence of writing ‘toDate(unix_timestamp)’, which otherwise would be an error and would require writing the more cumbersome ‘toDate(toDateTime(unix_timestamp))’. Conversion between a date and date with time is performed the natural way: by adding a null time or dropping the time. Conversion between numeric types uses the same rules as assignments between different numeric types in C++. Additionally, the toString function of the DateTime argument can take a second String argument containing the name of the time zone. Example: Asia/Yekaterinburg In this case, the time is formatted according to the specified time zone. Example Query: SELECT now() AS now_local, toString(now(), 'Asia/Yekaterinburg') AS now_yekat;  Result: ┌───────────now_local─┬─now_yekat───────────┐ │ 2016-06-15 00:11:21 │ 2016-06-15 02:11:21 │ └─────────────────────┴─────────────────────┘  Also see the toUnixTimestamp function. "},{"title":"toFixedString(s, N)​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#tofixedstrings-n","content":"Converts a String type argument to a FixedString(N) type (a string with fixed length N). N must be a constant. If the string has fewer bytes than N, it is padded with null bytes to the right. If the string has more bytes than N, an exception is thrown. "},{"title":"toStringCutToZero(s)​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#tostringcuttozeros","content":"Accepts a String or FixedString argument. Returns the String with the content truncated at the first zero byte found. Example Query: SELECT toFixedString('foo', 8) AS s, toStringCutToZero(s) AS s_cut;  Result: ┌─s─────────────┬─s_cut─┐ │ foo\\0\\0\\0\\0\\0 │ foo │ └───────────────┴───────┘  Query: SELECT toFixedString('foo\\0bar', 8) AS s, toStringCutToZero(s) AS s_cut;  Result: ┌─s──────────┬─s_cut─┐ │ foo\\0bar\\0 │ foo │ └────────────┴───────┘  "},{"title":"reinterpretAsUInt(8|16|32|64)​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#reinterpretasuint8163264","content":""},{"title":"reinterpretAsInt(8|16|32|64)​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#reinterpretasint8163264","content":""},{"title":"reinterpretAsFloat(32|64)​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#reinterpretasfloat3264","content":""},{"title":"reinterpretAsDate​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#reinterpretasdate","content":""},{"title":"reinterpretAsDateTime​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#reinterpretasdatetime","content":"These functions accept a string and interpret the bytes placed at the beginning of the string as a number in host order (little endian). If the string isn’t long enough, the functions work as if the string is padded with the necessary number of null bytes. If the string is longer than needed, the extra bytes are ignored. A date is interpreted as the number of days since the beginning of the Unix Epoch, and a date with time is interpreted as the number of seconds since the beginning of the Unix Epoch. "},{"title":"reinterpretAsString​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#type_conversion_functions-reinterpretAsString","content":"This function accepts a number or date or date with time, and returns a string containing bytes representing the corresponding value in host order (little endian). Null bytes are dropped from the end. For example, a UInt32 type value of 255 is a string that is one byte long. "},{"title":"reinterpretAsFixedString​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#reinterpretasfixedstring","content":"This function accepts a number or date or date with time, and returns a FixedString containing bytes representing the corresponding value in host order (little endian). Null bytes are dropped from the end. For example, a UInt32 type value of 255 is a FixedString that is one byte long. "},{"title":"reinterpretAsUUID​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#reinterpretasuuid","content":"Accepts 16 bytes string and returns UUID containing bytes representing the corresponding value in network byte order (big-endian). If the string isn't long enough, the function works as if the string is padded with the necessary number of null bytes to the end. If the string longer than 16 bytes, the extra bytes at the end are ignored. Syntax reinterpretAsUUID(fixed_string)  Arguments fixed_string — Big-endian byte string. FixedString. Returned value The UUID type value. UUID. Examples String to UUID. Query: SELECT reinterpretAsUUID(reverse(unhex('000102030405060708090a0b0c0d0e0f')));  Result: ┌─reinterpretAsUUID(reverse(unhex('000102030405060708090a0b0c0d0e0f')))─┐ │ 08090a0b-0c0d-0e0f-0001-020304050607 │ └───────────────────────────────────────────────────────────────────────┘  Going back and forth from String to UUID. Query: WITH generateUUIDv4() AS uuid, identity(lower(hex(reverse(reinterpretAsString(uuid))))) AS str, reinterpretAsUUID(reverse(unhex(str))) AS uuid2 SELECT uuid = uuid2;  Result: ┌─equals(uuid, uuid2)─┐ │ 1 │ └─────────────────────┘  "},{"title":"reinterpret(x, T)​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#type_conversion_function-reinterpret","content":"Uses the same source in-memory bytes sequence for x value and reinterprets it to destination type. Syntax reinterpret(x, type)  Arguments x — Any type.type — Destination type. String. Returned value Destination type value. Examples Query: SELECT reinterpret(toInt8(-1), 'UInt8') as int_to_uint, reinterpret(toInt8(1), 'Float32') as int_to_float, reinterpret('1', 'UInt32') as string_to_int;  Result: ┌─int_to_uint─┬─int_to_float─┬─string_to_int─┐ │ 255 │ 1e-45 │ 49 │ └─────────────┴──────────────┴───────────────┘  "},{"title":"CAST(x, T)​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#type_conversion_function-cast","content":"Converts an input value to the specified data type. Unlike the reinterpret function, CAST tries to present the same value using the new data type. If the conversion can not be done then an exception is raised. Several syntax variants are supported. Syntax CAST(x, T) CAST(x AS t) x::t  Arguments x — A value to convert. May be of any type.T — The name of the target data type. String.t — The target data type. Returned value Converted value. note If the input value does not fit the bounds of the target type, the result overflows. For example, CAST(-1, 'UInt8') returns 255. Examples Query: SELECT CAST(toInt8(-1), 'UInt8') AS cast_int_to_uint, CAST(1.5 AS Decimal(3,2)) AS cast_float_to_decimal, '1'::Int32 AS cast_string_to_int;  Result: ┌─cast_int_to_uint─┬─cast_float_to_decimal─┬─cast_string_to_int─┐ │ 255 │ 1.50 │ 1 │ └──────────────────┴───────────────────────┴────────────────────┘  Query: SELECT '2016-06-15 23:00:00' AS timestamp, CAST(timestamp AS DateTime) AS datetime, CAST(timestamp AS Date) AS date, CAST(timestamp, 'String') AS string, CAST(timestamp, 'FixedString(22)') AS fixed_string;  Result: ┌─timestamp───────────┬────────────datetime─┬───────date─┬─string──────────────┬─fixed_string──────────────┐ │ 2016-06-15 23:00:00 │ 2016-06-15 23:00:00 │ 2016-06-15 │ 2016-06-15 23:00:00 │ 2016-06-15 23:00:00\\0\\0\\0 │ └─────────────────────┴─────────────────────┴────────────┴─────────────────────┴───────────────────────────┘  Conversion to FixedString(N) only works for arguments of type String or FixedString. Type conversion to Nullable and back is supported. Example Query: SELECT toTypeName(x) FROM t_null;  Result: ┌─toTypeName(x)─┐ │ Int8 │ │ Int8 │ └───────────────┘  Query: SELECT toTypeName(CAST(x, 'Nullable(UInt16)')) FROM t_null;  Result: ┌─toTypeName(CAST(x, 'Nullable(UInt16)'))─┐ │ Nullable(UInt16) │ │ Nullable(UInt16) │ └─────────────────────────────────────────┘  See also cast_keep_nullable setting "},{"title":"accurateCast(x, T)​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#type_conversion_function-accurate-cast","content":"Converts x to the T data type. The difference from cast(x, T) is that accurateCast does not allow overflow of numeric types during cast if type value x does not fit the bounds of type T. For example, accurateCast(-1, 'UInt8') throws an exception. Example Query: SELECT cast(-1, 'UInt8') as uint8;  Result: ┌─uint8─┐ │ 255 │ └───────┘  Query: SELECT accurateCast(-1, 'UInt8') as uint8;  Result: Code: 70. DB::Exception: Received from localhost:9000. DB::Exception: Value in column Int8 cannot be safely converted into type UInt8: While processing accurateCast(-1, 'UInt8') AS uint8.  "},{"title":"accurateCastOrNull(x, T)​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#type_conversion_function-accurate-cast_or_null","content":"Converts input value x to the specified data type T. Always returns Nullable type and returns NULL if the casted value is not representable in the target type. Syntax accurateCastOrNull(x, T)  Parameters x — Input value.T — The name of the returned data type. Returned value The value, converted to the specified data type T. Example Query: SELECT toTypeName(accurateCastOrNull(5, 'UInt8'));  Result: ┌─toTypeName(accurateCastOrNull(5, 'UInt8'))─┐ │ Nullable(UInt8) │ └────────────────────────────────────────────┘  Query: SELECT accurateCastOrNull(-1, 'UInt8') as uint8, accurateCastOrNull(128, 'Int8') as int8, accurateCastOrNull('Test', 'FixedString(2)') as fixed_string;  Result: ┌─uint8─┬─int8─┬─fixed_string─┐ │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ └───────┴──────┴──────────────┘  "},{"title":"accurateCastOrDefault(x, T[, default_value])​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#type_conversion_function-accurate-cast_or_default","content":"Converts input value x to the specified data type T. Returns default type value or default_value if specified if the casted value is not representable in the target type. Syntax accurateCastOrDefault(x, T)  Parameters x — Input value.T — The name of the returned data type.default_value — Default value of returned data type. Returned value The value converted to the specified data type T. Example Query: SELECT toTypeName(accurateCastOrDefault(5, 'UInt8'));  Result: ┌─toTypeName(accurateCastOrDefault(5, 'UInt8'))─┐ │ UInt8 │ └───────────────────────────────────────────────┘  Query: SELECT accurateCastOrDefault(-1, 'UInt8') as uint8, accurateCastOrDefault(-1, 'UInt8', 5) as uint8_default, accurateCastOrDefault(128, 'Int8') as int8, accurateCastOrDefault(128, 'Int8', 5) as int8_default, accurateCastOrDefault('Test', 'FixedString(2)') as fixed_string, accurateCastOrDefault('Test', 'FixedString(2)', 'Te') as fixed_string_default;  Result: ┌─uint8─┬─uint8_default─┬─int8─┬─int8_default─┬─fixed_string─┬─fixed_string_default─┐ │ 0 │ 5 │ 0 │ 5 │ │ Te │ └───────┴───────────────┴──────┴──────────────┴──────────────┴──────────────────────┘  "},{"title":"toInterval(Year|Quarter|Month|Week|Day|Hour|Minute|Second)​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#function-tointerval","content":"Converts a Number type argument to an Interval data type. Syntax toIntervalSecond(number) toIntervalMinute(number) toIntervalHour(number) toIntervalDay(number) toIntervalWeek(number) toIntervalMonth(number) toIntervalQuarter(number) toIntervalYear(number)  Arguments number — Duration of interval. Positive integer number. Returned values The value in Interval data type. Example Query: WITH toDate('2019-01-01') AS date, INTERVAL 1 WEEK AS interval_week, toIntervalWeek(1) AS interval_to_week SELECT date + interval_week, date + interval_to_week;  Result: ┌─plus(date, interval_week)─┬─plus(date, interval_to_week)─┐ │ 2019-01-08 │ 2019-01-08 │ └───────────────────────────┴──────────────────────────────┘  "},{"title":"parseDateTimeBestEffort​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#parsedatetimebesteffort","content":""},{"title":"parseDateTime32BestEffort​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#parsedatetime32besteffort","content":"Converts a date and time in the String representation to DateTime data type. The function parses ISO 8601, RFC 1123 - 5.2.14 RFC-822 Date and Time Specification, ClickHouse’s and some other date and time formats. Syntax parseDateTimeBestEffort(time_string [, time_zone])  Arguments time_string — String containing a date and time to convert. String.time_zone — Time zone. The function parses time_string according to the time zone. String. Supported non-standard formats A string containing 9..10 digit unix timestamp.A string with a date and a time component: YYYYMMDDhhmmss, DD/MM/YYYY hh:mm:ss, DD-MM-YY hh:mm, YYYY-MM-DD hh:mm:ss, etc.A string with a date, but no time component: YYYY, YYYYMM, YYYY*MM, DD/MM/YYYY, DD-MM-YY etc.A string with a day and time: DD, DD hh, DD hh:mm. In this case YYYY-MM are substituted as 2000-01.A string that includes the date and time along with time zone offset information: YYYY-MM-DD hh:mm:ss ±h:mm, etc. For example, 2020-12-12 17:36:00 -5:00. For all of the formats with separator the function parses months names expressed by their full name or by the first three letters of a month name. Examples: 24/DEC/18, 24-Dec-18, 01-September-2018. Returned value time_string converted to the DateTime data type. Examples Query: SELECT parseDateTimeBestEffort('12/12/2020 12:12:57') AS parseDateTimeBestEffort;  Result: ┌─parseDateTimeBestEffort─┐ │ 2020-12-12 12:12:57 │ └─────────────────────────┘  Query: SELECT parseDateTimeBestEffort('Sat, 18 Aug 2018 07:22:16 GMT', 'Asia/Istanbul') AS parseDateTimeBestEffort;  Result: ┌─parseDateTimeBestEffort─┐ │ 2018-08-18 10:22:16 │ └─────────────────────────┘  Query: SELECT parseDateTimeBestEffort('1284101485') AS parseDateTimeBestEffort;  Result: ┌─parseDateTimeBestEffort─┐ │ 2015-07-07 12:04:41 │ └─────────────────────────┘  Query: SELECT parseDateTimeBestEffort('2018-12-12 10:12:12') AS parseDateTimeBestEffort;  Result: ┌─parseDateTimeBestEffort─┐ │ 2018-12-12 10:12:12 │ └─────────────────────────┘  Query: SELECT parseDateTimeBestEffort('10 20:19');  Result: ┌─parseDateTimeBestEffort('10 20:19')─┐ │ 2000-01-10 20:19:00 │ └─────────────────────────────────────┘  See Also ISO 8601 announcement by @xkcdRFC 1123toDatetoDateTime "},{"title":"parseDateTimeBestEffortUS​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#parsedatetimebesteffortUS","content":"This function is similar to parseDateTimeBestEffort, the only difference is that this function prefers US date format (MM/DD/YYYY etc.) in case of ambiguity. Syntax parseDateTimeBestEffortUS(time_string [, time_zone])  Arguments time_string — String containing a date and time to convert. String.time_zone — Time zone. The function parses time_string according to the time zone. String. Supported non-standard formats A string containing 9..10 digit unix timestamp.A string with a date and a time component: YYYYMMDDhhmmss, MM/DD/YYYY hh:mm:ss, MM-DD-YY hh:mm, YYYY-MM-DD hh:mm:ss, etc.A string with a date, but no time component: YYYY, YYYYMM, YYYY*MM, MM/DD/YYYY, MM-DD-YY etc.A string with a day and time: DD, DD hh, DD hh:mm. In this case, YYYY-MM are substituted as 2000-01.A string that includes the date and time along with time zone offset information: YYYY-MM-DD hh:mm:ss ±h:mm, etc. For example, 2020-12-12 17:36:00 -5:00. Returned value time_string converted to the DateTime data type. Examples Query: SELECT parseDateTimeBestEffortUS('09/12/2020 12:12:57') AS parseDateTimeBestEffortUS;  Result: ┌─parseDateTimeBestEffortUS─┐ │ 2020-09-12 12:12:57 │ └─────────────────────────——┘  Query: SELECT parseDateTimeBestEffortUS('09-12-2020 12:12:57') AS parseDateTimeBestEffortUS;  Result: ┌─parseDateTimeBestEffortUS─┐ │ 2020-09-12 12:12:57 │ └─────────────────────────——┘  Query: SELECT parseDateTimeBestEffortUS('09.12.2020 12:12:57') AS parseDateTimeBestEffortUS;  Result: ┌─parseDateTimeBestEffortUS─┐ │ 2020-09-12 12:12:57 │ └─────────────────────────——┘  "},{"title":"parseDateTimeBestEffortOrNull​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#parsedatetimebesteffortornull","content":""},{"title":"parseDateTime32BestEffortOrNull​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#parsedatetime32besteffortornull","content":"Same as for parseDateTimeBestEffort except that it returns NULL when it encounters a date format that cannot be processed. "},{"title":"parseDateTimeBestEffortOrZero​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#parsedatetimebesteffortorzero","content":""},{"title":"parseDateTime32BestEffortOrZero​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#parsedatetime32besteffortorzero","content":"Same as for parseDateTimeBestEffort except that it returns zero date or zero date time when it encounters a date format that cannot be processed. "},{"title":"parseDateTimeBestEffortUSOrNull​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#parsedatetimebesteffortusornull","content":"Same as parseDateTimeBestEffortUS function except that it returns NULL when it encounters a date format that cannot be processed. Syntax parseDateTimeBestEffortUSOrNull(time_string[, time_zone])  Parameters time_string — String containing a date or date with time to convert. The date must be in the US date format (MM/DD/YYYY, etc). String.time_zone — Timezone. The function parses time_string according to the timezone. Optional. String. Supported non-standard formats A string containing 9..10 digit unix timestamp.A string with a date and a time components: YYYYMMDDhhmmss, MM/DD/YYYY hh:mm:ss, MM-DD-YY hh:mm, YYYY-MM-DD hh:mm:ss, etc.A string with a date, but no time component: YYYY, YYYYMM, YYYY*MM, MM/DD/YYYY, MM-DD-YY, etc.A string with a day and time: DD, DD hh, DD hh:mm. In this case, YYYY-MM are substituted with 2000-01.A string that includes date and time along with timezone offset information: YYYY-MM-DD hh:mm:ss ±h:mm, etc. For example, 2020-12-12 17:36:00 -5:00. Returned values time_string converted to the DateTime data type.NULL if the input string cannot be converted to the DateTime data type. Examples Query: SELECT parseDateTimeBestEffortUSOrNull('02/10/2021 21:12:57') AS parseDateTimeBestEffortUSOrNull;  Result: ┌─parseDateTimeBestEffortUSOrNull─┐ │ 2021-02-10 21:12:57 │ └─────────────────────────────────┘  Query: SELECT parseDateTimeBestEffortUSOrNull('02-10-2021 21:12:57 GMT', 'Asia/Istanbul') AS parseDateTimeBestEffortUSOrNull;  Result: ┌─parseDateTimeBestEffortUSOrNull─┐ │ 2021-02-11 00:12:57 │ └─────────────────────────────────┘  Query: SELECT parseDateTimeBestEffortUSOrNull('02.10.2021') AS parseDateTimeBestEffortUSOrNull;  Result: ┌─parseDateTimeBestEffortUSOrNull─┐ │ 2021-02-10 00:00:00 │ └─────────────────────────────────┘  Query: SELECT parseDateTimeBestEffortUSOrNull('10.2021') AS parseDateTimeBestEffortUSOrNull;  Result: ┌─parseDateTimeBestEffortUSOrNull─┐ │ ᴺᵁᴸᴸ │ └─────────────────────────────────┘  "},{"title":"parseDateTimeBestEffortUSOrZero​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#parsedatetimebesteffortusorzero","content":"Same as parseDateTimeBestEffortUS function except that it returns zero date (1970-01-01) or zero date with time (1970-01-01 00:00:00) when it encounters a date format that cannot be processed. Syntax parseDateTimeBestEffortUSOrZero(time_string[, time_zone])  Parameters time_string — String containing a date or date with time to convert. The date must be in the US date format (MM/DD/YYYY, etc). String.time_zone — Timezone. The function parses time_string according to the timezone. Optional. String. Supported non-standard formats A string containing 9..10 digit unix timestamp.A string with a date and a time components: YYYYMMDDhhmmss, MM/DD/YYYY hh:mm:ss, MM-DD-YY hh:mm, YYYY-MM-DD hh:mm:ss, etc.A string with a date, but no time component: YYYY, YYYYMM, YYYY*MM, MM/DD/YYYY, MM-DD-YY, etc.A string with a day and time: DD, DD hh, DD hh:mm. In this case, YYYY-MM are substituted with 2000-01.A string that includes date and time along with timezone offset information: YYYY-MM-DD hh:mm:ss ±h:mm, etc. For example, 2020-12-12 17:36:00 -5:00. Returned values time_string converted to the DateTime data type.Zero date or zero date with time if the input string cannot be converted to the DateTime data type. Examples Query: SELECT parseDateTimeBestEffortUSOrZero('02/10/2021 21:12:57') AS parseDateTimeBestEffortUSOrZero;  Result: ┌─parseDateTimeBestEffortUSOrZero─┐ │ 2021-02-10 21:12:57 │ └─────────────────────────────────┘  Query: SELECT parseDateTimeBestEffortUSOrZero('02-10-2021 21:12:57 GMT', 'Asia/Istanbul') AS parseDateTimeBestEffortUSOrZero;  Result: ┌─parseDateTimeBestEffortUSOrZero─┐ │ 2021-02-11 00:12:57 │ └─────────────────────────────────┘  Query: SELECT parseDateTimeBestEffortUSOrZero('02.10.2021') AS parseDateTimeBestEffortUSOrZero;  Result: ┌─parseDateTimeBestEffortUSOrZero─┐ │ 2021-02-10 00:00:00 │ └─────────────────────────────────┘  Query: SELECT parseDateTimeBestEffortUSOrZero('02.2021') AS parseDateTimeBestEffortUSOrZero;  Result: ┌─parseDateTimeBestEffortUSOrZero─┐ │ 1970-01-01 00:00:00 │ └─────────────────────────────────┘  "},{"title":"parseDateTime64BestEffort​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#parsedatetime64besteffort","content":"Same as parseDateTimeBestEffort function but also parse milliseconds and microseconds and returns DateTime data type. Syntax parseDateTime64BestEffort(time_string [, precision [, time_zone]])  Parameters time_string — String containing a date or date with time to convert. String.precision — Required precision. 3 — for milliseconds, 6 — for microseconds. Default — 3. Optional. UInt8.time_zone — Timezone. The function parses time_string according to the timezone. Optional. String. Returned value time_string converted to the DateTime data type. Examples Query: SELECT parseDateTime64BestEffort('2021-01-01') AS a, toTypeName(a) AS t UNION ALL SELECT parseDateTime64BestEffort('2021-01-01 01:01:00.12346') AS a, toTypeName(a) AS t UNION ALL SELECT parseDateTime64BestEffort('2021-01-01 01:01:00.12346',6) AS a, toTypeName(a) AS t UNION ALL SELECT parseDateTime64BestEffort('2021-01-01 01:01:00.12346',3,'Asia/Istanbul') AS a, toTypeName(a) AS t FORMAT PrettyCompactMonoBlock;  Result: ┌──────────────────────────a─┬─t──────────────────────────────┐ │ 2021-01-01 01:01:00.123000 │ DateTime64(3) │ │ 2021-01-01 00:00:00.000000 │ DateTime64(3) │ │ 2021-01-01 01:01:00.123460 │ DateTime64(6) │ │ 2020-12-31 22:01:00.123000 │ DateTime64(3, 'Asia/Istanbul') │ └────────────────────────────┴────────────────────────────────┘  "},{"title":"parseDateTime64BestEffortOrNull​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#parsedatetime32besteffortornull","content":"Same as for parseDateTime64BestEffort except that it returns NULL when it encounters a date format that cannot be processed. "},{"title":"parseDateTime64BestEffortOrZero​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#parsedatetime64besteffortorzero","content":"Same as for parseDateTime64BestEffort except that it returns zero date or zero date time when it encounters a date format that cannot be processed. "},{"title":"toLowCardinality​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#tolowcardinality","content":"Converts input parameter to the LowCardianlity version of same data type. To convert data from the LowCardinality data type use the CAST function. For example, CAST(x as String). Syntax toLowCardinality(expr)  Arguments expr — Expression resulting in one of the supported data types. Returned values Result of expr. Type: LowCardinality(expr_result_type) Example Query: SELECT toLowCardinality('1');  Result: ┌─toLowCardinality('1')─┐ │ 1 │ └───────────────────────┘  "},{"title":"toUnixTimestamp64Milli​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#tounixtimestamp64milli","content":""},{"title":"toUnixTimestamp64Micro​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#tounixtimestamp64micro","content":""},{"title":"toUnixTimestamp64Nano​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#tounixtimestamp64nano","content":"Converts a DateTime64 to a Int64 value with fixed sub-second precision. Input value is scaled up or down appropriately depending on it precision. note The output value is a timestamp in UTC, not in the timezone of DateTime64. Syntax toUnixTimestamp64Milli(value)  Arguments value — DateTime64 value with any precision. Returned value value converted to the Int64 data type. Examples Query: WITH toDateTime64('2019-09-16 19:20:12.345678910', 6) AS dt64 SELECT toUnixTimestamp64Milli(dt64);  Result: ┌─toUnixTimestamp64Milli(dt64)─┐ │ 1568650812345 │ └──────────────────────────────┘  Query: WITH toDateTime64('2019-09-16 19:20:12.345678910', 6) AS dt64 SELECT toUnixTimestamp64Nano(dt64);  Result: ┌─toUnixTimestamp64Nano(dt64)─┐ │ 1568650812345678000 │ └─────────────────────────────┘  "},{"title":"fromUnixTimestamp64Milli​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#fromunixtimestamp64milli","content":""},{"title":"fromUnixTimestamp64Micro​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#fromunixtimestamp64micro","content":""},{"title":"fromUnixTimestamp64Nano​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#fromunixtimestamp64nano","content":"Converts an Int64 to a DateTime64 value with fixed sub-second precision and optional timezone. Input value is scaled up or down appropriately depending on it’s precision. Please note that input value is treated as UTC timestamp, not timestamp at given (or implicit) timezone. Syntax fromUnixTimestamp64Milli(value [, ti])  Arguments value — Int64 value with any precision.timezone — String (optional) timezone name of the result. Returned value value converted to the DateTime64 data type. Example Query: WITH CAST(1234567891011, 'Int64') AS i64 SELECT fromUnixTimestamp64Milli(i64, 'UTC');  Result: ┌─fromUnixTimestamp64Milli(i64, 'UTC')─┐ │ 2009-02-13 23:31:31.011 │ └──────────────────────────────────────┘  "},{"title":"formatRow​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#formatrow","content":"Converts arbitrary expressions into a string via given format. Syntax formatRow(format, x, y, ...)  Arguments format — Text format. For example, CSV, TSV.x,y, ... — Expressions. Returned value A formatted string (for text formats it's usually terminated with the new line character). Example Query: SELECT formatRow('CSV', number, 'good') FROM numbers(3);  Result: ┌─formatRow('CSV', number, 'good')─┐ │ 0,&quot;good&quot; │ │ 1,&quot;good&quot; │ │ 2,&quot;good&quot; │ └──────────────────────────────────┘  "},{"title":"formatRowNoNewline​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#formatrownonewline","content":"Converts arbitrary expressions into a string via given format. The function trims the last \\n if any. Syntax formatRowNoNewline(format, x, y, ...)  Arguments format — Text format. For example, CSV, TSV.x,y, ... — Expressions. Returned value A formatted string. Example Query: SELECT formatRowNoNewline('CSV', number, 'good') FROM numbers(3);  Result: ┌─formatRowNoNewline('CSV', number, 'good')─┐ │ 0,&quot;good&quot; │ │ 1,&quot;good&quot; │ │ 2,&quot;good&quot; │ └───────────────────────────────────────────┘  "},{"title":"snowflakeToDateTime​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#snowflaketodatetime","content":"Extracts time from Snowflake ID as DateTime format. Syntax snowflakeToDateTime(value [, time_zone])  Parameters value — Snowflake ID. Int64.time_zone — Timezone. The function parses time_string according to the timezone. Optional. String. Returned value Input value converted to the DateTime data type. Example Query: SELECT snowflakeToDateTime(CAST('1426860702823350272', 'Int64'), 'UTC');  Result:  ┌─snowflakeToDateTime(CAST('1426860702823350272', 'Int64'), 'UTC')─┐ │ 2021-08-15 10:57:56 │ └──────────────────────────────────────────────────────────────────┘  "},{"title":"snowflakeToDateTime64​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#snowflaketodatetime64","content":"Extracts time from Snowflake ID as DateTime64 format. Syntax snowflakeToDateTime64(value [, time_zone])  Parameters value — Snowflake ID. Int64.time_zone — Timezone. The function parses time_string according to the timezone. Optional. String. Returned value Input value converted to the DateTime64 data type. Example Query: SELECT snowflakeToDateTime64(CAST('1426860802823350272', 'Int64'), 'UTC');  Result:  ┌─snowflakeToDateTime64(CAST('1426860802823350272', 'Int64'), 'UTC')─┐ │ 2021-08-15 10:58:19.841 │ └────────────────────────────────────────────────────────────────────┘  "},{"title":"dateTimeToSnowflake​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#datetimetosnowflake","content":"Converts DateTime value to the first Snowflake ID at the giving time. Syntax dateTimeToSnowflake(value)  Parameters value — Date and time. DateTime. Returned value Input value converted to the Int64 data type as the first Snowflake ID at that time. Example Query: WITH toDateTime('2021-08-15 18:57:56', 'Asia/Shanghai') AS dt SELECT dateTimeToSnowflake(dt);  Result: ┌─dateTimeToSnowflake(dt)─┐ │ 1426860702823350272 │ └─────────────────────────┘  "},{"title":"dateTime64ToSnowflake​","type":1,"pageTitle":"Type Conversion Functions","url":"docs/en/sql-reference/functions/type-conversion-functions#datetime64tosnowflake","content":"Convert DateTime64 to the first Snowflake ID at the giving time. Syntax dateTime64ToSnowflake(value)  Parameters value — Date and time. DateTime64. Returned value Input value converted to the Int64 data type as the first Snowflake ID at that time. Example Query: WITH toDateTime64('2021-08-15 18:57:56.492', 3, 'Asia/Shanghai') AS dt64 SELECT dateTime64ToSnowflake(dt64);  Result: ┌─dateTime64ToSnowflake(dt64)─┐ │ 1426860704886947840 │ └─────────────────────────────┘  "},{"title":"ATTACH Statement","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/attach","content":"","keywords":""},{"title":"Attach Existing Table​","type":1,"pageTitle":"ATTACH Statement","url":"docs/en/sql-reference/statements/attach#attach-existing-table","content":"Syntax ATTACH TABLE [IF NOT EXISTS] [db.]name [ON CLUSTER cluster]  This query is used when starting the server. The server stores table metadata as files with ATTACH queries, which it simply runs at launch (with the exception of some system tables, which are explicitly created on the server). If the table was detached permanently, it won't be reattached at the server start, so you need to use ATTACH query explicitly. "},{"title":"Create New Table And Attach Data​","type":1,"pageTitle":"ATTACH Statement","url":"docs/en/sql-reference/statements/attach#create-new-table-and-attach-data","content":""},{"title":"With Specified Path to Table Data​","type":1,"pageTitle":"ATTACH Statement","url":"docs/en/sql-reference/statements/attach#attach-with-specified-path","content":"The query creates a new table with provided structure and attaches table data from the provided directory in user_files. Syntax ATTACH TABLE name FROM 'path/to/data/' (col1 Type1, ...)  Example Query: DROP TABLE IF EXISTS test; INSERT INTO TABLE FUNCTION file('01188_attach/test/data.TSV', 'TSV', 's String, n UInt8') VALUES ('test', 42); ATTACH TABLE test FROM '01188_attach/test' (s String, n UInt8) ENGINE = File(TSV); SELECT * FROM test;  Result: ┌─s────┬──n─┐ │ test │ 42 │ └──────┴────┘  "},{"title":"With Specified Table UUID​","type":1,"pageTitle":"ATTACH Statement","url":"docs/en/sql-reference/statements/attach#attach-with-specified-uuid","content":"This query creates a new table with provided structure and attaches data from the table with the specified UUID. It is supported by the Atomic database engine. Syntax ATTACH TABLE name UUID '&lt;uuid&gt;' (col1 Type1, ...)  "},{"title":"Attach Existing Dictionary​","type":1,"pageTitle":"ATTACH Statement","url":"docs/en/sql-reference/statements/attach#attach-existing-dictionary","content":"Attaches a previously detached dictionary. Syntax ATTACH DICTIONARY [IF NOT EXISTS] [db.]name [ON CLUSTER cluster]  "},{"title":"ALTER TABLE … UPDATE Statements","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/alter/update","content":"ALTER TABLE … UPDATE Statements ALTER TABLE [db.]table UPDATE column1 = expr1 [, ...] WHERE filter_expr Manipulates data matching the specified filtering expression. Implemented as a mutation. note The ALTER TABLE prefix makes this syntax different from most other systems supporting SQL. It is intended to signify that unlike similar queries in OLTP databases this is a heavy operation not designed for frequent use. The filter_expr must be of type UInt8. This query updates values of specified columns to the values of corresponding expressions in rows for which the filter_expr takes a non-zero value. Values are casted to the column type using the CAST operator. Updating columns that are used in the calculation of the primary or the partition key is not supported. One query can contain several commands separated by commas. The synchronicity of the query processing is defined by the mutations_sync setting. By default, it is asynchronous. See also MutationsSynchronicity of ALTER Queriesmutations_sync setting","keywords":""},{"title":"Manipulating Partitions and Parts","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/alter/partition","content":"","keywords":""},{"title":"DETACH PARTITION|PART​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"docs/en/sql-reference/statements/alter/partition#alter_detach-partition","content":"ALTER TABLE table_name DETACH PARTITION|PART partition_expr  Moves all data for the specified partition to the detached directory. The server forgets about the detached data partition as if it does not exist. The server will not know about this data until you make the ATTACH query. Example: ALTER TABLE mt DETACH PARTITION '2020-11-21'; ALTER TABLE mt DETACH PART 'all_2_2_0';  Read about setting the partition expression in a section How to specify the partition expression. After the query is executed, you can do whatever you want with the data in the detached directory — delete it from the file system, or just leave it. This query is replicated – it moves the data to the detached directory on all replicas. Note that you can execute this query only on a leader replica. To find out if a replica is a leader, perform the SELECT query to the system.replicas table. Alternatively, it is easier to make a DETACH query on all replicas - all the replicas throw an exception, except the leader replicas (as multiple leaders are allowed). "},{"title":"DROP PARTITION|PART​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"docs/en/sql-reference/statements/alter/partition#alter_drop-partition","content":"ALTER TABLE table_name DROP PARTITION|PART partition_expr  Deletes the specified partition from the table. This query tags the partition as inactive and deletes data completely, approximately in 10 minutes. Read about setting the partition expression in a section How to specify the partition expression. The query is replicated – it deletes data on all replicas. Example: ALTER TABLE mt DROP PARTITION '2020-11-21'; ALTER TABLE mt DROP PART 'all_4_4_0';  "},{"title":"DROP DETACHED PARTITION|PART​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"docs/en/sql-reference/statements/alter/partition#alter_drop-detached","content":"ALTER TABLE table_name DROP DETACHED PARTITION|PART partition_expr  Removes the specified part or all parts of the specified partition from detached. Read more about setting the partition expression in a section How to specify the partition expression. "},{"title":"ATTACH PARTITION|PART​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"docs/en/sql-reference/statements/alter/partition#alter_attach-partition","content":"ALTER TABLE table_name ATTACH PARTITION|PART partition_expr  Adds data to the table from the detached directory. It is possible to add data for an entire partition or for a separate part. Examples: ALTER TABLE visits ATTACH PARTITION 201901; ALTER TABLE visits ATTACH PART 201901_2_2_0;  Read more about setting the partition expression in a section How to specify the partition expression. This query is replicated. The replica-initiator checks whether there is data in the detached directory. If data exists, the query checks its integrity. If everything is correct, the query adds the data to the table. If the non-initiator replica, receiving the attach command, finds the part with the correct checksums in its own detached folder, it attaches the data without fetching it from other replicas. If there is no part with the correct checksums, the data is downloaded from any replica having the part. You can put data to the detached directory on one replica and use the ALTER ... ATTACH query to add it to the table on all replicas. "},{"title":"ATTACH PARTITION FROM​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"docs/en/sql-reference/statements/alter/partition#alter_attach-partition-from","content":"ALTER TABLE table2 ATTACH PARTITION partition_expr FROM table1  This query copies the data partition from table1 to table2. Note that data will be deleted neither from table1 nor from table2. For the query to run successfully, the following conditions must be met: Both tables must have the same structure.Both tables must have the same partition key. "},{"title":"REPLACE PARTITION​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"docs/en/sql-reference/statements/alter/partition#alter_replace-partition","content":"ALTER TABLE table2 REPLACE PARTITION partition_expr FROM table1  This query copies the data partition from the table1 to table2 and replaces existing partition in the table2. Note that data won’t be deleted from table1. For the query to run successfully, the following conditions must be met: Both tables must have the same structure.Both tables must have the same partition key. "},{"title":"MOVE PARTITION TO TABLE​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"docs/en/sql-reference/statements/alter/partition#alter_move_to_table-partition","content":"ALTER TABLE table_source MOVE PARTITION partition_expr TO TABLE table_dest  This query moves the data partition from the table_source to table_dest with deleting the data from table_source. For the query to run successfully, the following conditions must be met: Both tables must have the same structure.Both tables must have the same partition key.Both tables must be the same engine family (replicated or non-replicated).Both tables must have the same storage policy. "},{"title":"CLEAR COLUMN IN PARTITION​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"docs/en/sql-reference/statements/alter/partition#alter_clear-column-partition","content":"ALTER TABLE table_name CLEAR COLUMN column_name IN PARTITION partition_expr  Resets all values in the specified column in a partition. If the DEFAULT clause was determined when creating a table, this query sets the column value to a specified default value. Example: ALTER TABLE visits CLEAR COLUMN hour in PARTITION 201902  "},{"title":"FREEZE PARTITION​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"docs/en/sql-reference/statements/alter/partition#alter_freeze-partition","content":"ALTER TABLE table_name FREEZE [PARTITION partition_expr] [WITH NAME 'backup_name']  This query creates a local backup of a specified partition. If the PARTITION clause is omitted, the query creates the backup of all partitions at once. note The entire backup process is performed without stopping the server. Note that for old-styled tables you can specify the prefix of the partition name (for example, 2019) - then the query creates the backup for all the corresponding partitions. Read about setting the partition expression in a section How to specify the partition expression. At the time of execution, for a data snapshot, the query creates hardlinks to a table data. Hardlinks are placed in the directory /var/lib/clickhouse/shadow/N/..., where: /var/lib/clickhouse/ is the working ClickHouse directory specified in the config.N is the incremental number of the backup.if the WITH NAME parameter is specified, then the value of the 'backup_name' parameter is used instead of the incremental number.  note If you use a set of disks for data storage in a table, the shadow/N directory appears on every disk, storing data parts that matched by the PARTITION expression. The same structure of directories is created inside the backup as inside /var/lib/clickhouse/. The query performs chmod for all files, forbidding writing into them. After creating the backup, you can copy the data from /var/lib/clickhouse/shadow/ to the remote server and then delete it from the local server. Note that the ALTER t FREEZE PARTITION query is not replicated. It creates a local backup only on the local server. The query creates backup almost instantly (but first it waits for the current queries to the corresponding table to finish running). ALTER TABLE t FREEZE PARTITION copies only the data, not table metadata. To make a backup of table metadata, copy the file /var/lib/clickhouse/metadata/database/table.sql To restore data from a backup, do the following: Create the table if it does not exist. To view the query, use the .sql file (replace ATTACH in it with CREATE).Copy the data from the data/database/table/ directory inside the backup to the /var/lib/clickhouse/data/database/table/detached/ directory.Run ALTER TABLE t ATTACH PARTITION queries to add the data to a table. Restoring from a backup does not require stopping the server. For more information about backups and restoring data, see the Data Backup section. "},{"title":"UNFREEZE PARTITION​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"docs/en/sql-reference/statements/alter/partition#alter_unfreeze-partition","content":"ALTER TABLE 'table_name' UNFREEZE [PARTITION 'part_expr'] WITH NAME 'backup_name'  Removes freezed partitions with the specified name from the disk. If the PARTITION clause is omitted, the query removes the backup of all partitions at once. "},{"title":"CLEAR INDEX IN PARTITION​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"docs/en/sql-reference/statements/alter/partition#alter_clear-index-partition","content":"ALTER TABLE table_name CLEAR INDEX index_name IN PARTITION partition_expr  The query works similar to CLEAR COLUMN, but it resets an index instead of a column data. "},{"title":"FETCH PARTITION|PART​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"docs/en/sql-reference/statements/alter/partition#alter_fetch-partition","content":"ALTER TABLE table_name FETCH PARTITION|PART partition_expr FROM 'path-in-zookeeper'  Downloads a partition from another server. This query only works for the replicated tables. The query does the following: Downloads the partition|part from the specified shard. In ‘path-in-zookeeper’ you must specify a path to the shard in ZooKeeper.Then the query puts the downloaded data to the detached directory of the table_name table. Use the ATTACH PARTITION|PART query to add the data to the table. For example: FETCH PARTITION ALTER TABLE users FETCH PARTITION 201902 FROM '/clickhouse/tables/01-01/visits'; ALTER TABLE users ATTACH PARTITION 201902;  FETCH PART ALTER TABLE users FETCH PART 201901_2_2_0 FROM '/clickhouse/tables/01-01/visits'; ALTER TABLE users ATTACH PART 201901_2_2_0;  Note that: The ALTER ... FETCH PARTITION|PART query isn’t replicated. It places the part or partition to the detached directory only on the local server.The ALTER TABLE ... ATTACH query is replicated. It adds the data to all replicas. The data is added to one of the replicas from the detached directory, and to the others - from neighboring replicas. Before downloading, the system checks if the partition exists and the table structure matches. The most appropriate replica is selected automatically from the healthy replicas. Although the query is called ALTER TABLE, it does not change the table structure and does not immediately change the data available in the table. "},{"title":"MOVE PARTITION|PART​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"docs/en/sql-reference/statements/alter/partition#alter_move-partition","content":"Moves partitions or data parts to another volume or disk for MergeTree-engine tables. See Using Multiple Block Devices for Data Storage. ALTER TABLE table_name MOVE PARTITION|PART partition_expr TO DISK|VOLUME 'disk_name'  The ALTER TABLE t MOVE query: Not replicated, because different replicas can have different storage policies.Returns an error if the specified disk or volume is not configured. Query also returns an error if conditions of data moving, that specified in the storage policy, can’t be applied.Can return an error in the case, when data to be moved is already moved by a background process, concurrent ALTER TABLE t MOVE query or as a result of background data merging. A user shouldn’t perform any additional actions in this case. Example: ALTER TABLE hits MOVE PART '20190301_14343_16206_438' TO VOLUME 'slow' ALTER TABLE hits MOVE PARTITION '2019-09-01' TO DISK 'fast_ssd'  "},{"title":"UPDATE IN PARTITION​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"docs/en/sql-reference/statements/alter/partition#update-in-partition","content":"Manipulates data in the specifies partition matching the specified filtering expression. Implemented as a mutation. Syntax: ALTER TABLE [db.]table UPDATE column1 = expr1 [, ...] [IN PARTITION partition_id] WHERE filter_expr  "},{"title":"Example​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"docs/en/sql-reference/statements/alter/partition#example","content":"ALTER TABLE mt UPDATE x = x + 1 IN PARTITION 2 WHERE p = 2;  "},{"title":"See Also​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"docs/en/sql-reference/statements/alter/partition#see-also","content":"UPDATE "},{"title":"DELETE IN PARTITION​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"docs/en/sql-reference/statements/alter/partition#delete-in-partition","content":"Deletes data in the specifies partition matching the specified filtering expression. Implemented as a mutation. Syntax: ALTER TABLE [db.]table DELETE [IN PARTITION partition_id] WHERE filter_expr  "},{"title":"Example​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"docs/en/sql-reference/statements/alter/partition#example-1","content":"ALTER TABLE mt DELETE IN PARTITION 2 WHERE p = 2;  "},{"title":"See Also​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"docs/en/sql-reference/statements/alter/partition#see-also-1","content":"DELETE "},{"title":"How to Set Partition Expression​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"docs/en/sql-reference/statements/alter/partition#alter-how-to-specify-part-expr","content":"You can specify the partition expression in ALTER ... PARTITION queries in different ways: As a value from the partition column of the system.parts table. For example, ALTER TABLE visits DETACH PARTITION 201901.As a tuple of expressions or constants that matches (in types) the table partitioning keys tuple. In the case of a single element partitioning key, the expression should be wrapped in the tuple (...) function. For example, ALTER TABLE visits DETACH PARTITION tuple(toYYYYMM(toDate('2019-01-25'))).Using the partition ID. Partition ID is a string identifier of the partition (human-readable, if possible) that is used as the names of partitions in the file system and in ZooKeeper. The partition ID must be specified in the PARTITION ID clause, in a single quotes. For example, ALTER TABLE visits DETACH PARTITION ID '201901'.In the ALTER ATTACH PART and DROP DETACHED PART query, to specify the name of a part, use string literal with a value from the name column of the system.detached_parts table. For example, ALTER TABLE visits ATTACH PART '201901_1_1_0'. Usage of quotes when specifying the partition depends on the type of partition expression. For example, for the String type, you have to specify its name in quotes ('). For the Date and Int* types no quotes are needed. All the rules above are also true for the OPTIMIZE query. If you need to specify the only partition when optimizing a non-partitioned table, set the expression PARTITION tuple(). For example: OPTIMIZE TABLE table_not_partitioned PARTITION tuple() FINAL;  IN PARTITION specifies the partition to which the UPDATE or DELETE expressions are applied as a result of the ALTER TABLE query. New parts are created only from the specified partition. In this way, IN PARTITION helps to reduce the load when the table is divided into many partitions, and you only need to update the data point-by-point. The examples of ALTER ... PARTITION queries are demonstrated in the tests 00502_custom_partitioning_local and 00502_custom_partitioning_replicated_zookeeper. "},{"title":"ALTER TABLE … MODIFY QUERY Statement","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/alter/view","content":"","keywords":""},{"title":"ALTER LIVE VIEW Statement​","type":1,"pageTitle":"ALTER TABLE … MODIFY QUERY Statement","url":"docs/en/sql-reference/statements/alter/view#alter-live-view","content":"ALTER LIVE VIEW ... REFRESH statement refreshes a Live view. See Force Live View Refresh. "},{"title":"Column Manipulations","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/alter/column","content":"","keywords":""},{"title":"ADD COLUMN​","type":1,"pageTitle":"Column Manipulations","url":"docs/en/sql-reference/statements/alter/column#alter_add-column","content":"ADD COLUMN [IF NOT EXISTS] name [type] [default_expr] [codec] [AFTER name_after | FIRST]  Adds a new column to the table with the specified name, type, codec and default_expr (see the section Default expressions). If the IF NOT EXISTS clause is included, the query won’t return an error if the column already exists. If you specify AFTER name_after (the name of another column), the column is added after the specified one in the list of table columns. If you want to add a column to the beginning of the table use the FIRST clause. Otherwise, the column is added to the end of the table. For a chain of actions, name_after can be the name of a column that is added in one of the previous actions. Adding a column just changes the table structure, without performing any actions with data. The data does not appear on the disk after ALTER. If the data is missing for a column when reading from the table, it is filled in with default values (by performing the default expression if there is one, or using zeros or empty strings). The column appears on the disk after merging data parts (see MergeTree). This approach allows us to complete the ALTER query instantly, without increasing the volume of old data. Example: ALTER TABLE alter_test ADD COLUMN Added1 UInt32 FIRST; ALTER TABLE alter_test ADD COLUMN Added2 UInt32 AFTER NestedColumn; ALTER TABLE alter_test ADD COLUMN Added3 UInt32 AFTER ToDrop; DESC alter_test FORMAT TSV;  Added1 UInt32 CounterID UInt32 StartDate Date UserID UInt32 VisitID UInt32 NestedColumn.A Array(UInt8) NestedColumn.S Array(String) Added2 UInt32 ToDrop UInt32 Added3 UInt32  "},{"title":"DROP COLUMN​","type":1,"pageTitle":"Column Manipulations","url":"docs/en/sql-reference/statements/alter/column#alter_drop-column","content":"DROP COLUMN [IF EXISTS] name  Deletes the column with the name name. If the IF EXISTS clause is specified, the query won’t return an error if the column does not exist. Deletes data from the file system. Since this deletes entire files, the query is completed almost instantly. warning You can’t delete a column if it is referenced by materialized view. Otherwise, it returns an error. Example: ALTER TABLE visits DROP COLUMN browser  "},{"title":"RENAME COLUMN​","type":1,"pageTitle":"Column Manipulations","url":"docs/en/sql-reference/statements/alter/column#alter_rename-column","content":"RENAME COLUMN [IF EXISTS] name to new_name  Renames the column name to new_name. If the IF EXISTS clause is specified, the query won’t return an error if the column does not exist. Since renaming does not involve the underlying data, the query is completed almost instantly. NOTE: Columns specified in the key expression of the table (either with ORDER BY or PRIMARY KEY) cannot be renamed. Trying to change these columns will produce SQL Error [524]. Example: ALTER TABLE visits RENAME COLUMN webBrowser TO browser  "},{"title":"CLEAR COLUMN​","type":1,"pageTitle":"Column Manipulations","url":"docs/en/sql-reference/statements/alter/column#alter_clear-column","content":"CLEAR COLUMN [IF EXISTS] name IN PARTITION partition_name  Resets all data in a column for a specified partition. Read more about setting the partition name in the section How to specify the partition expression. If the IF EXISTS clause is specified, the query won’t return an error if the column does not exist. Example: ALTER TABLE visits CLEAR COLUMN browser IN PARTITION tuple()  "},{"title":"COMMENT COLUMN​","type":1,"pageTitle":"Column Manipulations","url":"docs/en/sql-reference/statements/alter/column#alter_comment-column","content":"COMMENT COLUMN [IF EXISTS] name 'Text comment'  Adds a comment to the column. If the IF EXISTS clause is specified, the query won’t return an error if the column does not exist. Each column can have one comment. If a comment already exists for the column, a new comment overwrites the previous comment. Comments are stored in the comment_expression column returned by the DESCRIBE TABLE query. Example: ALTER TABLE visits COMMENT COLUMN browser 'The table shows the browser used for accessing the site.'  "},{"title":"MODIFY COLUMN​","type":1,"pageTitle":"Column Manipulations","url":"docs/en/sql-reference/statements/alter/column#alter_modify-column","content":"MODIFY COLUMN [IF EXISTS] name [type] [default_expr] [codec] [TTL] [AFTER name_after | FIRST] ALTER COLUMN [IF EXISTS] name TYPE [type] [default_expr] [codec] [TTL] [AFTER name_after | FIRST]  This query changes the name column properties: Type Default expression Compression Codec TTL For examples of columns compression CODECS modifying, see Column Compression Codecs. For examples of columns TTL modifying, see Column TTL. If the IF EXISTS clause is specified, the query won’t return an error if the column does not exist. The query also can change the order of the columns using FIRST | AFTER clause, see ADD COLUMN description. When changing the type, values are converted as if the toType functions were applied to them. If only the default expression is changed, the query does not do anything complex, and is completed almost instantly. Example: ALTER TABLE visits MODIFY COLUMN browser Array(String)  Changing the column type is the only complex action – it changes the contents of files with data. For large tables, this may take a long time. The ALTER query is atomic. For MergeTree tables it is also lock-free. The ALTER query for changing columns is replicated. The instructions are saved in ZooKeeper, then each replica applies them. All ALTER queries are run in the same order. The query waits for the appropriate actions to be completed on the other replicas. However, a query to change columns in a replicated table can be interrupted, and all actions will be performed asynchronously. "},{"title":"MODIFY COLUMN REMOVE​","type":1,"pageTitle":"Column Manipulations","url":"docs/en/sql-reference/statements/alter/column#modify-remove","content":"Removes one of the column properties: DEFAULT, ALIAS, MATERIALIZED, CODEC, COMMENT, TTL. Syntax: ALTER TABLE table_name MODIFY column_name REMOVE property;  Example Remove TTL: ALTER TABLE table_with_ttl MODIFY COLUMN column_ttl REMOVE TTL;  See Also REMOVE TTL. "},{"title":"MATERIALIZE COLUMN​","type":1,"pageTitle":"Column Manipulations","url":"docs/en/sql-reference/statements/alter/column#materialize-column","content":"Materializes or updates a column with an expression for a default value (DEFAULT or MATERIALIZED). It is used if it is necessary to add or update a column with a complicated expression, because evaluating such an expression directly on SELECT executing turns out to be expensive. Syntax: ALTER TABLE table MATERIALIZE COLUMN col;  Example DROP TABLE IF EXISTS tmp; SET mutations_sync = 2; CREATE TABLE tmp (x Int64) ENGINE = MergeTree() ORDER BY tuple() PARTITION BY tuple(); INSERT INTO tmp SELECT * FROM system.numbers LIMIT 5; ALTER TABLE tmp ADD COLUMN s String MATERIALIZED toString(x); ALTER TABLE tmp MATERIALIZE COLUMN s; SELECT groupArray(x), groupArray(s) FROM (select x,s from tmp order by x); ┌─groupArray(x)─┬─groupArray(s)─────────┐ │ [0,1,2,3,4] │ ['0','1','2','3','4'] │ └───────────────┴───────────────────────┘ ALTER TABLE tmp MODIFY COLUMN s String MATERIALIZED toString(round(100/x)); INSERT INTO tmp SELECT * FROM system.numbers LIMIT 5,5; SELECT groupArray(x), groupArray(s) FROM tmp; ┌─groupArray(x)─────────┬─groupArray(s)──────────────────────────────────┐ │ [0,1,2,3,4,5,6,7,8,9] │ ['0','1','2','3','4','20','17','14','12','11'] │ └───────────────────────┴────────────────────────────────────────────────┘ ALTER TABLE tmp MATERIALIZE COLUMN s; SELECT groupArray(x), groupArray(s) FROM tmp; ┌─groupArray(x)─────────┬─groupArray(s)─────────────────────────────────────────┐ │ [0,1,2,3,4,5,6,7,8,9] │ ['inf','100','50','33','25','20','17','14','12','11'] │ └───────────────────────┴───────────────────────────────────────────────────────┘  See Also MATERIALIZED. "},{"title":"Limitations​","type":1,"pageTitle":"Column Manipulations","url":"docs/en/sql-reference/statements/alter/column#alter-query-limitations","content":"The ALTER query lets you create and delete separate elements (columns) in nested data structures, but not whole nested data structures. To add a nested data structure, you can add columns with a name like name.nested_name and the type Array(T). A nested data structure is equivalent to multiple array columns with a name that has the same prefix before the dot. There is no support for deleting columns in the primary key or the sampling key (columns that are used in the ENGINE expression). Changing the type for columns that are included in the primary key is only possible if this change does not cause the data to be modified (for example, you are allowed to add values to an Enum or to change a type from DateTime to UInt32). If the ALTER query is not sufficient to make the table changes you need, you can create a new table, copy the data to it using the INSERT SELECT query, then switch the tables using the RENAME query and delete the old table. You can use the clickhouse-copier as an alternative to the INSERT SELECT query. The ALTER query blocks all reads and writes for the table. In other words, if a long SELECT is running at the time of the ALTER query, the ALTER query will wait for it to complete. At the same time, all new queries to the same table will wait while this ALTER is running. For tables that do not store data themselves (such as Merge and Distributed), ALTER just changes the table structure, and does not change the structure of subordinate tables. For example, when running ALTER for a Distributed table, you will also need to run ALTER for the tables on all remote servers. "},{"title":"role","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/alter/role","content":"","keywords":""},{"title":"ALTER ROLE​","type":1,"pageTitle":"role","url":"docs/en/sql-reference/statements/alter/role#alter-role-statement","content":"Changes roles. Syntax: ALTER ROLE [IF EXISTS] name1 [ON CLUSTER cluster_name1] [RENAME TO new_name1] [, name2 [ON CLUSTER cluster_name2] [RENAME TO new_name2] ...] [SETTINGS variable [= value] [MIN [=] min_value] [MAX [=] max_value] [READONLY|WRITABLE] | PROFILE 'profile_name'] [,...]  "},{"title":"CREATE Queries","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/create/","content":"CREATE Queries Create queries make a new entity of one of the following kinds: DATABASETABLEVIEWDICTIONARYFUNCTIONUSERROLEROW POLICYQUOTASETTINGS PROFILE Original article","keywords":""},{"title":"CREATE DATABASE","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/create/database","content":"","keywords":""},{"title":"Clauses​","type":1,"pageTitle":"CREATE DATABASE","url":"docs/en/sql-reference/statements/create/database#clauses","content":""},{"title":"IF NOT EXISTS​","type":1,"pageTitle":"CREATE DATABASE","url":"docs/en/sql-reference/statements/create/database#if-not-exists","content":"If the db_name database already exists, then ClickHouse does not create a new database and: Doesn’t throw an exception if clause is specified.Throws an exception if clause isn’t specified. "},{"title":"ON CLUSTER​","type":1,"pageTitle":"CREATE DATABASE","url":"docs/en/sql-reference/statements/create/database#on-cluster","content":"ClickHouse creates the db_name database on all the servers of a specified cluster. More details in a Distributed DDL article. "},{"title":"ENGINE​","type":1,"pageTitle":"CREATE DATABASE","url":"docs/en/sql-reference/statements/create/database#engine","content":"By default, ClickHouse uses its own Atomic database engine. There are also Lazy, MySQL, PostgresSQL, MaterializedMySQL, MaterializedPostgreSQL, Replicated, SQLite. "},{"title":"COMMENT​","type":1,"pageTitle":"CREATE DATABASE","url":"docs/en/sql-reference/statements/create/database#comment","content":"You can add a comment to the database when you creating it. The comment is supported for all database engines. Syntax CREATE DATABASE db_name ENGINE = engine(...) COMMENT 'Comment'  Example Query: CREATE DATABASE db_comment ENGINE = Memory COMMENT 'The temporary database'; SELECT name, comment FROM system.databases WHERE name = 'db_comment';  Result: ┌─name───────┬─comment────────────────┐ │ db_comment │ The temporary database │ └────────────┴────────────────────────┘  "},{"title":"CREATE SETTINGS PROFILE","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/create/settings-profile","content":"","keywords":""},{"title":"Example​","type":1,"pageTitle":"CREATE SETTINGS PROFILE","url":"docs/en/sql-reference/statements/create/settings-profile#create-settings-profile-syntax","content":"Create the max_memory_usage_profile settings profile with value and constraints for the max_memory_usage setting and assign it to user robin: CREATE SETTINGS PROFILE max_memory_usage_profile SETTINGS max_memory_usage = 100000001 MIN 90000000 MAX 110000000 TO robin  "},{"title":"CREATE DICTIONARY","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/create/dictionary","content":"CREATE DICTIONARY Creates a new external dictionary with given structure, source, layout and lifetime. Syntax CREATE [OR REPLACE] DICTIONARY [IF NOT EXISTS] [db.]dictionary_name [ON CLUSTER cluster] ( key1 type1 [DEFAULT|EXPRESSION expr1] [IS_OBJECT_ID], key2 type2 [DEFAULT|EXPRESSION expr2], attr1 type2 [DEFAULT|EXPRESSION expr3] [HIERARCHICAL|INJECTIVE], attr2 type2 [DEFAULT|EXPRESSION expr4] [HIERARCHICAL|INJECTIVE] ) PRIMARY KEY key1, key2 SOURCE(SOURCE_NAME([param1 value1 ... paramN valueN])) LAYOUT(LAYOUT_NAME([param_name param_value])) LIFETIME({MIN min_val MAX max_val | max_val}) SETTINGS(setting_name = setting_value, setting_name = setting_value, ...) COMMENT 'Comment' External dictionary structure consists of attributes. Dictionary attributes are specified similarly to table columns. The only required attribute property is its type, all other properties may have default values. ON CLUSTER clause allows creating dictionary on a cluster, see Distributed DDL. Depending on dictionary layout one or more attributes can be specified as dictionary keys. For more information, see External Dictionaries section. You can add a comment to the dictionary when you creating it using COMMENT clause. Example Input table source_table: ┌─id─┬─value──┐ │ 1 │ First │ │ 2 │ Second │ └────┴────────┘ Creating the dictionary: CREATE DICTIONARY dictionary_with_comment ( id UInt64, value String ) PRIMARY KEY id SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() TABLE 'source_table')) LAYOUT(FLAT()) LIFETIME(MIN 0 MAX 1000) COMMENT 'The temporary dictionary'; Output the dictionary: SHOW CREATE DICTIONARY dictionary_with_comment; ┌─statement───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE DICTIONARY default.dictionary_with_comment ( `id` UInt64, `value` String ) PRIMARY KEY id SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() TABLE 'source_table')) LIFETIME(MIN 0 MAX 1000) LAYOUT(FLAT()) COMMENT 'The temporary dictionary' │ └─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ Output the comment to dictionary: SELECT comment FROM system.dictionaries WHERE name == 'dictionary_with_comment' AND database == currentDatabase(); ┌─comment──────────────────┐ │ The temporary dictionary │ └──────────────────────────┘ See Also system.dictionaries — This table contains information about external dictionaries.","keywords":""},{"title":"CREATE USER","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/create/user","content":"","keywords":""},{"title":"Identification​","type":1,"pageTitle":"CREATE USER","url":"docs/en/sql-reference/statements/create/user#identification","content":"There are multiple ways of user identification: IDENTIFIED WITH no_passwordIDENTIFIED WITH plaintext_password BY 'qwerty'IDENTIFIED WITH sha256_password BY 'qwerty' or IDENTIFIED BY 'password'IDENTIFIED WITH sha256_hash BY 'hash'IDENTIFIED WITH double_sha1_password BY 'qwerty'IDENTIFIED WITH double_sha1_hash BY 'hash'IDENTIFIED WITH ldap SERVER 'server_name'IDENTIFIED WITH kerberos or IDENTIFIED WITH kerberos REALM 'realm' "},{"title":"User Host​","type":1,"pageTitle":"CREATE USER","url":"docs/en/sql-reference/statements/create/user#user-host","content":"User host is a host from which a connection to ClickHouse server could be established. The host can be specified in the HOST query section in the following ways: HOST IP 'ip_address_or_subnetwork' — User can connect to ClickHouse server only from the specified IP address or a subnetwork. Examples: HOST IP '192.168.0.0/16', HOST IP '2001:DB8::/32'. For use in production, only specify HOST IP elements (IP addresses and their masks), since using host and host_regexp might cause extra latency.HOST ANY — User can connect from any location. This is a default option.HOST LOCAL — User can connect only locally.HOST NAME 'fqdn' — User host can be specified as FQDN. For example, HOST NAME 'mysite.com'.HOST REGEXP 'regexp' — You can use pcre regular expressions when specifying user hosts. For example, HOST REGEXP '.*\\.mysite\\.com'.HOST LIKE 'template' — Allows you to use the LIKE operator to filter the user hosts. For example, HOST LIKE '%' is equivalent to HOST ANY, HOST LIKE '%.mysite.com' filters all the hosts in the mysite.com domain. Another way of specifying host is to use @ syntax following the username. Examples: CREATE USER mira@'127.0.0.1' — Equivalent to the HOST IP syntax.CREATE USER mira@'localhost' — Equivalent to the HOST LOCAL syntax.CREATE USER mira@'192.168.%.%' — Equivalent to the HOST LIKE syntax. warning ClickHouse treats user_name@'address' as a username as a whole. Thus, technically you can create multiple users with the same user_name and different constructions after @. However, we do not recommend to do so. "},{"title":"GRANTEES Clause​","type":1,"pageTitle":"CREATE USER","url":"docs/en/sql-reference/statements/create/user#grantees","content":"Specifies users or roles which are allowed to receive privileges from this user on the condition this user has also all required access granted with GRANT OPTION. Options of the GRANTEES clause: user — Specifies a user this user can grant privileges to.role — Specifies a role this user can grant privileges to.ANY — This user can grant privileges to anyone. It's the default setting.NONE — This user can grant privileges to none. You can exclude any user or role by using the EXCEPT expression. For example, CREATE USER user1 GRANTEES ANY EXCEPT user2. It means if user1 has some privileges granted with GRANT OPTION it will be able to grant those privileges to anyone except user2. "},{"title":"Examples​","type":1,"pageTitle":"CREATE USER","url":"docs/en/sql-reference/statements/create/user#create-user-examples","content":"Create the user account mira protected by the password qwerty: CREATE USER mira HOST IP '127.0.0.1' IDENTIFIED WITH sha256_password BY 'qwerty';  mira should start client app at the host where the ClickHouse server runs. Create the user account john, assign roles to it and make this roles default: CREATE USER john DEFAULT ROLE role1, role2;  Create the user account john and make all his future roles default: CREATE USER john DEFAULT ROLE ALL;  When some role is assigned to john in the future, it will become default automatically. Create the user account john and make all his future roles default excepting role1 and role2: CREATE USER john DEFAULT ROLE ALL EXCEPT role1, role2;  Create the user account john and allow him to grant his privileges to the user with jack account: CREATE USER john GRANTEES jack;  "},{"title":"CHECK TABLE Statement","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/check-table","content":"","keywords":""},{"title":"Checking the MergeTree Family Tables​","type":1,"pageTitle":"CHECK TABLE Statement","url":"docs/en/sql-reference/statements/check-table#checking-mergetree-tables","content":"For MergeTree family engines, if check_query_single_value_result = 0, the CHECK TABLE query shows a check status for every individual data part of a table on the local server. SET check_query_single_value_result = 0; CHECK TABLE test_table;  ┌─part_path─┬─is_passed─┬─message─┐ │ all_1_4_1 │ 1 │ │ │ all_1_4_2 │ 1 │ │ └───────────┴───────────┴─────────┘  If check_query_single_value_result = 1, the CHECK TABLE query shows the general table check status. SET check_query_single_value_result = 1; CHECK TABLE test_table;  ┌─result─┐ │ 1 │ └────────┘  "},{"title":"If the Data Is Corrupted​","type":1,"pageTitle":"CHECK TABLE Statement","url":"docs/en/sql-reference/statements/check-table#if-data-is-corrupted","content":"If the table is corrupted, you can copy the non-corrupted data to another table. To do this: Create a new table with the same structure as damaged table. To do this execute the query CREATE TABLE &lt;new_table_name&gt; AS &lt;damaged_table_name&gt;.Set the max_threads value to 1 to process the next query in a single thread. To do this run the query SET max_threads = 1.Execute the query INSERT INTO &lt;new_table_name&gt; SELECT * FROM &lt;damaged_table_name&gt;. This request copies the non-corrupted data from the damaged table to another table. Only the data before the corrupted part will be copied.Restart the clickhouse-client to reset the max_threads value. "},{"title":"CREATE TABLE","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/create/table","content":"","keywords":""},{"title":"Syntax Forms​","type":1,"pageTitle":"CREATE TABLE","url":"docs/en/sql-reference/statements/create/table#syntax-forms","content":""},{"title":"With Explicit Schema​","type":1,"pageTitle":"CREATE TABLE","url":"docs/en/sql-reference/statements/create/table#with-explicit-schema","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [NULL|NOT NULL] [DEFAULT|MATERIALIZED|EPHEMERAL|ALIAS expr1] [compression_codec] [TTL expr1], name2 [type2] [NULL|NOT NULL] [DEFAULT|MATERIALIZED|EPHEMERAL|ALIAS expr2] [compression_codec] [TTL expr2], ... ) ENGINE = engine  Creates a table named table_name in the db database or the current database if db is not set, with the structure specified in brackets and the engine engine. The structure of the table is a list of column descriptions, secondary indexes and constraints . If primary key is supported by the engine, it will be indicated as parameter for the table engine. A column description is name type in the simplest case. Example: RegionID UInt32. Expressions can also be defined for default values (see below). If necessary, primary key can be specified, with one or more key expressions. "},{"title":"With a Schema Similar to Other Table​","type":1,"pageTitle":"CREATE TABLE","url":"docs/en/sql-reference/statements/create/table#with-a-schema-similar-to-other-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name AS [db2.]name2 [ENGINE = engine]  Creates a table with the same structure as another table. You can specify a different engine for the table. If the engine is not specified, the same engine will be used as for the db2.name2 table. "},{"title":"From a Table Function​","type":1,"pageTitle":"CREATE TABLE","url":"docs/en/sql-reference/statements/create/table#from-a-table-function","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name AS table_function()  Creates a table with the same result as that of the table function specified. The created table will also work in the same way as the corresponding table function that was specified. "},{"title":"From SELECT query​","type":1,"pageTitle":"CREATE TABLE","url":"docs/en/sql-reference/statements/create/table#from-select-query","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name[(name1 [type1], name2 [type2], ...)] ENGINE = engine AS SELECT ...  Creates a table with a structure like the result of the SELECT query, with the engine engine, and fills it with data from SELECT. Also you can explicitly specify columns description. If the table already exists and IF NOT EXISTS is specified, the query won’t do anything. There can be other clauses after the ENGINE clause in the query. See detailed documentation on how to create tables in the descriptions of table engines. Example Query: CREATE TABLE t1 (x String) ENGINE = Memory AS SELECT 1; SELECT x, toTypeName(x) FROM t1;  Result: ┌─x─┬─toTypeName(x)─┐ │ 1 │ String │ └───┴───────────────┘  "},{"title":"NULL Or NOT NULL Modifiers​","type":1,"pageTitle":"CREATE TABLE","url":"docs/en/sql-reference/statements/create/table#null-modifiers","content":"NULL and NOT NULL modifiers after data type in column definition allow or do not allow it to be Nullable. If the type is not Nullable and if NULL is specified, it will be treated as Nullable; if NOT NULL is specified, then no. For example, INT NULL is the same as Nullable(INT). If the type is Nullable and NULL or NOT NULL modifiers are specified, the exception will be thrown. See also data_type_default_nullable setting. "},{"title":"Default Values​","type":1,"pageTitle":"CREATE TABLE","url":"docs/en/sql-reference/statements/create/table#create-default-values","content":"The column description can specify an expression for a default value, in one of the following ways: DEFAULT expr, MATERIALIZED expr, ALIAS expr. Example: URLDomain String DEFAULT domain(URL). If an expression for the default value is not defined, the default values will be set to zeros for numbers, empty strings for strings, empty arrays for arrays, and 1970-01-01 for dates or zero unix timestamp for DateTime, NULL for Nullable. If the default expression is defined, the column type is optional. If there isn’t an explicitly defined type, the default expression type is used. Example: EventDate DEFAULT toDate(EventTime) – the ‘Date’ type will be used for the ‘EventDate’ column. If the data type and default expression are defined explicitly, this expression will be cast to the specified type using type casting functions. Example: Hits UInt32 DEFAULT 0 means the same thing as Hits UInt32 DEFAULT toUInt32(0). Default expressions may be defined as an arbitrary expression from table constants and columns. When creating and changing the table structure, it checks that expressions do not contain loops. For INSERT, it checks that expressions are resolvable – that all columns they can be calculated from have been passed. "},{"title":"DEFAULT​","type":1,"pageTitle":"CREATE TABLE","url":"docs/en/sql-reference/statements/create/table#default","content":"DEFAULT expr Normal default value. If the INSERT query does not specify the corresponding column, it will be filled in by computing the corresponding expression. "},{"title":"MATERIALIZED​","type":1,"pageTitle":"CREATE TABLE","url":"docs/en/sql-reference/statements/create/table#materialized","content":"MATERIALIZED expr Materialized expression. Such a column can’t be specified for INSERT, because it is always calculated. For an INSERT without a list of columns, these columns are not considered. In addition, this column is not substituted when using an asterisk in a SELECT query. This is to preserve the invariant that the dump obtained using SELECT * can be inserted back into the table using INSERT without specifying the list of columns. "},{"title":"EPHEMERAL​","type":1,"pageTitle":"CREATE TABLE","url":"docs/en/sql-reference/statements/create/table#ephemeral","content":"EPHEMERAL [expr] Ephemeral column. Such a column isn't stored in the table and cannot be SELECTed, but can be referenced in the defaults of CREATE statement. If expr is omitted type for column is required. INSERT without list of columns will skip such column, so SELECT/INSERT invariant is preserved - the dump obtained using SELECT * can be inserted back into the table using INSERT without specifying the list of columns. "},{"title":"ALIAS​","type":1,"pageTitle":"CREATE TABLE","url":"docs/en/sql-reference/statements/create/table#alias","content":"ALIAS expr Synonym. Such a column isn’t stored in the table at all. Its values can’t be inserted in a table, and it is not substituted when using an asterisk in a SELECT query. It can be used in SELECTs if the alias is expanded during query parsing. When using the ALTER query to add new columns, old data for these columns is not written. Instead, when reading old data that does not have values for the new columns, expressions are computed on the fly by default. However, if running the expressions requires different columns that are not indicated in the query, these columns will additionally be read, but only for the blocks of data that need it. If you add a new column to a table but later change its default expression, the values used for old data will change (for data where values were not stored on the disk). Note that when running background merges, data for columns that are missing in one of the merging parts is written to the merged part. It is not possible to set default values for elements in nested data structures. "},{"title":"Primary Key​","type":1,"pageTitle":"CREATE TABLE","url":"docs/en/sql-reference/statements/create/table#primary-key","content":"You can define a primary key when creating a table. Primary key can be specified in two ways: Inside the column list CREATE TABLE db.table_name ( name1 type1, name2 type2, ..., PRIMARY KEY(expr1[, expr2,...])] ) ENGINE = engine;  Outside the column list CREATE TABLE db.table_name ( name1 type1, name2 type2, ... ) ENGINE = engine PRIMARY KEY(expr1[, expr2,...]);  warning You can't combine both ways in one query. "},{"title":"Constraints​","type":1,"pageTitle":"CREATE TABLE","url":"docs/en/sql-reference/statements/create/table#constraints","content":"Along with columns descriptions constraints could be defined: CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [compression_codec] [TTL expr1], ... CONSTRAINT constraint_name_1 CHECK boolean_expr_1, ... ) ENGINE = engine  boolean_expr_1 could by any boolean expression. If constraints are defined for the table, each of them will be checked for every row in INSERT query. If any constraint is not satisfied — server will raise an exception with constraint name and checking expression. Adding large amount of constraints can negatively affect performance of big INSERT queries. "},{"title":"TTL Expression​","type":1,"pageTitle":"CREATE TABLE","url":"docs/en/sql-reference/statements/create/table#ttl-expression","content":"Defines storage time for values. Can be specified only for MergeTree-family tables. For the detailed description, see TTL for columns and tables. "},{"title":"Column Compression Codecs​","type":1,"pageTitle":"CREATE TABLE","url":"docs/en/sql-reference/statements/create/table#codecs","content":"By default, ClickHouse applies the lz4 compression method. For MergeTree-engine family you can change the default compression method in the compression section of a server configuration. You can also define the compression method for each individual column in the CREATE TABLE query. CREATE TABLE codec_example ( dt Date CODEC(ZSTD), ts DateTime CODEC(LZ4HC), float_value Float32 CODEC(NONE), double_value Float64 CODEC(LZ4HC(9)), value Float32 CODEC(Delta, ZSTD) ) ENGINE = &lt;Engine&gt; ...  The Default codec can be specified to reference default compression which may depend on different settings (and properties of data) in runtime. Example: value UInt64 CODEC(Default) — the same as lack of codec specification. Also you can remove current CODEC from the column and use default compression from config.xml: ALTER TABLE codec_example MODIFY COLUMN float_value CODEC(Default);  Codecs can be combined in a pipeline, for example, CODEC(Delta, Default). warning You can’t decompress ClickHouse database files with external utilities like lz4. Instead, use the special clickhouse-compressor utility. Compression is supported for the following table engines: MergeTree family. Supports column compression codecs and selecting the default compression method by compression settings.Log family. Uses the lz4 compression method by default and supports column compression codecs.Set. Only supported the default compression.Join. Only supported the default compression. ClickHouse supports general purpose codecs and specialized codecs. "},{"title":"General Purpose Codecs​","type":1,"pageTitle":"CREATE TABLE","url":"docs/en/sql-reference/statements/create/table#create-query-general-purpose-codecs","content":"Codecs: NONE — No compression.LZ4 — Lossless data compression algorithm used by default. Applies LZ4 fast compression.LZ4HC[(level)] — LZ4 HC (high compression) algorithm with configurable level. Default level: 9. Setting level &lt;= 0 applies the default level. Possible levels: [1, 12]. Recommended level range: [4, 9].ZSTD[(level)] — ZSTD compression algorithm with configurable level. Possible levels: [1, 22]. Default value: 1. High compression levels are useful for asymmetric scenarios, like compress once, decompress repeatedly. Higher levels mean better compression and higher CPU usage. "},{"title":"Specialized Codecs​","type":1,"pageTitle":"CREATE TABLE","url":"docs/en/sql-reference/statements/create/table#specialized-codecs","content":"These codecs are designed to make compression more effective by using specific features of data. Some of these codecs do not compress data themself. Instead, they prepare the data for a common purpose codec, which compresses it better than without this preparation. Specialized codecs: Delta(delta_bytes) — Compression approach in which raw values are replaced by the difference of two neighboring values, except for the first value that stays unchanged. Up to delta_bytes are used for storing delta values, so delta_bytes is the maximum size of raw values. Possible delta_bytes values: 1, 2, 4, 8. The default value for delta_bytes is sizeof(type) if equal to 1, 2, 4, or 8. In all other cases, it’s 1.DoubleDelta — Calculates delta of deltas and writes it in compact binary form. Optimal compression rates are achieved for monotonic sequences with a constant stride, such as time series data. Can be used with any fixed-width type. Implements the algorithm used in Gorilla TSDB, extending it to support 64-bit types. Uses 1 extra bit for 32-byte deltas: 5-bit prefixes instead of 4-bit prefixes. For additional information, see Compressing Time Stamps in Gorilla: A Fast, Scalable, In-Memory Time Series Database.Gorilla — Calculates XOR between current and previous value and writes it in compact binary form. Efficient when storing a series of floating point values that change slowly, because the best compression rate is achieved when neighboring values are binary equal. Implements the algorithm used in Gorilla TSDB, extending it to support 64-bit types. For additional information, see Compressing Values in Gorilla: A Fast, Scalable, In-Memory Time Series Database.T64 — Compression approach that crops unused high bits of values in integer data types (including Enum, Date and DateTime). At each step of its algorithm, codec takes a block of 64 values, puts them into 64x64 bit matrix, transposes it, crops the unused bits of values and returns the rest as a sequence. Unused bits are the bits, that do not differ between maximum and minimum values in the whole data part for which the compression is used. DoubleDelta and Gorilla codecs are used in Gorilla TSDB as the components of its compressing algorithm. Gorilla approach is effective in scenarios when there is a sequence of slowly changing values with their timestamps. Timestamps are effectively compressed by the DoubleDelta codec, and values are effectively compressed by the Gorilla codec. For example, to get an effectively stored table, you can create it in the following configuration: CREATE TABLE codec_example ( timestamp DateTime CODEC(DoubleDelta), slow_values Float32 CODEC(Gorilla) ) ENGINE = MergeTree()  "},{"title":"Encryption Codecs​","type":1,"pageTitle":"CREATE TABLE","url":"docs/en/sql-reference/statements/create/table#create-query-encryption-codecs","content":"These codecs don't actually compress data, but instead encrypt data on disk. These are only available when an encryption key is specified by encryption settings. Note that encryption only makes sense at the end of codec pipelines, because encrypted data usually can't be compressed in any meaningful way. Encryption codecs: CODEC('AES-128-GCM-SIV') — Encrypts data with AES-128 in RFC 8452 GCM-SIV mode. CODEC('AES-256-GCM-SIV') — Encrypts data with AES-256 in GCM-SIV mode. These codecs use a fixed nonce and encryption is therefore deterministic. This makes it compatible with deduplicating engines such as ReplicatedMergeTree but has a weakness: when the same data block is encrypted twice, the resulting ciphertext will be exactly the same so an adversary who can read the disk can see this equivalence (although only the equivalence, without getting its content). warning Most engines including the &quot;*MergeTree&quot; family create index files on disk without applying codecs. This means plaintext will appear on disk if an encrypted column is indexed. warning If you perform a SELECT query mentioning a specific value in an encrypted column (such as in its WHERE clause), the value may appear in system.query_log. You may want to disable the logging. Example CREATE TABLE mytable ( x String Codec(AES_128_GCM_SIV) ) ENGINE = MergeTree ORDER BY x;  note If compression needs to be applied, it must be explicitly specified. Otherwise, only encryption will be applied to data. Example CREATE TABLE mytable ( x String Codec(Delta, LZ4, AES_128_GCM_SIV) ) ENGINE = MergeTree ORDER BY x;  "},{"title":"Temporary Tables​","type":1,"pageTitle":"CREATE TABLE","url":"docs/en/sql-reference/statements/create/table#temporary-tables","content":"ClickHouse supports temporary tables which have the following characteristics: Temporary tables disappear when the session ends, including if the connection is lost.A temporary table uses the Memory engine only.The DB can’t be specified for a temporary table. It is created outside of databases.Impossible to create a temporary table with distributed DDL query on all cluster servers (by using ON CLUSTER): this table exists only in the current session.If a temporary table has the same name as another one and a query specifies the table name without specifying the DB, the temporary table will be used.For distributed query processing, temporary tables used in a query are passed to remote servers. To create a temporary table, use the following syntax: CREATE TEMPORARY TABLE [IF NOT EXISTS] table_name ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... )  In most cases, temporary tables are not created manually, but when using external data for a query, or for distributed (GLOBAL) IN. For more information, see the appropriate sections It’s possible to use tables with ENGINE = Memory instead of temporary tables. "},{"title":"REPLACE TABLE​","type":1,"pageTitle":"CREATE TABLE","url":"docs/en/sql-reference/statements/create/table#replace-table-query","content":"'REPLACE' query allows you to update the table atomically. note This query is supported only for Atomic database engine. If you need to delete some data from a table, you can create a new table and fill it with a SELECT statement that does not retrieve unwanted data, then drop the old table and rename the new one: CREATE TABLE myNewTable AS myOldTable; INSERT INTO myNewTable SELECT * FROM myOldTable WHERE CounterID &lt;12345; DROP TABLE myOldTable; RENAME TABLE myNewTable TO myOldTable;  Instead of above, you can use the following: REPLACE TABLE myOldTable SELECT * FROM myOldTable WHERE CounterID &lt;12345;  "},{"title":"Syntax​","type":1,"pageTitle":"CREATE TABLE","url":"docs/en/sql-reference/statements/create/table#syntax","content":"{CREATE [OR REPLACE] | REPLACE} TABLE [db.]table_name  All syntax forms for CREATE query also work for this query. REPLACE for a non-existent table will cause an error. "},{"title":"Examples:​","type":1,"pageTitle":"CREATE TABLE","url":"docs/en/sql-reference/statements/create/table#examples","content":"Consider the table: CREATE DATABASE base ENGINE = Atomic; CREATE OR REPLACE TABLE base.t1 (n UInt64, s String) ENGINE = MergeTree ORDER BY n; INSERT INTO base.t1 VALUES (1, 'test'); SELECT * FROM base.t1;  ┌─n─┬─s────┐ │ 1 │ test │ └───┴──────┘  Using REPLACE query to clear all data: CREATE OR REPLACE TABLE base.t1 (n UInt64, s Nullable(String)) ENGINE = MergeTree ORDER BY n; INSERT INTO base.t1 VALUES (2, null); SELECT * FROM base.t1;  ┌─n─┬─s──┐ │ 2 │ \\N │ └───┴────┘  Using REPLACE query to change table structure: REPLACE TABLE base.t1 (n UInt64) ENGINE = MergeTree ORDER BY n; INSERT INTO base.t1 VALUES (3); SELECT * FROM base.t1;  ┌─n─┐ │ 3 │ └───┘  "},{"title":"COMMENT Clause​","type":1,"pageTitle":"CREATE TABLE","url":"docs/en/sql-reference/statements/create/table#comment-table","content":"You can add a comment to the table when you creating it. note The comment is supported for all table engines except Kafka, RabbitMQ and EmbeddedRocksDB. Syntax CREATE TABLE db.table_name ( name1 type1, name2 type2, ... ) ENGINE = engine COMMENT 'Comment'  Example Query: CREATE TABLE t1 (x String) ENGINE = Memory COMMENT 'The temporary table'; SELECT name, comment FROM system.tables WHERE name = 't1';  Result: ┌─name─┬─comment─────────────┐ │ t1 │ The temporary table │ └──────┴─────────────────────┘  "},{"title":"CREATE ROLE","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/create/role","content":"","keywords":""},{"title":"Managing Roles​","type":1,"pageTitle":"CREATE ROLE","url":"docs/en/sql-reference/statements/create/role#managing-roles","content":"A user can be assigned multiple roles. Users can apply their assigned roles in arbitrary combinations by the SET ROLE statement. The final scope of privileges is a combined set of all the privileges of all the applied roles. If a user has privileges granted directly to it’s user account, they are also combined with the privileges granted by roles. User can have default roles which apply at user login. To set default roles, use the SET DEFAULT ROLE statement or the ALTER USER statement. To revoke a role, use the REVOKE statement. To delete role, use the DROP ROLE statement. The deleted role is being automatically revoked from all the users and roles to which it was assigned. "},{"title":"Examples​","type":1,"pageTitle":"CREATE ROLE","url":"docs/en/sql-reference/statements/create/role#create-role-examples","content":"CREATE ROLE accountant; GRANT SELECT ON db.* TO accountant;  This sequence of queries creates the role accountant that has the privilege of reading data from the db database. Assigning the role to the user mira: GRANT accountant TO mira;  After the role is assigned, the user can apply it and execute the allowed queries. For example: SET ROLE accountant; SELECT * FROM db.*;  "},{"title":"DESCRIBE TABLE","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/describe-table","content":"DESCRIBE TABLE Returns information about table columns. Syntax DESC|DESCRIBE TABLE [db.]table [INTO OUTFILE filename] [FORMAT format] The DESCRIBE statement returns a row for each table column with the following String values: name — A column name.type — A column type.default_type — A clause that is used in the column default expression: DEFAULT, MATERIALIZED or ALIAS. If there is no default expression, then empty string is returned.default_expression — An expression specified after the DEFAULT clause.comment — A column comment.codec_expression — A codec that is applied to the column.ttl_expression — A TTL expression.is_subcolumn — A flag that equals 1 for internal subcolumns. It is included into the result only if subcolumn description is enabled by the describe_include_subcolumns setting. All columns in Nested data structures are described separately. The name of each column is prefixed with a parent column name and a dot. To show internal subcolumns of other data types, use the describe_include_subcolumns setting. Example Query: CREATE TABLE describe_example ( id UInt64, text String DEFAULT 'unknown' CODEC(ZSTD), user Tuple (name String, age UInt8) ) ENGINE = MergeTree() ORDER BY id; DESCRIBE TABLE describe_example; DESCRIBE TABLE describe_example SETTINGS describe_include_subcolumns=1; Result: ┌─name─┬─type──────────────────────────┬─default_type─┬─default_expression─┬─comment─┬─codec_expression─┬─ttl_expression─┐ │ id │ UInt64 │ │ │ │ │ │ │ text │ String │ DEFAULT │ 'unknown' │ │ ZSTD(1) │ │ │ user │ Tuple(name String, age UInt8) │ │ │ │ │ │ └──────┴───────────────────────────────┴──────────────┴────────────────────┴─────────┴──────────────────┴────────────────┘ The second query additionally shows subcolumns: ┌─name──────┬─type──────────────────────────┬─default_type─┬─default_expression─┬─comment─┬─codec_expression─┬─ttl_expression─┬─is_subcolumn─┐ │ id │ UInt64 │ │ │ │ │ │ 0 │ │ text │ String │ DEFAULT │ 'unknown' │ │ ZSTD(1) │ │ 0 │ │ user │ Tuple(name String, age UInt8) │ │ │ │ │ │ 0 │ │ user.name │ String │ │ │ │ │ │ 1 │ │ user.age │ UInt8 │ │ │ │ │ │ 1 │ └───────────┴───────────────────────────────┴──────────────┴────────────────────┴─────────┴──────────────────┴────────────────┴──────────────┘ See Also describe_include_subcolumns setting.","keywords":""},{"title":"EXISTS Statement","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/exists","content":"EXISTS Statement EXISTS [TEMPORARY] [TABLE|DICTIONARY] [db.]name [INTO OUTFILE filename] [FORMAT format] Returns a single UInt8-type column, which contains the single value 0 if the table or database does not exist, or 1 if the table exists in the specified database.","keywords":""},{"title":"EXCHANGE Statement","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/exchange","content":"","keywords":""},{"title":"EXCHANGE TABLES​","type":1,"pageTitle":"EXCHANGE Statement","url":"docs/en/sql-reference/statements/exchange#exchange_tables","content":"Exchanges the names of two tables. Syntax EXCHANGE TABLES [db0.]table_A AND [db1.]table_B  "},{"title":"EXCHANGE DICTIONARIES​","type":1,"pageTitle":"EXCHANGE Statement","url":"docs/en/sql-reference/statements/exchange#exchange_dictionaries","content":"Exchanges the names of two dictionaries. Syntax EXCHANGE DICTIONARIES [db0.]dict_A AND [db1.]dict_B  See Also Dictionaries "},{"title":"DROP Statements","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/drop","content":"","keywords":""},{"title":"DROP DATABASE​","type":1,"pageTitle":"DROP Statements","url":"docs/en/sql-reference/statements/drop#drop-database","content":"Deletes all tables inside the db database, then deletes the db database itself. Syntax: DROP DATABASE [IF EXISTS] db [ON CLUSTER cluster]  "},{"title":"DROP TABLE​","type":1,"pageTitle":"DROP Statements","url":"docs/en/sql-reference/statements/drop#drop-table","content":"Deletes the table. Syntax: DROP [TEMPORARY] TABLE [IF EXISTS] [db.]name [ON CLUSTER cluster]  "},{"title":"DROP DICTIONARY​","type":1,"pageTitle":"DROP Statements","url":"docs/en/sql-reference/statements/drop#drop-dictionary","content":"Deletes the dictionary. Syntax: DROP DICTIONARY [IF EXISTS] [db.]name  "},{"title":"DROP USER​","type":1,"pageTitle":"DROP Statements","url":"docs/en/sql-reference/statements/drop#drop-user-statement","content":"Deletes a user. Syntax: DROP USER [IF EXISTS] name [,...] [ON CLUSTER cluster_name]  "},{"title":"DROP ROLE​","type":1,"pageTitle":"DROP Statements","url":"docs/en/sql-reference/statements/drop#drop-role-statement","content":"Deletes a role. The deleted role is revoked from all the entities where it was assigned. Syntax: DROP ROLE [IF EXISTS] name [,...] [ON CLUSTER cluster_name]  "},{"title":"DROP ROW POLICY​","type":1,"pageTitle":"DROP Statements","url":"docs/en/sql-reference/statements/drop#drop-row-policy-statement","content":"Deletes a row policy. Deleted row policy is revoked from all the entities where it was assigned. Syntax: DROP [ROW] POLICY [IF EXISTS] name [,...] ON [database.]table [,...] [ON CLUSTER cluster_name]  "},{"title":"DROP QUOTA​","type":1,"pageTitle":"DROP Statements","url":"docs/en/sql-reference/statements/drop#drop-quota-statement","content":"Deletes a quota. The deleted quota is revoked from all the entities where it was assigned. Syntax: DROP QUOTA [IF EXISTS] name [,...] [ON CLUSTER cluster_name]  "},{"title":"DROP SETTINGS PROFILE​","type":1,"pageTitle":"DROP Statements","url":"docs/en/sql-reference/statements/drop#drop-settings-profile-statement","content":"Deletes a settings profile. The deleted settings profile is revoked from all the entities where it was assigned. Syntax: DROP [SETTINGS] PROFILE [IF EXISTS] name [,...] [ON CLUSTER cluster_name]  "},{"title":"DROP VIEW​","type":1,"pageTitle":"DROP Statements","url":"docs/en/sql-reference/statements/drop#drop-view","content":"Deletes a view. Views can be deleted by a DROP TABLE command as well but DROP VIEW checks that [db.]name is a view. Syntax: DROP VIEW [IF EXISTS] [db.]name [ON CLUSTER cluster]  "},{"title":"DROP FUNCTION​","type":1,"pageTitle":"DROP Statements","url":"docs/en/sql-reference/statements/drop#drop-function","content":"Deletes a user defined function created by CREATE FUNCTION. System functions can not be dropped. Syntax DROP FUNCTION [IF EXISTS] function_name  Example CREATE FUNCTION linear_equation AS (x, k, b) -&gt; k*x + b; DROP FUNCTION linear_equation;  "},{"title":"CREATE QUOTA","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/create/quota","content":"CREATE QUOTA Creates a quota that can be assigned to a user or a role. Syntax: CREATE QUOTA [IF NOT EXISTS | OR REPLACE] name [ON CLUSTER cluster_name] [KEYED BY {user_name | ip_address | client_key | client_key,user_name | client_key,ip_address} | NOT KEYED] [FOR [RANDOMIZED] INTERVAL number {second | minute | hour | day | week | month | quarter | year} {MAX { {queries | query_selects | query_inserts | errors | result_rows | result_bytes | read_rows | read_bytes | execution_time} = number } [,...] | NO LIMITS | TRACKING ONLY} [,...]] [TO {role [,...] | ALL | ALL EXCEPT role [,...]}] Keys user_name, ip_address, client_key, client_key, user_name and client_key, ip_address correspond to the fields in the system.quotas table. Parameters queries, query_selects, query_inserts, errors, result_rows, result_bytes, read_rows, read_bytes, execution_time correspond to the fields in the system.quotas_usage table. ON CLUSTER clause allows creating quotas on a cluster, see Distributed DDL. Examples Limit the maximum number of queries for the current user with 123 queries in 15 months constraint: CREATE QUOTA qA FOR INTERVAL 15 month MAX queries = 123 TO CURRENT_USER; For the default user limit the maximum execution time with half a second in 30 minutes, and limit the maximum number of queries with 321 and the maximum number of errors with 10 in 5 quaters: CREATE QUOTA qB FOR INTERVAL 30 minute MAX execution_time = 0.5, FOR INTERVAL 5 quarter MAX queries = 321, errors = 10 TO default; ","keywords":""},{"title":"INSERT INTO Statement","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/insert-into","content":"","keywords":""},{"title":"Constraints​","type":1,"pageTitle":"INSERT INTO Statement","url":"docs/en/sql-reference/statements/insert-into#constraints","content":"If table has constraints, their expressions will be checked for each row of inserted data. If any of those constraints is not satisfied — server will raise an exception containing constraint name and expression, the query will be stopped. "},{"title":"Inserting the Results of SELECT​","type":1,"pageTitle":"INSERT INTO Statement","url":"docs/en/sql-reference/statements/insert-into#insert_query_insert-select","content":"Syntax INSERT INTO [db.]table [(c1, c2, c3)] SELECT ...  Columns are mapped according to their position in the SELECT clause. However, their names in the SELECT expression and the table for INSERT may differ. If necessary, type casting is performed. None of the data formats except Values allow setting values to expressions such as now(), 1 + 2, and so on. The Values format allows limited use of expressions, but this is not recommended, because in this case inefficient code is used for their execution. Other queries for modifying data parts are not supported: UPDATE, DELETE, REPLACE, MERGE, UPSERT, INSERT UPDATE. However, you can delete old data using ALTER TABLE ... DROP PARTITION. FORMAT clause must be specified in the end of query if SELECT clause contains table function input(). To insert a default value instead of NULL into a column with not nullable data type, enable insert_null_as_default setting. "},{"title":"Inserting Data from a File​","type":1,"pageTitle":"INSERT INTO Statement","url":"docs/en/sql-reference/statements/insert-into#inserting-data-from-a-file","content":"Syntax INSERT INTO [db.]table [(c1, c2, c3)] FROM INFILE file_name [COMPRESSION type] FORMAT format_name  Use the syntax above to insert data from a file stored on a client side. file_name and type are string literals. Input file format must be set in the FORMAT clause. Compressed files are supported. Compression type is detected by the extension of the file name. Or it can be explicitly specified in a COMPRESSION clause. Supported types are: 'none', 'gzip', 'deflate', 'br', 'xz', 'zstd', 'lz4', 'bz2'. This functionality is available in the command-line client and clickhouse-local. Example Execute the following queries using command-line client: echo 1,A &gt; input.csv ; echo 2,B &gt;&gt; input.csv clickhouse-client --query=&quot;CREATE TABLE table_from_file (id UInt32, text String) ENGINE=MergeTree() ORDER BY id;&quot; clickhouse-client --query=&quot;INSERT INTO table_from_file FROM INFILE 'input.csv' FORMAT CSV;&quot; clickhouse-client --query=&quot;SELECT * FROM table_from_file FORMAT PrettyCompact;&quot;  Result: ┌─id─┬─text─┐ │ 1 │ A │ │ 2 │ B │ └────┴──────┘  "},{"title":"Inserting into Table Function​","type":1,"pageTitle":"INSERT INTO Statement","url":"docs/en/sql-reference/statements/insert-into#inserting-into-table-function","content":"Data can be inserted into tables referenced by table functions. Syntax INSERT INTO [TABLE] FUNCTION table_func ...  Example remote table function is used in the following queries: CREATE TABLE simple_table (id UInt32, text String) ENGINE=MergeTree() ORDER BY id; INSERT INTO TABLE FUNCTION remote('localhost', default.simple_table) VALUES (100, 'inserted via remote()'); SELECT * FROM simple_table;  Result: ┌──id─┬─text──────────────────┐ │ 100 │ inserted via remote() │ └─────┴───────────────────────┘  "},{"title":"Performance Considerations​","type":1,"pageTitle":"INSERT INTO Statement","url":"docs/en/sql-reference/statements/insert-into#performance-considerations","content":"INSERT sorts the input data by primary key and splits them into partitions by a partition key. If you insert data into several partitions at once, it can significantly reduce the performance of the INSERT query. To avoid this: Add data in fairly large batches, such as 100,000 rows at a time.Group data by a partition key before uploading it to ClickHouse. Performance will not decrease if: Data is added in real time.You upload data that is usually sorted by time. It's also possible to asynchronously insert data in small but frequent inserts. The data from such insertions is combined into batches and then safely inserted into a table. To enable the asynchronous mode, switch on the async_insert setting. Note that asynchronous insertions are supported only over HTTP protocol, and deduplication is not supported for them. See Also async_insertasync_insert_threadswait_for_async_insertwait_for_async_insert_timeoutasync_insert_max_data_sizeasync_insert_busy_timeout_msasync_insert_stale_timeout_ms "},{"title":"Miscellaneous Statements","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/misc","content":"Miscellaneous Statements ATTACHCHECK TABLEDESCRIBE TABLEDETACHDROPEXISTSKILLOPTIMIZERENAMESETSET ROLETRUNCATEUSE","keywords":""},{"title":"DETACH Statement","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/detach","content":"DETACH Statement Makes the server &quot;forget&quot; about the existence of a table, a materialized view, or a dictionary. Syntax DETACH TABLE|VIEW|DICTIONARY [IF EXISTS] [db.]name [ON CLUSTER cluster] [PERMANENTLY] Detaching does not delete the data or metadata of a table, a materialized view or a dictionary. If an entity was not detached PERMANENTLY, on the next server launch the server will read the metadata and recall the table/view/dictionary again. If an entity was detached PERMANENTLY, there will be no automatic recall. Whether a table or a dictionary was detached permanently or not, in both cases you can reattach them using the ATTACH query. System log tables can be also attached back (e.g. query_log, text_log, etc). Other system tables can't be reattached. On the next server launch the server will recall those tables again. ATTACH MATERIALIZED VIEW does not work with short syntax (without SELECT), but you can attach it using the ATTACH TABLE query. Note that you can not detach permanently the table which is already detached (temporary). But you can attach it back and then detach permanently again. Also you can not DROP the detached table, or CREATE TABLE with the same name as detached permanently, or replace it with the other table with RENAME TABLE query. Example Creating a table: Query: CREATE TABLE test ENGINE = Log AS SELECT * FROM numbers(10); SELECT * FROM test; Result: ┌─number─┐ │ 0 │ │ 1 │ │ 2 │ │ 3 │ │ 4 │ │ 5 │ │ 6 │ │ 7 │ │ 8 │ │ 9 │ └────────┘ Detaching the table: Query: DETACH TABLE test; SELECT * FROM test; Result: Received exception from server (version 21.4.1): Code: 60. DB::Exception: Received from localhost:9000. DB::Exception: Table default.test does not exist. See Also Materialized ViewDictionaries","keywords":""},{"title":"KILL Statements","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/kill","content":"","keywords":""},{"title":"KILL QUERY​","type":1,"pageTitle":"KILL Statements","url":"docs/en/sql-reference/statements/kill#kill-query-statement","content":"KILL QUERY [ON CLUSTER cluster] WHERE &lt;where expression to SELECT FROM system.processes query&gt; [SYNC|ASYNC|TEST] [FORMAT format]  Attempts to forcibly terminate the currently running queries. The queries to terminate are selected from the system.processes table using the criteria defined in the WHERE clause of the KILL query. Examples: -- Forcibly terminates all queries with the specified query_id: KILL QUERY WHERE query_id='2-857d-4a57-9ee0-327da5d60a90' -- Synchronously terminates all queries run by 'username': KILL QUERY WHERE user='username' SYNC  Read-only users can only stop their own queries. By default, the asynchronous version of queries is used (ASYNC), which does not wait for confirmation that queries have stopped. The synchronous version (SYNC) waits for all queries to stop and displays information about each process as it stops. The response contains the kill_status column, which can take the following values: finished – The query was terminated successfully.waiting – Waiting for the query to end after sending it a signal to terminate.The other values ​​explain why the query can’t be stopped. A test query (TEST) only checks the user’s rights and displays a list of queries to stop. "},{"title":"KILL MUTATION​","type":1,"pageTitle":"KILL Statements","url":"docs/en/sql-reference/statements/kill#kill-mutation","content":"KILL MUTATION [ON CLUSTER cluster] WHERE &lt;where expression to SELECT FROM system.mutations query&gt; [TEST] [FORMAT format]  Tries to cancel and remove mutations that are currently executing. Mutations to cancel are selected from the system.mutations table using the filter specified by the WHERE clause of the KILL query. A test query (TEST) only checks the user’s rights and displays a list of mutations to stop. Examples: -- Cancel and remove all mutations of the single table: KILL MUTATION WHERE database = 'default' AND table = 'table' -- Cancel the specific mutation: KILL MUTATION WHERE database = 'default' AND table = 'table' AND mutation_id = 'mutation_3.txt'  The query is useful when a mutation is stuck and cannot finish (e.g. if some function in the mutation query throws an exception when applied to the data contained in the table). Changes already made by the mutation are not rolled back. "},{"title":"OPTIMIZE Statement","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/optimize","content":"","keywords":""},{"title":"BY expression​","type":1,"pageTitle":"OPTIMIZE Statement","url":"docs/en/sql-reference/statements/optimize#by-expression","content":"If you want to perform deduplication on custom set of columns rather than on all, you can specify list of columns explicitly or use any combination of *, COLUMNS or EXCEPT expressions. The explictly written or implicitly expanded list of columns must include all columns specified in row ordering expression (both primary and sorting keys) and partitioning expression (partitioning key). note Notice that * behaves just like in SELECT: MATERIALIZED and ALIAS columns are not used for expansion. Also, it is an error to specify empty list of columns, or write an expression that results in an empty list of columns, or deduplicate by an ALIAS column. Syntax OPTIMIZE TABLE table DEDUPLICATE; -- all columns OPTIMIZE TABLE table DEDUPLICATE BY *; -- excludes MATERIALIZED and ALIAS columns OPTIMIZE TABLE table DEDUPLICATE BY colX,colY,colZ; OPTIMIZE TABLE table DEDUPLICATE BY * EXCEPT colX; OPTIMIZE TABLE table DEDUPLICATE BY * EXCEPT (colX, colY); OPTIMIZE TABLE table DEDUPLICATE BY COLUMNS('column-matched-by-regex'); OPTIMIZE TABLE table DEDUPLICATE BY COLUMNS('column-matched-by-regex') EXCEPT colX; OPTIMIZE TABLE table DEDUPLICATE BY COLUMNS('column-matched-by-regex') EXCEPT (colX, colY);  Examples Consider the table: CREATE TABLE example ( primary_key Int32, secondary_key Int32, value UInt32, partition_key UInt32, materialized_value UInt32 MATERIALIZED 12345, aliased_value UInt32 ALIAS 2, PRIMARY KEY primary_key ) ENGINE=MergeTree PARTITION BY partition_key ORDER BY (primary_key, secondary_key);  INSERT INTO example (primary_key, secondary_key, value, partition_key) VALUES (0, 0, 0, 0), (0, 0, 0, 0), (1, 1, 2, 2), (1, 1, 2, 3), (1, 1, 3, 3);  SELECT * FROM example;  Result:  ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │ └─────────────┴───────────────┴───────┴───────────────┘ ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 1 │ 1 │ 2 │ 2 │ └─────────────┴───────────────┴───────┴───────────────┘ ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 1 │ 1 │ 2 │ 3 │ │ 1 │ 1 │ 3 │ 3 │ └─────────────┴───────────────┴───────┴───────────────┘  When columns for deduplication are not specified, all of them are taken into account. Row is removed only if all values in all columns are equal to corresponding values in previous row: OPTIMIZE TABLE example FINAL DEDUPLICATE;  SELECT * FROM example;  Result: ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 1 │ 1 │ 2 │ 2 │ └─────────────┴───────────────┴───────┴───────────────┘ ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 0 │ 0 │ 0 │ 0 │ └─────────────┴───────────────┴───────┴───────────────┘ ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 1 │ 1 │ 2 │ 3 │ │ 1 │ 1 │ 3 │ 3 │ └─────────────┴───────────────┴───────┴───────────────┘  When columns are specified implicitly, the table is deduplicated by all columns that are not ALIAS or MATERIALIZED. Considering the table above, these are primary_key, secondary_key, value, and partition_key columns: OPTIMIZE TABLE example FINAL DEDUPLICATE BY *;  SELECT * FROM example;  Result: ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 1 │ 1 │ 2 │ 2 │ └─────────────┴───────────────┴───────┴───────────────┘ ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 0 │ 0 │ 0 │ 0 │ └─────────────┴───────────────┴───────┴───────────────┘ ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 1 │ 1 │ 2 │ 3 │ │ 1 │ 1 │ 3 │ 3 │ └─────────────┴───────────────┴───────┴───────────────┘  Deduplicate by all columns that are not ALIAS or MATERIALIZED and explicitly not value: primary_key, secondary_key, and partition_key columns. OPTIMIZE TABLE example FINAL DEDUPLICATE BY * EXCEPT value;  SELECT * FROM example;  Result: ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 1 │ 1 │ 2 │ 2 │ └─────────────┴───────────────┴───────┴───────────────┘ ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 0 │ 0 │ 0 │ 0 │ └─────────────┴───────────────┴───────┴───────────────┘ ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 1 │ 1 │ 2 │ 3 │ └─────────────┴───────────────┴───────┴───────────────┘  Deduplicate explicitly by primary_key, secondary_key, and partition_key columns: OPTIMIZE TABLE example FINAL DEDUPLICATE BY primary_key, secondary_key, partition_key;  SELECT * FROM example;  Result: ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 1 │ 1 │ 2 │ 2 │ └─────────────┴───────────────┴───────┴───────────────┘ ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 0 │ 0 │ 0 │ 0 │ └─────────────┴───────────────┴───────┴───────────────┘ ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 1 │ 1 │ 2 │ 3 │ └─────────────┴───────────────┴───────┴───────────────┘  Deduplicate by any column matching a regex: primary_key, secondary_key, and partition_key columns: OPTIMIZE TABLE example FINAL DEDUPLICATE BY COLUMNS('.*_key');  SELECT * FROM example;  Result: ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 0 │ 0 │ 0 │ 0 │ └─────────────┴───────────────┴───────┴───────────────┘ ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 1 │ 1 │ 2 │ 2 │ └─────────────┴───────────────┴───────┴───────────────┘ ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 1 │ 1 │ 2 │ 3 │ └─────────────┴───────────────┴───────┴───────────────┘  "},{"title":"REVOKE Statement","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/revoke","content":"","keywords":""},{"title":"Syntax​","type":1,"pageTitle":"REVOKE Statement","url":"docs/en/sql-reference/statements/revoke#revoke-syntax","content":"Revoking privileges from users REVOKE [ON CLUSTER cluster_name] privilege[(column_name [,...])] [,...] ON {db.table|db.*|*.*|table|*} FROM {user | CURRENT_USER} [,...] | ALL | ALL EXCEPT {user | CURRENT_USER} [,...]  Revoking roles from users REVOKE [ON CLUSTER cluster_name] [ADMIN OPTION FOR] role [,...] FROM {user | role | CURRENT_USER} [,...] | ALL | ALL EXCEPT {user_name | role_name | CURRENT_USER} [,...]  "},{"title":"Description​","type":1,"pageTitle":"REVOKE Statement","url":"docs/en/sql-reference/statements/revoke#revoke-description","content":"To revoke some privilege you can use a privilege of a wider scope than you plan to revoke. For example, if a user has the SELECT (x,y) privilege, administrator can execute REVOKE SELECT(x,y) ..., or REVOKE SELECT * ..., or even REVOKE ALL PRIVILEGES ... query to revoke this privilege. "},{"title":"Partial Revokes​","type":1,"pageTitle":"REVOKE Statement","url":"docs/en/sql-reference/statements/revoke#partial-revokes-dscr","content":"You can revoke a part of a privilege. For example, if a user has the SELECT *.* privilege you can revoke from it a privilege to read data from some table or a database. "},{"title":"Examples​","type":1,"pageTitle":"REVOKE Statement","url":"docs/en/sql-reference/statements/revoke#revoke-example","content":"Grant the john user account with a privilege to select from all the databases, excepting the accounts one: GRANT SELECT ON *.* TO john; REVOKE SELECT ON accounts.* FROM john;  Grant the mira user account with a privilege to select from all the columns of the accounts.staff table, excepting the wage one. GRANT SELECT ON accounts.staff TO mira; REVOKE SELECT(wage) ON accounts.staff FROM mira;  Original article "},{"title":"EXPLAIN Statement","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/explain","content":"","keywords":""},{"title":"EXPLAIN Types​","type":1,"pageTitle":"EXPLAIN Statement","url":"docs/en/sql-reference/statements/explain#explain-types","content":"AST — Abstract syntax tree.SYNTAX — Query text after AST-level optimizations.PLAN — Query execution plan.PIPELINE — Query execution pipeline. "},{"title":"EXPLAIN AST​","type":1,"pageTitle":"EXPLAIN Statement","url":"docs/en/sql-reference/statements/explain#explain-ast","content":"Dump query AST. Supports all types of queries, not only SELECT. Examples: EXPLAIN AST SELECT 1;  SelectWithUnionQuery (children 1) ExpressionList (children 1) SelectQuery (children 1) ExpressionList (children 1) Literal UInt64_1  EXPLAIN AST ALTER TABLE t1 DELETE WHERE date = today();   explain AlterQuery t1 (children 1) ExpressionList (children 1) AlterCommand 27 (children 1) Function equals (children 1) ExpressionList (children 2) Identifier date Function today (children 1) ExpressionList  "},{"title":"EXPLAIN SYNTAX​","type":1,"pageTitle":"EXPLAIN Statement","url":"docs/en/sql-reference/statements/explain#explain-syntax","content":"Returns query after syntax optimizations. Example: EXPLAIN SYNTAX SELECT * FROM system.numbers AS a, system.numbers AS b, system.numbers AS c;  SELECT `--a.number` AS `a.number`, `--b.number` AS `b.number`, number AS `c.number` FROM ( SELECT number AS `--a.number`, b.number AS `--b.number` FROM system.numbers AS a CROSS JOIN system.numbers AS b ) AS `--.s` CROSS JOIN system.numbers AS c  "},{"title":"EXPLAIN PLAN​","type":1,"pageTitle":"EXPLAIN Statement","url":"docs/en/sql-reference/statements/explain#explain-plan","content":"Dump query plan steps. Settings: header — Prints output header for step. Default: 0.description — Prints step description. Default: 1.indexes — Shows used indexes, the number of filtered parts and the number of filtered granules for every index applied. Default: 0. Supported for MergeTree tables.actions — Prints detailed information about step actions. Default: 0.json — Prints query plan steps as a row in JSON format. Default: 0. It is recommended to use TSVRaw format to avoid unnecessary escaping. Example: EXPLAIN SELECT sum(number) FROM numbers(10) GROUP BY number % 4;  Union Expression (Projection) Expression (Before ORDER BY and SELECT) Aggregating Expression (Before GROUP BY) SettingQuotaAndLimits (Set limits and quota after reading from storage) ReadFromStorage (SystemNumbers)  note Step and query cost estimation is not supported. When json = 1, the query plan is represented in JSON format. Every node is a dictionary that always has the keys Node Type and Plans. Node Type is a string with a step name. Plans is an array with child step descriptions. Other optional keys may be added depending on node type and settings. Example: EXPLAIN json = 1, description = 0 SELECT 1 UNION ALL SELECT 2 FORMAT TSVRaw;  [ { &quot;Plan&quot;: { &quot;Node Type&quot;: &quot;Union&quot;, &quot;Plans&quot;: [ { &quot;Node Type&quot;: &quot;Expression&quot;, &quot;Plans&quot;: [ { &quot;Node Type&quot;: &quot;SettingQuotaAndLimits&quot;, &quot;Plans&quot;: [ { &quot;Node Type&quot;: &quot;ReadFromStorage&quot; } ] } ] }, { &quot;Node Type&quot;: &quot;Expression&quot;, &quot;Plans&quot;: [ { &quot;Node Type&quot;: &quot;SettingQuotaAndLimits&quot;, &quot;Plans&quot;: [ { &quot;Node Type&quot;: &quot;ReadFromStorage&quot; } ] } ] } ] } } ]  With description = 1, the Description key is added to the step: { &quot;Node Type&quot;: &quot;ReadFromStorage&quot;, &quot;Description&quot;: &quot;SystemOne&quot; }  With header = 1, the Header key is added to the step as an array of columns. Example: EXPLAIN json = 1, description = 0, header = 1 SELECT 1, 2 + dummy;  [ { &quot;Plan&quot;: { &quot;Node Type&quot;: &quot;Expression&quot;, &quot;Header&quot;: [ { &quot;Name&quot;: &quot;1&quot;, &quot;Type&quot;: &quot;UInt8&quot; }, { &quot;Name&quot;: &quot;plus(2, dummy)&quot;, &quot;Type&quot;: &quot;UInt16&quot; } ], &quot;Plans&quot;: [ { &quot;Node Type&quot;: &quot;SettingQuotaAndLimits&quot;, &quot;Header&quot;: [ { &quot;Name&quot;: &quot;dummy&quot;, &quot;Type&quot;: &quot;UInt8&quot; } ], &quot;Plans&quot;: [ { &quot;Node Type&quot;: &quot;ReadFromStorage&quot;, &quot;Header&quot;: [ { &quot;Name&quot;: &quot;dummy&quot;, &quot;Type&quot;: &quot;UInt8&quot; } ] } ] } ] } } ]  With indexes = 1, the Indexes key is added. It contains an array of used indexes. Each index is described as JSON with Type key (a string MinMax, Partition, PrimaryKey or Skip) and optional keys: Name — An index name (for now, is used only for Skip index).Keys — An array of columns used by the index.Condition — A string with condition used.Description — An index (for now, is used only for Skip index).Initial Parts — A number of parts before the index is applied.Selected Parts — A number of parts after the index is applied.Initial Granules — A number of granules before the index is applied.Selected Granulesis — A number of granules after the index is applied. Example: &quot;Node Type&quot;: &quot;ReadFromMergeTree&quot;, &quot;Indexes&quot;: [ { &quot;Type&quot;: &quot;MinMax&quot;, &quot;Keys&quot;: [&quot;y&quot;], &quot;Condition&quot;: &quot;(y in [1, +inf))&quot;, &quot;Initial Parts&quot;: 5, &quot;Selected Parts&quot;: 4, &quot;Initial Granules&quot;: 12, &quot;Selected Granules&quot;: 11 }, { &quot;Type&quot;: &quot;Partition&quot;, &quot;Keys&quot;: [&quot;y&quot;, &quot;bitAnd(z, 3)&quot;], &quot;Condition&quot;: &quot;and((bitAnd(z, 3) not in [1, 1]), and((y in [1, +inf)), (bitAnd(z, 3) not in [1, 1])))&quot;, &quot;Initial Parts&quot;: 4, &quot;Selected Parts&quot;: 3, &quot;Initial Granules&quot;: 11, &quot;Selected Granules&quot;: 10 }, { &quot;Type&quot;: &quot;PrimaryKey&quot;, &quot;Keys&quot;: [&quot;x&quot;, &quot;y&quot;], &quot;Condition&quot;: &quot;and((x in [11, +inf)), (y in [1, +inf)))&quot;, &quot;Initial Parts&quot;: 3, &quot;Selected Parts&quot;: 2, &quot;Initial Granules&quot;: 10, &quot;Selected Granules&quot;: 6 }, { &quot;Type&quot;: &quot;Skip&quot;, &quot;Name&quot;: &quot;t_minmax&quot;, &quot;Description&quot;: &quot;minmax GRANULARITY 2&quot;, &quot;Initial Parts&quot;: 2, &quot;Selected Parts&quot;: 1, &quot;Initial Granules&quot;: 6, &quot;Selected Granules&quot;: 2 }, { &quot;Type&quot;: &quot;Skip&quot;, &quot;Name&quot;: &quot;t_set&quot;, &quot;Description&quot;: &quot;set GRANULARITY 2&quot;, &quot;Initial Parts&quot;: 1, &quot;Selected Parts&quot;: 1, &quot;Initial Granules&quot;: 2, &quot;Selected Granules&quot;: 1 } ]  With actions = 1, added keys depend on step type. Example: EXPLAIN json = 1, actions = 1, description = 0 SELECT 1 FORMAT TSVRaw;  [ { &quot;Plan&quot;: { &quot;Node Type&quot;: &quot;Expression&quot;, &quot;Expression&quot;: { &quot;Inputs&quot;: [], &quot;Actions&quot;: [ { &quot;Node Type&quot;: &quot;Column&quot;, &quot;Result Type&quot;: &quot;UInt8&quot;, &quot;Result Type&quot;: &quot;Column&quot;, &quot;Column&quot;: &quot;Const(UInt8)&quot;, &quot;Arguments&quot;: [], &quot;Removed Arguments&quot;: [], &quot;Result&quot;: 0 } ], &quot;Outputs&quot;: [ { &quot;Name&quot;: &quot;1&quot;, &quot;Type&quot;: &quot;UInt8&quot; } ], &quot;Positions&quot;: [0], &quot;Project Input&quot;: true }, &quot;Plans&quot;: [ { &quot;Node Type&quot;: &quot;SettingQuotaAndLimits&quot;, &quot;Plans&quot;: [ { &quot;Node Type&quot;: &quot;ReadFromStorage&quot; } ] } ] } } ]  "},{"title":"EXPLAIN PIPELINE​","type":1,"pageTitle":"EXPLAIN Statement","url":"docs/en/sql-reference/statements/explain#explain-pipeline","content":"Settings: header — Prints header for each output port. Default: 0.graph — Prints a graph described in the DOT graph description language. Default: 0.compact — Prints graph in compact mode if graph setting is enabled. Default: 1. Example: EXPLAIN PIPELINE SELECT sum(number) FROM numbers_mt(100000) GROUP BY number % 4;  (Union) (Expression) ExpressionTransform (Expression) ExpressionTransform (Aggregating) Resize 2 → 1 AggregatingTransform × 2 (Expression) ExpressionTransform × 2 (SettingQuotaAndLimits) (ReadFromStorage) NumbersMt × 2 0 → 1  "},{"title":"EXPLAIN ESTIMATE​","type":1,"pageTitle":"EXPLAIN Statement","url":"docs/en/sql-reference/statements/explain#explain-estimate","content":"Shows the estimated number of rows, marks and parts to be read from the tables while processing the query. Works with tables in the MergeTree family. Example Creating a table: CREATE TABLE ttt (i Int64) ENGINE = MergeTree() ORDER BY i SETTINGS index_granularity = 16, write_final_mark = 0; INSERT INTO ttt SELECT number FROM numbers(128); OPTIMIZE TABLE ttt;  Query: EXPLAIN ESTIMATE SELECT * FROM ttt;  Result: ┌─database─┬─table─┬─parts─┬─rows─┬─marks─┐ │ default │ ttt │ 1 │ 128 │ 8 │ └──────────┴───────┴───────┴──────┴───────┘  "},{"title":"EXPLAIN TABLE OVERRIDE​","type":1,"pageTitle":"EXPLAIN Statement","url":"docs/en/sql-reference/statements/explain#explain-table-override","content":"Shows the result of a table override on a table schema accessed through a table function. Also does some validation, throwing an exception if the override would have caused some kind of failure. Example Assume you have a remote MySQL table like this: CREATE TABLE db.tbl ( id INT PRIMARY KEY, created DATETIME DEFAULT now() )  EXPLAIN TABLE OVERRIDE mysql('127.0.0.1:3306', 'db', 'tbl', 'root', 'clickhouse') PARTITION BY toYYYYMM(assumeNotNull(created))  Result: ┌─explain─────────────────────────────────────────────────┐ │ PARTITION BY uses columns: `created` Nullable(DateTime) │ └─────────────────────────────────────────────────────────┘  note The validation is not complete, so a successfull query does not guarantee that the override would not cause issues. Оriginal article "},{"title":"ALL Clause","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/select/all","content":"ALL Clause If there are multiple matching rows in the table, then ALL returns all of them. SELECT ALL is identical to SELECT without DISTINCT. If both ALL and DISTINCT specified, exception will be thrown. ALL can also be specified inside aggregate function with the same effect(noop), for instance: SELECT sum(ALL number) FROM numbers(10); equals to SELECT sum(number) FROM numbers(10); Original article","keywords":""},{"title":"RENAME Statement","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/rename","content":"","keywords":""},{"title":"RENAME DATABASE​","type":1,"pageTitle":"RENAME Statement","url":"docs/en/sql-reference/statements/rename#misc_operations-rename_database","content":"Renames databases. Syntax RENAME DATABASE atomic_database1 TO atomic_database2 [,...] [ON CLUSTER cluster]  "},{"title":"RENAME TABLE​","type":1,"pageTitle":"RENAME Statement","url":"docs/en/sql-reference/statements/rename#misc_operations-rename_table","content":"Renames one or more tables. Renaming tables is a light operation. If you pass a different database after TO, the table will be moved to this database. However, the directories with databases must reside in the same file system. Otherwise, an error is returned. If you rename multiple tables in one query, the operation is not atomic. It may be partially executed, and queries in other sessions may get Table ... does not exist ... error. Syntax RENAME TABLE [db1.]name1 TO [db2.]name2 [,...] [ON CLUSTER cluster]  Example RENAME TABLE table_A TO table_A_bak, table_B TO table_B_bak;  "},{"title":"RENAME DICTIONARY​","type":1,"pageTitle":"RENAME Statement","url":"docs/en/sql-reference/statements/rename#rename_dictionary","content":"Renames one or several dictionaries. This query can be used to move dictionaries between databases. Syntax RENAME DICTIONARY [db0.]dict_A TO [db1.]dict_B [,...] [ON CLUSTER cluster]  See Also Dictionaries "},{"title":"CREATE FUNCTION","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/create/function","content":"CREATE FUNCTION Creates a user defined function from a lambda expression. The expression must consist of function parameters, constants, operators, or other function calls. Syntax CREATE FUNCTION name AS (parameter0, ...) -&gt; expression A function can have an arbitrary number of parameters. There are a few restrictions: The name of a function must be unique among user defined and system functions.Recursive functions are not allowed.All variables used by a function must be specified in its parameter list. If any restriction is violated then an exception is raised. Example Query: CREATE FUNCTION linear_equation AS (x, k, b) -&gt; k*x + b; SELECT number, linear_equation(number, 2, 1) FROM numbers(3); Result: ┌─number─┬─plus(multiply(2, number), 1)─┐ │ 0 │ 1 │ │ 1 │ 3 │ │ 2 │ 5 │ └────────┴──────────────────────────────┘ A conditional function is called in a user defined function in the following query: CREATE FUNCTION parity_str AS (n) -&gt; if(n % 2, 'odd', 'even'); SELECT number, parity_str(number) FROM numbers(3); Result: ┌─number─┬─if(modulo(number, 2), 'odd', 'even')─┐ │ 0 │ even │ │ 1 │ odd │ │ 2 │ even │ └────────┴──────────────────────────────────────┘ ","keywords":""},{"title":"ARRAY JOIN Clause","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/select/array-join","content":"","keywords":""},{"title":"Basic ARRAY JOIN Examples​","type":1,"pageTitle":"ARRAY JOIN Clause","url":"docs/en/sql-reference/statements/select/array-join#basic-array-join-examples","content":"The examples below demonstrate the usage of the ARRAY JOIN and LEFT ARRAY JOIN clauses. Let’s create a table with an Array type column and insert values into it: CREATE TABLE arrays_test ( s String, arr Array(UInt8) ) ENGINE = Memory; INSERT INTO arrays_test VALUES ('Hello', [1,2]), ('World', [3,4,5]), ('Goodbye', []);  ┌─s───────────┬─arr─────┐ │ Hello │ [1,2] │ │ World │ [3,4,5] │ │ Goodbye │ [] │ └─────────────┴─────────┘  The example below uses the ARRAY JOIN clause: SELECT s, arr FROM arrays_test ARRAY JOIN arr;  ┌─s─────┬─arr─┐ │ Hello │ 1 │ │ Hello │ 2 │ │ World │ 3 │ │ World │ 4 │ │ World │ 5 │ └───────┴─────┘  The next example uses the LEFT ARRAY JOIN clause: SELECT s, arr FROM arrays_test LEFT ARRAY JOIN arr;  ┌─s───────────┬─arr─┐ │ Hello │ 1 │ │ Hello │ 2 │ │ World │ 3 │ │ World │ 4 │ │ World │ 5 │ │ Goodbye │ 0 │ └─────────────┴─────┘  "},{"title":"Using Aliases​","type":1,"pageTitle":"ARRAY JOIN Clause","url":"docs/en/sql-reference/statements/select/array-join#using-aliases","content":"An alias can be specified for an array in the ARRAY JOIN clause. In this case, an array item can be accessed by this alias, but the array itself is accessed by the original name. Example: SELECT s, arr, a FROM arrays_test ARRAY JOIN arr AS a;  ┌─s─────┬─arr─────┬─a─┐ │ Hello │ [1,2] │ 1 │ │ Hello │ [1,2] │ 2 │ │ World │ [3,4,5] │ 3 │ │ World │ [3,4,5] │ 4 │ │ World │ [3,4,5] │ 5 │ └───────┴─────────┴───┘  Using aliases, you can perform ARRAY JOIN with an external array. For example: SELECT s, arr_external FROM arrays_test ARRAY JOIN [1, 2, 3] AS arr_external;  ┌─s───────────┬─arr_external─┐ │ Hello │ 1 │ │ Hello │ 2 │ │ Hello │ 3 │ │ World │ 1 │ │ World │ 2 │ │ World │ 3 │ │ Goodbye │ 1 │ │ Goodbye │ 2 │ │ Goodbye │ 3 │ └─────────────┴──────────────┘  Multiple arrays can be comma-separated in the ARRAY JOIN clause. In this case, JOIN is performed with them simultaneously (the direct sum, not the cartesian product). Note that all the arrays must have the same size by default. Example: SELECT s, arr, a, num, mapped FROM arrays_test ARRAY JOIN arr AS a, arrayEnumerate(arr) AS num, arrayMap(x -&gt; x + 1, arr) AS mapped;  ┌─s─────┬─arr─────┬─a─┬─num─┬─mapped─┐ │ Hello │ [1,2] │ 1 │ 1 │ 2 │ │ Hello │ [1,2] │ 2 │ 2 │ 3 │ │ World │ [3,4,5] │ 3 │ 1 │ 4 │ │ World │ [3,4,5] │ 4 │ 2 │ 5 │ │ World │ [3,4,5] │ 5 │ 3 │ 6 │ └───────┴─────────┴───┴─────┴────────┘  The example below uses the arrayEnumerate function: SELECT s, arr, a, num, arrayEnumerate(arr) FROM arrays_test ARRAY JOIN arr AS a, arrayEnumerate(arr) AS num;  ┌─s─────┬─arr─────┬─a─┬─num─┬─arrayEnumerate(arr)─┐ │ Hello │ [1,2] │ 1 │ 1 │ [1,2] │ │ Hello │ [1,2] │ 2 │ 2 │ [1,2] │ │ World │ [3,4,5] │ 3 │ 1 │ [1,2,3] │ │ World │ [3,4,5] │ 4 │ 2 │ [1,2,3] │ │ World │ [3,4,5] │ 5 │ 3 │ [1,2,3] │ └───────┴─────────┴───┴─────┴─────────────────────┘  Multiple arrays with different sizes can be joined by using: SETTINGS enable_unaligned_array_join = 1. Example: SELECT s, arr, a, b FROM arrays_test ARRAY JOIN arr as a, [['a','b'],['c']] as b SETTINGS enable_unaligned_array_join = 1;  ┌─s───────┬─arr─────┬─a─┬─b─────────┐ │ Hello │ [1,2] │ 1 │ ['a','b'] │ │ Hello │ [1,2] │ 2 │ ['c'] │ │ World │ [3,4,5] │ 3 │ ['a','b'] │ │ World │ [3,4,5] │ 4 │ ['c'] │ │ World │ [3,4,5] │ 5 │ [] │ │ Goodbye │ [] │ 0 │ ['a','b'] │ │ Goodbye │ [] │ 0 │ ['c'] │ └─────────┴─────────┴───┴───────────┘  "},{"title":"ARRAY JOIN with Nested Data Structure​","type":1,"pageTitle":"ARRAY JOIN Clause","url":"docs/en/sql-reference/statements/select/array-join#array-join-with-nested-data-structure","content":"ARRAY JOIN also works with nested data structures: CREATE TABLE nested_test ( s String, nest Nested( x UInt8, y UInt32) ) ENGINE = Memory; INSERT INTO nested_test VALUES ('Hello', [1,2], [10,20]), ('World', [3,4,5], [30,40,50]), ('Goodbye', [], []);  ┌─s───────┬─nest.x──┬─nest.y─────┐ │ Hello │ [1,2] │ [10,20] │ │ World │ [3,4,5] │ [30,40,50] │ │ Goodbye │ [] │ [] │ └─────────┴─────────┴────────────┘  SELECT s, `nest.x`, `nest.y` FROM nested_test ARRAY JOIN nest;  ┌─s─────┬─nest.x─┬─nest.y─┐ │ Hello │ 1 │ 10 │ │ Hello │ 2 │ 20 │ │ World │ 3 │ 30 │ │ World │ 4 │ 40 │ │ World │ 5 │ 50 │ └───────┴────────┴────────┘  When specifying names of nested data structures in ARRAY JOIN, the meaning is the same as ARRAY JOIN with all the array elements that it consists of. Examples are listed below: SELECT s, `nest.x`, `nest.y` FROM nested_test ARRAY JOIN `nest.x`, `nest.y`;  ┌─s─────┬─nest.x─┬─nest.y─┐ │ Hello │ 1 │ 10 │ │ Hello │ 2 │ 20 │ │ World │ 3 │ 30 │ │ World │ 4 │ 40 │ │ World │ 5 │ 50 │ └───────┴────────┴────────┘  This variation also makes sense: SELECT s, `nest.x`, `nest.y` FROM nested_test ARRAY JOIN `nest.x`;  ┌─s─────┬─nest.x─┬─nest.y─────┐ │ Hello │ 1 │ [10,20] │ │ Hello │ 2 │ [10,20] │ │ World │ 3 │ [30,40,50] │ │ World │ 4 │ [30,40,50] │ │ World │ 5 │ [30,40,50] │ └───────┴────────┴────────────┘  An alias may be used for a nested data structure, in order to select either the JOIN result or the source array. Example: SELECT s, `n.x`, `n.y`, `nest.x`, `nest.y` FROM nested_test ARRAY JOIN nest AS n;  ┌─s─────┬─n.x─┬─n.y─┬─nest.x──┬─nest.y─────┐ │ Hello │ 1 │ 10 │ [1,2] │ [10,20] │ │ Hello │ 2 │ 20 │ [1,2] │ [10,20] │ │ World │ 3 │ 30 │ [3,4,5] │ [30,40,50] │ │ World │ 4 │ 40 │ [3,4,5] │ [30,40,50] │ │ World │ 5 │ 50 │ [3,4,5] │ [30,40,50] │ └───────┴─────┴─────┴─────────┴────────────┘  Example of using the arrayEnumerate function: SELECT s, `n.x`, `n.y`, `nest.x`, `nest.y`, num FROM nested_test ARRAY JOIN nest AS n, arrayEnumerate(`nest.x`) AS num;  ┌─s─────┬─n.x─┬─n.y─┬─nest.x──┬─nest.y─────┬─num─┐ │ Hello │ 1 │ 10 │ [1,2] │ [10,20] │ 1 │ │ Hello │ 2 │ 20 │ [1,2] │ [10,20] │ 2 │ │ World │ 3 │ 30 │ [3,4,5] │ [30,40,50] │ 1 │ │ World │ 4 │ 40 │ [3,4,5] │ [30,40,50] │ 2 │ │ World │ 5 │ 50 │ [3,4,5] │ [30,40,50] │ 3 │ └───────┴─────┴─────┴─────────┴────────────┴─────┘  "},{"title":"Implementation Details​","type":1,"pageTitle":"ARRAY JOIN Clause","url":"docs/en/sql-reference/statements/select/array-join#implementation-details","content":"The query execution order is optimized when running ARRAY JOIN. Although ARRAY JOIN must always be specified before the WHERE/PREWHERE clause in a query, technically they can be performed in any order, unless result of ARRAY JOIN is used for filtering. The processing order is controlled by the query optimizer. "},{"title":"CREATE VIEW","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/create/view","content":"","keywords":""},{"title":"Normal View​","type":1,"pageTitle":"CREATE VIEW","url":"docs/en/sql-reference/statements/create/view#normal","content":"Syntax: CREATE [OR REPLACE] VIEW [IF NOT EXISTS] [db.]table_name [ON CLUSTER] AS SELECT ...  Normal views do not store any data. They just perform a read from another table on each access. In other words, a normal view is nothing more than a saved query. When reading from a view, this saved query is used as a subquery in the FROM clause. As an example, assume you’ve created a view: CREATE VIEW view AS SELECT ...  and written a query: SELECT a, b, c FROM view  This query is fully equivalent to using the subquery: SELECT a, b, c FROM (SELECT ...)  "},{"title":"Materialized View​","type":1,"pageTitle":"CREATE VIEW","url":"docs/en/sql-reference/statements/create/view#materialized","content":"CREATE MATERIALIZED VIEW [IF NOT EXISTS] [db.]table_name [ON CLUSTER] [TO[db.]name] [ENGINE = engine] [POPULATE] AS SELECT ...  Materialized views store data transformed by the corresponding SELECT query. When creating a materialized view without TO [db].[table], you must specify ENGINE – the table engine for storing data. When creating a materialized view with TO [db].[table], you must not use POPULATE. A materialized view is implemented as follows: when inserting data to the table specified in SELECT, part of the inserted data is converted by this SELECT query, and the result is inserted in the view. note Materialized views in ClickHouse use column names instead of column order during insertion into destination table. If some column names are not present in the SELECT query result, ClickHouse uses a default value, even if the column is not Nullable. A safe practice would be to add aliases for every column when using Materialized views. Materialized views in ClickHouse are implemented more like insert triggers. If there’s some aggregation in the view query, it’s applied only to the batch of freshly inserted data. Any changes to existing data of source table (like update, delete, drop partition, etc.) does not change the materialized view. If you specify POPULATE, the existing table data is inserted into the view when creating it, as if making a CREATE TABLE ... AS SELECT ... . Otherwise, the query contains only the data inserted in the table after creating the view. We do not recommend using POPULATE, since data inserted in the table during the view creation will not be inserted in it. A SELECT query can contain DISTINCT, GROUP BY, ORDER BY, LIMIT. Note that the corresponding conversions are performed independently on each block of inserted data. For example, if GROUP BY is set, data is aggregated during insertion, but only within a single packet of inserted data. The data won’t be further aggregated. The exception is when using an ENGINE that independently performs data aggregation, such as SummingMergeTree. The execution of ALTER queries on materialized views has limitations, so they might be inconvenient. If the materialized view uses the construction TO [db.]name, you can DETACH the view, run ALTER for the target table, and then ATTACH the previously detached (DETACH) view. Note that materialized view is influenced by optimize_on_insert setting. The data is merged before the insertion into a view. Views look the same as normal tables. For example, they are listed in the result of the SHOW TABLES query. To delete a view, use DROP VIEW. Although DROP TABLE works for VIEWs as well. "},{"title":"Live View [Experimental]​","type":1,"pageTitle":"CREATE VIEW","url":"docs/en/sql-reference/statements/create/view#live-view","content":"note This is an experimental feature that may change in backwards-incompatible ways in the future releases. Enable usage of live views and WATCH query using allow_experimental_live_view setting. Input the command set allow_experimental_live_view = 1. CREATE LIVE VIEW [IF NOT EXISTS] [db.]table_name [WITH [TIMEOUT [value_in_sec] [AND]] [REFRESH [value_in_sec]]] AS SELECT ...  Live views store result of the corresponding SELECT query and are updated any time the result of the query changes. Query result as well as partial result needed to combine with new data are stored in memory providing increased performance for repeated queries. Live views can provide push notifications when query result changes using the WATCH query. Live views are triggered by insert into the innermost table specified in the query. Live views work similarly to how a query in a distributed table works. But instead of combining partial results from different servers they combine partial result from current data with partial result from the new data. When a live view query includes a subquery then the cached partial result is only stored for the innermost subquery. info Table function is not supported as the innermost table.Tables that do not have inserts such as a dictionary, system table, a normal view, or a materialized view will not trigger a live view.Only queries where one can combine partial result from the old data plus partial result from the new data will work. Live view will not work for queries that require the complete data set to compute the final result or aggregations where the state of the aggregation must be preserved.Does not work with replicated or distributed tables where inserts are performed on different nodes.Can't be triggered by multiple tables. See WITH REFRESH to force periodic updates of a live view that in some cases can be used as a workaround. "},{"title":"Monitoring Live View Changes​","type":1,"pageTitle":"CREATE VIEW","url":"docs/en/sql-reference/statements/create/view#live-view-monitoring","content":"You can monitor changes in the LIVE VIEW query result using WATCH query. WATCH [db.]live_view  Example: CREATE TABLE mt (x Int8) Engine = MergeTree ORDER BY x; CREATE LIVE VIEW lv AS SELECT sum(x) FROM mt;  Watch a live view while doing a parallel insert into the source table. WATCH lv;  ┌─sum(x)─┬─_version─┐ │ 1 │ 1 │ └────────┴──────────┘ ┌─sum(x)─┬─_version─┐ │ 3 │ 2 │ └────────┴──────────┘ ┌─sum(x)─┬─_version─┐ │ 6 │ 3 │ └────────┴──────────┘  INSERT INTO mt VALUES (1); INSERT INTO mt VALUES (2); INSERT INTO mt VALUES (3);  Or add EVENTS clause to just get change events. WATCH [db.]live_view EVENTS;  Example: WATCH lv EVENTS;  ┌─version─┐ │ 1 │ └─────────┘ ┌─version─┐ │ 2 │ └─────────┘ ┌─version─┐ │ 3 │ └─────────┘  You can execute SELECT query on a live view in the same way as for any regular view or a table. If the query result is cached it will return the result immediately without running the stored query on the underlying tables. SELECT * FROM [db.]live_view WHERE ...  "},{"title":"Force Live View Refresh​","type":1,"pageTitle":"CREATE VIEW","url":"docs/en/sql-reference/statements/create/view#live-view-alter-refresh","content":"You can force live view refresh using the ALTER LIVE VIEW [db.]table_name REFRESH statement. "},{"title":"WITH TIMEOUT Clause​","type":1,"pageTitle":"CREATE VIEW","url":"docs/en/sql-reference/statements/create/view#live-view-with-timeout","content":"When a live view is created with a WITH TIMEOUT clause then the live view will be dropped automatically after the specified number of seconds elapse since the end of the last WATCH query that was watching the live view. CREATE LIVE VIEW [db.]table_name WITH TIMEOUT [value_in_sec] AS SELECT ...  If the timeout value is not specified then the value specified by the temporary_live_view_timeout setting is used. Example: CREATE TABLE mt (x Int8) Engine = MergeTree ORDER BY x; CREATE LIVE VIEW lv WITH TIMEOUT 15 AS SELECT sum(x) FROM mt;  "},{"title":"WITH REFRESH Clause​","type":1,"pageTitle":"CREATE VIEW","url":"docs/en/sql-reference/statements/create/view#live-view-with-refresh","content":"When a live view is created with a WITH REFRESH clause then it will be automatically refreshed after the specified number of seconds elapse since the last refresh or trigger. CREATE LIVE VIEW [db.]table_name WITH REFRESH [value_in_sec] AS SELECT ...  If the refresh value is not specified then the value specified by the periodic_live_view_refresh setting is used. Example: CREATE LIVE VIEW lv WITH REFRESH 5 AS SELECT now(); WATCH lv  ┌───────────────now()─┬─_version─┐ │ 2021-02-21 08:47:05 │ 1 │ └─────────────────────┴──────────┘ ┌───────────────now()─┬─_version─┐ │ 2021-02-21 08:47:10 │ 2 │ └─────────────────────┴──────────┘ ┌───────────────now()─┬─_version─┐ │ 2021-02-21 08:47:15 │ 3 │ └─────────────────────┴──────────┘  You can combine WITH TIMEOUT and WITH REFRESH clauses using an AND clause. CREATE LIVE VIEW [db.]table_name WITH TIMEOUT [value_in_sec] AND REFRESH [value_in_sec] AS SELECT ...  Example: CREATE LIVE VIEW lv WITH TIMEOUT 15 AND REFRESH 5 AS SELECT now();  After 15 sec the live view will be automatically dropped if there are no active WATCH queries. WATCH lv  Code: 60. DB::Exception: Received from localhost:9000. DB::Exception: Table default.lv does not exist..  "},{"title":"Live View Usage​","type":1,"pageTitle":"CREATE VIEW","url":"docs/en/sql-reference/statements/create/view#live-view-usage","content":"Most common uses of live view tables include: Providing push notifications for query result changes to avoid polling.Caching results of most frequent queries to provide immediate query results.Watching for table changes and triggering a follow-up select queries.Watching metrics from system tables using periodic refresh. See Also ALTER LIVE VIEW "},{"title":"Window View [Experimental]​","type":1,"pageTitle":"CREATE VIEW","url":"docs/en/sql-reference/statements/create/view#window-view","content":"info This is an experimental feature that may change in backwards-incompatible ways in the future releases. Enable usage of window views and WATCH query using allow_experimental_window_view setting. Input the command set allow_experimental_window_view = 1. CREATE WINDOW VIEW [IF NOT EXISTS] [db.]table_name [TO [db.]table_name] [ENGINE = engine] [WATERMARK = strategy] [ALLOWED_LATENESS = interval_function] AS SELECT ... GROUP BY time_window_function  Window view can aggregate data by time window and output the results when the window is ready to fire. It stores the partial aggregation results in an inner(or specified) table to reduce latency and can push the processing result to a specified table or push notifications using the WATCH query. Creating a window view is similar to creating MATERIALIZED VIEW. Window view needs an inner storage engine to store intermediate data. The inner storage will use AggregatingMergeTree as the default engine. "},{"title":"Time Window Functions​","type":1,"pageTitle":"CREATE VIEW","url":"docs/en/sql-reference/statements/create/view#window-view-timewindowfunctions","content":"Time window functions are used to get the lower and upper window bound of records. The window view needs to be used with a time window function. "},{"title":"TIME ATTRIBUTES​","type":1,"pageTitle":"CREATE VIEW","url":"docs/en/sql-reference/statements/create/view#window-view-timeattributes","content":"Window view supports processing time and event time process. Processing time allows window view to produce results based on the local machine's time and is used by default. It is the most straightforward notion of time but does not provide determinism. The processing time attribute can be defined by setting the time_attr of the time window function to a table column or using the function now(). The following query creates a window view with processing time. CREATE WINDOW VIEW wv AS SELECT count(number), tumbleStart(w_id) as w_start from date GROUP BY tumble(now(), INTERVAL '5' SECOND) as w_id  Event time is the time that each individual event occurred on its producing device. This time is typically embedded within the records when it is generated. Event time processing allows for consistent results even in case of out-of-order events or late events. Window view supports event time processing by using WATERMARK syntax. Window view provides three watermark strategies: STRICTLY_ASCENDING: Emits a watermark of the maximum observed timestamp so far. Rows that have a timestamp smaller to the max timestamp are not late.ASCENDING: Emits a watermark of the maximum observed timestamp so far minus 1. Rows that have a timestamp equal and smaller to the max timestamp are not late.BOUNDED: WATERMARK=INTERVAL. Emits watermarks, which are the maximum observed timestamp minus the specified delay. The following queries are examples of creating a window view with WATERMARK: CREATE WINDOW VIEW wv WATERMARK=STRICTLY_ASCENDING AS SELECT count(number) FROM date GROUP BY tumble(timestamp, INTERVAL '5' SECOND); CREATE WINDOW VIEW wv WATERMARK=ASCENDING AS SELECT count(number) FROM date GROUP BY tumble(timestamp, INTERVAL '5' SECOND); CREATE WINDOW VIEW wv WATERMARK=INTERVAL '3' SECOND AS SELECT count(number) FROM date GROUP BY tumble(timestamp, INTERVAL '5' SECOND);  By default, the window will be fired when the watermark comes, and elements that arrived behind the watermark will be dropped. Window view supports late event processing by setting ALLOWED_LATENESS=INTERVAL. An example of lateness handling is: CREATE WINDOW VIEW test.wv TO test.dst WATERMARK=ASCENDING ALLOWED_LATENESS=INTERVAL '2' SECOND AS SELECT count(a) AS count, tumbleEnd(wid) AS w_end FROM test.mt GROUP BY tumble(timestamp, INTERVAL '5' SECOND) AS wid;  Note that elements emitted by a late firing should be treated as updated results of a previous computation. Instead of firing at the end of windows, the window view will fire immediately when the late event arrives. Thus, it will result in multiple outputs for the same window. Users need to take these duplicated results into account or deduplicate them. "},{"title":"Monitoring New Windows​","type":1,"pageTitle":"CREATE VIEW","url":"docs/en/sql-reference/statements/create/view#window-view-monitoring","content":"Window view supports the WATCH query to monitoring changes, or use TO syntax to output the results to a table. WATCH [db.]window_view [EVENTS] [LIMIT n] [FORMAT format]  WATCH query acts similar as in LIVE VIEW. A LIMIT can be specified to set the number of updates to receive before terminating the query. The EVENTS clause can be used to obtain a short form of the WATCH query where instead of the query result you will just get the latest query watermark. "},{"title":"Settings​","type":1,"pageTitle":"CREATE VIEW","url":"docs/en/sql-reference/statements/create/view#window-view-settings","content":"window_view_clean_interval: The clean interval of window view in seconds to free outdated data. The system will retain the windows that have not been fully triggered according to the system time or WATERMARK configuration, and the other data will be deleted.window_view_heartbeat_interval: The heartbeat interval in seconds to indicate the watch query is alive. "},{"title":"Example​","type":1,"pageTitle":"CREATE VIEW","url":"docs/en/sql-reference/statements/create/view#window-view-example","content":"Suppose we need to count the number of click logs per 10 seconds in a log table called data, and its table structure is: CREATE TABLE data ( `id` UInt64, `timestamp` DateTime) ENGINE = Memory;  First, we create a window view with tumble window of 10 seconds interval: CREATE WINDOW VIEW wv as select count(id), tumbleStart(w_id) as window_start from data group by tumble(timestamp, INTERVAL '10' SECOND) as w_id  Then, we use the WATCH query to get the results. WATCH wv  When logs are inserted into table data, INSERT INTO data VALUES(1,now())  The WATCH query should print the results as follows: ┌─count(id)─┬────────window_start─┐ │ 1 │ 2020-01-14 16:56:40 │ └───────────┴─────────────────────┘  Alternatively, we can attach the output to another table using TO syntax. CREATE WINDOW VIEW wv TO dst AS SELECT count(id), tumbleStart(w_id) as window_start FROM data GROUP BY tumble(timestamp, INTERVAL '10' SECOND) as w_id  Additional examples can be found among stateful tests of ClickHouse (they are named *window_view* there). "},{"title":"Window View Usage​","type":1,"pageTitle":"CREATE VIEW","url":"docs/en/sql-reference/statements/create/view#window-view-usage","content":"The window view is useful in the following scenarios: Monitoring: Aggregate and calculate the metrics logs by time, and output the results to a target table. The dashboard can use the target table as a source table.Analyzing: Automatically aggregate and preprocess data in the time window. This can be useful when analyzing a large number of logs. The preprocessing eliminates repeated calculations in multiple queries and reduces query latency. "},{"title":"FORMAT Clause","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/select/format","content":"","keywords":""},{"title":"Default Format​","type":1,"pageTitle":"FORMAT Clause","url":"docs/en/sql-reference/statements/select/format#default-format","content":"If the FORMAT clause is omitted, the default format is used, which depends on both the settings and the interface used for accessing the ClickHouse server. For the HTTP interface and the command-line client in batch mode, the default format is TabSeparated. For the command-line client in interactive mode, the default format is PrettyCompact (it produces compact human-readable tables). "},{"title":"Implementation Details​","type":1,"pageTitle":"FORMAT Clause","url":"docs/en/sql-reference/statements/select/format#implementation-details","content":"When using the command-line client, data is always passed over the network in an internal efficient format (Native). The client independently interprets the FORMAT clause of the query and formats the data itself (thus relieving the network and the server from the extra load). "},{"title":"CREATE ROW POLICY","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/create/row-policy","content":"","keywords":""},{"title":"USING Clause​","type":1,"pageTitle":"CREATE ROW POLICY","url":"docs/en/sql-reference/statements/create/row-policy#create-row-policy-using","content":"Allows to specify a condition to filter rows. An user will see a row if the condition is calculated to non-zero for the row. "},{"title":"TO Clause​","type":1,"pageTitle":"CREATE ROW POLICY","url":"docs/en/sql-reference/statements/create/row-policy#create-row-policy-to","content":"In the section TO you can provide a list of users and roles this policy should work for. For example, CREATE ROW POLICY ... TO accountant, john@localhost. Keyword ALL means all the ClickHouse users including current user. Keyword ALL EXCEPT allow to exclude some users from the all users list, for example, CREATE ROW POLICY ... TO ALL EXCEPT accountant, john@localhost note If there are no row policies defined for a table then any user can SELECT all the row from the table. Defining one or more row policies for the table makes the access to the table depending on the row policies no matter if those row policies are defined for the current user or not. For example, the following policy CREATE ROW POLICY pol1 ON mydb.table1 USING b=1 TO mira, peter forbids the users mira and peter to see the rows with b != 1, and any non-mentioned user (e.g., the user paul) will see no rows from mydb.table1 at all. If that's not desirable it can't be fixed by adding one more row policy, like the following: CREATE ROW POLICY pol2 ON mydb.table1 USING 1 TO ALL EXCEPT mira, peter "},{"title":"AS Clause​","type":1,"pageTitle":"CREATE ROW POLICY","url":"docs/en/sql-reference/statements/create/row-policy#create-row-policy-as","content":"It's allowed to have more than one policy enabled on the same table for the same user at the one time. So we need a way to combine the conditions from multiple policies. By default policies are combined using the boolean OR operator. For example, the following policies CREATE ROW POLICY pol1 ON mydb.table1 USING b=1 TO mira, peter CREATE ROW POLICY pol2 ON mydb.table1 USING c=2 TO peter, antonio  enables the user peter to see rows with either b=1 or c=2. The AS clause specifies how policies should be combined with other policies. Policies can be either permissive or restrictive. By default policies are permissive, which means they are combined using the boolean OR operator. A policy can be defined as restrictive as an alternative. Restrictive policies are combined using the boolean AND operator. Here is the general formula: row_is_visible = (one or more of the permissive policies' conditions are non-zero) AND (all of the restrictive policies's conditions are non-zero)  For example, the following policies CREATE ROW POLICY pol1 ON mydb.table1 USING b=1 TO mira, peter CREATE ROW POLICY pol2 ON mydb.table1 USING c=2 AS RESTRICTIVE TO peter, antonio  enables the user peter to see rows only if both b=1 AND c=2. "},{"title":"ON CLUSTER Clause​","type":1,"pageTitle":"CREATE ROW POLICY","url":"docs/en/sql-reference/statements/create/row-policy#create-row-policy-on-cluster","content":"Allows creating row policies on a cluster, see Distributed DDL. "},{"title":"Examples​","type":1,"pageTitle":"CREATE ROW POLICY","url":"docs/en/sql-reference/statements/create/row-policy#examples","content":"CREATE ROW POLICY filter1 ON mydb.mytable USING a&lt;1000 TO accountant, john@localhost CREATE ROW POLICY filter2 ON mydb.mytable USING a&lt;1000 AND b=5 TO ALL EXCEPT mira CREATE ROW POLICY filter3 ON mydb.mytable USING 1 TO admin "},{"title":"HAVING Clause","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/select/having","content":"","keywords":""},{"title":"Limitations​","type":1,"pageTitle":"HAVING Clause","url":"docs/en/sql-reference/statements/select/having#limitations","content":"HAVING can’t be used if aggregation is not performed. Use WHERE instead. "},{"title":"FROM Clause","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/select/from","content":"","keywords":""},{"title":"FINAL Modifier​","type":1,"pageTitle":"FROM Clause","url":"docs/en/sql-reference/statements/select/from#select-from-final","content":"When FINAL is specified, ClickHouse fully merges the data before returning the result and thus performs all data transformations that happen during merges for the given table engine. It is applicable when selecting data from tables that use the MergeTree-engine family. Also supported for: Replicated versions of MergeTree engines.View, Buffer, Distributed, and MaterializedView engines that operate over other engines, provided they were created over MergeTree-engine tables. Now SELECT queries with FINAL are executed in parallel and slightly faster. But there are drawbacks (see below). The max_final_threads setting limits the number of threads used. "},{"title":"Drawbacks​","type":1,"pageTitle":"FROM Clause","url":"docs/en/sql-reference/statements/select/from#drawbacks","content":"Queries that use FINAL are executed slightly slower than similar queries that do not, because: Data is merged during query execution.Queries with FINAL read primary key columns in addition to the columns specified in the query. In most cases, avoid using FINAL. The common approach is to use different queries that assume the background processes of the MergeTree engine have’t happened yet and deal with it by applying aggregation (for example, to discard duplicates). "},{"title":"Implementation Details​","type":1,"pageTitle":"FROM Clause","url":"docs/en/sql-reference/statements/select/from#implementation-details","content":"If the FROM clause is omitted, data will be read from the system.one table. The system.one table contains exactly one row (this table fulfills the same purpose as the DUAL table found in other DBMSs). To execute a query, all the columns listed in the query are extracted from the appropriate table. Any columns not needed for the external query are thrown out of the subqueries. If a query does not list any columns (for example, SELECT count() FROM t), some column is extracted from the table anyway (the smallest one is preferred), in order to calculate the number of rows. "},{"title":"GRANT Statement","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/grant","content":"","keywords":""},{"title":"Granting Privilege Syntax​","type":1,"pageTitle":"GRANT Statement","url":"docs/en/sql-reference/statements/grant#grant-privigele-syntax","content":"GRANT [ON CLUSTER cluster_name] privilege[(column_name [,...])] [,...] ON {db.table|db.*|*.*|table|*} TO {user | role | CURRENT_USER} [,...] [WITH GRANT OPTION] [WITH REPLACE OPTION]  privilege — Type of privilege.role — ClickHouse user role.user — ClickHouse user account. The WITH GRANT OPTION clause grants user or role with permission to execute the GRANT query. Users can grant privileges of the same scope they have and less. The WITH REPLACE OPTION clause replace old privileges by new privileges for the user or role, if is not specified it appends privileges. "},{"title":"Assigning Role Syntax​","type":1,"pageTitle":"GRANT Statement","url":"docs/en/sql-reference/statements/grant#assign-role-syntax","content":"GRANT [ON CLUSTER cluster_name] role [,...] TO {user | another_role | CURRENT_USER} [,...] [WITH ADMIN OPTION] [WITH REPLACE OPTION]  role — ClickHouse user role.user — ClickHouse user account. The WITH ADMIN OPTION clause grants ADMIN OPTION privilege to user or role. The WITH REPLACE OPTION clause replace old roles by new role for the user or role, if is not specified it appends roles. "},{"title":"Usage​","type":1,"pageTitle":"GRANT Statement","url":"docs/en/sql-reference/statements/grant#grant-usage","content":"To use GRANT, your account must have the GRANT OPTION privilege. You can grant privileges only inside the scope of your account privileges. For example, administrator has granted privileges to the john account by the query: GRANT SELECT(x,y) ON db.table TO john WITH GRANT OPTION  It means that john has the permission to execute: SELECT x,y FROM db.table.SELECT x FROM db.table.SELECT y FROM db.table. john can’t execute SELECT z FROM db.table. The SELECT * FROM db.table also is not available. Processing this query, ClickHouse does not return any data, even x and y. The only exception is if a table contains only x and y columns. In this case ClickHouse returns all the data. Also john has the GRANT OPTION privilege, so it can grant other users with privileges of the same or smaller scope. Specifying privileges you can use asterisk (*) instead of a table or a database name. For example, the GRANT SELECT ON db.* TO john query allows john to execute the SELECT query over all the tables in db database. Also, you can omit database name. In this case privileges are granted for current database. For example, GRANT SELECT ON * TO john grants the privilege on all the tables in the current database, GRANT SELECT ON mytable TO john grants the privilege on the mytable table in the current database. Access to the system database is always allowed (since this database is used for processing queries). You can grant multiple privileges to multiple accounts in one query. The query GRANT SELECT, INSERT ON *.* TO john, robin allows accounts john and robin to execute the INSERT and SELECT queries over all the tables in all the databases on the server. "},{"title":"Privileges​","type":1,"pageTitle":"GRANT Statement","url":"docs/en/sql-reference/statements/grant#grant-privileges","content":"Privilege is a permission to execute specific kind of queries. Privileges have a hierarchical structure. A set of permitted queries depends on the privilege scope. Hierarchy of privileges: SELECTINSERTALTER ALTER TABLE ALTER UPDATEALTER DELETEALTER COLUMN ALTER ADD COLUMNALTER DROP COLUMNALTER MODIFY COLUMNALTER COMMENT COLUMNALTER CLEAR COLUMNALTER RENAME COLUMN ALTER INDEX ALTER ORDER BYALTER SAMPLE BYALTER ADD INDEXALTER DROP INDEXALTER MATERIALIZE INDEXALTER CLEAR INDEX ALTER CONSTRAINT ALTER ADD CONSTRAINTALTER DROP CONSTRAINT ALTER TTL ALTER MATERIALIZE TTL ALTER SETTINGSALTER MOVE PARTITIONALTER FETCH PARTITIONALTER FREEZE PARTITION ALTER VIEW ALTER VIEW REFRESHALTER VIEW MODIFY QUERY CREATE CREATE DATABASECREATE TABLE CREATE TEMPORARY TABLE CREATE VIEWCREATE DICTIONARYCREATE FUNCTION DROP DROP DATABASEDROP TABLEDROP VIEWDROP DICTIONARYDROP FUNCTION TRUNCATEOPTIMIZESHOW SHOW DATABASESSHOW TABLESSHOW COLUMNSSHOW DICTIONARIES KILL QUERYACCESS MANAGEMENT CREATE USERALTER USERDROP USERCREATE ROLEALTER ROLEDROP ROLECREATE ROW POLICYALTER ROW POLICYDROP ROW POLICYCREATE QUOTAALTER QUOTADROP QUOTACREATE SETTINGS PROFILEALTER SETTINGS PROFILEDROP SETTINGS PROFILESHOW ACCESS SHOW_USERSSHOW_ROLESSHOW_ROW_POLICIESSHOW_QUOTASSHOW_SETTINGS_PROFILES ROLE ADMIN SYSTEM SYSTEM SHUTDOWNSYSTEM DROP CACHE SYSTEM DROP DNS CACHESYSTEM DROP MARK CACHESYSTEM DROP UNCOMPRESSED CACHE SYSTEM RELOAD SYSTEM RELOAD CONFIGSYSTEM RELOAD DICTIONARY SYSTEM RELOAD EMBEDDED DICTIONARIES SYSTEM RELOAD FUNCTIONSYSTEM RELOAD FUNCTIONS SYSTEM MERGESSYSTEM TTL MERGESSYSTEM FETCHESSYSTEM MOVESSYSTEM SENDS SYSTEM DISTRIBUTED SENDSSYSTEM REPLICATED SENDS SYSTEM REPLICATION QUEUESSYSTEM SYNC REPLICASYSTEM RESTART REPLICASYSTEM FLUSH SYSTEM FLUSH DISTRIBUTEDSYSTEM FLUSH LOGS INTROSPECTION addressToLineaddressToLineWithInlinesaddressToSymboldemangle SOURCES FILEURLREMOTEYSQLODBCJDBCHDFSS3 dictGet Examples of how this hierarchy is treated: The ALTER privilege includes all other ALTER* privileges.ALTER CONSTRAINT includes ALTER ADD CONSTRAINT and ALTER DROP CONSTRAINT privileges. Privileges are applied at different levels. Knowing of a level suggests syntax available for privilege. Levels (from lower to higher): COLUMN — Privilege can be granted for column, table, database, or globally.TABLE — Privilege can be granted for table, database, or globally.VIEW — Privilege can be granted for view, database, or globally.DICTIONARY — Privilege can be granted for dictionary, database, or globally.DATABASE — Privilege can be granted for database or globally.GLOBAL — Privilege can be granted only globally.GROUP — Groups privileges of different levels. When GROUP-level privilege is granted, only that privileges from the group are granted which correspond to the used syntax. Examples of allowed syntax: GRANT SELECT(x) ON db.table TO userGRANT SELECT ON db.* TO user Examples of disallowed syntax: GRANT CREATE USER(x) ON db.table TO userGRANT CREATE USER ON db.* TO user The special privilege ALL grants all the privileges to a user account or a role. By default, a user account or a role has no privileges. If a user or a role has no privileges, it is displayed as NONE privilege. Some queries by their implementation require a set of privileges. For example, to execute the RENAME query you need the following privileges: SELECT, CREATE TABLE, INSERT and DROP TABLE. "},{"title":"SELECT​","type":1,"pageTitle":"GRANT Statement","url":"docs/en/sql-reference/statements/grant#grant-select","content":"Allows executing SELECT queries. Privilege level: COLUMN. Description User granted with this privilege can execute SELECT queries over a specified list of columns in the specified table and database. If user includes other columns then specified a query returns no data. Consider the following privilege: GRANT SELECT(x,y) ON db.table TO john  This privilege allows john to execute any SELECT query that involves data from the x and/or y columns in db.table, for example, SELECT x FROM db.table. john can’t execute SELECT z FROM db.table. The SELECT * FROM db.table also is not available. Processing this query, ClickHouse does not return any data, even x and y. The only exception is if a table contains only x and y columns, in this case ClickHouse returns all the data. "},{"title":"INSERT​","type":1,"pageTitle":"GRANT Statement","url":"docs/en/sql-reference/statements/grant#grant-insert","content":"Allows executing INSERT queries. Privilege level: COLUMN. Description User granted with this privilege can execute INSERT queries over a specified list of columns in the specified table and database. If user includes other columns then specified a query does not insert any data. Example GRANT INSERT(x,y) ON db.table TO john  The granted privilege allows john to insert data to the x and/or y columns in db.table. "},{"title":"ALTER​","type":1,"pageTitle":"GRANT Statement","url":"docs/en/sql-reference/statements/grant#grant-alter","content":"Allows executing ALTER queries according to the following hierarchy of privileges: ALTER. Level: COLUMN. ALTER TABLE. Level: GROUP ALTER UPDATE. Level: COLUMN. Aliases: UPDATEALTER DELETE. Level: COLUMN. Aliases: DELETEALTER COLUMN. Level: GROUP ALTER ADD COLUMN. Level: COLUMN. Aliases: ADD COLUMNALTER DROP COLUMN. Level: COLUMN. Aliases: DROP COLUMNALTER MODIFY COLUMN. Level: COLUMN. Aliases: MODIFY COLUMNALTER COMMENT COLUMN. Level: COLUMN. Aliases: COMMENT COLUMNALTER CLEAR COLUMN. Level: COLUMN. Aliases: CLEAR COLUMNALTER RENAME COLUMN. Level: COLUMN. Aliases: RENAME COLUMN ALTER INDEX. Level: GROUP. Aliases: INDEX ALTER ORDER BY. Level: TABLE. Aliases: ALTER MODIFY ORDER BY, MODIFY ORDER BYALTER SAMPLE BY. Level: TABLE. Aliases: ALTER MODIFY SAMPLE BY, MODIFY SAMPLE BYALTER ADD INDEX. Level: TABLE. Aliases: ADD INDEXALTER DROP INDEX. Level: TABLE. Aliases: DROP INDEXALTER MATERIALIZE INDEX. Level: TABLE. Aliases: MATERIALIZE INDEXALTER CLEAR INDEX. Level: TABLE. Aliases: CLEAR INDEX ALTER CONSTRAINT. Level: GROUP. Aliases: CONSTRAINT ALTER ADD CONSTRAINT. Level: TABLE. Aliases: ADD CONSTRAINTALTER DROP CONSTRAINT. Level: TABLE. Aliases: DROP CONSTRAINT ALTER TTL. Level: TABLE. Aliases: ALTER MODIFY TTL, MODIFY TTL ALTER MATERIALIZE TTL. Level: TABLE. Aliases: MATERIALIZE TTL ALTER SETTINGS. Level: TABLE. Aliases: ALTER SETTING, ALTER MODIFY SETTING, MODIFY SETTINGALTER MOVE PARTITION. Level: TABLE. Aliases: ALTER MOVE PART, MOVE PARTITION, MOVE PARTALTER FETCH PARTITION. Level: TABLE. Aliases: ALTER FETCH PART, FETCH PARTITION, FETCH PARTALTER FREEZE PARTITION. Level: TABLE. Aliases: FREEZE PARTITION ALTER VIEW Level: GROUP ALTER VIEW REFRESH. Level: VIEW. Aliases: ALTER LIVE VIEW REFRESH, REFRESH VIEWALTER VIEW MODIFY QUERY. Level: VIEW. Aliases: ALTER TABLE MODIFY QUERY Examples of how this hierarchy is treated: The ALTER privilege includes all other ALTER* privileges.ALTER CONSTRAINT includes ALTER ADD CONSTRAINT and ALTER DROP CONSTRAINT privileges. Notes The MODIFY SETTING privilege allows modifying table engine settings. It does not affect settings or server configuration parameters.The ATTACH operation needs the CREATE privilege.The DETACH operation needs the DROP privilege.To stop mutation by the KILL MUTATION query, you need to have a privilege to start this mutation. For example, if you want to stop the ALTER UPDATE query, you need the ALTER UPDATE, ALTER TABLE, or ALTER privilege. "},{"title":"CREATE​","type":1,"pageTitle":"GRANT Statement","url":"docs/en/sql-reference/statements/grant#grant-create","content":"Allows executing CREATE and ATTACH DDL-queries according to the following hierarchy of privileges: CREATE. Level: GROUP CREATE DATABASE. Level: DATABASECREATE TABLE. Level: TABLE CREATE TEMPORARY TABLE. Level: GLOBAL CREATE VIEW. Level: VIEWCREATE DICTIONARY. Level: DICTIONARY Notes To delete the created table, a user needs DROP. "},{"title":"DROP​","type":1,"pageTitle":"GRANT Statement","url":"docs/en/sql-reference/statements/grant#grant-drop","content":"Allows executing DROP and DETACH queries according to the following hierarchy of privileges: DROP. Level: GROUP DROP DATABASE. Level: DATABASEDROP TABLE. Level: TABLEDROP VIEW. Level: VIEWDROP DICTIONARY. Level: DICTIONARY "},{"title":"TRUNCATE​","type":1,"pageTitle":"GRANT Statement","url":"docs/en/sql-reference/statements/grant#grant-truncate","content":"Allows executing TRUNCATE queries. Privilege level: TABLE. "},{"title":"OPTIMIZE​","type":1,"pageTitle":"GRANT Statement","url":"docs/en/sql-reference/statements/grant#grant-optimize","content":"Allows executing OPTIMIZE TABLE queries. Privilege level: TABLE. "},{"title":"SHOW​","type":1,"pageTitle":"GRANT Statement","url":"docs/en/sql-reference/statements/grant#grant-show","content":"Allows executing SHOW, DESCRIBE, USE, and EXISTS queries according to the following hierarchy of privileges: SHOW. Level: GROUP SHOW DATABASES. Level: DATABASE. Allows to execute SHOW DATABASES, SHOW CREATE DATABASE, USE &lt;database&gt; queries.SHOW TABLES. Level: TABLE. Allows to execute SHOW TABLES, EXISTS &lt;table&gt;, CHECK &lt;table&gt; queries.SHOW COLUMNS. Level: COLUMN. Allows to execute SHOW CREATE TABLE, DESCRIBE queries.SHOW DICTIONARIES. Level: DICTIONARY. Allows to execute SHOW DICTIONARIES, SHOW CREATE DICTIONARY, EXISTS &lt;dictionary&gt; queries. Notes A user has the SHOW privilege if it has any other privilege concerning the specified table, dictionary or database. "},{"title":"KILL QUERY​","type":1,"pageTitle":"GRANT Statement","url":"docs/en/sql-reference/statements/grant#grant-kill-query","content":"Allows executing KILL queries according to the following hierarchy of privileges: Privilege level: GLOBAL. Notes KILL QUERY privilege allows one user to kill queries of other users. "},{"title":"ACCESS MANAGEMENT​","type":1,"pageTitle":"GRANT Statement","url":"docs/en/sql-reference/statements/grant#grant-access-management","content":"Allows a user to execute queries that manage users, roles and row policies. ACCESS MANAGEMENT. Level: GROUP CREATE USER. Level: GLOBALALTER USER. Level: GLOBALDROP USER. Level: GLOBALCREATE ROLE. Level: GLOBALALTER ROLE. Level: GLOBALDROP ROLE. Level: GLOBALROLE ADMIN. Level: GLOBALCREATE ROW POLICY. Level: GLOBAL. Aliases: CREATE POLICYALTER ROW POLICY. Level: GLOBAL. Aliases: ALTER POLICYDROP ROW POLICY. Level: GLOBAL. Aliases: DROP POLICYCREATE QUOTA. Level: GLOBALALTER QUOTA. Level: GLOBALDROP QUOTA. Level: GLOBALCREATE SETTINGS PROFILE. Level: GLOBAL. Aliases: CREATE PROFILEALTER SETTINGS PROFILE. Level: GLOBAL. Aliases: ALTER PROFILEDROP SETTINGS PROFILE. Level: GLOBAL. Aliases: DROP PROFILESHOW ACCESS. Level: GROUP SHOW_USERS. Level: GLOBAL. Aliases: SHOW CREATE USERSHOW_ROLES. Level: GLOBAL. Aliases: SHOW CREATE ROLESHOW_ROW_POLICIES. Level: GLOBAL. Aliases: SHOW POLICIES, SHOW CREATE ROW POLICY, SHOW CREATE POLICYSHOW_QUOTAS. Level: GLOBAL. Aliases: SHOW CREATE QUOTASHOW_SETTINGS_PROFILES. Level: GLOBAL. Aliases: SHOW PROFILES, SHOW CREATE SETTINGS PROFILE, SHOW CREATE PROFILE The ROLE ADMIN privilege allows a user to assign and revoke any roles including those which are not assigned to the user with the admin option. "},{"title":"SYSTEM​","type":1,"pageTitle":"GRANT Statement","url":"docs/en/sql-reference/statements/grant#grant-system","content":"Allows a user to execute SYSTEM queries according to the following hierarchy of privileges. SYSTEM. Level: GROUP SYSTEM SHUTDOWN. Level: GLOBAL. Aliases: SYSTEM KILL, SHUTDOWNSYSTEM DROP CACHE. Aliases: DROP CACHE SYSTEM DROP DNS CACHE. Level: GLOBAL. Aliases: SYSTEM DROP DNS, DROP DNS CACHE, DROP DNSSYSTEM DROP MARK CACHE. Level: GLOBAL. Aliases: SYSTEM DROP MARK, DROP MARK CACHE, DROP MARKSSYSTEM DROP UNCOMPRESSED CACHE. Level: GLOBAL. Aliases: SYSTEM DROP UNCOMPRESSED, DROP UNCOMPRESSED CACHE, DROP UNCOMPRESSED SYSTEM RELOAD. Level: GROUP SYSTEM RELOAD CONFIG. Level: GLOBAL. Aliases: RELOAD CONFIGSYSTEM RELOAD DICTIONARY. Level: GLOBAL. Aliases: SYSTEM RELOAD DICTIONARIES, RELOAD DICTIONARY, RELOAD DICTIONARIES SYSTEM RELOAD EMBEDDED DICTIONARIES. Level: GLOBAL. Aliases: RELOAD EMBEDDED DICTIONARIES SYSTEM MERGES. Level: TABLE. Aliases: SYSTEM STOP MERGES, SYSTEM START MERGES, STOP MERGES, START MERGESSYSTEM TTL MERGES. Level: TABLE. Aliases: SYSTEM STOP TTL MERGES, SYSTEM START TTL MERGES, STOP TTL MERGES, START TTL MERGESSYSTEM FETCHES. Level: TABLE. Aliases: SYSTEM STOP FETCHES, SYSTEM START FETCHES, STOP FETCHES, START FETCHESSYSTEM MOVES. Level: TABLE. Aliases: SYSTEM STOP MOVES, SYSTEM START MOVES, STOP MOVES, START MOVESSYSTEM SENDS. Level: GROUP. Aliases: SYSTEM STOP SENDS, SYSTEM START SENDS, STOP SENDS, START SENDS SYSTEM DISTRIBUTED SENDS. Level: TABLE. Aliases: SYSTEM STOP DISTRIBUTED SENDS, SYSTEM START DISTRIBUTED SENDS, STOP DISTRIBUTED SENDS, START DISTRIBUTED SENDSSYSTEM REPLICATED SENDS. Level: TABLE. Aliases: SYSTEM STOP REPLICATED SENDS, SYSTEM START REPLICATED SENDS, STOP REPLICATED SENDS, START REPLICATED SENDS SYSTEM REPLICATION QUEUES. Level: TABLE. Aliases: SYSTEM STOP REPLICATION QUEUES, SYSTEM START REPLICATION QUEUES, STOP REPLICATION QUEUES, START REPLICATION QUEUESSYSTEM SYNC REPLICA. Level: TABLE. Aliases: SYNC REPLICASYSTEM RESTART REPLICA. Level: TABLE. Aliases: RESTART REPLICASYSTEM FLUSH. Level: GROUP SYSTEM FLUSH DISTRIBUTED. Level: TABLE. Aliases: FLUSH DISTRIBUTEDSYSTEM FLUSH LOGS. Level: GLOBAL. Aliases: FLUSH LOGS The SYSTEM RELOAD EMBEDDED DICTIONARIES privilege implicitly granted by the SYSTEM RELOAD DICTIONARY ON *.* privilege. "},{"title":"INTROSPECTION​","type":1,"pageTitle":"GRANT Statement","url":"docs/en/sql-reference/statements/grant#grant-introspection","content":"Allows using introspection functions. INTROSPECTION. Level: GROUP. Aliases: INTROSPECTION FUNCTIONS addressToLine. Level: GLOBALaddressToLineWithInlines. Level: GLOBALaddressToSymbol. Level: GLOBALdemangle. Level: GLOBAL "},{"title":"SOURCES​","type":1,"pageTitle":"GRANT Statement","url":"docs/en/sql-reference/statements/grant#grant-sources","content":"Allows using external data sources. Applies to table engines and table functions. SOURCES. Level: GROUP FILE. Level: GLOBALURL. Level: GLOBALREMOTE. Level: GLOBALYSQL. Level: GLOBALODBC. Level: GLOBALJDBC. Level: GLOBALHDFS. Level: GLOBALS3. Level: GLOBAL The SOURCES privilege enables use of all the sources. Also you can grant a privilege for each source individually. To use sources, you need additional privileges. Examples: To create a table with the MySQL table engine, you need CREATE TABLE (ON db.table_name) and MYSQL privileges.To use the mysql table function, you need CREATE TEMPORARY TABLE and MYSQL privileges. "},{"title":"dictGet​","type":1,"pageTitle":"GRANT Statement","url":"docs/en/sql-reference/statements/grant#grant-dictget","content":"dictGet. Aliases: dictHas, dictGetHierarchy, dictIsIn Allows a user to execute dictGet, dictHas, dictGetHierarchy, dictIsIn functions. Privilege level: DICTIONARY. Examples GRANT dictGet ON mydb.mydictionary TO johnGRANT dictGet ON mydictionary TO john "},{"title":"ALL​","type":1,"pageTitle":"GRANT Statement","url":"docs/en/sql-reference/statements/grant#grant-all","content":"Grants all the privileges on regulated entity to a user account or a role. "},{"title":"NONE​","type":1,"pageTitle":"GRANT Statement","url":"docs/en/sql-reference/statements/grant#grant-none","content":"Doesn’t grant any privileges. "},{"title":"ADMIN OPTION​","type":1,"pageTitle":"GRANT Statement","url":"docs/en/sql-reference/statements/grant#admin-option-privilege","content":"The ADMIN OPTION privilege allows a user to grant their role to another user. "},{"title":"GROUP BY Clause","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/select/group-by","content":"","keywords":""},{"title":"NULL Processing​","type":1,"pageTitle":"GROUP BY Clause","url":"docs/en/sql-reference/statements/select/group-by#null-processing","content":"For grouping, ClickHouse interprets NULL as a value, and NULL==NULL. It differs from NULL processing in most other contexts. Here’s an example to show what this means. Assume you have this table: ┌─x─┬────y─┐ │ 1 │ 2 │ │ 2 │ ᴺᵁᴸᴸ │ │ 3 │ 2 │ │ 3 │ 3 │ │ 3 │ ᴺᵁᴸᴸ │ └───┴──────┘  The query SELECT sum(x), y FROM t_null_big GROUP BY y results in: ┌─sum(x)─┬────y─┐ │ 4 │ 2 │ │ 3 │ 3 │ │ 5 │ ᴺᵁᴸᴸ │ └────────┴──────┘  You can see that GROUP BY for y = NULL summed up x, as if NULL is this value. If you pass several keys to GROUP BY, the result will give you all the combinations of the selection, as if NULL were a specific value. "},{"title":"WITH ROLLUP Modifier​","type":1,"pageTitle":"GROUP BY Clause","url":"docs/en/sql-reference/statements/select/group-by#with-rollup-modifier","content":"WITH ROLLUP modifier is used to calculate subtotals for the key expressions, based on their order in the GROUP BY list. The subtotals rows are added after the result table. The subtotals are calculated in the reverse order: at first subtotals are calculated for the last key expression in the list, then for the previous one, and so on up to the first key expression. In the subtotals rows the values of already &quot;grouped&quot; key expressions are set to 0 or empty line. note Mind that HAVING clause can affect the subtotals results. Example Consider the table t: ┌─year─┬─month─┬─day─┐ │ 2019 │ 1 │ 5 │ │ 2019 │ 1 │ 15 │ │ 2020 │ 1 │ 5 │ │ 2020 │ 1 │ 15 │ │ 2020 │ 10 │ 5 │ │ 2020 │ 10 │ 15 │ └──────┴───────┴─────┘  Query: SELECT year, month, day, count(*) FROM t GROUP BY year, month, day WITH ROLLUP;  As GROUP BY section has three key expressions, the result contains four tables with subtotals &quot;rolled up&quot; from right to left: GROUP BY year, month, day;GROUP BY year, month (and day column is filled with zeros);GROUP BY year (now month, day columns are both filled with zeros);and totals (and all three key expression columns are zeros). ┌─year─┬─month─┬─day─┬─count()─┐ │ 2020 │ 10 │ 15 │ 1 │ │ 2020 │ 1 │ 5 │ 1 │ │ 2019 │ 1 │ 5 │ 1 │ │ 2020 │ 1 │ 15 │ 1 │ │ 2019 │ 1 │ 15 │ 1 │ │ 2020 │ 10 │ 5 │ 1 │ └──────┴───────┴─────┴─────────┘ ┌─year─┬─month─┬─day─┬─count()─┐ │ 2019 │ 1 │ 0 │ 2 │ │ 2020 │ 1 │ 0 │ 2 │ │ 2020 │ 10 │ 0 │ 2 │ └──────┴───────┴─────┴─────────┘ ┌─year─┬─month─┬─day─┬─count()─┐ │ 2019 │ 0 │ 0 │ 2 │ │ 2020 │ 0 │ 0 │ 4 │ └──────┴───────┴─────┴─────────┘ ┌─year─┬─month─┬─day─┬─count()─┐ │ 0 │ 0 │ 0 │ 6 │ └──────┴───────┴─────┴─────────┘  "},{"title":"WITH CUBE Modifier​","type":1,"pageTitle":"GROUP BY Clause","url":"docs/en/sql-reference/statements/select/group-by#with-cube-modifier","content":"WITH CUBE modifier is used to calculate subtotals for every combination of the key expressions in the GROUP BY list. The subtotals rows are added after the result table. In the subtotals rows the values of all &quot;grouped&quot; key expressions are set to 0 or empty line. note Mind that HAVING clause can affect the subtotals results. Example Consider the table t: ┌─year─┬─month─┬─day─┐ │ 2019 │ 1 │ 5 │ │ 2019 │ 1 │ 15 │ │ 2020 │ 1 │ 5 │ │ 2020 │ 1 │ 15 │ │ 2020 │ 10 │ 5 │ │ 2020 │ 10 │ 15 │ └──────┴───────┴─────┘  Query: SELECT year, month, day, count(*) FROM t GROUP BY year, month, day WITH CUBE;  As GROUP BY section has three key expressions, the result contains eight tables with subtotals for all key expression combinations: GROUP BY year, month, dayGROUP BY year, monthGROUP BY year, dayGROUP BY yearGROUP BY month, dayGROUP BY monthGROUP BY dayand totals. Columns, excluded from GROUP BY, are filled with zeros. ┌─year─┬─month─┬─day─┬─count()─┐ │ 2020 │ 10 │ 15 │ 1 │ │ 2020 │ 1 │ 5 │ 1 │ │ 2019 │ 1 │ 5 │ 1 │ │ 2020 │ 1 │ 15 │ 1 │ │ 2019 │ 1 │ 15 │ 1 │ │ 2020 │ 10 │ 5 │ 1 │ └──────┴───────┴─────┴─────────┘ ┌─year─┬─month─┬─day─┬─count()─┐ │ 2019 │ 1 │ 0 │ 2 │ │ 2020 │ 1 │ 0 │ 2 │ │ 2020 │ 10 │ 0 │ 2 │ └──────┴───────┴─────┴─────────┘ ┌─year─┬─month─┬─day─┬─count()─┐ │ 2020 │ 0 │ 5 │ 2 │ │ 2019 │ 0 │ 5 │ 1 │ │ 2020 │ 0 │ 15 │ 2 │ │ 2019 │ 0 │ 15 │ 1 │ └──────┴───────┴─────┴─────────┘ ┌─year─┬─month─┬─day─┬─count()─┐ │ 2019 │ 0 │ 0 │ 2 │ │ 2020 │ 0 │ 0 │ 4 │ └──────┴───────┴─────┴─────────┘ ┌─year─┬─month─┬─day─┬─count()─┐ │ 0 │ 1 │ 5 │ 2 │ │ 0 │ 10 │ 15 │ 1 │ │ 0 │ 10 │ 5 │ 1 │ │ 0 │ 1 │ 15 │ 2 │ └──────┴───────┴─────┴─────────┘ ┌─year─┬─month─┬─day─┬─count()─┐ │ 0 │ 1 │ 0 │ 4 │ │ 0 │ 10 │ 0 │ 2 │ └──────┴───────┴─────┴─────────┘ ┌─year─┬─month─┬─day─┬─count()─┐ │ 0 │ 0 │ 5 │ 3 │ │ 0 │ 0 │ 15 │ 3 │ └──────┴───────┴─────┴─────────┘ ┌─year─┬─month─┬─day─┬─count()─┐ │ 0 │ 0 │ 0 │ 6 │ └──────┴───────┴─────┴─────────┘  "},{"title":"WITH TOTALS Modifier​","type":1,"pageTitle":"GROUP BY Clause","url":"docs/en/sql-reference/statements/select/group-by#with-totals-modifier","content":"If the WITH TOTALS modifier is specified, another row will be calculated. This row will have key columns containing default values (zeros or empty lines), and columns of aggregate functions with the values calculated across all the rows (the “total” values). This extra row is only produced in JSON*, TabSeparated*, and Pretty* formats, separately from the other rows: In JSON* formats, this row is output as a separate ‘totals’ field.In TabSeparated* formats, the row comes after the main result, preceded by an empty row (after the other data).In Pretty* formats, the row is output as a separate table after the main result.In the other formats it is not available. note totals is output in the results of SELECT queries, and is not output in INSERT INTO ... SELECT. WITH TOTALS can be run in different ways when HAVING is present. The behavior depends on the totals_mode setting. "},{"title":"Configuring Totals Processing​","type":1,"pageTitle":"GROUP BY Clause","url":"docs/en/sql-reference/statements/select/group-by#configuring-totals-processing","content":"By default, totals_mode = 'before_having'. In this case, ‘totals’ is calculated across all rows, including the ones that do not pass through HAVING and max_rows_to_group_by. The other alternatives include only the rows that pass through HAVING in ‘totals’, and behave differently with the setting max_rows_to_group_by and group_by_overflow_mode = 'any'. after_having_exclusive – Don’t include rows that didn’t pass through max_rows_to_group_by. In other words, ‘totals’ will have less than or the same number of rows as it would if max_rows_to_group_by were omitted. after_having_inclusive – Include all the rows that didn’t pass through ‘max_rows_to_group_by’ in ‘totals’. In other words, ‘totals’ will have more than or the same number of rows as it would if max_rows_to_group_by were omitted. after_having_auto – Count the number of rows that passed through HAVING. If it is more than a certain amount (by default, 50%), include all the rows that didn’t pass through ‘max_rows_to_group_by’ in ‘totals’. Otherwise, do not include them. totals_auto_threshold – By default, 0.5. The coefficient for after_having_auto. If max_rows_to_group_by and group_by_overflow_mode = 'any' are not used, all variations of after_having are the same, and you can use any of them (for example, after_having_auto). You can use WITH TOTALS in subqueries, including subqueries in the JOIN clause (in this case, the respective total values are combined). "},{"title":"Examples​","type":1,"pageTitle":"GROUP BY Clause","url":"docs/en/sql-reference/statements/select/group-by#examples","content":"Example: SELECT count(), median(FetchTiming &gt; 60 ? 60 : FetchTiming), count() - sum(Refresh) FROM hits  As opposed to MySQL (and conforming to standard SQL), you can’t get some value of some column that is not in a key or aggregate function (except constant expressions). To work around this, you can use the ‘any’ aggregate function (get the first encountered value) or ‘min/max’. Example: SELECT domainWithoutWWW(URL) AS domain, count(), any(Title) AS title -- getting the first occurred page header for each domain. FROM hits GROUP BY domain  For every different key value encountered, GROUP BY calculates a set of aggregate function values. "},{"title":"Implementation Details​","type":1,"pageTitle":"GROUP BY Clause","url":"docs/en/sql-reference/statements/select/group-by#implementation-details","content":"Aggregation is one of the most important features of a column-oriented DBMS, and thus it’s implementation is one of the most heavily optimized parts of ClickHouse. By default, aggregation is done in memory using a hash-table. It has 40+ specializations that are chosen automatically depending on “grouping key” data types. "},{"title":"GROUP BY Optimization Depending on Table Sorting Key​","type":1,"pageTitle":"GROUP BY Clause","url":"docs/en/sql-reference/statements/select/group-by#aggregation-in-order","content":"The aggregation can be performed more effectively, if a table is sorted by some key, and GROUP BY expression contains at least prefix of sorting key or injective functions. In this case when a new key is read from table, the in-between result of aggregation can be finalized and sent to client. This behaviour is switched on by the optimize_aggregation_in_order setting. Such optimization reduces memory usage during aggregation, but in some cases may slow down the query execution. "},{"title":"GROUP BY in External Memory​","type":1,"pageTitle":"GROUP BY Clause","url":"docs/en/sql-reference/statements/select/group-by#select-group-by-in-external-memory","content":"You can enable dumping temporary data to the disk to restrict memory usage during GROUP BY. The max_bytes_before_external_group_by setting determines the threshold RAM consumption for dumping GROUP BY temporary data to the file system. If set to 0 (the default), it is disabled. When using max_bytes_before_external_group_by, we recommend that you set max_memory_usage about twice as high. This is necessary because there are two stages to aggregation: reading the data and forming intermediate data (1) and merging the intermediate data (2). Dumping data to the file system can only occur during stage 1. If the temporary data wasn’t dumped, then stage 2 might require up to the same amount of memory as in stage 1. For example, if max_memory_usage was set to 10000000000 and you want to use external aggregation, it makes sense to set max_bytes_before_external_group_by to 10000000000, and max_memory_usage to 20000000000. When external aggregation is triggered (if there was at least one dump of temporary data), maximum consumption of RAM is only slightly more than max_bytes_before_external_group_by. With distributed query processing, external aggregation is performed on remote servers. In order for the requester server to use only a small amount of RAM, set distributed_aggregation_memory_efficient to 1. When merging data flushed to the disk, as well as when merging results from remote servers when the distributed_aggregation_memory_efficient setting is enabled, consumes up to 1/256 * the_number_of_threads from the total amount of RAM. When external aggregation is enabled, if there was less than max_bytes_before_external_group_by of data (i.e. data was not flushed), the query runs just as fast as without external aggregation. If any temporary data was flushed, the run time will be several times longer (approximately three times). If you have an ORDER BY with a LIMIT after GROUP BY, then the amount of used RAM depends on the amount of data in LIMIT, not in the whole table. But if the ORDER BY does not have LIMIT, do not forget to enable external sorting (max_bytes_before_external_sort). "},{"title":"DISTINCT Clause","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/select/distinct","content":"","keywords":""},{"title":"DISTINCT and ORDER BY​","type":1,"pageTitle":"DISTINCT Clause","url":"docs/en/sql-reference/statements/select/distinct#distinct-orderby","content":"ClickHouse supports using the DISTINCT and ORDER BY clauses for different columns in one query. The DISTINCT clause is executed before the ORDER BY clause. Consider the table: ┌─a─┬─b─┐ │ 2 │ 1 │ │ 1 │ 2 │ │ 3 │ 3 │ │ 2 │ 4 │ └───┴───┘  Selecting data: SELECT DISTINCT a FROM t1 ORDER BY b ASC;  ┌─a─┐ │ 2 │ │ 1 │ │ 3 │ └───┘  Selecting data with the different sorting direction: SELECT DISTINCT a FROM t1 ORDER BY b DESC;  ┌─a─┐ │ 3 │ │ 1 │ │ 2 │ └───┘  Row 2, 4 was cut before sorting. Take this implementation specificity into account when programming queries. "},{"title":"Null Processing​","type":1,"pageTitle":"DISTINCT Clause","url":"docs/en/sql-reference/statements/select/distinct#null-processing","content":"DISTINCT works with NULL as if NULL were a specific value, and NULL==NULL. In other words, in the DISTINCT results, different combinations with NULL occur only once. It differs from NULL processing in most other contexts. "},{"title":"Alternatives​","type":1,"pageTitle":"DISTINCT Clause","url":"docs/en/sql-reference/statements/select/distinct#alternatives","content":"It is possible to obtain the same result by applying GROUP BY across the same set of values as specified as SELECT clause, without using any aggregate functions. But there are few differences from GROUP BY approach: DISTINCT can be applied together with GROUP BY.When ORDER BY is omitted and LIMIT is defined, the query stops running immediately after the required number of different rows has been read.Data blocks are output as they are processed, without waiting for the entire query to finish running. "},{"title":"EXCEPT Clause","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/select/except","content":"EXCEPT Clause The EXCEPT clause returns only those rows that result from the first query without the second. The queries must match the number of columns, order, and type. The result of EXCEPT can contain duplicate rows. Multiple EXCEPT statements are executed left to right if parenthesis are not specified. The EXCEPT operator has the same priority as the UNION clause and lower priority than the INTERSECT clause. SELECT column1 [, column2 ] FROM table1 [WHERE condition] EXCEPT SELECT column1 [, column2 ] FROM table2 [WHERE condition] The condition could be any expression based on your requirements. Examples Query: SELECT number FROM numbers(1,10) EXCEPT SELECT number FROM numbers(3,6); Result: ┌─number─┐ │ 1 │ │ 2 │ │ 9 │ │ 10 │ └────────┘ Query: CREATE TABLE t1(one String, two String, three String) ENGINE=Memory(); CREATE TABLE t2(four String, five String, six String) ENGINE=Memory(); INSERT INTO t1 VALUES ('q', 'm', 'b'), ('s', 'd', 'f'), ('l', 'p', 'o'), ('s', 'd', 'f'), ('s', 'd', 'f'), ('k', 't', 'd'), ('l', 'p', 'o'); INSERT INTO t2 VALUES ('q', 'm', 'b'), ('b', 'd', 'k'), ('s', 'y', 't'), ('s', 'd', 'f'), ('m', 'f', 'o'), ('k', 'k', 'd'); SELECT * FROM t1 EXCEPT SELECT * FROM t2; Result: ┌─one─┬─two─┬─three─┐ │ l │ p │ o │ │ k │ t │ d │ │ l │ p │ o │ └─────┴─────┴───────┘ See Also UNIONINTERSECT","keywords":""},{"title":"SELECT Query","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/select/","content":"","keywords":""},{"title":"Syntax​","type":1,"pageTitle":"SELECT Query","url":"docs/en/sql-reference/statements/select/#syntax","content":"[WITH expr_list|(subquery)] SELECT [DISTINCT [ON (column1, column2, ...)]] expr_list [FROM [db.]table | (subquery) | table_function] [FINAL] [SAMPLE sample_coeff] [ARRAY JOIN ...] [GLOBAL] [ANY|ALL|ASOF] [INNER|LEFT|RIGHT|FULL|CROSS] [OUTER|SEMI|ANTI] JOIN (subquery)|table (ON &lt;expr_list&gt;)|(USING &lt;column_list&gt;) [PREWHERE expr] [WHERE expr] [GROUP BY expr_list] [WITH ROLLUP|WITH CUBE] [WITH TOTALS] [HAVING expr] [ORDER BY expr_list] [WITH FILL] [FROM expr] [TO expr] [STEP expr] [LIMIT [offset_value, ]n BY columns] [LIMIT [n, ]m] [WITH TIES] [SETTINGS ...] [UNION ...] [INTO OUTFILE filename [COMPRESSION type] ] [FORMAT format]  All clauses are optional, except for the required list of expressions immediately after SELECT which is covered in more detail below. Specifics of each optional clause are covered in separate sections, which are listed in the same order as they are executed: WITH clauseSELECT clauseDISTINCT clauseFROM clauseSAMPLE clauseJOIN clausePREWHERE clauseWHERE clauseGROUP BY clauseLIMIT BY clauseHAVING clauseLIMIT clauseOFFSET clauseUNION clauseINTERSECT clauseEXCEPT clauseINTO OUTFILE clauseFORMAT clause "},{"title":"SELECT Clause​","type":1,"pageTitle":"SELECT Query","url":"docs/en/sql-reference/statements/select/#select-clause","content":"Expressions specified in the SELECT clause are calculated after all the operations in the clauses described above are finished. These expressions work as if they apply to separate rows in the result. If expressions in the SELECT clause contain aggregate functions, then ClickHouse processes aggregate functions and expressions used as their arguments during the GROUP BY aggregation. If you want to include all columns in the result, use the asterisk (*) symbol. For example, SELECT * FROM .... "},{"title":"COLUMNS expression​","type":1,"pageTitle":"SELECT Query","url":"docs/en/sql-reference/statements/select/#columns-expression","content":"To match some columns in the result with a re2 regular expression, you can use the COLUMNS expression. COLUMNS('regexp')  For example, consider the table: CREATE TABLE default.col_names (aa Int8, ab Int8, bc Int8) ENGINE = TinyLog  The following query selects data from all the columns containing the a symbol in their name. SELECT COLUMNS('a') FROM col_names  ┌─aa─┬─ab─┐ │ 1 │ 1 │ └────┴────┘  The selected columns are returned not in the alphabetical order. You can use multiple COLUMNS expressions in a query and apply functions to them. For example: SELECT COLUMNS('a'), COLUMNS('c'), toTypeName(COLUMNS('c')) FROM col_names  ┌─aa─┬─ab─┬─bc─┬─toTypeName(bc)─┐ │ 1 │ 1 │ 1 │ Int8 │ └────┴────┴────┴────────────────┘  Each column returned by the COLUMNS expression is passed to the function as a separate argument. Also you can pass other arguments to the function if it supports them. Be careful when using functions. If a function does not support the number of arguments you have passed to it, ClickHouse throws an exception. For example: SELECT COLUMNS('a') + COLUMNS('c') FROM col_names  Received exception from server (version 19.14.1): Code: 42. DB::Exception: Received from localhost:9000. DB::Exception: Number of arguments for function plus does not match: passed 3, should be 2.  In this example, COLUMNS('a') returns two columns: aa and ab. COLUMNS('c') returns the bc column. The + operator can’t apply to 3 arguments, so ClickHouse throws an exception with the relevant message. Columns that matched the COLUMNS expression can have different data types. If COLUMNS does not match any columns and is the only expression in SELECT, ClickHouse throws an exception. "},{"title":"Asterisk​","type":1,"pageTitle":"SELECT Query","url":"docs/en/sql-reference/statements/select/#asterisk","content":"You can put an asterisk in any part of a query instead of an expression. When the query is analyzed, the asterisk is expanded to a list of all table columns (excluding the MATERIALIZED and ALIAS columns). There are only a few cases when using an asterisk is justified: When creating a table dump.For tables containing just a few columns, such as system tables.For getting information about what columns are in a table. In this case, set LIMIT 1. But it is better to use the DESC TABLE query.When there is strong filtration on a small number of columns using PREWHERE.In subqueries (since columns that aren’t needed for the external query are excluded from subqueries). In all other cases, we do not recommend using the asterisk, since it only gives you the drawbacks of a columnar DBMS instead of the advantages. In other words using the asterisk is not recommended. "},{"title":"Extreme Values​","type":1,"pageTitle":"SELECT Query","url":"docs/en/sql-reference/statements/select/#extreme-values","content":"In addition to results, you can also get minimum and maximum values for the results columns. To do this, set the extremes setting to 1. Minimums and maximums are calculated for numeric types, dates, and dates with times. For other columns, the default values are output. An extra two rows are calculated – the minimums and maximums, respectively. These extra two rows are output in JSON*, TabSeparated*, and Pretty* formats, separate from the other rows. They are not output for other formats. In JSON* formats, the extreme values are output in a separate ‘extremes’ field. In TabSeparated* formats, the row comes after the main result, and after ‘totals’ if present. It is preceded by an empty row (after the other data). In Pretty* formats, the row is output as a separate table after the main result, and after totals if present. Extreme values are calculated for rows before LIMIT, but after LIMIT BY. However, when using LIMIT offset, size, the rows before offset are included in extremes. In stream requests, the result may also include a small number of rows that passed through LIMIT. "},{"title":"Notes​","type":1,"pageTitle":"SELECT Query","url":"docs/en/sql-reference/statements/select/#notes","content":"You can use synonyms (AS aliases) in any part of a query. The GROUP BY, ORDER BY, and LIMIT BY clauses can support positional arguments. To enable this, switch on the enable_positional_arguments setting. Then, for example, ORDER BY 1,2 will be sorting rows in the table on the first and then the second column. "},{"title":"Implementation Details​","type":1,"pageTitle":"SELECT Query","url":"docs/en/sql-reference/statements/select/#implementation-details","content":"If the query omits the DISTINCT, GROUP BY and ORDER BY clauses and the IN and JOIN subqueries, the query will be completely stream processed, using O(1) amount of RAM. Otherwise, the query might consume a lot of RAM if the appropriate restrictions are not specified: max_memory_usagemax_rows_to_group_bymax_rows_to_sortmax_rows_in_distinctmax_bytes_in_distinctmax_rows_in_setmax_bytes_in_setmax_rows_in_joinmax_bytes_in_joinmax_bytes_before_external_sortmax_bytes_before_external_group_by For more information, see the section “Settings”. It is possible to use external sorting (saving temporary tables to a disk) and external aggregation. "},{"title":"SELECT modifiers​","type":1,"pageTitle":"SELECT Query","url":"docs/en/sql-reference/statements/select/#select-modifiers","content":"You can use the following modifiers in SELECT queries. "},{"title":"APPLY​","type":1,"pageTitle":"SELECT Query","url":"docs/en/sql-reference/statements/select/#apply-modifier","content":"Allows you to invoke some function for each row returned by an outer table expression of a query. Syntax: SELECT &lt;expr&gt; APPLY( &lt;func&gt; ) FROM [db.]table_name  Example: CREATE TABLE columns_transformers (i Int64, j Int16, k Int64) ENGINE = MergeTree ORDER by (i); INSERT INTO columns_transformers VALUES (100, 10, 324), (120, 8, 23); SELECT * APPLY(sum) FROM columns_transformers;  ┌─sum(i)─┬─sum(j)─┬─sum(k)─┐ │ 220 │ 18 │ 347 │ └────────┴────────┴────────┘  "},{"title":"EXCEPT​","type":1,"pageTitle":"SELECT Query","url":"docs/en/sql-reference/statements/select/#except-modifier","content":"Specifies the names of one or more columns to exclude from the result. All matching column names are omitted from the output. Syntax: SELECT &lt;expr&gt; EXCEPT ( col_name1 [, col_name2, col_name3, ...] ) FROM [db.]table_name  Example: SELECT * EXCEPT (i) from columns_transformers;  ┌──j─┬───k─┐ │ 10 │ 324 │ │ 8 │ 23 │ └────┴─────┘  "},{"title":"REPLACE​","type":1,"pageTitle":"SELECT Query","url":"docs/en/sql-reference/statements/select/#replace-modifier","content":"Specifies one or more expression aliases. Each alias must match a column name from the SELECT * statement. In the output column list, the column that matches the alias is replaced by the expression in that REPLACE. This modifier does not change the names or order of columns. However, it can change the value and the value type. Syntax: SELECT &lt;expr&gt; REPLACE( &lt;expr&gt; AS col_name) from [db.]table_name  Example: SELECT * REPLACE(i + 1 AS i) from columns_transformers;  ┌───i─┬──j─┬───k─┐ │ 101 │ 10 │ 324 │ │ 121 │ 8 │ 23 │ └─────┴────┴─────┘  "},{"title":"Modifier Combinations​","type":1,"pageTitle":"SELECT Query","url":"docs/en/sql-reference/statements/select/#modifier-combinations","content":"You can use each modifier separately or combine them. Examples: Using the same modifier multiple times. SELECT COLUMNS('[jk]') APPLY(toString) APPLY(length) APPLY(max) from columns_transformers;  ┌─max(length(toString(j)))─┬─max(length(toString(k)))─┐ │ 2 │ 3 │ └──────────────────────────┴──────────────────────────┘  Using multiple modifiers in a single query. SELECT * REPLACE(i + 1 AS i) EXCEPT (j) APPLY(sum) from columns_transformers;  ┌─sum(plus(i, 1))─┬─sum(k)─┐ │ 222 │ 347 │ └─────────────────┴────────┘  "},{"title":"SETTINGS in SELECT Query​","type":1,"pageTitle":"SELECT Query","url":"docs/en/sql-reference/statements/select/#settings-in-select","content":"You can specify the necessary settings right in the SELECT query. The setting value is applied only to this query and is reset to default or previous value after the query is executed. Other ways to make settings see here. Example SELECT * FROM some_table SETTINGS optimize_read_in_order=1, cast_keep_nullable=1;  Original article "},{"title":"INTERSECT Clause","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/select/intersect","content":"INTERSECT Clause The INTERSECT clause returns only those rows that result from both the first and the second queries. The queries must match the number of columns, order, and type. The result of INTERSECT can contain duplicate rows. Multiple INTERSECT statements are executes left to right if parenthesis are not specified. The INTERSECT operator has a higher priority than the UNION and EXCEPT clause. SELECT column1 [, column2 ] FROM table1 [WHERE condition] INTERSECT SELECT column1 [, column2 ] FROM table2 [WHERE condition] The condition could be any expression based on your requirements. Examples Query: SELECT number FROM numbers(1,10) INTERSECT SELECT number FROM numbers(3,6); Result: ┌─number─┐ │ 3 │ │ 4 │ │ 5 │ │ 6 │ │ 7 │ │ 8 │ └────────┘ Query: CREATE TABLE t1(one String, two String, three String) ENGINE=Memory(); CREATE TABLE t2(four String, five String, six String) ENGINE=Memory(); INSERT INTO t1 VALUES ('q', 'm', 'b'), ('s', 'd', 'f'), ('l', 'p', 'o'), ('s', 'd', 'f'), ('s', 'd', 'f'), ('k', 't', 'd'), ('l', 'p', 'o'); INSERT INTO t2 VALUES ('q', 'm', 'b'), ('b', 'd', 'k'), ('s', 'y', 't'), ('s', 'd', 'f'), ('m', 'f', 'o'), ('k', 'k', 'd'); SELECT * FROM t1 INTERSECT SELECT * FROM t2; Result: ┌─one─┬─two─┬─three─┐ │ q │ m │ b │ │ s │ d │ f │ │ s │ d │ f │ │ s │ d │ f │ └─────┴─────┴───────┘ See Also UNIONEXCEPT","keywords":""},{"title":"LIMIT Clause","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/select/limit","content":"","keywords":""},{"title":"LIMIT … WITH TIES Modifier​","type":1,"pageTitle":"LIMIT Clause","url":"docs/en/sql-reference/statements/select/limit#limit-with-ties","content":"When you set WITH TIES modifier for LIMIT n[,m] and specify ORDER BY expr_list, you will get in result first n or n,m rows and all rows with same ORDER BY fields values equal to row at position n for LIMIT n and m for LIMIT n,m. This modifier also can be combined with ORDER BY … WITH FILL modifier. For example, the following query SELECT * FROM ( SELECT number%50 AS n FROM numbers(100) ) ORDER BY n LIMIT 0,5  returns ┌─n─┐ │ 0 │ │ 0 │ │ 1 │ │ 1 │ │ 2 │ └───┘  but after apply WITH TIES modifier SELECT * FROM ( SELECT number%50 AS n FROM numbers(100) ) ORDER BY n LIMIT 0,5 WITH TIES  it returns another rows set ┌─n─┐ │ 0 │ │ 0 │ │ 1 │ │ 1 │ │ 2 │ │ 2 │ └───┘  cause row number 6 have same value “2” for field n as row number 5 "},{"title":"PREWHERE Clause","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/select/prewhere","content":"","keywords":""},{"title":"Controlling Prewhere Manually​","type":1,"pageTitle":"PREWHERE Clause","url":"docs/en/sql-reference/statements/select/prewhere#controlling-prewhere-manually","content":"The clause has the same meaning as the WHERE clause. The difference is in which data is read from the table. When manually controlling PREWHERE for filtration conditions that are used by a minority of the columns in the query, but that provide strong data filtration. This reduces the volume of data to read. A query may simultaneously specify PREWHERE and WHERE. In this case, PREWHERE precedes WHERE. If the optimize_move_to_prewhere setting is set to 0, heuristics to automatically move parts of expressions from WHERE to PREWHERE are disabled. If query has FINAL modifier, the PREWHERE optimization is not always correct. It is enabled only if both settings optimize_move_to_prewhere and optimize_move_to_prewhere_if_final are turned on. note The PREWHERE section is executed before FINAL, so the results of FROM ... FINAL queries may be skewed when using PREWHERE with fields not in the ORDER BY section of a table. "},{"title":"Limitations​","type":1,"pageTitle":"PREWHERE Clause","url":"docs/en/sql-reference/statements/select/prewhere#limitations","content":"PREWHERE is only supported by tables from the *MergeTree family. "},{"title":"OFFSET FETCH Clause","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/select/offset","content":"","keywords":""},{"title":"Examples​","type":1,"pageTitle":"OFFSET FETCH Clause","url":"docs/en/sql-reference/statements/select/offset#examples","content":"Input table: ┌─a─┬─b─┐ │ 1 │ 1 │ │ 2 │ 1 │ │ 3 │ 4 │ │ 1 │ 3 │ │ 5 │ 4 │ │ 0 │ 6 │ │ 5 │ 7 │ └───┴───┘  Usage of the ONLY option: SELECT * FROM test_fetch ORDER BY a OFFSET 3 ROW FETCH FIRST 3 ROWS ONLY;  Result: ┌─a─┬─b─┐ │ 2 │ 1 │ │ 3 │ 4 │ │ 5 │ 4 │ └───┴───┘  Usage of the WITH TIES option: SELECT * FROM test_fetch ORDER BY a OFFSET 3 ROW FETCH FIRST 3 ROWS WITH TIES;  Result: ┌─a─┬─b─┐ │ 2 │ 1 │ │ 3 │ 4 │ │ 5 │ 4 │ │ 5 │ 7 │ └───┴───┘  "},{"title":"WHERE Clause","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/select/where","content":"WHERE Clause WHERE clause allows to filter the data that is coming from FROM clause of SELECT. If there is a WHERE clause, it must contain an expression with the UInt8 type. This is usually an expression with comparison and logical operators. Rows where this expression evaluates to 0 are excluded from further transformations or result. WHERE expression is evaluated on the ability to use indexes and partition pruning, if the underlying table engine supports that. note There is a filtering optimization called PREWHERE. If you need to test a value for NULL, use IS NULL and IS NOT NULL operators or isNull and isNotNull functions. Otherwise an expression with NULL never passes. Example To find numbers that are multiples of 3 and are greater than 10 execute the following query on the numbers table: SELECT number FROM numbers(20) WHERE (number &gt; 10) AND (number % 3 == 0); Result: ┌─number─┐ │ 12 │ │ 15 │ │ 18 │ └────────┘ Queries with NULL values: CREATE TABLE t_null(x Int8, y Nullable(Int8)) ENGINE=MergeTree() ORDER BY x; INSERT INTO t_null VALUES (1, NULL), (2, 3); SELECT * FROM t_null WHERE y IS NULL; SELECT * FROM t_null WHERE y != 0; Result: ┌─x─┬────y─┐ │ 1 │ ᴺᵁᴸᴸ │ └───┴──────┘ ┌─x─┬─y─┐ │ 2 │ 3 │ └───┴───┘ ","keywords":""},{"title":"UNION Clause","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/select/union","content":"UNION Clause You can use UNION with explicitly specifying UNION ALL or UNION DISTINCT. If you don't specify ALL or DISTINCT, it will depend on the union_default_mode setting. The difference between UNION ALL and UNION DISTINCT is that UNION DISTINCT will do a distinct transform for union result, it is equivalent to SELECT DISTINCT from a subquery containing UNION ALL. You can use UNION to combine any number of SELECT queries by extending their results. Example: SELECT CounterID, 1 AS table, toInt64(count()) AS c FROM test.hits GROUP BY CounterID UNION ALL SELECT CounterID, 2 AS table, sum(Sign) AS c FROM test.visits GROUP BY CounterID HAVING c &gt; 0 Result columns are matched by their index (order inside SELECT). If column names do not match, names for the final result are taken from the first query. Type casting is performed for unions. For example, if two queries being combined have the same field with non-Nullable and Nullable types from a compatible type, the resulting UNION has a Nullable type field. Queries that are parts of UNION can be enclosed in round brackets. ORDER BY and LIMIT are applied to separate queries, not to the final result. If you need to apply a conversion to the final result, you can put all the queries with UNION in a subquery in the FROM clause. If you use UNION without explicitly specifying UNION ALL or UNION DISTINCT, you can specify the union mode using the union_default_mode setting. The setting values can be ALL, DISTINCT or an empty string. However, if you use UNION with union_default_mode setting to empty string, it will throw an exception. The following examples demonstrate the results of queries with different values setting. Query: SET union_default_mode = 'DISTINCT'; SELECT 1 UNION SELECT 2 UNION SELECT 3 UNION SELECT 2; Result: ┌─1─┐ │ 1 │ └───┘ ┌─1─┐ │ 2 │ └───┘ ┌─1─┐ │ 3 │ └───┘ Query: SET union_default_mode = 'ALL'; SELECT 1 UNION SELECT 2 UNION SELECT 3 UNION SELECT 2; Result: ┌─1─┐ │ 1 │ └───┘ ┌─1─┐ │ 2 │ └───┘ ┌─1─┐ │ 2 │ └───┘ ┌─1─┐ │ 3 │ └───┘ Queries that are parts of UNION/UNION ALL/UNION DISTINCT can be run simultaneously, and their results can be mixed together. See Also insert_null_as_default setting.union_default_mode setting. Original article","keywords":""},{"title":"JOIN Clause","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/select/join","content":"","keywords":""},{"title":"Supported Types of JOIN​","type":1,"pageTitle":"JOIN Clause","url":"docs/en/sql-reference/statements/select/join#select-join-types","content":"All standard SQL JOIN types are supported: INNER JOIN, only matching rows are returned.LEFT OUTER JOIN, non-matching rows from left table are returned in addition to matching rows.RIGHT OUTER JOIN, non-matching rows from right table are returned in addition to matching rows.FULL OUTER JOIN, non-matching rows from both tables are returned in addition to matching rows.CROSS JOIN, produces cartesian product of whole tables, “join keys” are not specified. JOIN without specified type implies INNER. Keyword OUTER can be safely omitted. Alternative syntax for CROSS JOIN is specifying multiple tables in FROM clause separated by commas. Additional join types available in ClickHouse: LEFT SEMI JOIN and RIGHT SEMI JOIN, a whitelist on “join keys”, without producing a cartesian product.LEFT ANTI JOIN and RIGHT ANTI JOIN, a blacklist on “join keys”, without producing a cartesian product.LEFT ANY JOIN, RIGHT ANY JOIN and INNER ANY JOIN, partially (for opposite side of LEFT and RIGHT) or completely (for INNER and FULL) disables the cartesian product for standard JOIN types.ASOF JOIN and LEFT ASOF JOIN, joining sequences with a non-exact match. ASOF JOIN usage is described below. note When join_algorithm is set to partial_merge, RIGHT JOIN and FULL JOIN are supported only with ALL strictness (SEMI, ANTI, ANY, and ASOF are not supported). "},{"title":"Settings​","type":1,"pageTitle":"JOIN Clause","url":"docs/en/sql-reference/statements/select/join#join-settings","content":"The default join type can be overridden using join_default_strictness setting. The behavior of ClickHouse server for ANY JOIN operations depends on the any_join_distinct_right_table_keys setting. See also join_algorithmjoin_any_take_last_rowjoin_use_nullspartial_merge_join_optimizationspartial_merge_join_rows_in_right_blocksjoin_on_disk_max_files_to_mergeany_join_distinct_right_table_keys "},{"title":"ON Section Conditions​","type":1,"pageTitle":"JOIN Clause","url":"docs/en/sql-reference/statements/select/join#on-section-conditions","content":"An ON section can contain several conditions combined using the AND and OR operators. Conditions specifying join keys must refer both left and right tables and must use the equality operator. Other conditions may use other logical operators but they must refer either the left or the right table of a query. Rows are joined if the whole complex condition is met. If the conditions are not met, still rows may be included in the result depending on the JOIN type. Note that if the same conditions are placed in a WHERE section and they are not met, then rows are always filtered out from the result. The OR operator inside the ON clause works using the hash join algorithm — for each OR argument with join keys for JOIN, a separate hash table is created, so memory consumption and query execution time grow linearly with an increase in the number of expressions OR of the ON clause. note If a condition refers columns from different tables, then only the equality operator (=) is supported so far. Example Consider table_1 and table_2: ┌─Id─┬─name─┐ ┌─Id─┬─text───────────┬─scores─┐ │ 1 │ A │ │ 1 │ Text A │ 10 │ │ 2 │ B │ │ 1 │ Another text A │ 12 │ │ 3 │ C │ │ 2 │ Text B │ 15 │ └────┴──────┘ └────┴────────────────┴────────┘  Query with one join key condition and an additional condition for table_2: SELECT name, text FROM table_1 LEFT OUTER JOIN table_2 ON table_1.Id = table_2.Id AND startsWith(table_2.text, 'Text');  Note that the result contains the row with the name C and the empty text column. It is included into the result because an OUTER type of a join is used. ┌─name─┬─text───┐ │ A │ Text A │ │ B │ Text B │ │ C │ │ └──────┴────────┘  Query with INNER type of a join and multiple conditions: SELECT name, text, scores FROM table_1 INNER JOIN table_2 ON table_1.Id = table_2.Id AND table_2.scores &gt; 10 AND startsWith(table_2.text, 'Text');  Result: ┌─name─┬─text───┬─scores─┐ │ B │ Text B │ 15 │ └──────┴────────┴────────┘  Query with INNER type of a join and condition with OR: CREATE TABLE t1 (`a` Int64, `b` Int64) ENGINE = MergeTree() ORDER BY a; CREATE TABLE t2 (`key` Int32, `val` Int64) ENGINE = MergeTree() ORDER BY key; INSERT INTO t1 SELECT number as a, -a as b from numbers(5); INSERT INTO t2 SELECT if(number % 2 == 0, toInt64(number), -number) as key, number as val from numbers(5); SELECT a, b, val FROM t1 INNER JOIN t2 ON t1.a = t2.key OR t1.b = t2.key;  Result: ┌─a─┬──b─┬─val─┐ │ 0 │ 0 │ 0 │ │ 1 │ -1 │ 1 │ │ 2 │ -2 │ 2 │ │ 3 │ -3 │ 3 │ │ 4 │ -4 │ 4 │ └───┴────┴─────┘  Query with INNER type of a join and conditions with OR and AND: SELECT a, b, val FROM t1 INNER JOIN t2 ON t1.a = t2.key OR t1.b = t2.key AND t2.val &gt; 3;  Result: ┌─a─┬──b─┬─val─┐ │ 0 │ 0 │ 0 │ │ 2 │ -2 │ 2 │ │ 4 │ -4 │ 4 │ └───┴────┴─────┘  "},{"title":"ASOF JOIN Usage​","type":1,"pageTitle":"JOIN Clause","url":"docs/en/sql-reference/statements/select/join#asof-join-usage","content":"ASOF JOIN is useful when you need to join records that have no exact match. Algorithm requires the special column in tables. This column: Must contain an ordered sequence.Can be one of the following types: Int, UInt, Float, Date, DateTime, Decimal.Can’t be the only column in the JOIN clause. Syntax ASOF JOIN ... ON: SELECT expressions_list FROM table_1 ASOF LEFT JOIN table_2 ON equi_cond AND closest_match_cond  You can use any number of equality conditions and exactly one closest match condition. For example, SELECT count() FROM table_1 ASOF LEFT JOIN table_2 ON table_1.a == table_2.b AND table_2.t &lt;= table_1.t. Conditions supported for the closest match: &gt;, &gt;=, &lt;, &lt;=. Syntax ASOF JOIN ... USING: SELECT expressions_list FROM table_1 ASOF JOIN table_2 USING (equi_column1, ... equi_columnN, asof_column)  ASOF JOIN uses equi_columnX for joining on equality and asof_column for joining on the closest match with the table_1.asof_column &gt;= table_2.asof_column condition. The asof_column column is always the last one in the USING clause. For example, consider the following tables:  table_1 table_2 event | ev_time | user_id event | ev_time | user_id ----------|---------|---------- ----------|---------|---------- ... ... event_1_1 | 12:00 | 42 event_2_1 | 11:59 | 42 ... event_2_2 | 12:30 | 42 event_1_2 | 13:00 | 42 event_2_3 | 13:00 | 42 ... ...  ASOF JOIN can take the timestamp of a user event from table_1 and find an event in table_2 where the timestamp is closest to the timestamp of the event from table_1 corresponding to the closest match condition. Equal timestamp values are the closest if available. Here, the user_id column can be used for joining on equality and the ev_time column can be used for joining on the closest match. In our example, event_1_1 can be joined with event_2_1 and event_1_2 can be joined with event_2_3, but event_2_2 can’t be joined. note ASOF join is not supported in the Join table engine. "},{"title":"Distributed JOIN​","type":1,"pageTitle":"JOIN Clause","url":"docs/en/sql-reference/statements/select/join#global-join","content":"There are two ways to execute join involving distributed tables: When using a normal JOIN, the query is sent to remote servers. Subqueries are run on each of them in order to make the right table, and the join is performed with this table. In other words, the right table is formed on each server separately.When using GLOBAL ... JOIN, first the requestor server runs a subquery to calculate the right table. This temporary table is passed to each remote server, and queries are run on them using the temporary data that was transmitted. Be careful when using GLOBAL. For more information, see the Distributed subqueries section. "},{"title":"Implicit Type Conversion​","type":1,"pageTitle":"JOIN Clause","url":"docs/en/sql-reference/statements/select/join#implicit-type-conversion","content":"INNER JOIN, LEFT JOIN, RIGHT JOIN, and FULL JOIN queries support the implicit type conversion for &quot;join keys&quot;. However the query can not be executed, if join keys from the left and the right tables cannot be converted to a single type (for example, there is no data type that can hold all values from both UInt64 and Int64, or String and Int32). Example Consider the table t_1: ┌─a─┬─b─┬─toTypeName(a)─┬─toTypeName(b)─┐ │ 1 │ 1 │ UInt16 │ UInt8 │ │ 2 │ 2 │ UInt16 │ UInt8 │ └───┴───┴───────────────┴───────────────┘  and the table t_2: ┌──a─┬────b─┬─toTypeName(a)─┬─toTypeName(b)───┐ │ -1 │ 1 │ Int16 │ Nullable(Int64) │ │ 1 │ -1 │ Int16 │ Nullable(Int64) │ │ 1 │ 1 │ Int16 │ Nullable(Int64) │ └────┴──────┴───────────────┴─────────────────┘  The query SELECT a, b, toTypeName(a), toTypeName(b) FROM t_1 FULL JOIN t_2 USING (a, b);  returns the set: ┌──a─┬────b─┬─toTypeName(a)─┬─toTypeName(b)───┐ │ 1 │ 1 │ Int32 │ Nullable(Int64) │ │ 2 │ 2 │ Int32 │ Nullable(Int64) │ │ -1 │ 1 │ Int32 │ Nullable(Int64) │ │ 1 │ -1 │ Int32 │ Nullable(Int64) │ └────┴──────┴───────────────┴─────────────────┘  "},{"title":"Usage Recommendations​","type":1,"pageTitle":"JOIN Clause","url":"docs/en/sql-reference/statements/select/join#usage-recommendations","content":""},{"title":"Processing of Empty or NULL Cells​","type":1,"pageTitle":"JOIN Clause","url":"docs/en/sql-reference/statements/select/join#processing-of-empty-or-null-cells","content":"While joining tables, the empty cells may appear. The setting join_use_nulls define how ClickHouse fills these cells. If the JOIN keys are Nullable fields, the rows where at least one of the keys has the value NULL are not joined. "},{"title":"Syntax​","type":1,"pageTitle":"JOIN Clause","url":"docs/en/sql-reference/statements/select/join#syntax","content":"The columns specified in USING must have the same names in both subqueries, and the other columns must be named differently. You can use aliases to change the names of columns in subqueries. The USING clause specifies one or more columns to join, which establishes the equality of these columns. The list of columns is set without brackets. More complex join conditions are not supported. "},{"title":"Syntax Limitations​","type":1,"pageTitle":"JOIN Clause","url":"docs/en/sql-reference/statements/select/join#syntax-limitations","content":"For multiple JOIN clauses in a single SELECT query: Taking all the columns via * is available only if tables are joined, not subqueries.The PREWHERE clause is not available. For ON, WHERE, and GROUP BY clauses: Arbitrary expressions cannot be used in ON, WHERE, and GROUP BY clauses, but you can define an expression in a SELECT clause and then use it in these clauses via an alias. "},{"title":"Performance​","type":1,"pageTitle":"JOIN Clause","url":"docs/en/sql-reference/statements/select/join#performance","content":"When running a JOIN, there is no optimization of the order of execution in relation to other stages of the query. The join (a search in the right table) is run before filtering in WHERE and before aggregation. Each time a query is run with the same JOIN, the subquery is run again because the result is not cached. To avoid this, use the special Join table engine, which is a prepared array for joining that is always in RAM. In some cases, it is more efficient to use IN instead of JOIN. If you need a JOIN for joining with dimension tables (these are relatively small tables that contain dimension properties, such as names for advertising campaigns), a JOIN might not be very convenient due to the fact that the right table is re-accessed for every query. For such cases, there is an “external dictionaries” feature that you should use instead of JOIN. For more information, see the External dictionaries section. "},{"title":"Memory Limitations​","type":1,"pageTitle":"JOIN Clause","url":"docs/en/sql-reference/statements/select/join#memory-limitations","content":"By default, ClickHouse uses the hash join algorithm. ClickHouse takes the right_table and creates a hash table for it in RAM. If join_algorithm = 'auto' is enabled, then after some threshold of memory consumption, ClickHouse falls back to merge join algorithm. For JOIN algorithms description see the join_algorithm setting. If you need to restrict JOIN operation memory consumption use the following settings: max_rows_in_join — Limits number of rows in the hash table.max_bytes_in_join — Limits size of the hash table. When any of these limits is reached, ClickHouse acts as the join_overflow_mode setting instructs. "},{"title":"Examples​","type":1,"pageTitle":"JOIN Clause","url":"docs/en/sql-reference/statements/select/join#examples","content":"Example: SELECT CounterID, hits, visits FROM ( SELECT CounterID, count() AS hits FROM test.hits GROUP BY CounterID ) ANY LEFT JOIN ( SELECT CounterID, sum(Sign) AS visits FROM test.visits GROUP BY CounterID ) USING CounterID ORDER BY hits DESC LIMIT 10  ┌─CounterID─┬───hits─┬─visits─┐ │ 1143050 │ 523264 │ 13665 │ │ 731962 │ 475698 │ 102716 │ │ 722545 │ 337212 │ 108187 │ │ 722889 │ 252197 │ 10547 │ │ 2237260 │ 196036 │ 9522 │ │ 23057320 │ 147211 │ 7689 │ │ 722818 │ 90109 │ 17847 │ │ 48221 │ 85379 │ 4652 │ │ 19762435 │ 77807 │ 7026 │ │ 722884 │ 77492 │ 11056 │ └───────────┴────────┴────────┘  "},{"title":"INTO OUTFILE Clause","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/select/into-outfile","content":"","keywords":""},{"title":"Implementation Details​","type":1,"pageTitle":"INTO OUTFILE Clause","url":"docs/en/sql-reference/statements/select/into-outfile#implementation-details","content":"This functionality is available in the command-line client and clickhouse-local. Thus a query sent via HTTP interface will fail.The query will fail if a file with the same file name already exists.The default output format is TabSeparated (like in the command-line client batch mode). Use FORMAT clause to change it. Example Execute the following query using command-line client: clickhouse-client --query=&quot;SELECT 1,'ABC' INTO OUTFILE 'select.gz' FORMAT CSV;&quot; zcat select.gz  Result: 1,&quot;ABC&quot;  "},{"title":"SET ROLE Statement","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/set-role","content":"","keywords":""},{"title":"SET DEFAULT ROLE​","type":1,"pageTitle":"SET ROLE Statement","url":"docs/en/sql-reference/statements/set-role#set-default-role-statement","content":"Sets default roles to a user. Default roles are automatically activated at user login. You can set as default only the previously granted roles. If the role isn’t granted to a user, ClickHouse throws an exception. SET DEFAULT ROLE {NONE | role [,...] | ALL | ALL EXCEPT role [,...]} TO {user|CURRENT_USER} [,...]  "},{"title":"Examples​","type":1,"pageTitle":"SET ROLE Statement","url":"docs/en/sql-reference/statements/set-role#set-default-role-examples","content":"Set multiple default roles to a user: SET DEFAULT ROLE role1, role2, ... TO user  Set all the granted roles as default to a user: SET DEFAULT ROLE ALL TO user  Purge default roles from a user: SET DEFAULT ROLE NONE TO user  Set all the granted roles as default excepting some of them: SET DEFAULT ROLE ALL EXCEPT role1, role2 TO user  "},{"title":"WITH Clause","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/select/with","content":"","keywords":""},{"title":"Syntax​","type":1,"pageTitle":"WITH Clause","url":"docs/en/sql-reference/statements/select/with#syntax","content":"WITH &lt;expression&gt; AS &lt;identifier&gt;  or WITH &lt;identifier&gt; AS &lt;subquery expression&gt;  "},{"title":"Examples​","type":1,"pageTitle":"WITH Clause","url":"docs/en/sql-reference/statements/select/with#examples","content":"Example 1: Using constant expression as “variable” WITH '2019-08-01 15:23:00' as ts_upper_bound SELECT * FROM hits WHERE EventDate = toDate(ts_upper_bound) AND EventTime &lt;= ts_upper_bound;  Example 2: Evicting a sum(bytes) expression result from the SELECT clause column list WITH sum(bytes) as s SELECT formatReadableSize(s), table FROM system.parts GROUP BY table ORDER BY s;  Example 3: Using results of a scalar subquery /* this example would return TOP 10 of most huge tables */ WITH ( SELECT sum(bytes) FROM system.parts WHERE active ) AS total_disk_usage SELECT (sum(bytes) / total_disk_usage) * 100 AS table_disk_usage, table FROM system.parts GROUP BY table ORDER BY table_disk_usage DESC LIMIT 10;  Example 4: Reusing expression in a subquery WITH test1 AS (SELECT i + 1, j + 1 FROM test1) SELECT * FROM test1;  Original article "},{"title":"TRUNCATE Statement","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/truncate","content":"TRUNCATE Statement TRUNCATE TABLE [IF EXISTS] [db.]name [ON CLUSTER cluster] Removes all data from a table. When the clause IF EXISTS is omitted, the query returns an error if the table does not exist. The TRUNCATE query is not supported for View, File, URL, Buffer and Null table engines. You can use the replication_alter_partitions_sync setting to set up waiting for actions to be executed on replicas. You can specify how long (in seconds) to wait for inactive replicas to execute TRUNCATE queries with the replication_wait_for_inactive_replica_timeout setting. note If the replication_alter_partitions_sync is set to 2 and some replicas are not active for more than the time, specified by the replication_wait_for_inactive_replica_timeout setting, then an exception UNFINISHED is thrown.","keywords":""},{"title":"WATCH Statement (Experimental)","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/watch","content":"","keywords":""},{"title":"Virtual columns​","type":1,"pageTitle":"WATCH Statement (Experimental)","url":"docs/en/sql-reference/statements/watch#watch-virtual-columns","content":"The virtual _version column in the query result indicates the current result version. Example: CREATE LIVE VIEW lv WITH REFRESH 5 AS SELECT now(); WATCH lv;  ┌───────────────now()─┬─_version─┐ │ 2021-02-21 09:17:21 │ 1 │ └─────────────────────┴──────────┘ ┌───────────────now()─┬─_version─┐ │ 2021-02-21 09:17:26 │ 2 │ └─────────────────────┴──────────┘ ┌───────────────now()─┬─_version─┐ │ 2021-02-21 09:17:31 │ 3 │ └─────────────────────┴──────────┘ ...  By default, the requested data is returned to the client, while in conjunction with INSERT INTO it can be forwarded to a different table. Example: INSERT INTO [db.]table WATCH [db.]live_view ...  "},{"title":"EVENTS Clause​","type":1,"pageTitle":"WATCH Statement (Experimental)","url":"docs/en/sql-reference/statements/watch#events-clause","content":"The EVENTS clause can be used to obtain a short form of the WATCH query where instead of the query result you will just get the latest query result version. WATCH [db.]live_view EVENTS;  Example: CREATE LIVE VIEW lv WITH REFRESH 5 AS SELECT now(); WATCH lv EVENTS;  ┌─version─┐ │ 1 │ └─────────┘ ┌─version─┐ │ 2 │ └─────────┘ ...  "},{"title":"LIMIT Clause​","type":1,"pageTitle":"WATCH Statement (Experimental)","url":"docs/en/sql-reference/statements/watch#limit-clause","content":"The LIMIT n clause specifies the number of updates the WATCH query should wait for before terminating. By default there is no limit on the number of updates and therefore the query will not terminate. The value of 0 indicates that the WATCH query should not wait for any new query results and therefore will return immediately once query result is evaluated. WATCH [db.]live_view LIMIT 1;  Example: CREATE LIVE VIEW lv WITH REFRESH 5 AS SELECT now(); WATCH lv EVENTS LIMIT 1;  ┌─version─┐ │ 1 │ └─────────┘  "},{"title":"FORMAT Clause​","type":1,"pageTitle":"WATCH Statement (Experimental)","url":"docs/en/sql-reference/statements/watch#format-clause","content":"The FORMAT clause works the same way as for the SELECT. note The JSONEachRowWithProgress format should be used when watching LIVE VIEW tables over the HTTP interface. The progress messages will be added to the output to keep the long-lived HTTP connection alive until the query result changes. The interval between progress messages is controlled using the live_view_heartbeat_interval setting. "},{"title":"LIMIT BY Clause","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/select/limit-by","content":"","keywords":""},{"title":"Examples​","type":1,"pageTitle":"LIMIT BY Clause","url":"docs/en/sql-reference/statements/select/limit-by#examples","content":"Sample table: CREATE TABLE limit_by(id Int, val Int) ENGINE = Memory; INSERT INTO limit_by VALUES (1, 10), (1, 11), (1, 12), (2, 20), (2, 21);  Queries: SELECT * FROM limit_by ORDER BY id, val LIMIT 2 BY id  ┌─id─┬─val─┐ │ 1 │ 10 │ │ 1 │ 11 │ │ 2 │ 20 │ │ 2 │ 21 │ └────┴─────┘  SELECT * FROM limit_by ORDER BY id, val LIMIT 1, 2 BY id  ┌─id─┬─val─┐ │ 1 │ 11 │ │ 1 │ 12 │ │ 2 │ 21 │ └────┴─────┘  The SELECT * FROM limit_by ORDER BY id, val LIMIT 2 OFFSET 1 BY id query returns the same result. The following query returns the top 5 referrers for each domain, device_type pair with a maximum of 100 rows in total (LIMIT n BY + LIMIT). SELECT domainWithoutWWW(URL) AS domain, domainWithoutWWW(REFERRER_URL) AS referrer, device_type, count() cnt FROM hits GROUP BY domain, referrer, device_type ORDER BY cnt DESC LIMIT 5 BY domain, device_type LIMIT 100  "},{"title":"SAMPLE Clause","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/select/sample","content":"","keywords":""},{"title":"SAMPLE K​","type":1,"pageTitle":"SAMPLE Clause","url":"docs/en/sql-reference/statements/select/sample#select-sample-k","content":"Here k is the number from 0 to 1 (both fractional and decimal notations are supported). For example, SAMPLE 1/2 or SAMPLE 0.5. In a SAMPLE k clause, the sample is taken from the k fraction of data. The example is shown below: SELECT Title, count() * 10 AS PageViews FROM hits_distributed SAMPLE 0.1 WHERE CounterID = 34 GROUP BY Title ORDER BY PageViews DESC LIMIT 1000  In this example, the query is executed on a sample from 0.1 (10%) of data. Values of aggregate functions are not corrected automatically, so to get an approximate result, the value count() is manually multiplied by 10. "},{"title":"SAMPLE N​","type":1,"pageTitle":"SAMPLE Clause","url":"docs/en/sql-reference/statements/select/sample#select-sample-n","content":"Here n is a sufficiently large integer. For example, SAMPLE 10000000. In this case, the query is executed on a sample of at least n rows (but not significantly more than this). For example, SAMPLE 10000000 runs the query on a minimum of 10,000,000 rows. Since the minimum unit for data reading is one granule (its size is set by the index_granularity setting), it makes sense to set a sample that is much larger than the size of the granule. When using the SAMPLE n clause, you do not know which relative percent of data was processed. So you do not know the coefficient the aggregate functions should be multiplied by. Use the _sample_factor virtual column to get the approximate result. The _sample_factor column contains relative coefficients that are calculated dynamically. This column is created automatically when you create a table with the specified sampling key. The usage examples of the _sample_factor column are shown below. Let’s consider the table visits, which contains the statistics about site visits. The first example shows how to calculate the number of page views: SELECT sum(PageViews * _sample_factor) FROM visits SAMPLE 10000000  The next example shows how to calculate the total number of visits: SELECT sum(_sample_factor) FROM visits SAMPLE 10000000  The example below shows how to calculate the average session duration. Note that you do not need to use the relative coefficient to calculate the average values. SELECT avg(Duration) FROM visits SAMPLE 10000000  "},{"title":"SAMPLE K OFFSET M​","type":1,"pageTitle":"SAMPLE Clause","url":"docs/en/sql-reference/statements/select/sample#select-sample-offset","content":"Here k and m are numbers from 0 to 1. Examples are shown below. Example 1 SAMPLE 1/10  In this example, the sample is 1/10th of all data: [++------------] Example 2 SAMPLE 1/10 OFFSET 1/2  Here, a sample of 10% is taken from the second half of the data. [------++------] "},{"title":"Syntax","type":0,"sectionRef":"#","url":"docs/en/sql-reference/syntax","content":"","keywords":""},{"title":"Spaces​","type":1,"pageTitle":"Syntax","url":"docs/en/sql-reference/syntax#spaces","content":"There may be any number of space symbols between syntactical constructions (including the beginning and end of a query). Space symbols include the space, tab, line feed, CR, and form feed. "},{"title":"Comments​","type":1,"pageTitle":"Syntax","url":"docs/en/sql-reference/syntax#comments","content":"ClickHouse supports either SQL-style and C-style comments: SQL-style comments start with --, #! or # and continue to the end of the line, a space after -- and #! can be omitted.C-style are from /* to */and can be multiline, spaces are not required either. "},{"title":"Keywords​","type":1,"pageTitle":"Syntax","url":"docs/en/sql-reference/syntax#syntax-keywords","content":"Keywords are case-insensitive when they correspond to: SQL standard. For example, SELECT, select and SeLeCt are all valid.Implementation in some popular DBMS (MySQL or Postgres). For example, DateTime is the same as datetime. You can check whether a data type name is case-sensitive in the system.data_type_families table. In contrast to standard SQL, all other keywords (including functions names) are case-sensitive. Keywords are not reserved; they are treated as such only in the corresponding context. If you use identifiers with the same name as the keywords, enclose them into double-quotes or backticks. For example, the query SELECT &quot;FROM&quot; FROM table_name is valid if the table table_name has column with the name &quot;FROM&quot;. "},{"title":"Identifiers​","type":1,"pageTitle":"Syntax","url":"docs/en/sql-reference/syntax#syntax-identifiers","content":"Identifiers are: Cluster, database, table, partition, and column names.Functions.Data types.Expression aliases. Identifiers can be quoted or non-quoted. The latter is preferred. Non-quoted identifiers must match the regex ^[a-zA-Z_][0-9a-zA-Z_]*$ and can not be equal to keywords. Examples: x, _1, X_y__Z123_. If you want to use identifiers the same as keywords or you want to use other symbols in identifiers, quote it using double quotes or backticks, for example, &quot;id&quot;, `id`. "},{"title":"Literals​","type":1,"pageTitle":"Syntax","url":"docs/en/sql-reference/syntax#literals","content":"There are numeric, string, compound, and NULL literals. "},{"title":"Numeric​","type":1,"pageTitle":"Syntax","url":"docs/en/sql-reference/syntax#numeric","content":"Numeric literal tries to be parsed: First, as a 64-bit signed number, using the strtoull function.If unsuccessful, as a 64-bit unsigned number, using the strtoll function.If unsuccessful, as a floating-point number using the strtod function.Otherwise, it returns an error. Literal value has the smallest type that the value fits in. For example, 1 is parsed as UInt8, but 256 is parsed as UInt16. For more information, see Data types. Examples: 1, 18446744073709551615, 0xDEADBEEF, 01, 0.1, 1e100, -1e-100, inf, nan. "},{"title":"String​","type":1,"pageTitle":"Syntax","url":"docs/en/sql-reference/syntax#syntax-string-literal","content":"Only string literals in single quotes are supported. The enclosed characters can be backslash-escaped. The following escape sequences have a corresponding special value: \\b, \\f, \\r, \\n, \\t, \\0, \\a, \\v, \\xHH. In all other cases, escape sequences in the format \\c, where c is any character, are converted to c. It means that you can use the sequences \\'and\\\\. The value will have the String type. In string literals, you need to escape at least ' and \\. Single quotes can be escaped with the single quote, literals 'It\\'s' and 'It''s' are equal. "},{"title":"Compound​","type":1,"pageTitle":"Syntax","url":"docs/en/sql-reference/syntax#compound","content":"Arrays are constructed with square brackets [1, 2, 3]. Tuples are constructed with round brackets (1, 'Hello, world!', 2). Technically these are not literals, but expressions with the array creation operator and the tuple creation operator, respectively. An array must consist of at least one item, and a tuple must have at least two items. There’s a separate case when tuples appear in the IN clause of a SELECT query. Query results can include tuples, but tuples can’t be saved to a database (except of tables with Memory engine). "},{"title":"NULL​","type":1,"pageTitle":"Syntax","url":"docs/en/sql-reference/syntax#null-literal","content":"Indicates that the value is missing. In order to store NULL in a table field, it must be of the Nullable type. Depending on the data format (input or output), NULL may have a different representation. For more information, see the documentation for data formats. There are many nuances to processing NULL. For example, if at least one of the arguments of a comparison operation is NULL, the result of this operation is also NULL. The same is true for multiplication, addition, and other operations. For more information, read the documentation for each operation. In queries, you can check NULL using the IS NULL and IS NOT NULL operators and the related functions isNull and isNotNull. "},{"title":"Heredoc​","type":1,"pageTitle":"Syntax","url":"docs/en/sql-reference/syntax#heredeoc","content":"A heredoc is a way to define a string (often multiline), while maintaining the original formatting. A heredoc is defined as a custom string literal, placed between two $ symbols, for example $heredoc$. A value between two heredocs is processed &quot;as-is&quot;. You can use a heredoc to embed snippets of SQL, HTML, or XML code, etc. Example Query: SELECT $smth$SHOW CREATE VIEW my_view$smth$;  Result: ┌─'SHOW CREATE VIEW my_view'─┐ │ SHOW CREATE VIEW my_view │ └────────────────────────────┘  "},{"title":"Functions​","type":1,"pageTitle":"Syntax","url":"docs/en/sql-reference/syntax#functions","content":"Function calls are written like an identifier with a list of arguments (possibly empty) in round brackets. In contrast to standard SQL, the brackets are required, even for an empty argument list. Example: now(). There are regular and aggregate functions (see the section “Aggregate functions”). Some aggregate functions can contain two lists of arguments in brackets. Example: quantile (0.9) (x). These aggregate functions are called “parametric” functions, and the arguments in the first list are called “parameters”. The syntax of aggregate functions without parameters is the same as for regular functions. "},{"title":"Operators​","type":1,"pageTitle":"Syntax","url":"docs/en/sql-reference/syntax#operators","content":"Operators are converted to their corresponding functions during query parsing, taking their priority and associativity into account. For example, the expression 1 + 2 * 3 + 4 is transformed to plus(plus(1, multiply(2, 3)), 4). "},{"title":"Data Types and Database Table Engines​","type":1,"pageTitle":"Syntax","url":"docs/en/sql-reference/syntax#data_types-and-database-table-engines","content":"Data types and table engines in the CREATE query are written the same way as identifiers or functions. In other words, they may or may not contain an argument list in brackets. For more information, see the sections “Data types,” “Table engines,” and “CREATE”. "},{"title":"Expression Aliases​","type":1,"pageTitle":"Syntax","url":"docs/en/sql-reference/syntax#syntax-expression_aliases","content":"An alias is a user-defined name for expression in a query. expr AS alias  AS — The keyword for defining aliases. You can define the alias for a table name or a column name in a SELECT clause without using the AS keyword. For example, `SELECT table_name_alias.column_name FROM table_name table_name_alias`. In the [CAST](sql_reference/functions/type_conversion_functions.md#type_conversion_function-cast) function, the `AS` keyword has another meaning. See the description of the function. expr — Any expression supported by ClickHouse. For example, `SELECT column_name * 2 AS double FROM some_table`. alias — Name for expr. Aliases should comply with the identifiers syntax. For example, `SELECT &quot;table t&quot;.column_name FROM table_name AS &quot;table t&quot;`.  "},{"title":"Notes on Usage​","type":1,"pageTitle":"Syntax","url":"docs/en/sql-reference/syntax#notes-on-usage","content":"Aliases are global for a query or subquery, and you can define an alias in any part of a query for any expression. For example, SELECT (1 AS n) + 2, n. Aliases are not visible in subqueries and between subqueries. For example, while executing the query SELECT (SELECT sum(b.a) + num FROM b) - a.a AS num FROM a ClickHouse generates the exception Unknown identifier: num. If an alias is defined for the result columns in the SELECT clause of a subquery, these columns are visible in the outer query. For example, SELECT n + m FROM (SELECT 1 AS n, 2 AS m). Be careful with aliases that are the same as column or table names. Let’s consider the following example: CREATE TABLE t ( a Int, b Int ) ENGINE = TinyLog()  SELECT argMax(a, b), sum(b) AS b FROM t  Received exception from server (version 18.14.17): Code: 184. DB::Exception: Received from localhost:9000, 127.0.0.1. DB::Exception: Aggregate function sum(b) is found inside another aggregate function in query.  In this example, we declared table t with column b. Then, when selecting data, we defined the sum(b) AS b alias. As aliases are global, ClickHouse substituted the literal b in the expression argMax(a, b) with the expression sum(b). This substitution caused the exception. You can change this default behavior by setting prefer_column_name_to_alias to 1. "},{"title":"Asterisk​","type":1,"pageTitle":"Syntax","url":"docs/en/sql-reference/syntax#asterisk","content":"In a SELECT query, an asterisk can replace the expression. For more information, see the section “SELECT”. "},{"title":"Expressions​","type":1,"pageTitle":"Syntax","url":"docs/en/sql-reference/syntax#syntax-expressions","content":"An expression is a function, identifier, literal, application of an operator, expression in brackets, subquery, or asterisk. It can also contain an alias. A list of expressions is one or more expressions separated by commas. Functions and operators, in turn, can have expressions as arguments. Original article "},{"title":"SHOW Statements","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/show","content":"","keywords":""},{"title":"SHOW CREATE TABLE​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-create-table","content":"SHOW CREATE [TEMPORARY] [TABLE|DICTIONARY|VIEW] [db.]table|view [INTO OUTFILE filename] [FORMAT format]  Returns a single String-type ‘statement’ column, which contains a single value – the CREATE query used for creating the specified object. Note that if you use this statement to get CREATE query of system tables, you will get a fake query, which only declares table structure, but cannot be used to create table. "},{"title":"SHOW DATABASES​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-databases","content":"Prints a list of all databases. SHOW DATABASES [LIKE | ILIKE | NOT LIKE '&lt;pattern&gt;'] [LIMIT &lt;N&gt;] [INTO OUTFILE filename] [FORMAT format]  This statement is identical to the query: SELECT name FROM system.databases [WHERE name LIKE | ILIKE | NOT LIKE '&lt;pattern&gt;'] [LIMIT &lt;N&gt;] [INTO OUTFILE filename] [FORMAT format]  "},{"title":"Examples​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#examples","content":"Getting database names, containing the symbols sequence 'de' in their names: SHOW DATABASES LIKE '%de%'  Result: ┌─name────┐ │ default │ └─────────┘  Getting database names, containing symbols sequence 'de' in their names, in the case insensitive manner: SHOW DATABASES ILIKE '%DE%'  Result: ┌─name────┐ │ default │ └─────────┘  Getting database names, not containing the symbols sequence 'de' in their names: SHOW DATABASES NOT LIKE '%de%'  Result: ┌─name───────────────────────────┐ │ _temporary_and_external_tables │ │ system │ │ test │ │ tutorial │ └────────────────────────────────┘  Getting the first two rows from database names: SHOW DATABASES LIMIT 2  Result: ┌─name───────────────────────────┐ │ _temporary_and_external_tables │ │ default │ └────────────────────────────────┘  "},{"title":"See Also​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#see-also","content":"CREATE DATABASE "},{"title":"SHOW PROCESSLIST​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-processlist","content":"SHOW PROCESSLIST [INTO OUTFILE filename] [FORMAT format]  Outputs the content of the system.processes table, that contains a list of queries that is being processed at the moment, excepting SHOW PROCESSLIST queries. The SELECT * FROM system.processes query returns data about all the current queries. Tip (execute in the console): $ watch -n1 &quot;clickhouse-client --query='SHOW PROCESSLIST'&quot;  "},{"title":"SHOW TABLES​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-tables","content":"Displays a list of tables. SHOW [TEMPORARY] TABLES [{FROM | IN} &lt;db&gt;] [LIKE | ILIKE | NOT LIKE '&lt;pattern&gt;'] [LIMIT &lt;N&gt;] [INTO OUTFILE &lt;filename&gt;] [FORMAT &lt;format&gt;]  If the FROM clause is not specified, the query returns the list of tables from the current database. This statement is identical to the query: SELECT name FROM system.tables [WHERE name LIKE | ILIKE | NOT LIKE '&lt;pattern&gt;'] [LIMIT &lt;N&gt;] [INTO OUTFILE &lt;filename&gt;] [FORMAT &lt;format&gt;]  "},{"title":"Examples​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#examples","content":"Getting table names, containing the symbols sequence 'user' in their names: SHOW TABLES FROM system LIKE '%user%'  Result: ┌─name─────────────┐ │ user_directories │ │ users │ └──────────────────┘  Getting table names, containing sequence 'user' in their names, in the case insensitive manner: SHOW TABLES FROM system ILIKE '%USER%'  Result: ┌─name─────────────┐ │ user_directories │ │ users │ └──────────────────┘  Getting table names, not containing the symbol sequence 's' in their names: SHOW TABLES FROM system NOT LIKE '%s%'  Result: ┌─name─────────┐ │ metric_log │ │ metric_log_0 │ │ metric_log_1 │ └──────────────┘  Getting the first two rows from table names: SHOW TABLES FROM system LIMIT 2  Result: ┌─name───────────────────────────┐ │ aggregate_function_combinators │ │ asynchronous_metric_log │ └────────────────────────────────┘  "},{"title":"See Also​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#see-also","content":"Create TablesSHOW CREATE TABLE "},{"title":"SHOW DICTIONARIES​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-dictionaries","content":"Displays a list of external dictionaries. SHOW DICTIONARIES [FROM &lt;db&gt;] [LIKE '&lt;pattern&gt;'] [LIMIT &lt;N&gt;] [INTO OUTFILE &lt;filename&gt;] [FORMAT &lt;format&gt;]  If the FROM clause is not specified, the query returns the list of dictionaries from the current database. You can get the same results as the SHOW DICTIONARIES query in the following way: SELECT name FROM system.dictionaries WHERE database = &lt;db&gt; [AND name LIKE &lt;pattern&gt;] [LIMIT &lt;N&gt;] [INTO OUTFILE &lt;filename&gt;] [FORMAT &lt;format&gt;]  Example The following query selects the first two rows from the list of tables in the system database, whose names contain reg. SHOW DICTIONARIES FROM db LIKE '%reg%' LIMIT 2  ┌─name─────────┐ │ regions │ │ region_names │ └──────────────┘  "},{"title":"SHOW GRANTS​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-grants-statement","content":"Shows privileges for a user. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-grants-syntax","content":"SHOW GRANTS [FOR user1 [, user2 ...]]  If user is not specified, the query returns privileges for the current user. "},{"title":"SHOW CREATE USER​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-create-user-statement","content":"Shows parameters that were used at a user creation. SHOW CREATE USER does not output user passwords. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-create-user-syntax","content":"SHOW CREATE USER [name1 [, name2 ...] | CURRENT_USER]  "},{"title":"SHOW CREATE ROLE​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-create-role-statement","content":"Shows parameters that were used at a role creation. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-create-role-syntax","content":"SHOW CREATE ROLE name1 [, name2 ...]  "},{"title":"SHOW CREATE ROW POLICY​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-create-row-policy-statement","content":"Shows parameters that were used at a row policy creation. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-create-row-policy-syntax","content":"SHOW CREATE [ROW] POLICY name ON [database1.]table1 [, [database2.]table2 ...]  "},{"title":"SHOW CREATE QUOTA​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-create-quota-statement","content":"Shows parameters that were used at a quota creation. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-create-quota-syntax","content":"SHOW CREATE QUOTA [name1 [, name2 ...] | CURRENT]  "},{"title":"SHOW CREATE SETTINGS PROFILE​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-create-settings-profile-statement","content":"Shows parameters that were used at a settings profile creation. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-create-settings-profile-syntax","content":"SHOW CREATE [SETTINGS] PROFILE name1 [, name2 ...]  "},{"title":"SHOW USERS​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-users-statement","content":"Returns a list of user account names. To view user accounts parameters, see the system table system.users. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-users-syntax","content":"SHOW USERS  "},{"title":"SHOW ROLES​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-roles-statement","content":"Returns a list of roles. To view another parameters, see system tables system.roles and system.role-grants. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-roles-syntax","content":"SHOW [CURRENT|ENABLED] ROLES  "},{"title":"SHOW PROFILES​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-profiles-statement","content":"Returns a list of setting profiles. To view user accounts parameters, see the system table settings_profiles. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-profiles-syntax","content":"SHOW [SETTINGS] PROFILES  "},{"title":"SHOW POLICIES​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-policies-statement","content":"Returns a list of row policies for the specified table. To view user accounts parameters, see the system table system.row_policies. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-policies-syntax","content":"SHOW [ROW] POLICIES [ON [db.]table]  "},{"title":"SHOW QUOTAS​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-quotas-statement","content":"Returns a list of quotas. To view quotas parameters, see the system table system.quotas. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-quotas-syntax","content":"SHOW QUOTAS  "},{"title":"SHOW QUOTA​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-quota-statement","content":"Returns a quota consumption for all users or for current user. To view another parameters, see system tables system.quotas_usage and system.quota_usage. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-quota-syntax","content":"SHOW [CURRENT] QUOTA  "},{"title":"SHOW ACCESS​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-access-statement","content":"Shows all users, roles, profiles, etc. and all their grants. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-access-syntax","content":"SHOW ACCESS  "},{"title":"SHOW CLUSTER(s)​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-cluster-statement","content":"Returns a list of clusters. All available clusters are listed in the system.clusters table. note SHOW CLUSTER name query displays the contents of system.clusters table for this cluster. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-cluster-syntax","content":"SHOW CLUSTER '&lt;name&gt;' SHOW CLUSTERS [LIKE|NOT LIKE '&lt;pattern&gt;'] [LIMIT &lt;N&gt;]  "},{"title":"Examples​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-cluster-examples","content":"Query: SHOW CLUSTERS;  Result: ┌─cluster──────────────────────────────────────┐ │ test_cluster_two_shards │ │ test_cluster_two_shards_internal_replication │ │ test_cluster_two_shards_localhost │ │ test_shard_localhost │ │ test_shard_localhost_secure │ │ test_unavailable_shard │ └──────────────────────────────────────────────┘  Query: SHOW CLUSTERS LIKE 'test%' LIMIT 1;  Result: ┌─cluster─────────────────┐ │ test_cluster_two_shards │ └─────────────────────────┘  Query: SHOW CLUSTER 'test_shard_localhost' FORMAT Vertical;  Result: Row 1: ────── cluster: test_shard_localhost shard_num: 1 shard_weight: 1 replica_num: 1 host_name: localhost host_address: 127.0.0.1 port: 9000 is_local: 1 user: default default_database: errors_count: 0 estimated_recovery_time: 0  "},{"title":"SHOW SETTINGS​","type":1,"pageTitle":"SHOW Statements","url":"docs/en/sql-reference/statements/show#show-settings","content":"Returns a list of system settings and their values. Selects data from the system.settings table. Syntax SHOW [CHANGED] SETTINGS LIKE|ILIKE &lt;name&gt;  Clauses LIKE|ILIKE allow to specify a matching pattern for the setting name. It can contain globs such as % or _. LIKE clause is case-sensitive, ILIKE — case insensitive. When the CHANGED clause is used, the query returns only settings changed from their default values. Examples Query with the LIKE clause: SHOW SETTINGS LIKE 'send_timeout';  Result: ┌─name─────────┬─type────┬─value─┐ │ send_timeout │ Seconds │ 300 │ └──────────────┴─────────┴───────┘  Query with the ILIKE clause: SHOW SETTINGS ILIKE '%CONNECT_timeout%'  Result: ┌─name────────────────────────────────────┬─type─────────┬─value─┐ │ connect_timeout │ Seconds │ 10 │ │ connect_timeout_with_failover_ms │ Milliseconds │ 50 │ │ connect_timeout_with_failover_secure_ms │ Milliseconds │ 100 │ └─────────────────────────────────────────┴──────────────┴───────┘  Query with the CHANGED clause: SHOW CHANGED SETTINGS ILIKE '%MEMORY%'  Result: ┌─name─────────────┬─type───┬─value───────┐ │ max_memory_usage │ UInt64 │ 10000000000 │ └──────────────────┴────────┴─────────────┘  See Also system.settings table Original article "},{"title":"SET Statement","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/set","content":"SET Statement SET param = value Assigns value to the param setting for the current session. You cannot change server settings this way. You can also set all the values from the specified settings profile in a single query. SET profile = 'profile-name-from-the-settings-file' For more information, see Settings.","keywords":""},{"title":"SYSTEM Statements","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/system","content":"","keywords":""},{"title":"RELOAD EMBEDDED DICTIONARIES​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-reload-emdedded-dictionaries","content":"Reload all Internal dictionaries. By default, internal dictionaries are disabled. Always returns Ok. regardless of the result of the internal dictionary update. "},{"title":"RELOAD DICTIONARIES​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-reload-dictionaries","content":"Reloads all dictionaries that have been successfully loaded before. By default, dictionaries are loaded lazily (see dictionaries_lazy_load), so instead of being loaded automatically at startup, they are initialized on first access through dictGet function or SELECT from tables with ENGINE = Dictionary. The SYSTEM RELOAD DICTIONARIES query reloads such dictionaries (LOADED). Always returns Ok. regardless of the result of the dictionary update. "},{"title":"RELOAD DICTIONARY​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-reload-dictionary","content":"Completely reloads a dictionary dictionary_name, regardless of the state of the dictionary (LOADED / NOT_LOADED / FAILED). Always returns Ok. regardless of the result of updating the dictionary. The status of the dictionary can be checked by querying the system.dictionaries table. SELECT name, status FROM system.dictionaries;  "},{"title":"RELOAD MODELS​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-reload-models","content":"Reloads all CatBoost models if the configuration was updated without restarting the server. Syntax SYSTEM RELOAD MODELS [ON CLUSTER cluster_name]  "},{"title":"RELOAD MODEL​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-reload-model","content":"Completely reloads a CatBoost model model_name if the configuration was updated without restarting the server. Syntax SYSTEM RELOAD MODEL [ON CLUSTER cluster_name] &lt;model_name&gt;  "},{"title":"RELOAD FUNCTIONS​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-reload-functions","content":"Reloads all registered executable user defined functions or one of them from a configuration file. Syntax RELOAD FUNCTIONS [ON CLUSTER cluster_name] RELOAD FUNCTION [ON CLUSTER cluster_name] function_name  "},{"title":"DROP DNS CACHE​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-drop-dns-cache","content":"Resets ClickHouse’s internal DNS cache. Sometimes (for old ClickHouse versions) it is necessary to use this command when changing the infrastructure (changing the IP address of another ClickHouse server or the server used by dictionaries). For more convenient (automatic) cache management, see disable_internal_dns_cache, dns_cache_update_period parameters. "},{"title":"DROP MARK CACHE​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-drop-mark-cache","content":"Resets the mark cache. Used in development of ClickHouse and performance tests. "},{"title":"DROP REPLICA​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-drop-replica","content":"Dead replicas can be dropped using following syntax: SYSTEM DROP REPLICA 'replica_name' FROM TABLE database.table; SYSTEM DROP REPLICA 'replica_name' FROM DATABASE database; SYSTEM DROP REPLICA 'replica_name'; SYSTEM DROP REPLICA 'replica_name' FROM ZKPATH '/path/to/table/in/zk';  Queries will remove the replica path in ZooKeeper. It is useful when the replica is dead and its metadata cannot be removed from ZooKeeper by DROP TABLE because there is no such table anymore. It will only drop the inactive/stale replica, and it cannot drop local replica, please use DROP TABLE for that. DROP REPLICA does not drop any tables and does not remove any data or metadata from disk. The first one removes metadata of 'replica_name' replica of database.table table. The second one does the same for all replicated tables in the database. The third one does the same for all replicated tables on the local server. The fourth one is useful to remove metadata of dead replica when all other replicas of a table were dropped. It requires the table path to be specified explicitly. It must be the same path as was passed to the first argument of ReplicatedMergeTree engine on table creation. "},{"title":"DROP UNCOMPRESSED CACHE​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-drop-uncompressed-cache","content":"Reset the uncompressed data cache. Used in development of ClickHouse and performance tests. For manage uncompressed data cache parameters use following server level settings uncompressed_cache_size and query/user/profile level settings use_uncompressed_cache "},{"title":"DROP COMPILED EXPRESSION CACHE​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-drop-compiled-expression-cache","content":"Reset the compiled expression cache. Used in development of ClickHouse and performance tests. Compiled expression cache used when query/user/profile enable option compile-expressions "},{"title":"FLUSH LOGS​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-flush_logs","content":"Flushes buffers of log messages to system tables (e.g. system.query_log). Allows you to not wait 7.5 seconds when debugging. This will also create system tables even if message queue is empty. "},{"title":"RELOAD CONFIG​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-reload-config","content":"Reloads ClickHouse configuration. Used when configuration is stored in ZooKeeper. "},{"title":"SHUTDOWN​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-shutdown","content":"Normally shuts down ClickHouse (like service clickhouse-server stop / kill {$pid_clickhouse-server}) "},{"title":"KILL​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-kill","content":"Aborts ClickHouse process (like kill -9 {$ pid_clickhouse-server}) "},{"title":"Managing Distributed Tables​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query-language-system-distributed","content":"ClickHouse can manage distributed tables. When a user inserts data into these tables, ClickHouse first creates a queue of the data that should be sent to cluster nodes, then asynchronously sends it. You can manage queue processing with the STOP DISTRIBUTED SENDS, FLUSH DISTRIBUTED, and START DISTRIBUTED SENDS queries. You can also synchronously insert distributed data with the insert_distributed_sync setting. "},{"title":"STOP DISTRIBUTED SENDS​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-stop-distributed-sends","content":"Disables background data distribution when inserting data into distributed tables. SYSTEM STOP DISTRIBUTED SENDS [db.]&lt;distributed_table_name&gt;  "},{"title":"FLUSH DISTRIBUTED​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-flush-distributed","content":"Forces ClickHouse to send data to cluster nodes synchronously. If any nodes are unavailable, ClickHouse throws an exception and stops query execution. You can retry the query until it succeeds, which will happen when all nodes are back online. SYSTEM FLUSH DISTRIBUTED [db.]&lt;distributed_table_name&gt;  "},{"title":"START DISTRIBUTED SENDS​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-start-distributed-sends","content":"Enables background data distribution when inserting data into distributed tables. SYSTEM START DISTRIBUTED SENDS [db.]&lt;distributed_table_name&gt;  "},{"title":"Managing MergeTree Tables​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query-language-system-mergetree","content":"ClickHouse can manage background processes in MergeTree tables. "},{"title":"STOP MERGES​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-stop-merges","content":"Provides possibility to stop background merges for tables in the MergeTree family: SYSTEM STOP MERGES [ON VOLUME &lt;volume_name&gt; | [db.]merge_tree_family_table_name]  note DETACH / ATTACH table will start background merges for the table even in case when merges have been stopped for all MergeTree tables before. "},{"title":"START MERGES​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-start-merges","content":"Provides possibility to start background merges for tables in the MergeTree family: SYSTEM START MERGES [ON VOLUME &lt;volume_name&gt; | [db.]merge_tree_family_table_name]  "},{"title":"STOP TTL MERGES​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-stop-ttl-merges","content":"Provides possibility to stop background delete old data according to TTL expression for tables in the MergeTree family: Returns Ok. even if table does not exist or table has not MergeTree engine. Returns error when database does not exist: SYSTEM STOP TTL MERGES [[db.]merge_tree_family_table_name]  "},{"title":"START TTL MERGES​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-start-ttl-merges","content":"Provides possibility to start background delete old data according to TTL expression for tables in the MergeTree family: Returns Ok. even if table does not exist. Returns error when database does not exist: SYSTEM START TTL MERGES [[db.]merge_tree_family_table_name]  "},{"title":"STOP MOVES​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-stop-moves","content":"Provides possibility to stop background move data according to TTL table expression with TO VOLUME or TO DISK clause for tables in the MergeTree family: Returns Ok. even if table does not exist. Returns error when database does not exist: SYSTEM STOP MOVES [[db.]merge_tree_family_table_name]  "},{"title":"START MOVES​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-start-moves","content":"Provides possibility to start background move data according to TTL table expression with TO VOLUME and TO DISK clause for tables in the MergeTree family: Returns Ok. even if table does not exist. Returns error when database does not exist: SYSTEM START MOVES [[db.]merge_tree_family_table_name]  "},{"title":"Managing ReplicatedMergeTree Tables​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query-language-system-replicated","content":"ClickHouse can manage background replication related processes in ReplicatedMergeTree tables. "},{"title":"STOP FETCHES​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-stop-fetches","content":"Provides possibility to stop background fetches for inserted parts for tables in the ReplicatedMergeTree family: Always returns Ok. regardless of the table engine and even if table or database does not exist. SYSTEM STOP FETCHES [[db.]replicated_merge_tree_family_table_name]  "},{"title":"START FETCHES​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-start-fetches","content":"Provides possibility to start background fetches for inserted parts for tables in the ReplicatedMergeTree family: Always returns Ok. regardless of the table engine and even if table or database does not exist. SYSTEM START FETCHES [[db.]replicated_merge_tree_family_table_name]  "},{"title":"STOP REPLICATED SENDS​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-start-replicated-sends","content":"Provides possibility to stop background sends to other replicas in cluster for new inserted parts for tables in the ReplicatedMergeTree family: SYSTEM STOP REPLICATED SENDS [[db.]replicated_merge_tree_family_table_name]  "},{"title":"START REPLICATED SENDS​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-start-replicated-sends","content":"Provides possibility to start background sends to other replicas in cluster for new inserted parts for tables in the ReplicatedMergeTree family: SYSTEM START REPLICATED SENDS [[db.]replicated_merge_tree_family_table_name]  "},{"title":"STOP REPLICATION QUEUES​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-stop-replication-queues","content":"Provides possibility to stop background fetch tasks from replication queues which stored in Zookeeper for tables in the ReplicatedMergeTree family. Possible background tasks types - merges, fetches, mutation, DDL statements with ON CLUSTER clause: SYSTEM STOP REPLICATION QUEUES [[db.]replicated_merge_tree_family_table_name]  "},{"title":"START REPLICATION QUEUES​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-start-replication-queues","content":"Provides possibility to start background fetch tasks from replication queues which stored in Zookeeper for tables in the ReplicatedMergeTree family. Possible background tasks types - merges, fetches, mutation, DDL statements with ON CLUSTER clause: SYSTEM START REPLICATION QUEUES [[db.]replicated_merge_tree_family_table_name]  "},{"title":"SYNC REPLICA​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-sync-replica","content":"Wait until a ReplicatedMergeTree table will be synced with other replicas in a cluster. Will run until receive_timeout if fetches currently disabled for the table. SYSTEM SYNC REPLICA [db.]replicated_merge_tree_family_table_name  After running this statement the [db.]replicated_merge_tree_family_table_name fetches commands from the common replicated log into its own replication queue, and then the query waits till the replica processes all of the fetched commands. "},{"title":"RESTART REPLICA​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-restart-replica","content":"Provides possibility to reinitialize Zookeeper sessions state for ReplicatedMergeTree table, will compare current state with Zookeeper as source of true and add tasks to Zookeeper queue if needed. Initialization replication queue based on ZooKeeper date happens in the same way as ATTACH TABLE statement. For a short time the table will be unavailable for any operations. SYSTEM RESTART REPLICA [db.]replicated_merge_tree_family_table_name  "},{"title":"RESTORE REPLICA​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-restore-replica","content":"Restores a replica if data is [possibly] present but Zookeeper metadata is lost. Works only on readonly ReplicatedMergeTree tables. One may execute query after: ZooKeeper root / loss.Replicas path /replicas loss.Individual replica path /replicas/replica_name/ loss. Replica attaches locally found parts and sends info about them to Zookeeper. Parts present on a replica before metadata loss are not re-fetched from other ones if not being outdated (so replica restoration does not mean re-downloading all data over the network). warning Parts in all states are moved to detached/ folder. Parts active before data loss (committed) are attached. Syntax SYSTEM RESTORE REPLICA [db.]replicated_merge_tree_family_table_name [ON CLUSTER cluster_name]  Alternative syntax: SYSTEM RESTORE REPLICA [ON CLUSTER cluster_name] [db.]replicated_merge_tree_family_table_name  Example Creating a table on multiple servers. After the replica's metadata in ZooKeeper is lost, the table will attach as read-only as metadata is missing. The last query needs to execute on every replica. CREATE TABLE test(n UInt32) ENGINE = ReplicatedMergeTree('/clickhouse/tables/test/', '{replica}') ORDER BY n PARTITION BY n % 10; INSERT INTO test SELECT * FROM numbers(1000); -- zookeeper_delete_path(&quot;/clickhouse/tables/test&quot;, recursive=True) &lt;- root loss. SYSTEM RESTART REPLICA test; SYSTEM RESTORE REPLICA test;  Another way: SYSTEM RESTORE REPLICA test ON CLUSTER cluster;  "},{"title":"RESTART REPLICAS​","type":1,"pageTitle":"SYSTEM Statements","url":"docs/en/sql-reference/statements/system#query_language-system-restart-replicas","content":"Provides possibility to reinitialize Zookeeper sessions state for all ReplicatedMergeTree tables, will compare current state with Zookeeper as source of true and add tasks to Zookeeper queue if needed "},{"title":"USE Statement","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/use","content":"USE Statement USE db Lets you set the current database for the session. The current database is used for searching for tables if the database is not explicitly defined in the query with a dot before the table name. This query can’t be made when using the HTTP protocol, since there is no concept of a session.","keywords":""},{"title":"cluster, clusterAllReplicas","type":0,"sectionRef":"#","url":"docs/en/sql-reference/table-functions/cluster","content":"cluster, clusterAllReplicas Allows to access all shards in an existing cluster which configured in remote_servers section without creating a Distributed table. One replica of each shard is queried. clusterAllReplicas function — same as cluster, but all replicas are queried. Each replica in a cluster is used as a separate shard/connection. note All available clusters are listed in the system.clusters table. Syntax cluster('cluster_name', db.table[, sharding_key]) cluster('cluster_name', db, table[, sharding_key]) clusterAllReplicas('cluster_name', db.table[, sharding_key]) clusterAllReplicas('cluster_name', db, table[, sharding_key]) Arguments cluster_name – Name of a cluster that is used to build a set of addresses and connection parameters to remote and local servers. db.table or db, table - Name of a database and a table. sharding_key - A sharding key. Optional. Needs to be specified if the cluster has more than one shard. Returned value The dataset from clusters. Using Macros cluster_name can contain macros — substitution in curly brackets. The substituted value is taken from the macros section of the server configuration file. Example: SELECT * FROM cluster('{cluster}', default.example_table); Usage and Recommendations Using the cluster and clusterAllReplicas table functions are less efficient than creating a Distributed table because in this case, the server connection is re-established for every request. When processing a large number of queries, please always create the Distributed table ahead of time, and do not use the cluster and clusterAllReplicas table functions. The cluster and clusterAllReplicas table functions can be useful in the following cases: Accessing a specific cluster for data comparison, debugging, and testing.Queries to various ClickHouse clusters and replicas for research purposes.Infrequent distributed requests that are made manually. Connection settings like host, port, user, password, compression, secure are taken from &lt;remote_servers&gt; config section. See details in Distributed engine. See Also skip_unavailable_shardsload_balancing","keywords":""},{"title":"Table Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/table-functions/","content":"Table Functions Table functions are methods for constructing tables. You can use table functions in: FROM clause of the SELECT query. The method for creating a temporary table that is available only in the current query. The table is deleted when the query finishes. CREATE TABLE AS table_function() query. It's one of the methods of creating a table. INSERT INTO TABLE FUNCTION query. warning You can’t use table functions if the allow_ddl setting is disabled. Function\tDescriptionfile\tCreates a File-engine table. merge\tCreates a Merge-engine table. numbers\tCreates a table with a single column filled with integer numbers. remote\tAllows you to access remote servers without creating a Distributed-engine table. url\tCreates a Url-engine table. mysql\tCreates a MySQL-engine table. postgresql\tCreates a PostgreSQL-engine table. jdbc\tCreates a JDBC-engine table. odbc\tCreates a ODBC-engine table. hdfs\tCreates a HDFS-engine table. s3\tCreates a S3-engine table. sqlite\tCreates a sqlite-engine table. Original article","keywords":""},{"title":"file","type":0,"sectionRef":"#","url":"docs/en/sql-reference/table-functions/file","content":"","keywords":""},{"title":"Globs in Path​","type":1,"pageTitle":"file","url":"docs/en/sql-reference/table-functions/file#globs-in-path","content":"Multiple path components can have globs. For being processed file must exist and match to the whole path pattern (not only suffix or prefix). * — Substitutes any number of any characters except / including empty string.? — Substitutes any single character.{some_string,another_string,yet_another_one} — Substitutes any of strings 'some_string', 'another_string', 'yet_another_one'.{N..M} — Substitutes any number in range from N to M including both borders. Constructions with {} are similar to the remote table function. Example Suppose we have several files with the following relative paths: 'some_dir/some_file_1''some_dir/some_file_2''some_dir/some_file_3''another_dir/some_file_1''another_dir/some_file_2''another_dir/some_file_3' Query the number of rows in these files: SELECT count(*) FROM file('{some,another}_dir/some_file_{1..3}', 'TSV', 'name String, value UInt32');  Query the number of rows in all files of these two directories: SELECT count(*) FROM file('{some,another}_dir/*', 'TSV', 'name String, value UInt32');  warning If your listing of files contains number ranges with leading zeros, use the construction with braces for each digit separately or use ?. Example Query the data from files named file000, file001, … , file999: SELECT count(*) FROM file('big_dir/file{0..9}{0..9}{0..9}', 'CSV', 'name String, value UInt32');  "},{"title":"Virtual Columns​","type":1,"pageTitle":"file","url":"docs/en/sql-reference/table-functions/file#virtual-columns","content":"_path — Path to the file._file — Name of the file. See Also Virtual columns Original article "},{"title":"generateRandom","type":0,"sectionRef":"#","url":"docs/en/sql-reference/table-functions/generate","content":"","keywords":""},{"title":"Usage Example​","type":1,"pageTitle":"generateRandom","url":"docs/en/sql-reference/table-functions/generate#usage-example","content":"SELECT * FROM generateRandom('a Array(Int8), d Decimal32(4), c Tuple(DateTime64(3), UUID)', 1, 10, 2) LIMIT 3;  ┌─a────────┬────────────d─┬─c──────────────────────────────────────────────────────────────────┐ │ [77] │ -124167.6723 │ ('2061-04-17 21:59:44.573','3f72f405-ec3e-13c8-44ca-66ef335f7835') │ │ [32,110] │ -141397.7312 │ ('1979-02-09 03:43:48.526','982486d1-5a5d-a308-e525-7bd8b80ffa73') │ │ [68] │ -67417.0770 │ ('2080-03-12 14:17:31.269','110425e5-413f-10a6-05ba-fa6b3e929f15') │ └──────────┴──────────────┴────────────────────────────────────────────────────────────────────┘  "},{"title":"merge","type":0,"sectionRef":"#","url":"docs/en/sql-reference/table-functions/merge","content":"merge Creates a temporary Merge table. The table structure is taken from the first table encountered that matches the regular expression. Syntax merge('db_name', 'tables_regexp') Arguments db_name — Possible values: database name, constant expression that returns a string with a database name, for example, currentDatabase(),REGEXP(expression), where expression is a regular expression to match the DB names. tables_regexp — A regular expression to match the table names in the specified DB or DBs. See Also Merge table engine","keywords":""},{"title":"hdfsCluster Table Function","type":0,"sectionRef":"#","url":"docs/en/sql-reference/table-functions/hdfsCluster","content":"hdfsCluster Table Function Allows processing files from HDFS in parallel from many nodes in a specified cluster. On initiator it creates a connection to all nodes in the cluster, discloses asterics in HDFS file path, and dispatches each file dynamically. On the worker node it asks the initiator about the next task to process and processes it. This is repeated until all tasks are finished. Syntax hdfsCluster(cluster_name, URI, format, structure) Arguments cluster_name — Name of a cluster that is used to build a set of addresses and connection parameters to remote and local servers.URI — URI to a file or a bunch of files. Supports following wildcards in readonly mode: *, ?, {'abc','def'} and {N..M} where N, M — numbers, abc, def — strings. For more information see Wildcards In Path.format — The format of the file.structure — Structure of the table. Format 'column1_name column1_type, column2_name column2_type, ...'. Returned value A table with the specified structure for reading data in the specified file. Examples Suppose that we have a ClickHouse cluster named cluster_simple, and several files with following URIs on HDFS: ‘hdfs://hdfs1:9000/some_dir/some_file_1’‘hdfs://hdfs1:9000/some_dir/some_file_2’‘hdfs://hdfs1:9000/some_dir/some_file_3’‘hdfs://hdfs1:9000/another_dir/some_file_1’‘hdfs://hdfs1:9000/another_dir/some_file_2’‘hdfs://hdfs1:9000/another_dir/some_file_3’ Query the amount of rows in these files: SELECT count(*) FROM hdfsCluster('cluster_simple', 'hdfs://hdfs1:9000/{some,another}_dir/some_file_{1..3}', 'TSV', 'name String, value UInt32') Query the amount of rows in all files of these two directories: SELECT count(*) FROM hdfsCluster('cluster_simple', 'hdfs://hdfs1:9000/{some,another}_dir/*', 'TSV', 'name String, value UInt32') warning If your listing of files contains number ranges with leading zeros, use the construction with braces for each digit separately or use ?. See Also HDFS engineHDFS table function","keywords":""},{"title":"jdbc","type":0,"sectionRef":"#","url":"docs/en/sql-reference/table-functions/jdbc","content":"jdbc jdbc(datasource, schema, table) - returns table that is connected via JDBC driver. This table function requires separate clickhouse-jdbc-bridge program to be running. It supports Nullable types (based on DDL of remote table that is queried). Examples SELECT * FROM jdbc('jdbc:mysql://localhost:3306/?user=root&amp;password=root', 'schema', 'table') SELECT * FROM jdbc('mysql://localhost:3306/?user=root&amp;password=root', 'select * from schema.table') SELECT * FROM jdbc('mysql-dev?p1=233', 'num Int32', 'select toInt32OrZero(''{{p1}}'') as num') SELECT * FROM jdbc('mysql-dev?p1=233', 'num Int32', 'select toInt32OrZero(''{{p1}}'') as num') SELECT a.datasource AS server1, b.datasource AS server2, b.name AS db FROM jdbc('mysql-dev?datasource_column', 'show databases') a INNER JOIN jdbc('self?datasource_column', 'show databases') b ON a.Database = b.name Original article","keywords":""},{"title":"null","type":0,"sectionRef":"#","url":"docs/en/sql-reference/table-functions/null","content":"null Creates a temporary table of the specified structure with the Null table engine. According to the Null-engine properties, the table data is ignored and the table itself is immediately droped right after the query execution. The function is used for the convenience of test writing and demonstrations. Syntax null('structure') Parameter structure — A list of columns and column types. String. Returned value A temporary Null-engine table with the specified structure. Example Query with the null function: INSERT INTO function null('x UInt64') SELECT * FROM numbers_mt(1000000000); can replace three queries: CREATE TABLE t (x UInt64) ENGINE = Null; INSERT INTO t SELECT * FROM numbers_mt(1000000000); DROP TABLE IF EXISTS t; See also: Null table engine Original article","keywords":""},{"title":"numbers","type":0,"sectionRef":"#","url":"docs/en/sql-reference/table-functions/numbers","content":"numbers numbers(N) – Returns a table with the single ‘number’ column (UInt64) that contains integers from 0 to N-1.numbers(N, M) - Returns a table with the single ‘number’ column (UInt64) that contains integers from N to (N + M - 1). Similar to the system.numbers table, it can be used for testing and generating successive values, numbers(N, M) more efficient than system.numbers. The following queries are equivalent: SELECT * FROM numbers(10); SELECT * FROM numbers(0, 10); SELECT * FROM system.numbers LIMIT 10; Examples: -- Generate a sequence of dates from 2010-01-01 to 2010-12-31 select toDate('2010-01-01') + number as d FROM numbers(365); ","keywords":""},{"title":"ORDER BY Clause","type":0,"sectionRef":"#","url":"docs/en/sql-reference/statements/select/order-by","content":"","keywords":""},{"title":"Sorting of Special Values​","type":1,"pageTitle":"ORDER BY Clause","url":"docs/en/sql-reference/statements/select/order-by#sorting-of-special-values","content":"There are two approaches to NaN and NULL sorting order: By default or with the NULLS LAST modifier: first the values, then NaN, then NULL.With the NULLS FIRST modifier: first NULL, then NaN, then other values. "},{"title":"Example​","type":1,"pageTitle":"ORDER BY Clause","url":"docs/en/sql-reference/statements/select/order-by#example","content":"For the table ┌─x─┬────y─┐ │ 1 │ ᴺᵁᴸᴸ │ │ 2 │ 2 │ │ 1 │ nan │ │ 2 │ 2 │ │ 3 │ 4 │ │ 5 │ 6 │ │ 6 │ nan │ │ 7 │ ᴺᵁᴸᴸ │ │ 6 │ 7 │ │ 8 │ 9 │ └───┴──────┘  Run the query SELECT * FROM t_null_nan ORDER BY y NULLS FIRST to get: ┌─x─┬────y─┐ │ 1 │ ᴺᵁᴸᴸ │ │ 7 │ ᴺᵁᴸᴸ │ │ 1 │ nan │ │ 6 │ nan │ │ 2 │ 2 │ │ 2 │ 2 │ │ 3 │ 4 │ │ 5 │ 6 │ │ 6 │ 7 │ │ 8 │ 9 │ └───┴──────┘  When floating point numbers are sorted, NaNs are separate from the other values. Regardless of the sorting order, NaNs come at the end. In other words, for ascending sorting they are placed as if they are larger than all the other numbers, while for descending sorting they are placed as if they are smaller than the rest. "},{"title":"Collation Support​","type":1,"pageTitle":"ORDER BY Clause","url":"docs/en/sql-reference/statements/select/order-by#collation-support","content":"For sorting by String values, you can specify collation (comparison). Example: ORDER BY SearchPhrase COLLATE 'tr' - for sorting by keyword in ascending order, using the Turkish alphabet, case insensitive, assuming that strings are UTF-8 encoded. COLLATE can be specified or not for each expression in ORDER BY independently. If ASC or DESC is specified, COLLATE is specified after it. When using COLLATE, sorting is always case-insensitive. Collate is supported in LowCardinality, Nullable, Array and Tuple. We only recommend using COLLATE for final sorting of a small number of rows, since sorting with COLLATE is less efficient than normal sorting by bytes. "},{"title":"Collation Examples​","type":1,"pageTitle":"ORDER BY Clause","url":"docs/en/sql-reference/statements/select/order-by#collation-examples","content":"Example only with String values: Input table: ┌─x─┬─s────┐ │ 1 │ bca │ │ 2 │ ABC │ │ 3 │ 123a │ │ 4 │ abc │ │ 5 │ BCA │ └───┴──────┘  Query: SELECT * FROM collate_test ORDER BY s ASC COLLATE 'en';  Result: ┌─x─┬─s────┐ │ 3 │ 123a │ │ 4 │ abc │ │ 2 │ ABC │ │ 1 │ bca │ │ 5 │ BCA │ └───┴──────┘  Example with Nullable: Input table: ┌─x─┬─s────┐ │ 1 │ bca │ │ 2 │ ᴺᵁᴸᴸ │ │ 3 │ ABC │ │ 4 │ 123a │ │ 5 │ abc │ │ 6 │ ᴺᵁᴸᴸ │ │ 7 │ BCA │ └───┴──────┘  Query: SELECT * FROM collate_test ORDER BY s ASC COLLATE 'en';  Result: ┌─x─┬─s────┐ │ 4 │ 123a │ │ 5 │ abc │ │ 3 │ ABC │ │ 1 │ bca │ │ 7 │ BCA │ │ 6 │ ᴺᵁᴸᴸ │ │ 2 │ ᴺᵁᴸᴸ │ └───┴──────┘  Example with Array: Input table: ┌─x─┬─s─────────────┐ │ 1 │ ['Z'] │ │ 2 │ ['z'] │ │ 3 │ ['a'] │ │ 4 │ ['A'] │ │ 5 │ ['z','a'] │ │ 6 │ ['z','a','a'] │ │ 7 │ [''] │ └───┴───────────────┘  Query: SELECT * FROM collate_test ORDER BY s ASC COLLATE 'en';  Result: ┌─x─┬─s─────────────┐ │ 7 │ [''] │ │ 3 │ ['a'] │ │ 4 │ ['A'] │ │ 2 │ ['z'] │ │ 5 │ ['z','a'] │ │ 6 │ ['z','a','a'] │ │ 1 │ ['Z'] │ └───┴───────────────┘  Example with LowCardinality string: Input table: ┌─x─┬─s───┐ │ 1 │ Z │ │ 2 │ z │ │ 3 │ a │ │ 4 │ A │ │ 5 │ za │ │ 6 │ zaa │ │ 7 │ │ └───┴─────┘  Query: SELECT * FROM collate_test ORDER BY s ASC COLLATE 'en';  Result: ┌─x─┬─s───┐ │ 7 │ │ │ 3 │ a │ │ 4 │ A │ │ 2 │ z │ │ 1 │ Z │ │ 5 │ za │ │ 6 │ zaa │ └───┴─────┘  Example with Tuple: ┌─x─┬─s───────┐ │ 1 │ (1,'Z') │ │ 2 │ (1,'z') │ │ 3 │ (1,'a') │ │ 4 │ (2,'z') │ │ 5 │ (1,'A') │ │ 6 │ (2,'Z') │ │ 7 │ (2,'A') │ └───┴─────────┘  Query: SELECT * FROM collate_test ORDER BY s ASC COLLATE 'en';  Result: ┌─x─┬─s───────┐ │ 3 │ (1,'a') │ │ 5 │ (1,'A') │ │ 2 │ (1,'z') │ │ 1 │ (1,'Z') │ │ 7 │ (2,'A') │ │ 4 │ (2,'z') │ │ 6 │ (2,'Z') │ └───┴─────────┘  "},{"title":"Implementation Details​","type":1,"pageTitle":"ORDER BY Clause","url":"docs/en/sql-reference/statements/select/order-by#implementation-details","content":"Less RAM is used if a small enough LIMIT is specified in addition to ORDER BY. Otherwise, the amount of memory spent is proportional to the volume of data for sorting. For distributed query processing, if GROUP BY is omitted, sorting is partially done on remote servers, and the results are merged on the requestor server. This means that for distributed sorting, the volume of data to sort can be greater than the amount of memory on a single server. If there is not enough RAM, it is possible to perform sorting in external memory (creating temporary files on a disk). Use the setting max_bytes_before_external_sort for this purpose. If it is set to 0 (the default), external sorting is disabled. If it is enabled, when the volume of data to sort reaches the specified number of bytes, the collected data is sorted and dumped into a temporary file. After all data is read, all the sorted files are merged and the results are output. Files are written to the /var/lib/clickhouse/tmp/ directory in the config (by default, but you can use the tmp_path parameter to change this setting). Running a query may use more memory than max_bytes_before_external_sort. For this reason, this setting must have a value significantly smaller than max_memory_usage. As an example, if your server has 128 GB of RAM and you need to run a single query, set max_memory_usage to 100 GB, and max_bytes_before_external_sort to 80 GB. External sorting works much less effectively than sorting in RAM. "},{"title":"Optimization of Data Reading​","type":1,"pageTitle":"ORDER BY Clause","url":"docs/en/sql-reference/statements/select/order-by#optimize_read_in_order","content":"If ORDER BY expression has a prefix that coincides with the table sorting key, you can optimize the query by using the optimize_read_in_order setting. When the optimize_read_in_order setting is enabled, the ClickHouse server uses the table index and reads the data in order of the ORDER BY key. This allows to avoid reading all data in case of specified LIMIT. So queries on big data with small limit are processed faster. Optimization works with both ASC and DESC and does not work together with GROUP BY clause and FINAL modifier. When the optimize_read_in_order setting is disabled, the ClickHouse server does not use the table index while processing SELECT queries. Consider disabling optimize_read_in_order manually, when running queries that have ORDER BY clause, large LIMIT and WHERE condition that requires to read huge amount of records before queried data is found. Optimization is supported in the following table engines: MergeTreeMerge, Buffer, and MaterializedView table engines over MergeTree-engine tables In MaterializedView-engine tables the optimization works with views like SELECT ... FROM merge_tree_table ORDER BY pk. But it is not supported in the queries like SELECT ... FROM view ORDER BY pk if the view query does not have the ORDER BY clause. "},{"title":"ORDER BY Expr WITH FILL Modifier​","type":1,"pageTitle":"ORDER BY Clause","url":"docs/en/sql-reference/statements/select/order-by#orderby-with-fill","content":"This modifier also can be combined with LIMIT … WITH TIES modifier. WITH FILL modifier can be set after ORDER BY expr with optional FROM expr, TO expr and STEP expr parameters. All missed values of expr column will be filled sequentially and other columns will be filled as defaults. To fill multiple columns, add WITH FILL modifier with optional parameters after each field name in ORDER BY section. ORDER BY expr [WITH FILL] [FROM const_expr] [TO const_expr] [STEP const_numeric_expr], ... exprN [WITH FILL] [FROM expr] [TO expr] [STEP numeric_expr]  WITH FILL can be applied for fields with Numeric (all kinds of float, decimal, int) or Date/DateTime types. When applied for String fields, missed values are filled with empty strings. When FROM const_expr not defined sequence of filling use minimal expr field value from ORDER BY. When TO const_expr not defined sequence of filling use maximum expr field value from ORDER BY. When STEP const_numeric_expr defined then const_numeric_expr interprets as is for numeric types, as days for Date type, as seconds for DateTime type. It also supports INTERVAL data type representing time and date intervals. When STEP const_numeric_expr omitted then sequence of filling use 1.0 for numeric type, 1 day for Date type and 1 second for DateTime type. Example of a query without WITH FILL: SELECT n, source FROM ( SELECT toFloat32(number % 10) AS n, 'original' AS source FROM numbers(10) WHERE number % 3 = 1 ) ORDER BY n;  Result: ┌─n─┬─source───┐ │ 1 │ original │ │ 4 │ original │ │ 7 │ original │ └───┴──────────┘  Same query after applying WITH FILL modifier: SELECT n, source FROM ( SELECT toFloat32(number % 10) AS n, 'original' AS source FROM numbers(10) WHERE number % 3 = 1 ) ORDER BY n WITH FILL FROM 0 TO 5.51 STEP 0.5;  Result: ┌───n─┬─source───┐ │ 0 │ │ │ 0.5 │ │ │ 1 │ original │ │ 1.5 │ │ │ 2 │ │ │ 2.5 │ │ │ 3 │ │ │ 3.5 │ │ │ 4 │ original │ │ 4.5 │ │ │ 5 │ │ │ 5.5 │ │ │ 7 │ original │ └─────┴──────────┘  For the case with multiple fields ORDER BY field2 WITH FILL, field1 WITH FILL order of filling will follow the order of fields in the ORDER BY clause. Example: SELECT toDate((number * 10) * 86400) AS d1, toDate(number * 86400) AS d2, 'original' AS source FROM numbers(10) WHERE (number % 3) = 1 ORDER BY d2 WITH FILL, d1 WITH FILL STEP 5;  Result: ┌───d1───────┬───d2───────┬─source───┐ │ 1970-01-11 │ 1970-01-02 │ original │ │ 1970-01-01 │ 1970-01-03 │ │ │ 1970-01-01 │ 1970-01-04 │ │ │ 1970-02-10 │ 1970-01-05 │ original │ │ 1970-01-01 │ 1970-01-06 │ │ │ 1970-01-01 │ 1970-01-07 │ │ │ 1970-03-12 │ 1970-01-08 │ original │ └────────────┴────────────┴──────────┘  Field d1 does not fill in and use the default value cause we do not have repeated values for d2 value, and the sequence for d1 can’t be properly calculated. The following query with the changed field in ORDER BY: SELECT toDate((number * 10) * 86400) AS d1, toDate(number * 86400) AS d2, 'original' AS source FROM numbers(10) WHERE (number % 3) = 1 ORDER BY d1 WITH FILL STEP 5, d2 WITH FILL;  Result: ┌───d1───────┬───d2───────┬─source───┐ │ 1970-01-11 │ 1970-01-02 │ original │ │ 1970-01-16 │ 1970-01-01 │ │ │ 1970-01-21 │ 1970-01-01 │ │ │ 1970-01-26 │ 1970-01-01 │ │ │ 1970-01-31 │ 1970-01-01 │ │ │ 1970-02-05 │ 1970-01-01 │ │ │ 1970-02-10 │ 1970-01-05 │ original │ │ 1970-02-15 │ 1970-01-01 │ │ │ 1970-02-20 │ 1970-01-01 │ │ │ 1970-02-25 │ 1970-01-01 │ │ │ 1970-03-02 │ 1970-01-01 │ │ │ 1970-03-07 │ 1970-01-01 │ │ │ 1970-03-12 │ 1970-01-08 │ original │ └────────────┴────────────┴──────────┘  The following query uses the INTERVAL data type of 1 day for each data filled on column d1: SELECT toDate((number * 10) * 86400) AS d1, toDate(number * 86400) AS d2, 'original' AS source FROM numbers(10) WHERE (number % 3) = 1 ORDER BY d1 WITH FILL STEP INTERVAL 1 DAY, d2 WITH FILL;  Result: ┌─────────d1─┬─────────d2─┬─source───┐ │ 1970-01-11 │ 1970-01-02 │ original │ │ 1970-01-12 │ 1970-01-01 │ │ │ 1970-01-13 │ 1970-01-01 │ │ │ 1970-01-14 │ 1970-01-01 │ │ │ 1970-01-15 │ 1970-01-01 │ │ │ 1970-01-16 │ 1970-01-01 │ │ │ 1970-01-17 │ 1970-01-01 │ │ │ 1970-01-18 │ 1970-01-01 │ │ │ 1970-01-19 │ 1970-01-01 │ │ │ 1970-01-20 │ 1970-01-01 │ │ │ 1970-01-21 │ 1970-01-01 │ │ │ 1970-01-22 │ 1970-01-01 │ │ │ 1970-01-23 │ 1970-01-01 │ │ │ 1970-01-24 │ 1970-01-01 │ │ │ 1970-01-25 │ 1970-01-01 │ │ │ 1970-01-26 │ 1970-01-01 │ │ │ 1970-01-27 │ 1970-01-01 │ │ │ 1970-01-28 │ 1970-01-01 │ │ │ 1970-01-29 │ 1970-01-01 │ │ │ 1970-01-30 │ 1970-01-01 │ │ │ 1970-01-31 │ 1970-01-01 │ │ │ 1970-02-01 │ 1970-01-01 │ │ │ 1970-02-02 │ 1970-01-01 │ │ │ 1970-02-03 │ 1970-01-01 │ │ │ 1970-02-04 │ 1970-01-01 │ │ │ 1970-02-05 │ 1970-01-01 │ │ │ 1970-02-06 │ 1970-01-01 │ │ │ 1970-02-07 │ 1970-01-01 │ │ │ 1970-02-08 │ 1970-01-01 │ │ │ 1970-02-09 │ 1970-01-01 │ │ │ 1970-02-10 │ 1970-01-05 │ original │ │ 1970-02-11 │ 1970-01-01 │ │ │ 1970-02-12 │ 1970-01-01 │ │ │ 1970-02-13 │ 1970-01-01 │ │ │ 1970-02-14 │ 1970-01-01 │ │ │ 1970-02-15 │ 1970-01-01 │ │ │ 1970-02-16 │ 1970-01-01 │ │ │ 1970-02-17 │ 1970-01-01 │ │ │ 1970-02-18 │ 1970-01-01 │ │ │ 1970-02-19 │ 1970-01-01 │ │ │ 1970-02-20 │ 1970-01-01 │ │ │ 1970-02-21 │ 1970-01-01 │ │ │ 1970-02-22 │ 1970-01-01 │ │ │ 1970-02-23 │ 1970-01-01 │ │ │ 1970-02-24 │ 1970-01-01 │ │ │ 1970-02-25 │ 1970-01-01 │ │ │ 1970-02-26 │ 1970-01-01 │ │ │ 1970-02-27 │ 1970-01-01 │ │ │ 1970-02-28 │ 1970-01-01 │ │ │ 1970-03-01 │ 1970-01-01 │ │ │ 1970-03-02 │ 1970-01-01 │ │ │ 1970-03-03 │ 1970-01-01 │ │ │ 1970-03-04 │ 1970-01-01 │ │ │ 1970-03-05 │ 1970-01-01 │ │ │ 1970-03-06 │ 1970-01-01 │ │ │ 1970-03-07 │ 1970-01-01 │ │ │ 1970-03-08 │ 1970-01-01 │ │ │ 1970-03-09 │ 1970-01-01 │ │ │ 1970-03-10 │ 1970-01-01 │ │ │ 1970-03-11 │ 1970-01-01 │ │ │ 1970-03-12 │ 1970-01-08 │ original │ └────────────┴────────────┴──────────┘  Original article "},{"title":"odbc","type":0,"sectionRef":"#","url":"docs/en/sql-reference/table-functions/odbc","content":"","keywords":""},{"title":"Usage Example​","type":1,"pageTitle":"odbc","url":"docs/en/sql-reference/table-functions/odbc#usage-example","content":"Getting data from the local MySQL installation via ODBC This example is checked for Ubuntu Linux 18.04 and MySQL server 5.7. Ensure that unixODBC and MySQL Connector are installed. By default (if installed from packages), ClickHouse starts as user clickhouse. Thus you need to create and configure this user in the MySQL server. $ sudo mysql  mysql&gt; CREATE USER 'clickhouse'@'localhost' IDENTIFIED BY 'clickhouse'; mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'clickhouse'@'clickhouse' WITH GRANT OPTION;  Then configure the connection in /etc/odbc.ini. $ cat /etc/odbc.ini [mysqlconn] DRIVER = /usr/local/lib/libmyodbc5w.so SERVER = 127.0.0.1 PORT = 3306 DATABASE = test USERNAME = clickhouse PASSWORD = clickhouse  You can check the connection using the isql utility from the unixODBC installation. $ isql -v mysqlconn +-------------------------+ | Connected! | | | ...  Table in MySQL: mysql&gt; CREATE TABLE `test`.`test` ( -&gt; `int_id` INT NOT NULL AUTO_INCREMENT, -&gt; `int_nullable` INT NULL DEFAULT NULL, -&gt; `float` FLOAT NOT NULL, -&gt; `float_nullable` FLOAT NULL DEFAULT NULL, -&gt; PRIMARY KEY (`int_id`)); Query OK, 0 rows affected (0,09 sec) mysql&gt; insert into test (`int_id`, `float`) VALUES (1,2); Query OK, 1 row affected (0,00 sec) mysql&gt; select * from test; +------+----------+-----+----------+ | int_id | int_nullable | float | float_nullable | +------+----------+-----+----------+ | 1 | NULL | 2 | NULL | +------+----------+-----+----------+ 1 row in set (0,00 sec)  Retrieving data from the MySQL table in ClickHouse: SELECT * FROM odbc('DSN=mysqlconn', 'test', 'test')  ┌─int_id─┬─int_nullable─┬─float─┬─float_nullable─┐ │ 1 │ 0 │ 2 │ 0 │ └────────┴──────────────┴───────┴────────────────┘  "},{"title":"See Also​","type":1,"pageTitle":"odbc","url":"docs/en/sql-reference/table-functions/odbc#see-also","content":"ODBC external dictionariesODBC table engine. "},{"title":"input","type":0,"sectionRef":"#","url":"docs/en/sql-reference/table-functions/input","content":"input input(structure) - table function that allows effectively convert and insert data sent to the server with given structure to the table with another structure. structure - structure of data sent to the server in following format 'column1_name column1_type, column2_name column2_type, ...'. For example, 'id UInt32, name String'. This function can be used only in INSERT SELECT query and only once but otherwise behaves like ordinary table function (for example, it can be used in subquery, etc.). Data can be sent in any way like for ordinary INSERT query and passed in any available formatthat must be specified in the end of query (unlike ordinary INSERT SELECT). The main feature of this function is that when server receives data from client it simultaneously converts it according to the list of expressions in the SELECT clause and inserts into the target table. Temporary table with all transferred data is not created. Examples Let the test table has the following structure (a String, b String)and data in data.csv has a different structure (col1 String, col2 Date, col3 Int32). Query for insert data from the data.csv into the test table with simultaneous conversion looks like this: $ cat data.csv | clickhouse-client --query=&quot;INSERT INTO test SELECT lower(col1), col3 * col3 FROM input('col1 String, col2 Date, col3 Int32') FORMAT CSV&quot;; If data.csv contains data of the same structure test_structure as the table test then these two queries are equal: $ cat data.csv | clickhouse-client --query=&quot;INSERT INTO test FORMAT CSV&quot; $ cat data.csv | clickhouse-client --query=&quot;INSERT INTO test SELECT * FROM input('test_structure') FORMAT CSV&quot; ","keywords":""},{"title":"remote, remoteSecure","type":0,"sectionRef":"#","url":"docs/en/sql-reference/table-functions/remote","content":"","keywords":""},{"title":"Globs in Addresses {globs-in-addresses}​","type":1,"pageTitle":"remote, remoteSecure","url":"docs/en/sql-reference/table-functions/remote#globs-in-addresses-globs-in-addresses","content":"Patterns in curly brackets { } are used to generate a set of shards and to specify replicas. If there are multiple pairs of curly brackets, then the direct product of the corresponding sets is generated. The following pattern types are supported. {a,b} - Any number of variants separated by a comma. The pattern is replaced with a in the first shard address and it is replaced with b in the second shard address and so on. For instance, example0{1,2}-1 generates addresses example01-1 and example02-1.{n..m} - A range of numbers. This pattern generates shard addresses with incrementing indices from n to m. example0{1..2}-1 generates example01-1 and example02-1.{0n..0m} - A range of numbers with leading zeroes. This modification preserves leading zeroes in indices. The pattern example{01..03}-1 generates example01-1, example02-1 and example03-1.{a|b} - Any number of variants separated by a |. The pattern specifies replicas. For instance, example01-{1|2} generates replicas example01-1 and example01-2. The query will be sent to the first healthy replica. However, for remote the replicas are iterated in the order currently set in the load_balancing setting. The number of generated addresses is limited by table_function_remote_max_addresses setting. "},{"title":"url","type":0,"sectionRef":"#","url":"docs/en/sql-reference/table-functions/url","content":"","keywords":""},{"title":"Globs in URL {globs-in-url}​","type":1,"pageTitle":"url","url":"docs/en/sql-reference/table-functions/url#globs-in-url-globs-in-url","content":"Patterns in curly brackets { } are used to generate a set of shards or to specify failover addresses. Supported pattern types and examples see in the description of the remote function. Character | inside patterns is used to specify failover addresses. They are iterated in the same order as listed in the pattern. The number of generated addresses is limited by glob_expansion_max_elements setting. "},{"title":"postgresql","type":0,"sectionRef":"#","url":"docs/en/sql-reference/table-functions/postgresql","content":"","keywords":""},{"title":"Implementation Details​","type":1,"pageTitle":"postgresql","url":"docs/en/sql-reference/table-functions/postgresql#implementation-details","content":"SELECT queries on PostgreSQL side run as COPY (SELECT ...) TO STDOUT inside read-only PostgreSQL transaction with commit after each SELECT query. Simple WHERE clauses such as =, !=, &gt;, &gt;=, &lt;, &lt;=, and IN are executed on the PostgreSQL server. All joins, aggregations, sorting, IN [ array ] conditions and the LIMIT sampling constraint are executed in ClickHouse only after the query to PostgreSQL finishes. INSERT queries on PostgreSQL side run as COPY &quot;table_name&quot; (field1, field2, ... fieldN) FROM STDIN inside PostgreSQL transaction with auto-commit after each INSERT statement. PostgreSQL Array types converts into ClickHouse arrays. note Be careful, in PostgreSQL an array data type column like Integer[] may contain arrays of different dimensions in different rows, but in ClickHouse it is only allowed to have multidimensional arrays of the same dimension in all rows. Supports multiple replicas that must be listed by |. For example: SELECT name FROM postgresql(`postgres{1|2|3}:5432`, 'postgres_database', 'postgres_table', 'user', 'password');  or SELECT name FROM postgresql(`postgres1:5431|postgres2:5432`, 'postgres_database', 'postgres_table', 'user', 'password');  Supports replicas priority for PostgreSQL dictionary source. The bigger the number in map, the less the priority. The highest priority is 0. Examples Table in PostgreSQL: postgres=# CREATE TABLE &quot;public&quot;.&quot;test&quot; ( &quot;int_id&quot; SERIAL, &quot;int_nullable&quot; INT NULL DEFAULT NULL, &quot;float&quot; FLOAT NOT NULL, &quot;str&quot; VARCHAR(100) NOT NULL DEFAULT '', &quot;float_nullable&quot; FLOAT NULL DEFAULT NULL, PRIMARY KEY (int_id)); CREATE TABLE postgres=# INSERT INTO test (int_id, str, &quot;float&quot;) VALUES (1,'test',2); INSERT 0 1 postgresql&gt; SELECT * FROM test; int_id | int_nullable | float | str | float_nullable --------+--------------+-------+------+---------------- 1 | | 2 | test | (1 row)  Selecting data from ClickHouse: SELECT * FROM postgresql('localhost:5432', 'test', 'test', 'postgresql_user', 'password') WHERE str IN ('test');  ┌─int_id─┬─int_nullable─┬─float─┬─str──┬─float_nullable─┐ │ 1 │ ᴺᵁᴸᴸ │ 2 │ test │ ᴺᵁᴸᴸ │ └────────┴──────────────┴───────┴──────┴────────────────┘  Inserting: INSERT INTO TABLE FUNCTION postgresql('localhost:5432', 'test', 'test', 'postgrsql_user', 'password') (int_id, float) VALUES (2, 3); SELECT * FROM postgresql('localhost:5432', 'test', 'test', 'postgresql_user', 'password');  ┌─int_id─┬─int_nullable─┬─float─┬─str──┬─float_nullable─┐ │ 1 │ ᴺᵁᴸᴸ │ 2 │ test │ ᴺᵁᴸᴸ │ │ 2 │ ᴺᵁᴸᴸ │ 3 │ │ ᴺᵁᴸᴸ │ └────────┴──────────────┴───────┴──────┴────────────────┘  Using Non-default Schema: postgres=# CREATE SCHEMA &quot;nice.schema&quot;; postgres=# CREATE TABLE &quot;nice.schema&quot;.&quot;nice.table&quot; (a integer); postgres=# INSERT INTO &quot;nice.schema&quot;.&quot;nice.table&quot; SELECT i FROM generate_series(0, 99) as t(i)  CREATE TABLE pg_table_schema_with_dots (a UInt32) ENGINE PostgreSQL('localhost:5432', 'clickhouse', 'nice.table', 'postgrsql_user', 'password', 'nice.schema');  See Also The PostgreSQL table engineUsing PostgreSQL as a source of external dictionary Original article "},{"title":"s3Cluster Table Function","type":0,"sectionRef":"#","url":"docs/en/sql-reference/table-functions/s3Cluster","content":"s3Cluster Table Function Allows processing files from Amazon S3 in parallel from many nodes in a specified cluster. On initiator it creates a connection to all nodes in the cluster, discloses asterics in S3 file path, and dispatches each file dynamically. On the worker node it asks the initiator about the next task to process and processes it. This is repeated until all tasks are finished. Syntax s3Cluster(cluster_name, source, [access_key_id, secret_access_key,] format, structure) Arguments cluster_name — Name of a cluster that is used to build a set of addresses and connection parameters to remote and local servers.source — URL to a file or a bunch of files. Supports following wildcards in readonly mode: *, ?, {'abc','def'} and {N..M} where N, M — numbers, abc, def — strings. For more information see Wildcards In Path.access_key_id and secret_access_key — Keys that specify credentials to use with given endpoint. Optional.format — The format of the file.structure — Structure of the table. Format 'column1_name column1_type, column2_name column2_type, ...'. Returned value A table with the specified structure for reading or writing data in the specified file. Examples Select the data from all files in the cluster cluster_simple: SELECT * FROM s3Cluster('cluster_simple', 'http://minio1:9001/root/data/{clickhouse,database}/*', 'minio', 'minio123', 'CSV', 'name String, value UInt32, polygon Array(Array(Tuple(Float64, Float64)))') ORDER BY (name, value, polygon); Count the total amount of rows in all files in the cluster cluster_simple: SELECT count(*) FROM s3Cluster('cluster_simple', 'http://minio1:9001/root/data/{clickhouse,database}/*', 'minio', 'minio123', 'CSV', 'name String, value UInt32, polygon Array(Array(Tuple(Float64, Float64)))'); warning If your listing of files contains number ranges with leading zeros, use the construction with braces for each digit separately or use ?. See Also S3 engines3 table function","keywords":""},{"title":"Window Functions","type":0,"sectionRef":"#","url":"docs/en/sql-reference/window-functions/","content":"","keywords":""},{"title":"References​","type":1,"pageTitle":"Window Functions","url":"docs/en/sql-reference/window-functions/#references","content":""},{"title":"GitHub Issues​","type":1,"pageTitle":"Window Functions","url":"docs/en/sql-reference/window-functions/#github-issues","content":"The roadmap for the initial support of window functions is in this issue. All GitHub issues related to window funtions have the comp-window-functions tag. "},{"title":"Tests​","type":1,"pageTitle":"Window Functions","url":"docs/en/sql-reference/window-functions/#tests","content":"These tests contain the examples of the currently supported grammar: https://github.com/ClickHouse/ClickHouse/blob/master/tests/performance/window_functions.xml https://github.com/ClickHouse/ClickHouse/blob/master/tests/queries/0_stateless/01591_window_functions.sql "},{"title":"Postgres Docs​","type":1,"pageTitle":"Window Functions","url":"docs/en/sql-reference/window-functions/#postgres-docs","content":"https://www.postgresql.org/docs/current/sql-select.html#SQL-WINDOW https://www.postgresql.org/docs/devel/sql-expressions.html#SYNTAX-WINDOW-FUNCTIONS https://www.postgresql.org/docs/devel/functions-window.html https://www.postgresql.org/docs/devel/tutorial-window.html "},{"title":"MySQL Docs​","type":1,"pageTitle":"Window Functions","url":"docs/en/sql-reference/window-functions/#mysql-docs","content":"https://dev.mysql.com/doc/refman/8.0/en/window-function-descriptions.html https://dev.mysql.com/doc/refman/8.0/en/window-functions-usage.html https://dev.mysql.com/doc/refman/8.0/en/window-functions-frames.html "},{"title":"view","type":0,"sectionRef":"#","url":"docs/en/sql-reference/table-functions/view","content":"","keywords":""},{"title":"view​","type":1,"pageTitle":"view","url":"docs/en/sql-reference/table-functions/view#view","content":"Turns a subquery into a table. The function implements views (see CREATE VIEW). The resulting table does not store data, but only stores the specified SELECT query. When reading from the table, ClickHouse executes the query and deletes all unnecessary columns from the result. Syntax view(subquery)  Arguments subquery — SELECT query. Returned value A table. Example Input table: ┌─id─┬─name─────┬─days─┐ │ 1 │ January │ 31 │ │ 2 │ February │ 29 │ │ 3 │ March │ 31 │ │ 4 │ April │ 30 │ └────┴──────────┴──────┘  Query: SELECT * FROM view(SELECT name FROM months);  Result: ┌─name─────┐ │ January │ │ February │ │ March │ │ April │ └──────────┘  You can use the view function as a parameter of the remote and cluster table functions: SELECT * FROM remote(`127.0.0.1`, view(SELECT a, b, c FROM table_name));  SELECT * FROM cluster(`cluster_name`, view(SELECT a, b, c FROM table_name));  See Also View Table Engine Original article "},{"title":"dictionary","type":0,"sectionRef":"#","url":"docs/en/sql-reference/table-functions/dictionary","content":"dictionary Displays the dictionary data as a ClickHouse table. Works the same way as Dictionary engine. Syntax dictionary('dict') Arguments dict — A dictionary name. String. Returned value A ClickHouse table. Example Input table dictionary_source_table: ┌─id─┬─value─┐ │ 0 │ 0 │ │ 1 │ 1 │ └────┴───────┘ Create a dictionary: CREATE DICTIONARY new_dictionary(id UInt64, value UInt64 DEFAULT 0) PRIMARY KEY id SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'dictionary_source_table')) LAYOUT(DIRECT()); Query: SELECT * FROM dictionary('new_dictionary'); Result: ┌─id─┬─value─┐ │ 0 │ 0 │ │ 1 │ 1 │ └────┴───────┘ See Also Dictionary engine","keywords":""},{"title":"s3 Table Function","type":0,"sectionRef":"#","url":"docs/en/sql-reference/table-functions/s3","content":"","keywords":""},{"title":"Usage​","type":1,"pageTitle":"s3 Table Function","url":"docs/en/sql-reference/table-functions/s3#usage-examples","content":"Suppose that we have several files with following URIs on S3: 'https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/some_prefix/some_file_1.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/some_prefix/some_file_2.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/some_prefix/some_file_3.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/some_prefix/some_file_4.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/another_prefix/some_file_1.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/another_prefix/some_file_2.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/another_prefix/some_file_3.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/another_prefix/some_file_4.csv' Count the amount of rows in files ending with numbers from 1 to 3: SELECT count(*) FROM s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/{some,another}_prefix/some_file_{1..3}.csv', 'CSV', 'name String, value UInt32')  ┌─count()─┐ │ 18 │ └─────────┘  Count the total amount of rows in all files in these two directories: SELECT count(*) FROM s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/{some,another}_prefix/*', 'CSV', 'name String, value UInt32')  ┌─count()─┐ │ 24 │ └─────────┘  warning If your listing of files contains number ranges with leading zeros, use the construction with braces for each digit separately or use ?. Count the total amount of rows in files named file-000.csv, file-001.csv, … , file-999.csv: SELECT count(*) FROM s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/big_prefix/file-{000..999}.csv', 'CSV', 'name String, value UInt32');  ┌─count()─┐ │ 12 │ └─────────┘  Insert data into file test-data.csv.gz: INSERT INTO FUNCTION s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'name String, value UInt32', 'gzip') VALUES ('test-data', 1), ('test-data-2', 2);  Insert data into file test-data.csv.gz from existing table: INSERT INTO FUNCTION s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'name String, value UInt32', 'gzip') SELECT name, value FROM existing_table;  "},{"title":"Partitioned Write​","type":1,"pageTitle":"s3 Table Function","url":"docs/en/sql-reference/table-functions/s3#partitioned-write","content":"If you specify PARTITION BY expression when inserting data into S3 table, a separate file is created for each partition value. Splitting the data into separate files helps to improve reading operations efficiency. Examples Using partition ID in a key creates separate files: INSERT INTO TABLE FUNCTION s3('http://bucket.amazonaws.com/my_bucket/file_{_partition_id}.csv', 'CSV', 'a String, b UInt32, c UInt32') PARTITION BY a VALUES ('x', 2, 3), ('x', 4, 5), ('y', 11, 12), ('y', 13, 14), ('z', 21, 22), ('z', 23, 24);  As a result, the data is written into three files: file_x.csv, file_y.csv, and file_z.csv. Using partition ID in a bucket name creates files in different buckets: INSERT INTO TABLE FUNCTION s3('http://bucket.amazonaws.com/my_bucket_{_partition_id}/file.csv', 'CSV', 'a UInt32, b UInt32, c UInt32') PARTITION BY a VALUES (1, 2, 3), (1, 4, 5), (10, 11, 12), (10, 13, 14), (20, 21, 22), (20, 23, 24);  As a result, the data is written into three files in different buckets: my_bucket_1/file.csv, my_bucket_10/file.csv, and my_bucket_20/file.csv. See Also S3 engine Original article "},{"title":"2017","type":0,"sectionRef":"#","url":"docs/en/whats-new/changelog/2017","content":"","keywords":""},{"title":"ClickHouse Release 1.1.54327, 2017-12-21​","type":1,"pageTitle":"2017","url":"docs/en/whats-new/changelog/2017#clickhouse-release-1-1-54327-2017-12-21","content":"This release contains bug fixes for the previous release 1.1.54318: Fixed bug with possible race condition in replication that could lead to data loss. This issue affects versions 1.1.54310 and 1.1.54318. If you use one of these versions with Replicated tables, the update is strongly recommended. This issue shows in logs in Warning messages like Part ... from own log does not exist. The issue is relevant even if you do not see these messages in logs. "},{"title":"ClickHouse Release 1.1.54318, 2017-11-30​","type":1,"pageTitle":"2017","url":"docs/en/whats-new/changelog/2017#clickhouse-release-1-1-54318-2017-11-30","content":"This release contains bug fixes for the previous release 1.1.54310: Fixed incorrect row deletions during merges in the SummingMergeTree engineFixed a memory leak in unreplicated MergeTree enginesFixed performance degradation with frequent inserts in MergeTree enginesFixed an issue that was causing the replication queue to stop runningFixed rotation and archiving of server logs "},{"title":"ClickHouse Release 1.1.54310, 2017-11-01​","type":1,"pageTitle":"2017","url":"docs/en/whats-new/changelog/2017#clickhouse-release-1-1-54310-2017-11-01","content":"New Features:​ Custom partitioning key for the MergeTree family of table engines.Kafka table engine.Added support for loading CatBoost models and applying them to data stored in ClickHouse.Added support for time zones with non-integer offsets from UTC.Added support for arithmetic operations with time intervals.The range of values for the Date and DateTime types is extended to the year 2105.Added the CREATE MATERIALIZED VIEW x TO y query (specifies an existing table for storing the data of a materialized view).Added the ATTACH TABLE query without arguments.The processing logic for Nested columns with names ending in -Map in a SummingMergeTree table was extracted to the sumMap aggregate function. You can now specify such columns explicitly.Max size of the IP trie dictionary is increased to 128M entries.Added the getSizeOfEnumType function.Added the sumWithOverflow aggregate function.Added support for the Cap’n Proto input format.You can now customize compression level when using the zstd algorithm. Backward Incompatible Changes:​ Creation of temporary tables with an engine other than Memory is not allowed.Explicit creation of tables with the View or MaterializedView engine is not allowed.During table creation, a new check verifies that the sampling key expression is included in the primary key. Bug Fixes:​ Fixed hangups when synchronously inserting into a Distributed table.Fixed nonatomic adding and removing of parts in Replicated tables.Data inserted into a materialized view is not subjected to unnecessary deduplication.Executing a query to a Distributed table for which the local replica is lagging and remote replicas are unavailable does not result in an error anymore.Users do not need access permissions to the default database to create temporary tables anymore.Fixed crashing when specifying the Array type without arguments.Fixed hangups when the disk volume containing server logs is full.Fixed an overflow in the toRelativeWeekNum function for the first week of the Unix epoch. Build Improvements:​ Several third-party libraries (notably Poco) were updated and converted to git submodules. "},{"title":"ClickHouse Release 1.1.54304, 2017-10-19​","type":1,"pageTitle":"2017","url":"docs/en/whats-new/changelog/2017#clickhouse-release-1-1-54304-2017-10-19","content":"New Features:​ TLS support in the native protocol (to enable, set tcp_ssl_port in config.xml ). Bug Fixes:​ ALTER for replicated tables now tries to start running as soon as possible.Fixed crashing when reading data with the setting preferred_block_size_bytes=0.Fixed crashes of clickhouse-client when pressing Page DownCorrect interpretation of certain complex queries with GLOBAL IN and UNION ALLFREEZE PARTITION always works atomically now.Empty POST requests now return a response with code 411.Fixed interpretation errors for expressions like CAST(1 AS Nullable(UInt8)).Fixed an error when reading Array(Nullable(String)) columns from MergeTree tables.Fixed crashing when parsing queries like SELECT dummy AS dummy, dummy AS bUsers are updated correctly with invalid users.xmlCorrect handling when an executable dictionary returns a non-zero response code. "},{"title":"ClickHouse Release 1.1.54292, 2017-09-20​","type":1,"pageTitle":"2017","url":"docs/en/whats-new/changelog/2017#clickhouse-release-1-1-54292-2017-09-20","content":"New Features:​ Added the pointInPolygon function for working with coordinates on a coordinate plane.Added the sumMap aggregate function for calculating the sum of arrays, similar to SummingMergeTree.Added the trunc function. Improved performance of the rounding functions (round, floor, ceil, roundToExp2) and corrected the logic of how they work. Changed the logic of the roundToExp2 function for fractions and negative numbers.The ClickHouse executable file is now less dependent on the libc version. The same ClickHouse executable file can run on a wide variety of Linux systems. There is still a dependency when using compiled queries (with the setting compile = 1 , which is not used by default).Reduced the time needed for dynamic compilation of queries. Bug Fixes:​ Fixed an error that sometimes produced part ... intersects previous part messages and weakened replica consistency.Fixed an error that caused the server to lock up if ZooKeeper was unavailable during shutdown.Removed excessive logging when restoring replicas.Fixed an error in the UNION ALL implementation.Fixed an error in the concat function that occurred if the first column in a block has the Array type.Progress is now displayed correctly in the system.merges table. "},{"title":"ClickHouse Release 1.1.54289, 2017-09-13​","type":1,"pageTitle":"2017","url":"docs/en/whats-new/changelog/2017#clickhouse-release-1-1-54289-2017-09-13","content":"New Features:​ SYSTEM queries for server administration: SYSTEM RELOAD DICTIONARY, SYSTEM RELOAD DICTIONARIES, SYSTEM DROP DNS CACHE, SYSTEM SHUTDOWN, SYSTEM KILL.Added functions for working with arrays: concat, arraySlice, arrayPushBack, arrayPushFront, arrayPopBack, arrayPopFront.Added root and identity parameters for the ZooKeeper configuration. This allows you to isolate individual users on the same ZooKeeper cluster.Added aggregate functions groupBitAnd, groupBitOr, and groupBitXor (for compatibility, they are also available under the names BIT_AND, BIT_OR, and BIT_XOR).External dictionaries can be loaded from MySQL by specifying a socket in the filesystem.External dictionaries can be loaded from MySQL over SSL (ssl_cert, ssl_key, ssl_ca parameters).Added the max_network_bandwidth_for_user setting to restrict the overall bandwidth use for queries per user.Support for DROP TABLE for temporary tables.Support for reading DateTime values in Unix timestamp format from the CSV and JSONEachRow formats.Lagging replicas in distributed queries are now excluded by default (the default threshold is 5 minutes).FIFO locking is used during ALTER: an ALTER query isn’t blocked indefinitely for continuously running queries.Option to set umask in the config file.Improved performance for queries with DISTINCT . Bug Fixes:​ Improved the process for deleting old nodes in ZooKeeper. Previously, old nodes sometimes didn’t get deleted if there were very frequent inserts, which caused the server to be slow to shut down, among other things.Fixed randomization when choosing hosts for the connection to ZooKeeper.Fixed the exclusion of lagging replicas in distributed queries if the replica is localhost.Fixed an error where a data part in a ReplicatedMergeTree table could be broken after running ALTER MODIFY on an element in a Nested structure.Fixed an error that could cause SELECT queries to “hang”.Improvements to distributed DDL queries.Fixed the query CREATE TABLE ... AS &lt;materialized view&gt;.Resolved the deadlock in the ALTER ... CLEAR COLUMN IN PARTITION query for Buffer tables.Fixed the invalid default value for Enum s (0 instead of the minimum) when using the JSONEachRow and TSKV formats.Resolved the appearance of zombie processes when using a dictionary with an executable source.Fixed segfault for the HEAD query. Improved Workflow for Developing and Assembling ClickHouse:​ You can use pbuilder to build ClickHouse.You can use libc++ instead of libstdc++ for builds on Linux.Added instructions for using static code analysis tools: Coverage, clang-tidy, cppcheck. Please Note When Upgrading:​ There is now a higher default value for the MergeTree setting max_bytes_to_merge_at_max_space_in_pool (the maximum total size of data parts to merge, in bytes): it has increased from 100 GiB to 150 GiB. This might result in large merges running after the server upgrade, which could cause an increased load on the disk subsystem. If the free space available on the server is less than twice the total amount of the merges that are running, this will cause all other merges to stop running, including merges of small data parts. As a result, INSERT queries will fail with the message “Merges are processing significantly slower than inserts.” Use the SELECT * FROM system.merges query to monitor the situation. You can also check the DiskSpaceReservedForMerge metric in the system.metrics table, or in Graphite. You do not need to do anything to fix this, since the issue will resolve itself once the large merges finish. If you find this unacceptable, you can restore the previous value for the max_bytes_to_merge_at_max_space_in_pool setting. To do this, go to the &lt;merge_tree&gt; section in config.xml, set &lt;merge_tree&gt;``&lt;max_bytes_to_merge_at_max_space_in_pool&gt;107374182400&lt;/max_bytes_to_merge_at_max_space_in_pool&gt; and restart the server. "},{"title":"ClickHouse Release 1.1.54284, 2017-08-29​","type":1,"pageTitle":"2017","url":"docs/en/whats-new/changelog/2017#clickhouse-release-1-1-54284-2017-08-29","content":"This is a bugfix release for the previous 1.1.54282 release. It fixes leaks in the parts directory in ZooKeeper. "},{"title":"ClickHouse Release 1.1.54282, 2017-08-23​","type":1,"pageTitle":"2017","url":"docs/en/whats-new/changelog/2017#clickhouse-release-1-1-54282-2017-08-23","content":"This release contains bug fixes for the previous release 1.1.54276: Fixed DB::Exception: Assertion violation: !_path.empty() when inserting into a Distributed table.Fixed parsing when inserting in RowBinary format if input data starts with’;’.Errors during runtime compilation of certain aggregate functions (e.g. groupArray()). "},{"title":"ClickHouse Release 1.1.54276, 2017-08-16​","type":1,"pageTitle":"2017","url":"docs/en/whats-new/changelog/2017#clickhouse-release-1-1-54276-2017-08-16","content":"New Features:​ Added an optional WITH section for a SELECT query. Example query: WITH 1+1 AS a SELECT a, a*aINSERT can be performed synchronously in a Distributed table: OK is returned only after all the data is saved on all the shards. This is activated by the setting insert_distributed_sync=1.Added the UUID data type for working with 16-byte identifiers.Added aliases of CHAR, FLOAT and other types for compatibility with the Tableau.Added the functions toYYYYMM, toYYYYMMDD, and toYYYYMMDDhhmmss for converting time into numbers.You can use IP addresses (together with the hostname) to identify servers for clustered DDL queries.Added support for non-constant arguments and negative offsets in the function substring(str, pos, len).Added the max_size parameter for the groupArray(max_size)(column) aggregate function, and optimized its performance. Main Changes:​ Security improvements: all server files are created with 0640 permissions (can be changed via &lt;umask&gt; config parameter).Improved error messages for queries with invalid syntax.Significantly reduced memory consumption and improved performance when merging large sections of MergeTree data.Significantly increased the performance of data merges for the ReplacingMergeTree engine.Improved performance for asynchronous inserts from a Distributed table by combining multiple source inserts. To enable this functionality, use the setting distributed_directory_monitor_batch_inserts=1. Backward Incompatible Changes:​ Changed the binary format of aggregate states of groupArray(array_column) functions for arrays. Complete List of Changes:​ Added the output_format_json_quote_denormals setting, which enables outputting nan and inf values in JSON format.Optimized stream allocation when reading from a Distributed table.Settings can be configured in readonly mode if the value does not change.Added the ability to retrieve non-integer granules of the MergeTree engine in order to meet restrictions on the block size specified in the preferred_block_size_bytes setting. The purpose is to reduce the consumption of RAM and increase cache locality when processing queries from tables with large columns.Efficient use of indexes that contain expressions like toStartOfHour(x) for conditions like toStartOfHour(x) op сonstexpr.Added new settings for MergeTree engines (the merge_tree section in config.xml): replicated_deduplication_window_seconds sets the number of seconds allowed for deduplicating inserts in Replicated tables.cleanup_delay_period sets how often to start cleanup to remove outdated data.replicated_can_become_leader can prevent a replica from becoming the leader (and assigning merges). Accelerated cleanup to remove outdated data from ZooKeeper.Multiple improvements and fixes for clustered DDL queries. Of particular interest is the new setting distributed_ddl_task_timeout, which limits the time to wait for a response from the servers in the cluster. If a ddl request has not been performed on all hosts, a response will contain a timeout error and a request will be executed in an async mode.Improved display of stack traces in the server logs.Added the “none” value for the compression method.You can use multiple dictionaries_config sections in config.xml.It is possible to connect to MySQL through a socket in the file system.The system.parts table has a new column with information about the size of marks, in bytes. Bug Fixes:​ Distributed tables using a Merge table now work correctly for a SELECT query with a condition on the _table field.Fixed a rare race condition in ReplicatedMergeTree when checking data parts.Fixed possible freezing on “leader election” when starting a server.The max_replica_delay_for_distributed_queries setting was ignored when using a local replica of the data source. This has been fixed.Fixed incorrect behavior of ALTER TABLE CLEAR COLUMN IN PARTITION when attempting to clean a non-existing column.Fixed an exception in the multiIf function when using empty arrays or strings.Fixed excessive memory allocations when deserializing Native format.Fixed incorrect auto-update of Trie dictionaries.Fixed an exception when running queries with a GROUP BY clause from a Merge table when using SAMPLE.Fixed a crash of GROUP BY when using distributed_aggregation_memory_efficient=1.Now you can specify the database.table in the right side of IN and JOIN.Too many threads were used for parallel aggregation. This has been fixed.Fixed how the “if” function works with FixedString arguments.SELECT worked incorrectly from a Distributed table for shards with a weight of 0. This has been fixed.Running CREATE VIEW IF EXISTS no longer causes crashes.Fixed incorrect behavior when input_format_skip_unknown_fields=1 is set and there are negative numbers.Fixed an infinite loop in the dictGetHierarchy() function if there is some invalid data in the dictionary.Fixed Syntax error: unexpected (...) errors when running distributed queries with subqueries in an IN or JOIN clause and Merge tables.Fixed an incorrect interpretation of a SELECT query from Dictionary tables.Fixed the “Cannot mremap” error when using arrays in IN and JOIN clauses with more than 2 billion elements.Fixed the failover for dictionaries with MySQL as the source. Improved Workflow for Developing and Assembling ClickHouse:​ Builds can be assembled in Arcadia.You can use gcc 7 to compile ClickHouse.Parallel builds using ccache+distcc are faster now. "},{"title":"ClickHouse Release 1.1.54245, 2017-07-04​","type":1,"pageTitle":"2017","url":"docs/en/whats-new/changelog/2017#clickhouse-release-1-1-54245-2017-07-04","content":"New Features:​ Distributed DDL (for example, CREATE TABLE ON CLUSTER)The replicated query ALTER TABLE CLEAR COLUMN IN PARTITION.The engine for Dictionary tables (access to dictionary data in the form of a table).Dictionary database engine (this type of database automatically has Dictionary tables available for all the connected external dictionaries).You can check for updates to the dictionary by sending a request to the source.Qualified column namesQuoting identifiers using double quotation marks.Sessions in the HTTP interface.The OPTIMIZE query for a Replicated table can can run not only on the leader. Backward Incompatible Changes:​ Removed SET GLOBAL. Minor Changes:​ Now after an alert is triggered, the log prints the full stack trace.Relaxed the verification of the number of damaged/extra data parts at startup (there were too many false positives). Bug Fixes:​ Fixed a bad connection “sticking” when inserting into a Distributed table.GLOBAL IN now works for a query from a Merge table that looks at a Distributed table.The incorrect number of cores was detected on a Google Compute Engine virtual machine. This has been fixed.Changes in how an executable source of cached external dictionaries works.Fixed the comparison of strings containing null characters.Fixed the comparison of Float32 primary key fields with constants.Previously, an incorrect estimate of the size of a field could lead to overly large allocations.Fixed a crash when querying a Nullable column added to a table using ALTER.Fixed a crash when sorting by a Nullable column, if the number of rows is less than LIMIT.Fixed an ORDER BY subquery consisting of only constant values.Previously, a Replicated table could remain in the invalid state after a failed DROP TABLE.Aliases for scalar subqueries with empty results are no longer lost.Now a query that used compilation does not fail with an error if the .so file gets damaged. "},{"title":"mysql","type":0,"sectionRef":"#","url":"docs/en/sql-reference/table-functions/mysql","content":"mysql Allows SELECT and INSERT queries to be performed on data that is stored on a remote MySQL server. Syntax mysql('host:port', 'database', 'table', 'user', 'password'[, replace_query, 'on_duplicate_clause']) Arguments host:port — MySQL server address. database — Remote database name. table — Remote table name. user — MySQL user. password — User password. replace_query — Flag that converts INSERT INTO queries to REPLACE INTO. Possible values: 0 - The query is executed as INSERT INTO.1 - The query is executed as REPLACE INTO. on_duplicate_clause — The ON DUPLICATE KEY on_duplicate_clause expression that is added to the INSERT query. Can be specified only with replace_query = 0 (if you simultaneously pass replace_query = 1 and on_duplicate_clause, ClickHouse generates an exception). Example: INSERT INTO t (c1,c2) VALUES ('a', 2) ON DUPLICATE KEY UPDATE c2 = c2 + 1; on_duplicate_clause here is UPDATE c2 = c2 + 1. See the MySQL documentation to find which on_duplicate_clause you can use with the ON DUPLICATE KEY clause. Simple WHERE clauses such as =, !=, &gt;, &gt;=, &lt;, &lt;= are currently executed on the MySQL server. The rest of the conditions and the LIMIT sampling constraint are executed in ClickHouse only after the query to MySQL finishes. Supports multiple replicas that must be listed by |. For example: SELECT name FROM mysql(`mysql{1|2|3}:3306`, 'mysql_database', 'mysql_table', 'user', 'password'); or SELECT name FROM mysql(`mysql1:3306|mysql2:3306|mysql3:3306`, 'mysql_database', 'mysql_table', 'user', 'password'); Returned Value A table object with the same columns as the original MySQL table. note In the INSERT query to distinguish table function mysql(...) from table name with column names list, you must use keywords FUNCTION or TABLE FUNCTION. See examples below. Examples Table in MySQL: mysql&gt; CREATE TABLE `test`.`test` ( -&gt; `int_id` INT NOT NULL AUTO_INCREMENT, -&gt; `float` FLOAT NOT NULL, -&gt; PRIMARY KEY (`int_id`)); mysql&gt; INSERT INTO test (`int_id`, `float`) VALUES (1,2); mysql&gt; SELECT * FROM test; +--------+-------+ | int_id | float | +--------+-------+ | 1 | 2 | +--------+-------+ Selecting data from ClickHouse: SELECT * FROM mysql('localhost:3306', 'test', 'test', 'bayonet', '123'); ┌─int_id─┬─float─┐ │ 1 │ 2 │ └────────┴───────┘ Replacing and inserting: INSERT INTO FUNCTION mysql('localhost:3306', 'test', 'test', 'bayonet', '123', 1) (int_id, float) VALUES (1, 3); INSERT INTO TABLE FUNCTION mysql('localhost:3306', 'test', 'test', 'bayonet', '123', 0, 'UPDATE int_id = int_id + 1') (int_id, float) VALUES (1, 4); SELECT * FROM mysql('localhost:3306', 'test', 'test', 'bayonet', '123'); ┌─int_id─┬─float─┐ │ 1 │ 3 │ │ 2 │ 4 │ └────────┴───────┘ See Also The ‘MySQL’ table engineUsing MySQL as a source of external dictionary Original article","keywords":""},{"title":"hdfs","type":0,"sectionRef":"#","url":"docs/en/sql-reference/table-functions/hdfs","content":"","keywords":""},{"title":"Virtual Columns​","type":1,"pageTitle":"hdfs","url":"docs/en/sql-reference/table-functions/hdfs#virtual-columns","content":"_path — Path to the file._file — Name of the file. See Also Virtual columns "},{"title":"What’s New in ClickHouse","type":0,"sectionRef":"#","url":"docs/en/whats-new/","content":"What’s New in ClickHouse There’s a short high-level roadmap and a detailed changelog for releases that have already been published.","keywords":"clickhouse what's new roadmap changelog"},{"title":"2018","type":0,"sectionRef":"#","url":"docs/en/whats-new/changelog/2018","content":"","keywords":""},{"title":"ClickHouse Release 18.16​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-16","content":""},{"title":"ClickHouse Release 18.16.1, 2018-12-21​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-16-1-2018-12-21","content":"Bug Fixes:​ Fixed an error that led to problems with updating dictionaries with the ODBC source. #3825, #3829JIT compilation of aggregate functions now works with LowCardinality columns. #3838 Improvements:​ Added the low_cardinality_allow_in_native_format setting (enabled by default). When disabled, LowCardinality columns will be converted to ordinary columns for SELECT queries and ordinary columns will be expected for INSERT queries. #3879 Build Improvements:​ Fixes for builds on macOS and ARM. "},{"title":"ClickHouse Release 18.16.0, 2018-12-14​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-16-0-2018-12-14","content":"New Features:​ DEFAULT expressions are evaluated for missing fields when loading data in semi-structured input formats (JSONEachRow, TSKV). The feature is enabled with the insert_sample_with_metadata setting. #3555The ALTER TABLE query now has the MODIFY ORDER BY action for changing the sorting key when adding or removing a table column. This is useful for tables in the MergeTree family that perform additional tasks when merging based on this sorting key, such as SummingMergeTree, AggregatingMergeTree, and so on. #3581 #3755For tables in the MergeTree family, now you can specify a different sorting key (ORDER BY) and index (PRIMARY KEY). The sorting key can be longer than the index. #3581Added the hdfs table function and the HDFS table engine for importing and exporting data to HDFS. chenxing-xcAdded functions for working with base64: base64Encode, base64Decode, tryBase64Decode. Alexander KrasheninnikovNow you can use a parameter to configure the precision of the uniqCombined aggregate function (select the number of HyperLogLog cells). #3406Added the system.contributors table that contains the names of everyone who made commits in ClickHouse. #3452Added the ability to omit the partition for the ALTER TABLE ... FREEZE query in order to back up all partitions at once. #3514Added dictGet and dictGetOrDefault functions that do not require specifying the type of return value. The type is determined automatically from the dictionary description. Amos BirdNow you can specify comments for a column in the table description and change it using ALTER. #3377Reading is supported for Join type tables with simple keys. Amos BirdNow you can specify the options join_use_nulls, max_rows_in_join, max_bytes_in_join, and join_overflow_mode when creating a Join type table. Amos BirdAdded the joinGet function that allows you to use a Join type table like a dictionary. Amos BirdAdded the partition_key, sorting_key, primary_key, and sampling_key columns to the system.tables table in order to provide information about table keys. #3609Added the is_in_partition_key, is_in_sorting_key, is_in_primary_key, and is_in_sampling_key columns to the system.columns table. #3609Added the min_time and max_time columns to the system.parts table. These columns are populated when the partitioning key is an expression consisting of DateTime columns. Emmanuel Donin de Rosière Bug Fixes:​ Fixes and performance improvements for the LowCardinality data type. GROUP BY using LowCardinality(Nullable(...)). Getting the values of extremes. Processing high-order functions. LEFT ARRAY JOIN. Distributed GROUP BY. Functions that return Array. Execution of ORDER BY. Writing to Distributed tables (nicelulu). Backward compatibility for INSERT queries from old clients that implement the Native protocol. Support for LowCardinality for JOIN. Improved performance when working in a single stream. #3823 #3803 #3799 #3769 #3744 #3681 #3651 #3649 #3641 #3632 #3568 #3523 #3518Fixed how the select_sequential_consistency option works. Previously, when this setting was enabled, an incomplete result was sometimes returned after beginning to write to a new partition. #2863Databases are correctly specified when executing DDL ON CLUSTER queries and ALTER UPDATE/DELETE. #3772 #3460Databases are correctly specified for subqueries inside a VIEW. #3521Fixed a bug in PREWHERE with FINAL for VersionedCollapsingMergeTree. 7167bfd7Now you can use KILL QUERY to cancel queries that have not started yet because they are waiting for the table to be locked. #3517Corrected date and time calculations if the clocks were moved back at midnight (this happens in Iran, and happened in Moscow from 1981 to 1983). Previously, this led to the time being reset a day earlier than necessary, and also caused incorrect formatting of the date and time in text format. #3819Fixed bugs in some cases of VIEW and subqueries that omit the database. Winter ZhangFixed a race condition when simultaneously reading from a MATERIALIZED VIEW and deleting a MATERIALIZED VIEW due to not locking the internal MATERIALIZED VIEW. #3404 #3694Fixed the error Lock handler cannot be nullptr. #3689Fixed query processing when the compile_expressions option is enabled (it’s enabled by default). Nondeterministic constant expressions like the now function are no longer unfolded. #3457Fixed a crash when specifying a non-constant scale argument in toDecimal32/64/128 functions.Fixed an error when trying to insert an array with NULL elements in the Values format into a column of type Array without Nullable (if input_format_values_interpret_expressions = 1). #3487 #3503Fixed continuous error logging in DDLWorker if ZooKeeper is not available. 8f50c620Fixed the return type for quantile* functions from Date and DateTime types of arguments. #3580Fixed the WITH clause if it specifies a simple alias without expressions. #3570Fixed processing of queries with named sub-queries and qualified column names when enable_optimize_predicate_expression is enabled. Winter ZhangFixed the error Attempt to attach to nullptr thread group when working with materialized views. Marek VavrušaFixed a crash when passing certain incorrect arguments to the arrayReverse function. 73e3a7b6Fixed the buffer overflow in the extractURLParameter function. Improved performance. Added correct processing of strings containing zero bytes. 141e9799Fixed buffer overflow in the lowerUTF8 and upperUTF8 functions. Removed the ability to execute these functions over FixedString type arguments. #3662Fixed a rare race condition when deleting MergeTree tables. #3680Fixed a race condition when reading from Buffer tables and simultaneously performing ALTER or DROP on the target tables. #3719Fixed a segfault if the max_temporary_non_const_columns limit was exceeded. #3788 Improvements:​ The server does not write the processed configuration files to the /etc/clickhouse-server/ directory. Instead, it saves them in the preprocessed_configs directory inside path. This means that the /etc/clickhouse-server/ directory does not have write access for the clickhouse user, which improves security. #2443The min_merge_bytes_to_use_direct_io option is set to 10 GiB by default. A merge that forms large parts of tables from the MergeTree family will be performed in O_DIRECT mode, which prevents excessive page cache eviction. #3504Accelerated server start when there is a very large number of tables. #3398Added a connection pool and HTTP Keep-Alive for connections between replicas. #3594If the query syntax is invalid, the 400 Bad Request code is returned in the HTTP interface (500 was returned previously). 31bc680aThe join_default_strictness option is set to ALL by default for compatibility. 120e2cbeRemoved logging to stderr from the re2 library for invalid or complex regular expressions. #3723Added for the Kafka table engine: checks for subscriptions before beginning to read from Kafka; the kafka_max_block_size setting for the table. Marek VavrušaThe cityHash64, farmHash64, metroHash64, sipHash64, halfMD5, murmurHash2_32, murmurHash2_64, murmurHash3_32, and murmurHash3_64 functions now work for any number of arguments and for arguments in the form of tuples. #3451 #3519The arrayReverse function now works with any types of arrays. 73e3a7b6Added an optional parameter: the slot size for the timeSlots function. Kirill ShvakovFor FULL and RIGHT JOIN, the max_block_size setting is used for a stream of non-joined data from the right table. Amos BirdAdded the --secure command line parameter in clickhouse-benchmark and clickhouse-performance-test to enable TLS. #3688 #3690Type conversion when the structure of a Buffer type table does not match the structure of the destination table. Vitaly BaranovAdded the tcp_keep_alive_timeout option to enable keep-alive packets after inactivity for the specified time interval. #3441Removed unnecessary quoting of values for the partition key in the system.parts table if it consists of a single column. #3652The modulo function works for Date and DateTime data types. #3385Added synonyms for the POWER, LN, LCASE, UCASE, REPLACE, LOCATE, SUBSTR, and MID functions. #3774 #3763 Some function names are case-insensitive for compatibility with the SQL standard. Added syntactic sugar SUBSTRING(expr FROM start FOR length) for compatibility with SQL. #3804Added the ability to mlock memory pages corresponding to clickhouse-server executable code to prevent it from being forced out of memory. This feature is disabled by default. #3553Improved performance when reading from O_DIRECT (with the min_bytes_to_use_direct_io option enabled). #3405Improved performance of the dictGet...OrDefault function for a constant key argument and a non-constant default argument. Amos BirdThe firstSignificantSubdomain function now processes the domains gov, mil, and edu. Igor Hatarist Improved performance. #3628Ability to specify custom environment variables for starting clickhouse-server using the SYS-V init.d script by defining CLICKHOUSE_PROGRAM_ENV in /etc/default/clickhouse.Pavlo BashynskyiCorrect return code for the clickhouse-server init script. #3516The system.metrics table now has the VersionInteger metric, and system.build_options has the added line VERSION_INTEGER, which contains the numeric form of the ClickHouse version, such as 18016000. #3644Removed the ability to compare the Date type with a number to avoid potential errors like date = 2018-12-17, where quotes around the date are omitted by mistake. #3687Fixed the behavior of stateful functions like rowNumberInAllBlocks. They previously output a result that was one number larger due to starting during query analysis. Amos BirdIf the force_restore_data file can’t be deleted, an error message is displayed. Amos Bird Build Improvements:​ Updated the jemalloc library, which fixes a potential memory leak. Amos BirdProfiling with jemalloc is enabled by default in order to debug builds. 2cc82f5cAdded the ability to run integration tests when only Docker is installed on the system. #3650Added the fuzz expression test in SELECT queries. #3442Added a stress test for commits, which performs functional tests in parallel and in random order to detect more race conditions. #3438Improved the method for starting clickhouse-server in a Docker image. Elghazal AhmedFor a Docker image, added support for initializing databases using files in the /docker-entrypoint-initdb.d directory. Konstantin LebedevFixes for builds on ARM. #3709 Backward Incompatible Changes:​ Removed the ability to compare the Date type with a number. Instead of toDate('2018-12-18') = 17883, you must use explicit type conversion = toDate(17883) #3687 "},{"title":"ClickHouse Release 18.14​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-14","content":""},{"title":"ClickHouse Release 18.14.19, 2018-12-19​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-14-19-2018-12-19","content":"Bug Fixes:​ Fixed an error that led to problems with updating dictionaries with the ODBC source. #3825, #3829Databases are correctly specified when executing DDL ON CLUSTER queries. #3460Fixed a segfault if the max_temporary_non_const_columns limit was exceeded. #3788 Build Improvements:​ Fixes for builds on ARM. "},{"title":"ClickHouse Release 18.14.18, 2018-12-04​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-14-18-2018-12-04","content":"Bug Fixes:​ Fixed error in dictGet... function for dictionaries of type range, if one of the arguments is constant and other is not. #3751Fixed error that caused messages netlink: '...': attribute type 1 has an invalid length to be printed in Linux kernel log, that was happening only on fresh enough versions of Linux kernel. #3749Fixed segfault in function empty for argument of FixedString type. Daniel, Dao Quang MinhFixed excessive memory allocation when using large value of max_query_size setting (a memory chunk of max_query_size bytes was preallocated at once). #3720 Build Changes:​ Fixed build with LLVM/Clang libraries of version 7 from the OS packages (these libraries are used for runtime query compilation). #3582 "},{"title":"ClickHouse Release 18.14.17, 2018-11-30​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-14-17-2018-11-30","content":"Bug Fixes:​ Fixed cases when the ODBC bridge process did not terminate with the main server process. #3642Fixed synchronous insertion into the Distributed table with a columns list that differs from the column list of the remote table. #3673Fixed a rare race condition that can lead to a crash when dropping a MergeTree table. #3643Fixed a query deadlock in case when query thread creation fails with the Resource temporarily unavailable error. #3643Fixed parsing of the ENGINE clause when the CREATE AS table syntax was used and the ENGINE clause was specified before the AS table (the error resulted in ignoring the specified engine). #3692 "},{"title":"ClickHouse Release 18.14.15, 2018-11-21​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-14-15-2018-11-21","content":"Bug Fixes:​ The size of memory chunk was overestimated while deserializing the column of type Array(String) that leads to “Memory limit exceeded” errors. The issue appeared in version 18.12.13. #3589 "},{"title":"ClickHouse Release 18.14.14, 2018-11-20​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-14-14-2018-11-20","content":"Bug Fixes:​ Fixed ON CLUSTER queries when cluster configured as secure (flag &lt;secure&gt;). #3599 Build Changes:​ Fixed problems (llvm-7 from system, macos) #3582 "},{"title":"ClickHouse Release 18.14.13, 2018-11-08​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-14-13-2018-11-08","content":"Bug Fixes:​ Fixed the Block structure mismatch in MergingSorted stream error. #3162Fixed ON CLUSTER queries in case when secure connections were turned on in the cluster config (the &lt;secure&gt; flag). #3465Fixed an error in queries that used SAMPLE, PREWHERE and alias columns. #3543Fixed a rare unknown compression method error when the min_bytes_to_use_direct_io setting was enabled. 3544 Performance Improvements:​ Fixed performance regression of queries with GROUP BY of columns of UInt16 or Date type when executing on AMD EPYC processors. Igor LapkoFixed performance regression of queries that process long strings. #3530 Build Improvements:​ Improvements for simplifying the Arcadia build. #3475, #3535 "},{"title":"ClickHouse Release 18.14.12, 2018-11-02​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-14-12-2018-11-02","content":"Bug Fixes:​ Fixed a crash on joining two unnamed subqueries. #3505Fixed generating incorrect queries (with an empty WHERE clause) when querying external databases. hotidFixed using an incorrect timeout value in ODBC dictionaries. Marek Vavruša "},{"title":"ClickHouse Release 18.14.11, 2018-10-29​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-14-11-2018-10-29","content":"Bug Fixes:​ Fixed the error Block structure mismatch in UNION stream: different number of columns in LIMIT queries. #2156Fixed errors when merging data in tables containing arrays inside Nested structures. #3397Fixed incorrect query results if the merge_tree_uniform_read_distribution setting is disabled (it is enabled by default). #3429Fixed an error on inserts to a Distributed table in Native format. #3411 "},{"title":"ClickHouse Release 18.14.10, 2018-10-23​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-14-10-2018-10-23","content":"The compile_expressions setting (JIT compilation of expressions) is disabled by default. #3410The enable_optimize_predicate_expression setting is disabled by default. "},{"title":"ClickHouse Release 18.14.9, 2018-10-16​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-14-9-2018-10-16","content":"New Features:​ The WITH CUBE modifier for GROUP BY (the alternative syntax GROUP BY CUBE(...) is also available). #3172Added the formatDateTime function. Alexandr KrasheninnikovAdded the JDBC table engine and jdbc table function (requires installing clickhouse-jdbc-bridge). Alexandr KrasheninnikovAdded functions for working with the ISO week number: toISOWeek, toISOYear, toStartOfISOYear, and toDayOfYear. #3146Now you can use Nullable columns for MySQL and ODBC tables. #3362Nested data structures can be read as nested objects in JSONEachRow format. Added the input_format_import_nested_json setting. Veloman YunkanParallel processing is available for many MATERIALIZED VIEWs when inserting data. See the parallel_view_processing setting. Marek VavrušaAdded the SYSTEM FLUSH LOGS query (forced log flushes to system tables such as query_log) #3321Now you can use pre-defined database and table macros when declaring Replicated tables. #3251Added the ability to read Decimal type values in engineering notation (indicating powers of ten). #3153 Experimental Features:​ Optimization of the GROUP BY clause for LowCardinality data types. #3138Optimized calculation of expressions for LowCardinality data types. #3200 Improvements:​ Significantly reduced memory consumption for queries with ORDER BY and LIMIT. See the max_bytes_before_remerge_sort setting. #3205In the absence of JOIN (LEFT, INNER, …), INNER JOIN is assumed. #3147Qualified asterisks work correctly in queries with JOIN. Winter ZhangThe ODBC table engine correctly chooses the method for quoting identifiers in the SQL dialect of a remote database. Alexandr KrasheninnikovThe compile_expressions setting (JIT compilation of expressions) is enabled by default.Fixed behavior for simultaneous DROP DATABASE/TABLE IF EXISTS and CREATE DATABASE/TABLE IF NOT EXISTS. Previously, a CREATE DATABASE ... IF NOT EXISTS query could return the error message “File … already exists”, and the CREATE TABLE ... IF NOT EXISTS and DROP TABLE IF EXISTS queries could return Table ... is creating or attaching right now. #3101LIKE and IN expressions with a constant right half are passed to the remote server when querying from MySQL or ODBC tables. #3182Comparisons with constant expressions in a WHERE clause are passed to the remote server when querying from MySQL and ODBC tables. Previously, only comparisons with constants were passed. #3182Correct calculation of row width in the terminal for Pretty formats, including strings with hieroglyphs. Amos Bird.ON CLUSTER can be specified for ALTER UPDATE queries.Improved performance for reading data in JSONEachRow format. #3332Added synonyms for the LENGTH and CHARACTER_LENGTH functions for compatibility. The CONCAT function is no longer case-sensitive. #3306Added the TIMESTAMP synonym for the DateTime type. #3390There is always space reserved for query_id in the server logs, even if the log line is not related to a query. This makes it easier to parse server text logs with third-party tools.Memory consumption by a query is logged when it exceeds the next level of an integer number of gigabytes. #3205Added compatibility mode for the case when the client library that uses the Native protocol sends fewer columns by mistake than the server expects for the INSERT query. This scenario was possible when using the clickhouse-cpp library. Previously, this scenario caused the server to crash. #3171In a user-defined WHERE expression in clickhouse-copier, you can now use a partition_key alias (for additional filtering by source table partition). This is useful if the partitioning scheme changes during copying, but only changes slightly. #3166The workflow of the Kafka engine has been moved to a background thread pool in order to automatically reduce the speed of data reading at high loads. Marek Vavruša.Support for reading Tuple and Nested values of structures like struct in the Cap'n'Proto format. Marek VavrušaThe list of top-level domains for the firstSignificantSubdomain function now includes the domain biz. decasealIn the configuration of external dictionaries, null_value is interpreted as the value of the default data type. #3330Support for the intDiv and intDivOrZero functions for Decimal. b48402e8Support for the Date, DateTime, UUID, and Decimal types as a key for the sumMap aggregate function. #3281Support for the Decimal data type in external dictionaries. #3324Support for the Decimal data type in SummingMergeTree tables. #3348Added specializations for UUID in if. #3366Reduced the number of open and close system calls when reading from a MergeTree table. #3283A TRUNCATE TABLE query can be executed on any replica (the query is passed to the leader replica). Kirill Shvakov Bug Fixes:​ Fixed an issue with Dictionary tables for range_hashed dictionaries. This error occurred in version 18.12.17. #1702Fixed an error when loading range_hashed dictionaries (the message Unsupported type Nullable (...)). This error occurred in version 18.12.17. #3362Fixed errors in the pointInPolygon function due to the accumulation of inaccurate calculations for polygons with a large number of vertices located close to each other. #3331 #3341If after merging data parts, the checksum for the resulting part differs from the result of the same merge in another replica, the result of the merge is deleted and the data part is downloaded from the other replica (this is the correct behavior). But after downloading the data part, it couldn’t be added to the working set because of an error that the part already exists (because the data part was deleted with some delay after the merge). This led to cyclical attempts to download the same data. #3194Fixed incorrect calculation of total memory consumption by queries (because of incorrect calculation, the max_memory_usage_for_all_queries setting worked incorrectly and the MemoryTracking metric had an incorrect value). This error occurred in version 18.12.13. Marek VavrušaFixed the functionality of CREATE TABLE ... ON CLUSTER ... AS SELECT ... This error occurred in version 18.12.13. #3247Fixed unnecessary preparation of data structures for JOINs on the server that initiates the query if the JOIN is only performed on remote servers. #3340Fixed bugs in the Kafka engine: deadlocks after exceptions when starting to read data, and locks upon completion Marek Vavruša.For Kafka tables, the optional schema parameter was not passed (the schema of the Cap'n'Proto format). Vojtech SplichalIf the ensemble of ZooKeeper servers has servers that accept the connection but then immediately close it instead of responding to the handshake, ClickHouse chooses to connect another server. Previously, this produced the error Cannot read all data. Bytes read: 0. Bytes expected: 4. and the server couldn’t start. 8218cf3aIf the ensemble of ZooKeeper servers contains servers for which the DNS query returns an error, these servers are ignored. 17b8e209Fixed type conversion between Date and DateTime when inserting data in the VALUES format (if input_format_values_interpret_expressions = 1). Previously, the conversion was performed between the numerical value of the number of days in Unix Epoch time and the Unix timestamp, which led to unexpected results. #3229Corrected type conversion between Decimal and integer numbers. #3211Fixed errors in the enable_optimize_predicate_expression setting. Winter ZhangFixed a parsing error in CSV format with floating-point numbers if a non-default CSV separator is used, such as ; #3155Fixed the arrayCumSumNonNegative function (it does not accumulate negative values if the accumulator is less than zero). Aleksey StudnevFixed how Merge tables work on top of Distributed tables when using PREWHERE. #3165Bug fixes in the ALTER UPDATE query.Fixed bugs in the odbc table function that appeared in version 18.12. #3197Fixed the operation of aggregate functions with StateArray combinators. #3188Fixed a crash when dividing a Decimal value by zero. 69dd6609Fixed output of types for operations using Decimal and integer arguments. #3224Fixed the segfault during GROUP BY on Decimal128. 3359ba06The log_query_threads setting (logging information about each thread of query execution) now takes effect only if the log_queries option (logging information about queries) is set to 1. Since the log_query_threads option is enabled by default, information about threads was previously logged even if query logging was disabled. #3241Fixed an error in the distributed operation of the quantiles aggregate function (the error message Not found column quantile...). 292a8855Fixed the compatibility problem when working on a cluster of version 18.12.17 servers and older servers at the same time. For distributed queries with GROUP BY keys of both fixed and non-fixed length, if there was a large amount of data to aggregate, the returned data was not always fully aggregated (two different rows contained the same aggregation keys). #3254Fixed handling of substitutions in clickhouse-performance-test, if the query contains only part of the substitutions declared in the test. #3263Fixed an error when using FINAL with PREWHERE. #3298Fixed an error when using PREWHERE over columns that were added during ALTER. #3298Added a check for the absence of arrayJoin for DEFAULT and MATERIALIZED expressions. Previously, arrayJoin led to an error when inserting data. #3337Added a check for the absence of arrayJoin in a PREWHERE clause. Previously, this led to messages like Size ... does not match or Unknown compression method when executing queries. #3357Fixed segfault that could occur in rare cases after optimization that replaced AND chains from equality evaluations with the corresponding IN expression. liuyimin-bytedanceMinor corrections to clickhouse-benchmark: previously, client information was not sent to the server; now the number of queries executed is calculated more accurately when shutting down and for limiting the number of iterations. #3351 #3352 Backward Incompatible Changes:​ Removed the allow_experimental_decimal_type option. The Decimal data type is available for default use. #3329 "},{"title":"ClickHouse Release 18.12​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-12","content":""},{"title":"ClickHouse Release 18.12.17, 2018-09-16​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-12-17-2018-09-16","content":"New Features:​ invalidate_query (the ability to specify a query to check whether an external dictionary needs to be updated) is implemented for the clickhouse source. #3126Added the ability to use UInt*, Int*, and DateTime data types (along with the Date type) as a range_hashed external dictionary key that defines the boundaries of ranges. Now NULL can be used to designate an open range. Vasily NemkovThe Decimal type now supports var* and stddev* aggregate functions. #3129The Decimal type now supports mathematical functions (exp, sin and so on.) #3129The system.part_log table now has the partition_id column. #3089 Bug Fixes:​ Merge now works correctly on Distributed tables. Winter ZhangFixed incompatibility (unnecessary dependency on the glibc version) that made it impossible to run ClickHouse on Ubuntu Precise and older versions. The incompatibility arose in version 18.12.13. #3130Fixed errors in the enable_optimize_predicate_expression setting. Winter ZhangFixed a minor issue with backwards compatibility that appeared when working with a cluster of replicas on versions earlier than 18.12.13 and simultaneously creating a new replica of a table on a server with a newer version (shown in the message Can not clone replica, because the ... updated to new ClickHouse version, which is logical, but shouldn’t happen). #3122 Backward Incompatible Changes:​ The enable_optimize_predicate_expression option is enabled by default (which is rather optimistic). If query analysis errors occur that are related to searching for the column names, set enable_optimize_predicate_expression to 0. Winter Zhang "},{"title":"ClickHouse Release 18.12.14, 2018-09-13​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-12-14-2018-09-13","content":"New Features:​ Added support for ALTER UPDATE queries. #3035Added the allow_ddl option, which restricts the user’s access to DDL queries. #3104Added the min_merge_bytes_to_use_direct_io option for MergeTree engines, which allows you to set a threshold for the total size of the merge (when above the threshold, data part files will be handled using O_DIRECT). #3117The system.merges system table now contains the partition_id column. #3099 Improvements​ If a data part remains unchanged during mutation, it isn’t downloaded by replicas. #3103Autocomplete is available for names of settings when working with clickhouse-client. #3106 Bug Fixes:​ Added a check for the sizes of arrays that are elements of Nested type fields when inserting. #3118Fixed an error updating external dictionaries with the ODBC source and hashed storage. This error occurred in version 18.12.13.Fixed a crash when creating a temporary table from a query with an IN condition. Winter ZhangFixed an error in aggregate functions for arrays that can have NULL elements. Winter Zhang "},{"title":"ClickHouse Release 18.12.13, 2018-09-10​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-12-13-2018-09-10","content":"New Features:​ Added the DECIMAL(digits, scale) data type (Decimal32(scale), Decimal64(scale), Decimal128(scale)). To enable it, use the setting allow_experimental_decimal_type. #2846 #2970 #3008 #3047New WITH ROLLUP modifier for GROUP BY (alternative syntax: GROUP BY ROLLUP(...)). #2948In queries with JOIN, the star character expands to a list of columns in all tables, in compliance with the SQL standard. You can restore the old behavior by setting asterisk_left_columns_only to 1 on the user configuration level. Winter ZhangAdded support for JOIN with table functions. Winter ZhangAutocomplete by pressing Tab in clickhouse-client. Sergey ShcherbinCtrl+C in clickhouse-client clears a query that was entered. #2877Added the join_default_strictness setting (values: &quot;, 'any', 'all'). This allows you to not specify ANY or ALL for JOIN. #2982Each line of the server log related to query processing shows the query ID. #2482Now you can get query execution logs in clickhouse-client (use the send_logs_level setting). With distributed query processing, logs are cascaded from all the servers. #2482The system.query_log and system.processes (SHOW PROCESSLIST) tables now have information about all changed settings when you run a query (the nested structure of the Settings data). Added the log_query_settings setting. #2482The system.query_log and system.processes tables now show information about the number of threads that are participating in query execution (see the thread_numbers column). #2482Added ProfileEvents counters that measure the time spent on reading and writing over the network and reading and writing to disk, the number of network errors, and the time spent waiting when network bandwidth is limited. #2482Added ProfileEventscounters that contain the system metrics from rusage (you can use them to get information about CPU usage in userspace and the kernel, page faults, and context switches), as well as taskstats metrics (use these to obtain information about I/O wait time, CPU wait time, and the amount of data read and recorded, both with and without page cache). #2482The ProfileEvents counters are applied globally and for each query, as well as for each query execution thread, which allows you to profile resource consumption by query in detail. #2482Added the system.query_thread_log table, which contains information about each query execution thread. Added the log_query_threads setting. #2482The system.metrics and system.events tables now have built-in documentation. #3016Added the arrayEnumerateDense function. Amos BirdAdded the arrayCumSumNonNegative and arrayDifference functions. Aleksey StudnevAdded the retention aggregate function. Sundy LiNow you can add (merge) states of aggregate functions by using the plus operator, and multiply the states of aggregate functions by a nonnegative constant. #3062 #3034Tables in the MergeTree family now have the virtual column _partition_id. #3089 Experimental Features:​ Added the LowCardinality(T) data type. This data type automatically creates a local dictionary of values and allows data processing without unpacking the dictionary. #2830Added a cache of JIT-compiled functions and a counter for the number of uses before compiling. To JIT compile expressions, enable the compile_expressions setting. #2990 #3077 Improvements:​ Fixed the problem with unlimited accumulation of the replication log when there are abandoned replicas. Added an effective recovery mode for replicas with a long lag.Improved performance of GROUP BY with multiple aggregation fields when one of them is string and the others are fixed length.Improved performance when using PREWHERE and with implicit transfer of expressions in PREWHERE.Improved parsing performance for text formats (CSV, TSV). Amos Bird #2980Improved performance of reading strings and arrays in binary formats. Amos BirdIncreased performance and reduced memory consumption for queries to system.tables and system.columns when there is a very large number of tables on a single server. #2953Fixed a performance problem in the case of a large stream of queries that result in an error (the _dl_addr function is visible in perf top, but the server isn’t using much CPU). #2938Conditions are cast into the View (when enable_optimize_predicate_expression is enabled). Winter ZhangImprovements to the functionality for the UUID data type. #3074 #2985The UUID data type is supported in The-Alchemist dictionaries. #2822The visitParamExtractRaw function works correctly with nested structures. Winter ZhangWhen the input_format_skip_unknown_fields setting is enabled, object fields in JSONEachRow format are skipped correctly. BlahGeekFor a CASE expression with conditions, you can now omit ELSE, which is equivalent to ELSE NULL. #2920The operation timeout can now be configured when working with ZooKeeper. urykhyYou can specify an offset for LIMIT n, m as LIMIT n OFFSET m. #2840You can use the SELECT TOP n syntax as an alternative for LIMIT. #2840Increased the size of the queue to write to system tables, so the SystemLog parameter queue is full error does not happen as often.The windowFunnel aggregate function now supports events that meet multiple conditions. Amos BirdDuplicate columns can be used in a USING clause for JOIN. #3006Pretty formats now have a limit on column alignment by width. Use the output_format_pretty_max_column_pad_width setting. If a value is wider, it will still be displayed in its entirety, but the other cells in the table will not be too wide. #3003The odbc table function now allows you to specify the database/schema name. Amos BirdAdded the ability to use a username specified in the clickhouse-client config file. Vladimir KozbinThe ZooKeeperExceptions counter has been split into three counters: ZooKeeperUserExceptions, ZooKeeperHardwareExceptions, and ZooKeeperOtherExceptions.ALTER DELETE queries work for materialized views.Added randomization when running the cleanup thread periodically for ReplicatedMergeTree tables in order to avoid periodic load spikes when there are a very large number of ReplicatedMergeTree tables.Support for ATTACH TABLE ... ON CLUSTER queries. #3025 Bug Fixes:​ Fixed an issue with Dictionary tables (throws the Size of offsets does not match size of column or Unknown compression method exception). This bug appeared in version 18.10.3. #2913Fixed a bug when merging CollapsingMergeTree tables if one of the data parts is empty (these parts are formed during merge or ALTER DELETE if all data was deleted), and the vertical algorithm was used for the merge. #3049Fixed a race condition during DROP or TRUNCATE for Memory tables with a simultaneous SELECT, which could lead to server crashes. This bug appeared in version 1.1.54388. #3038Fixed the possibility of data loss when inserting in Replicated tables if the Session is expired error is returned (data loss can be detected by the ReplicatedDataLoss metric). This error occurred in version 1.1.54378. #2939 #2949 #2964Fixed a segfault during JOIN ... ON. #3000Fixed the error searching column names when the WHERE expression consists entirely of a qualified column name, such as WHERE table.column. #2994Fixed the “Not found column” error that occurred when executing distributed queries if a single column consisting of an IN expression with a subquery is requested from a remote server. #3087Fixed the Block structure mismatch in UNION stream: different number of columns error that occurred for distributed queries if one of the shards is local and the other is not, and optimization of the move to PREWHERE is triggered. #2226 #3037 #3055 #3065 #3073 #3090 #3093Fixed the pointInPolygon function for certain cases of non-convex polygons. #2910Fixed the incorrect result when comparing nan with integers. #3024Fixed an error in the zlib-ng library that could lead to segfault in rare cases. #2854Fixed a memory leak when inserting into a table with AggregateFunction columns, if the state of the aggregate function is not simple (allocates memory separately), and if a single insertion request results in multiple small blocks. #3084Fixed a race condition when creating and deleting the same Buffer or MergeTree table simultaneously.Fixed the possibility of a segfault when comparing tuples made up of certain non-trivial types, such as tuples. #2989Fixed the possibility of a segfault when running certain ON CLUSTER queries. Winter ZhangFixed an error in the arrayDistinct function for Nullable array elements. #2845 #2937The enable_optimize_predicate_expression option now correctly supports cases with SELECT *. Winter ZhangFixed the segfault when re-initializing the ZooKeeper session. #2917Fixed potential blocking when working with ZooKeeper.Fixed incorrect code for adding nested data structures in a SummingMergeTree.When allocating memory for states of aggregate functions, alignment is correctly taken into account, which makes it possible to use operations that require alignment when implementing states of aggregate functions. chenxing-xc Security Fix:​ Safe use of ODBC data sources. Interaction with ODBC drivers uses a separate clickhouse-odbc-bridge process. Errors in third-party ODBC drivers no longer cause problems with server stability or vulnerabilities. #2828 #2879 #2886 #2893 #2921Fixed incorrect validation of the file path in the catBoostPool table function. #2894The contents of system tables (tables, databases, parts, columns, parts_columns, merges, mutations, replicas, and replication_queue) are filtered according to the user’s configured access to databases (allow_databases). Winter Zhang Backward Incompatible Changes:​ In queries with JOIN, the star character expands to a list of columns in all tables, in compliance with the SQL standard. You can restore the old behavior by setting asterisk_left_columns_only to 1 on the user configuration level. Build Changes:​ Most integration tests can now be run by commit.Code style checks can also be run by commit.The memcpy implementation is chosen correctly when building on CentOS7/Fedora. Etienne ChampetierWhen using clang to build, some warnings from -Weverything have been added, in addition to the regular -Wall-Wextra -Werror. #2957Debugging the build uses the jemalloc debug option.The interface of the library for interacting with ZooKeeper is declared abstract. #2950 "},{"title":"ClickHouse Release 18.10​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-10","content":""},{"title":"ClickHouse Release 18.10.3, 2018-08-13​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-10-3-2018-08-13","content":"New Features:​ HTTPS can be used for replication. #2760Added the functions murmurHash2_64, murmurHash3_32, murmurHash3_64, and murmurHash3_128 in addition to the existing murmurHash2_32. #2791Support for Nullable types in the ClickHouse ODBC driver (ODBCDriver2 output format). #2834Support for UUID in the key columns. Improvements:​ Clusters can be removed without restarting the server when they are deleted from the config files. #2777External dictionaries can be removed without restarting the server when they are removed from config files. #2779Added SETTINGS support for the Kafka table engine. Alexander MarshalovImprovements for the UUID data type (not yet complete). #2618Support for empty parts after merges in the SummingMergeTree, CollapsingMergeTree and VersionedCollapsingMergeTree engines. #2815Old records of completed mutations are deleted (ALTER DELETE). #2784Added the system.merge_tree_settings table. Kirill ShvakovThe system.tables table now has dependency columns: dependencies_database and dependencies_table. Winter ZhangAdded the max_partition_size_to_drop config option. #2782Added the output_format_json_escape_forward_slashes option. Alexander BocharovAdded the max_fetch_partition_retries_count setting. #2831Added the prefer_localhost_replica setting for disabling the preference for a local replica and going to a local replica without inter-process interaction. #2832The quantileExact aggregate function returns nan in the case of aggregation on an empty Float32 or Float64 set. Sundy Li Bug Fixes:​ Removed unnecessary escaping of the connection string parameters for ODBC, which made it impossible to establish a connection. This error occurred in version 18.6.0.Fixed the logic for processing REPLACE PARTITION commands in the replication queue. If there are two REPLACE commands for the same partition, the incorrect logic could cause one of them to remain in the replication queue and not be executed. #2814Fixed a merge bug when all data parts were empty (parts that were formed from a merge or from ALTER DELETE if all data was deleted). This bug appeared in version 18.1.0. #2930Fixed an error for concurrent Set or Join. Amos BirdFixed the Block structure mismatch in UNION stream: different number of columns error that occurred for UNION ALL queries inside a sub-query if one of the SELECT queries contains duplicate column names. Winter ZhangFixed a memory leak if an exception occurred when connecting to a MySQL server.Fixed incorrect clickhouse-client response code in case of a query error.Fixed incorrect behavior of materialized views containing DISTINCT. #2795 Backward Incompatible Changes​ Removed support for CHECK TABLE queries for Distributed tables. Build Changes:​ The allocator has been replaced: jemalloc is now used instead of tcmalloc. In some scenarios, this increases speed up to 20%. However, there are queries that have slowed by up to 20%. Memory consumption has been reduced by approximately 10% in some scenarios, with improved stability. With highly competitive loads, CPU usage in userspace and in system shows just a slight increase. #2773Use of libressl from a submodule. #1983 #2807Use of unixodbc from a submodule. #2789Use of mariadb-connector-c from a submodule. #2785Added functional test files to the repository that depend on the availability of test data (for the time being, without the test data itself). "},{"title":"ClickHouse Release 18.6​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-6","content":""},{"title":"ClickHouse Release 18.6.0, 2018-08-02​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-6-0-2018-08-02","content":"New Features:​ Added support for ON expressions for the JOIN ON syntax:JOIN ON Expr([table.]column ...) = Expr([table.]column, ...) [AND Expr([table.]column, ...) = Expr([table.]column, ...) ...]The expression must be a chain of equalities joined by the AND operator. Each side of the equality can be an arbitrary expression over the columns of one of the tables. The use of fully qualified column names is supported (table.name, database.table.name, table_alias.name, subquery_alias.name) for the right table. #2742HTTPS can be enabled for replication. #2760 Improvements:​ The server passes the patch component of its version to the client. Data about the patch version component is in system.processes and query_log. #2646 "},{"title":"ClickHouse Release 18.5​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-5","content":""},{"title":"ClickHouse Release 18.5.1, 2018-07-31​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-5-1-2018-07-31","content":"New Features:​ Added the hash function murmurHash2_32 #2756. Improvements:​ Now you can use the from_env #2741 attribute to set values in config files from environment variables.Added case-insensitive versions of the coalesce, ifNull, and nullIf functions #2752. Bug Fixes:​ Fixed a possible bug when starting a replica #2759. "},{"title":"ClickHouse Release 18.4​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-4","content":""},{"title":"ClickHouse Release 18.4.0, 2018-07-28​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-4-0-2018-07-28","content":"New Features:​ Added system tables: formats, data_type_families, aggregate_function_combinators, table_functions, table_engines, collations #2721.Added the ability to use a table function instead of a table as an argument of a remote or cluster table function #2708.Support for HTTP Basic authentication in the replication protocol #2727.The has function now allows searching for a numeric value in an array of Enum values Maxim Khrisanfov.Support for adding arbitrary message separators when reading from Kafka Amos Bird. Improvements:​ The ALTER TABLE t DELETE WHERE query does not rewrite data parts that were not affected by the WHERE condition #2694.The use_minimalistic_checksums_in_zookeeper option for ReplicatedMergeTree tables is enabled by default. This setting was added in version 1.1.54378, 2018-04-16. Versions that are older than 1.1.54378 can no longer be installed.Support for running KILL and OPTIMIZE queries that specify ON CLUSTER Winter Zhang. Bug Fixes:​ Fixed the error Column ... is not under an aggregate function and not in GROUP BY for aggregation with an IN expression. This bug appeared in version 18.1.0. (bbdd780b)Fixed a bug in the windowFunnel aggregate function Winter Zhang.Fixed a bug in the anyHeavy aggregate function (a2101df2)Fixed server crash when using the countArray() aggregate function. Backward Incompatible Changes:​ Parameters for Kafka engine was changed from Kafka(kafka_broker_list, kafka_topic_list, kafka_group_name, kafka_format[, kafka_schema, kafka_num_consumers]) to Kafka(kafka_broker_list, kafka_topic_list, kafka_group_name, kafka_format[, kafka_row_delimiter, kafka_schema, kafka_num_consumers]). If your tables use kafka_schema or kafka_num_consumers parameters, you have to manually edit the metadata files path/metadata/database/table.sql and add kafka_row_delimiter parameter with '' value. "},{"title":"ClickHouse Release 18.1​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-1","content":""},{"title":"ClickHouse Release 18.1.0, 2018-07-23​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-18-1-0-2018-07-23","content":"New Features:​ Support for the ALTER TABLE t DELETE WHERE query for non-replicated MergeTree tables (#2634).Support for arbitrary types for the uniq* family of aggregate functions (#2010).Support for arbitrary types in comparison operators (#2026).The users.xml file allows setting a subnet mask in the format 10.0.0.1/255.255.255.0. This is necessary for using masks for IPv6 networks with zeros in the middle (#2637).Added the arrayDistinct function (#2670).The SummingMergeTree engine can now work with AggregateFunction type columns (Constantin S. Pan). Improvements:​ Changed the numbering scheme for release versions. Now the first part contains the year of release (A.D., Moscow timezone, minus 2000), the second part contains the number for major changes (increases for most releases), and the third part is the patch version. Releases are still backward compatible, unless otherwise stated in the changelog.Faster conversions of floating-point numbers to a string (Amos Bird).If some rows were skipped during an insert due to parsing errors (this is possible with the input_allow_errors_num and input_allow_errors_ratio settings enabled), the number of skipped rows is now written to the server log (Leonardo Cecchi). Bug Fixes:​ Fixed the TRUNCATE command for temporary tables (Amos Bird).Fixed a rare deadlock in the ZooKeeper client library that occurred when there was a network error while reading the response (c315200).Fixed an error during a CAST to Nullable types (#1322).Fixed the incorrect result of the maxIntersection() function when the boundaries of intervals coincided (Michael Furmur).Fixed incorrect transformation of the OR expression chain in a function argument (chenxing-xc).Fixed performance degradation for queries containing IN (subquery) expressions inside another subquery (#2571).Fixed incompatibility between servers with different versions in distributed queries that use a CAST function that isn’t in uppercase letters (fe8c4d6).Added missing quoting of identifiers for queries to an external DBMS (#2635). Backward Incompatible Changes:​ Converting a string containing the number zero to DateTime does not work. Example: SELECT toDateTime('0'). This is also the reason that DateTime DEFAULT '0' does not work in tables, as well as &lt;null_value&gt;0&lt;/null_value&gt; in dictionaries. Solution: replace 0 with 0000-00-00 00:00:00. "},{"title":"ClickHouse Release 1.1​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-1-1","content":""},{"title":"ClickHouse Release 1.1.54394, 2018-07-12​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-1-1-54394-2018-07-12","content":"New Features:​ Added the histogram aggregate function (Mikhail Surin).Now OPTIMIZE TABLE ... FINAL can be used without specifying partitions for ReplicatedMergeTree (Amos Bird). Bug Fixes:​ Fixed a problem with a very small timeout for sockets (one second) for reading and writing when sending and downloading replicated data, which made it impossible to download larger parts if there is a load on the network or disk (it resulted in cyclical attempts to download parts). This error occurred in version 1.1.54388.Fixed issues when using chroot in ZooKeeper if you inserted duplicate data blocks in the table.The has function now works correctly for an array with Nullable elements (#2115).The system.tables table now works correctly when used in distributed queries. The metadata_modification_time and engine_full columns are now non-virtual. Fixed an error that occurred if only these columns were queried from the table.Fixed how an empty TinyLog table works after inserting an empty data block (#2563).The system.zookeeper table works if the value of the node in ZooKeeper is NULL. "},{"title":"ClickHouse Release 1.1.54390, 2018-07-06​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-1-1-54390-2018-07-06","content":"New Features:​ Queries can be sent in multipart/form-data format (in the query field), which is useful if external data is also sent for query processing (Olga Hvostikova).Added the ability to enable or disable processing single or double quotes when reading data in CSV format. You can configure this in the format_csv_allow_single_quotes and format_csv_allow_double_quotes settings (Amos Bird).Now OPTIMIZE TABLE ... FINAL can be used without specifying the partition for non-replicated variants of MergeTree (Amos Bird). Improvements:​ Improved performance, reduced memory consumption, and correct memory consumption tracking with use of the IN operator when a table index could be used (#2584).Removed redundant checking of checksums when adding a data part. This is important when there are a large number of replicas, because in these cases the total number of checks was equal to N^2.Added support for Array(Tuple(...)) arguments for the arrayEnumerateUniq function (#2573).Added Nullable support for the runningDifference function (#2594).Improved query analysis performance when there is a very large number of expressions (#2572).Faster selection of data parts for merging in ReplicatedMergeTree tables. Faster recovery of the ZooKeeper session (#2597).The format_version.txt file for MergeTree tables is re-created if it is missing, which makes sense if ClickHouse is launched after copying the directory structure without files (Ciprian Hacman). Bug Fixes:​ Fixed a bug when working with ZooKeeper that could make it impossible to recover the session and readonly states of tables before restarting the server.Fixed a bug when working with ZooKeeper that could result in old nodes not being deleted if the session is interrupted.Fixed an error in the quantileTDigest function for Float arguments (this bug was introduced in version 1.1.54388) (Mikhail Surin).Fixed a bug in the index for MergeTree tables if the primary key column is located inside the function for converting types between signed and unsigned integers of the same size (#2603).Fixed segfault if macros are used but they aren’t in the config file (#2570).Fixed switching to the default database when reconnecting the client (#2583).Fixed a bug that occurred when the use_index_for_in_with_subqueries setting was disabled. Security Fix:​ Sending files is no longer possible when connected to MySQL (LOAD DATA LOCAL INFILE). "},{"title":"ClickHouse Release 1.1.54388, 2018-06-28​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-1-1-54388-2018-06-28","content":"New Features:​ Support for the ALTER TABLE t DELETE WHERE query for replicated tables. Added the system.mutations table to track progress of this type of queries.Support for the ALTER TABLE t [REPLACE|ATTACH] PARTITION query for *MergeTree tables.Support for the TRUNCATE TABLE query (Winter Zhang)Several new SYSTEM queries for replicated tables (RESTART REPLICAS, SYNC REPLICA, [STOP|START] [MERGES|FETCHES|SENDS REPLICATED|REPLICATION QUEUES]).Added the ability to write to a table with the MySQL engine and the corresponding table function (sundy-li).Added the url() table function and the URL table engine (Alexander Sapin).Added the windowFunnel aggregate function (sundy-li).New startsWith and endsWith functions for strings (Vadim Plakhtinsky).The numbers() table function now allows you to specify the offset (Winter Zhang).The password to clickhouse-client can be entered interactively.Server logs can now be sent to syslog (Alexander Krasheninnikov).Support for logging in dictionaries with a shared library source (Alexander Sapin).Support for custom CSV delimiters (Ivan Zhukov)Added the date_time_input_format setting. If you switch this setting to 'best_effort', DateTime values will be read in a wide range of formats.Added the clickhouse-obfuscator utility for data obfuscation. Usage example: publishing data used in performance tests. Experimental Features:​ Added the ability to calculate and arguments only where they are needed (Anastasia Tsarkova)JIT compilation to native code is now available for some expressions (pyos). Bug Fixes:​ Duplicates no longer appear for a query with DISTINCT and ORDER BY.Queries with ARRAY JOIN and arrayFilter no longer return an incorrect result.Fixed an error when reading an array column from a Nested structure (#2066).Fixed an error when analyzing queries with a HAVING clause like HAVING tuple IN (...).Fixed an error when analyzing queries with recursive aliases.Fixed an error when reading from ReplacingMergeTree with a condition in PREWHERE that filters all rows (#2525).User profile settings were not applied when using sessions in the HTTP interface.Fixed how settings are applied from the command line parameters in clickhouse-local.The ZooKeeper client library now uses the session timeout received from the server.Fixed a bug in the ZooKeeper client library when the client waited for the server response longer than the timeout.Fixed pruning of parts for queries with conditions on partition key columns (#2342).Merges are now possible after CLEAR COLUMN IN PARTITION (#2315).Type mapping in the ODBC table function has been fixed (sundy-li).Type comparisons have been fixed for DateTime with and without the time zone (Alexander Bocharov).Fixed syntactic parsing and formatting of the CAST operator.Fixed insertion into a materialized view for the Distributed table engine (Babacar Diassé).Fixed a race condition when writing data from the Kafka engine to materialized views (Yangkuan Liu).Fixed SSRF in the remote() table function.Fixed exit behavior of clickhouse-client in multiline mode (#2510). Improvements:​ Background tasks in replicated tables are now performed in a thread pool instead of in separate threads (Silviu Caragea).Improved LZ4 compression performance.Faster analysis for queries with a large number of JOINs and sub-queries.The DNS cache is now updated automatically when there are too many network errors.Table inserts no longer occur if the insert into one of the materialized views is not possible because it has too many parts.Corrected the discrepancy in the event counters Query, SelectQuery, and InsertQuery.Expressions like tuple IN (SELECT tuple) are allowed if the tuple types match.A server with replicated tables can start even if you haven’t configured ZooKeeper.When calculating the number of available CPU cores, limits on cgroups are now taken into account (Atri Sharma).Added chown for config directories in the systemd config file (Mikhail Shiryaev). Build Changes:​ The gcc8 compiler can be used for builds.Added the ability to build llvm from submodule.The version of the librdkafka library has been updated to v0.11.4.Added the ability to use the system libcpuid library. The library version has been updated to 0.4.0.Fixed the build using the vectorclass library (Babacar Diassé).Cmake now generates files for ninja by default (like when using -G Ninja).Added the ability to use the libtinfo library instead of libtermcap (Georgy Kondratiev).Fixed a header file conflict in Fedora Rawhide (#2520). Backward Incompatible Changes:​ Removed escaping in Vertical and Pretty* formats and deleted the VerticalRaw format.If servers with version 1.1.54388 (or newer) and servers with an older version are used simultaneously in a distributed query and the query has the cast(x, 'Type') expression without the AS keyword and does not have the word cast in uppercase, an exception will be thrown with a message like Not found column cast(0, 'UInt8') in block. Solution: Update the server on the entire cluster. "},{"title":"ClickHouse Release 1.1.54385, 2018-06-01​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-1-1-54385-2018-06-01","content":"Bug Fixes:​ Fixed an error that in some cases caused ZooKeeper operations to block. "},{"title":"ClickHouse Release 1.1.54383, 2018-05-22​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-1-1-54383-2018-05-22","content":"Bug Fixes:​ Fixed a slowdown of replication queue if a table has many replicas. "},{"title":"ClickHouse Release 1.1.54381, 2018-05-14​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-1-1-54381-2018-05-14","content":"Bug Fixes:​ Fixed a nodes leak in ZooKeeper when ClickHouse loses connection to ZooKeeper server. "},{"title":"ClickHouse Release 1.1.54380, 2018-04-21​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-1-1-54380-2018-04-21","content":"New Features:​ Added the table function file(path, format, structure). An example reading bytes from /dev/urandom: ln -s /dev/urandom /var/lib/clickhouse/user_files/random``clickhouse-client -q &quot;SELECT * FROM file('random', 'RowBinary', 'd UInt8') LIMIT 10&quot;. Improvements:​ Subqueries can be wrapped in () brackets to enhance query readability. For example: (SELECT 1) UNION ALL (SELECT 1).Simple SELECT queries from the system.processes table are not included in the max_concurrent_queries limit. Bug Fixes:​ Fixed incorrect behavior of the IN operator when select from MATERIALIZED VIEW.Fixed incorrect filtering by partition index in expressions like partition_key_column IN (...).Fixed inability to execute OPTIMIZE query on non-leader replica if REANAME was performed on the table.Fixed the authorization error when executing OPTIMIZE or ALTER queries on a non-leader replica.Fixed freezing of KILL QUERY.Fixed an error in ZooKeeper client library which led to loss of watches, freezing of distributed DDL queue, and slowdowns in the replication queue if a non-empty chroot prefix is used in the ZooKeeper configuration. Backward Incompatible Changes:​ Removed support for expressions like (a, b) IN (SELECT (a, b)) (you can use the equivalent expression (a, b) IN (SELECT a, b)). In previous releases, these expressions led to undetermined WHERE filtering or caused errors. "},{"title":"ClickHouse Release 1.1.54378, 2018-04-16​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-1-1-54378-2018-04-16","content":"New Features:​ Logging level can be changed without restarting the server.Added the SHOW CREATE DATABASE query.The query_id can be passed to clickhouse-client (elBroom).New setting: max_network_bandwidth_for_all_users.Added support for ALTER TABLE ... PARTITION ... for MATERIALIZED VIEW.Added information about the size of data parts in uncompressed form in the system table.Server-to-server encryption support for distributed tables (&lt;secure&gt;1&lt;/secure&gt; in the replica config in &lt;remote_servers&gt;).Configuration of the table level for the ReplicatedMergeTree family in order to minimize the amount of data stored in Zookeeper: : use_minimalistic_checksums_in_zookeeper = 1Configuration of the clickhouse-client prompt. By default, server names are now output to the prompt. The server’s display name can be changed. It’s also sent in the X-ClickHouse-Display-Name HTTP header (Kirill Shvakov).Multiple comma-separated topics can be specified for the Kafka engine (Tobias Adamson)When a query is stopped by KILL QUERY or replace_running_query, the client receives the Query was canceled exception instead of an incomplete result. Improvements:​ ALTER TABLE ... DROP/DETACH PARTITION queries are run at the front of the replication queue.SELECT ... FINAL and OPTIMIZE ... FINAL can be used even when the table has a single data part.A query_log table is recreated on the fly if it was deleted manually (Kirill Shvakov).The lengthUTF8 function runs faster (zhang2014).Improved performance of synchronous inserts in Distributed tables (insert_distributed_sync = 1) when there is a very large number of shards.The server accepts the send_timeout and receive_timeout settings from the client and applies them when connecting to the client (they are applied in reverse order: the server socket’s send_timeout is set to the receive_timeout value received from the client, and vice versa).More robust crash recovery for asynchronous insertion into Distributed tables.The return type of the countEqual function changed from UInt32 to UInt64 (谢磊). Bug Fixes:​ Fixed an error with IN when the left side of the expression is Nullable.Correct results are now returned when using tuples with IN when some of the tuple components are in the table index.The max_execution_time limit now works correctly with distributed queries.Fixed errors when calculating the size of composite columns in the system.columns table.Fixed an error when creating a temporary table CREATE TEMPORARY TABLE IF NOT EXISTS.Fixed errors in StorageKafka (##2075)Fixed server crashes from invalid arguments of certain aggregate functions.Fixed the error that prevented the DETACH DATABASE query from stopping background tasks for ReplicatedMergeTree tables.Too many parts state is less likely to happen when inserting into aggregated materialized views (##2084).Corrected recursive handling of substitutions in the config if a substitution must be followed by another substitution on the same level.Corrected the syntax in the metadata file when creating a VIEW that uses a query with UNION ALL.SummingMergeTree now works correctly for summation of nested data structures with a composite key.Fixed the possibility of a race condition when choosing the leader for ReplicatedMergeTree tables. Build Changes:​ The build supports ninja instead of make and uses ninja by default for building releases.Renamed packages: clickhouse-server-base in clickhouse-common-static; clickhouse-server-common in clickhouse-server; clickhouse-common-dbg in clickhouse-common-static-dbg. To install, use clickhouse-server clickhouse-client. Packages with the old names will still load in the repositories for backward compatibility. Backward Incompatible Changes:​ Removed the special interpretation of an IN expression if an array is specified on the left side. Previously, the expression arr IN (set) was interpreted as “at least one arr element belongs to the set”. To get the same behavior in the new version, write arrayExists(x -&gt; x IN (set), arr).Disabled the incorrect use of the socket option SO_REUSEPORT, which was incorrectly enabled by default in the Poco library. Note that on Linux there is no longer any reason to simultaneously specify the addresses :: and 0.0.0.0 for listen – use just ::, which allows listening to the connection both over IPv4 and IPv6 (with the default kernel config settings). You can also revert to the behavior from previous versions by specifying &lt;listen_reuse_port&gt;1&lt;/listen_reuse_port&gt; in the config. "},{"title":"ClickHouse Release 1.1.54370, 2018-03-16​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-1-1-54370-2018-03-16","content":"New Features:​ Added the system.macros table and auto updating of macros when the config file is changed.Added the SYSTEM RELOAD CONFIG query.Added the maxIntersections(left_col, right_col) aggregate function, which returns the maximum number of simultaneously intersecting intervals [left; right]. The maxIntersectionsPosition(left, right) function returns the beginning of the “maximum” interval. (Michael Furmur). Improvements:​ When inserting data in a Replicated table, fewer requests are made to ZooKeeper (and most of the user-level errors have disappeared from the ZooKeeper log).Added the ability to create aliases for data sets. Example: WITH (1, 2, 3) AS set SELECT number IN set FROM system.numbers LIMIT 10. Bug Fixes:​ Fixed the Illegal PREWHERE error when reading from Merge tables for Distributedtables.Added fixes that allow you to start clickhouse-server in IPv4-only Docker containers.Fixed a race condition when reading from system system.parts_columns tables.Removed double buffering during a synchronous insert to a Distributed table, which could have caused the connection to timeout.Fixed a bug that caused excessively long waits for an unavailable replica before beginning a SELECT query.Fixed incorrect dates in the system.parts table.Fixed a bug that made it impossible to insert data in a Replicated table if chroot was non-empty in the configuration of the ZooKeeper cluster.Fixed the vertical merging algorithm for an empty ORDER BY table.Restored the ability to use dictionaries in queries to remote tables, even if these dictionaries are not present on the requestor server. This functionality was lost in release 1.1.54362.Restored the behavior for queries like SELECT * FROM remote('server2', default.table) WHERE col IN (SELECT col2 FROM default.table) when the right side of the IN should use a remote default.table instead of a local one. This behavior was broken in version 1.1.54358.Removed extraneous error-level logging of Not found column ... in block. "},{"title":"ClickHouse Release 1.1.54362, 2018-03-11​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-1-1-54362-2018-03-11","content":"New Features:​ Aggregation without GROUP BY for an empty set (such as SELECT count(*) FROM table WHERE 0) now returns a result with one row with null values for aggregate functions, in compliance with the SQL standard. To restore the old behavior (return an empty result), set empty_result_for_aggregation_by_empty_set to 1.Added type conversion for UNION ALL. Different alias names are allowed in SELECT positions in UNION ALL, in compliance with the SQL standard.Arbitrary expressions are supported in LIMIT BY clauses. Previously, it was only possible to use columns resulting from SELECT.An index of MergeTree tables is used when IN is applied to a tuple of expressions from the columns of the primary key. Example: WHERE (UserID, EventDate) IN ((123, '2000-01-01'), ...) (Anastasiya Tsarkova).Added the clickhouse-copier tool for copying between clusters and resharding data (beta).Added consistent hashing functions: yandexConsistentHash, jumpConsistentHash, sumburConsistentHash. They can be used as a sharding key in order to reduce the amount of network traffic during subsequent reshardings.Added functions: arrayAny, arrayAll, hasAny, hasAll, arrayIntersect, arrayResize.Added the arrayCumSum function (Javi Santana).Added the parseDateTimeBestEffort, parseDateTimeBestEffortOrZero, and parseDateTimeBestEffortOrNull functions to read the DateTime from a string containing text in a wide variety of possible formats.Data can be partially reloaded from external dictionaries during updating (load just the records in which the value of the specified field greater than in the previous download) (Arsen Hakobyan).Added the cluster table function. Example: cluster(cluster_name, db, table). The remote table function can accept the cluster name as the first argument, if it is specified as an identifier.The remote and cluster table functions can be used in INSERT queries.Added the create_table_query and engine_full virtual columns to the system.tablestable . The metadata_modification_time column is virtual.Added the data_path and metadata_path columns to system.tablesandsystem.databases tables, and added the path column to the system.parts and system.parts_columns tables.Added additional information about merges in the system.part_log table.An arbitrary partitioning key can be used for the system.query_log table (Kirill Shvakov).The SHOW TABLES query now also shows temporary tables. Added temporary tables and the is_temporary column to system.tables (zhang2014).Added DROP TEMPORARY TABLE and EXISTS TEMPORARY TABLE queries (zhang2014).Support for SHOW CREATE TABLE for temporary tables (zhang2014).Added the system_profile configuration parameter for the settings used by internal processes.Support for loading object_id as an attribute in MongoDB dictionaries (Pavel Litvinenko).Reading null as the default value when loading data for an external dictionary with the MongoDB source (Pavel Litvinenko).Reading DateTime values in the Values format from a Unix timestamp without single quotes.Failover is supported in remote table functions for cases when some of the replicas are missing the requested table.Configuration settings can be overridden in the command line when you run clickhouse-server. Example: clickhouse-server -- --logger.level=information.Implemented the empty function from a FixedString argument: the function returns 1 if the string consists entirely of null bytes (zhang2014).Added the listen_tryconfiguration parameter for listening to at least one of the listen addresses without quitting, if some of the addresses can’t be listened to (useful for systems with disabled support for IPv4 or IPv6).Added the VersionedCollapsingMergeTree table engine.Support for rows and arbitrary numeric types for the library dictionary source.MergeTree tables can be used without a primary key (you need to specify ORDER BY tuple()).A Nullable type can be CAST to a non-Nullable type if the argument is not NULL.RENAME TABLE can be performed for VIEW.Added the throwIf function.Added the odbc_default_field_size option, which allows you to extend the maximum size of the value loaded from an ODBC source (by default, it is 1024).The system.processes table and SHOW PROCESSLIST now have the is_cancelled and peak_memory_usage columns. Improvements:​ Limits and quotas on the result are no longer applied to intermediate data for INSERT SELECT queries or for SELECT subqueries.Fewer false triggers of force_restore_data when checking the status of Replicated tables when the server starts.Added the allow_distributed_ddl option.Nondeterministic functions are not allowed in expressions for MergeTree table keys.Files with substitutions from config.d directories are loaded in alphabetical order.Improved performance of the arrayElement function in the case of a constant multidimensional array with an empty array as one of the elements. Example: [[1], []][x].The server starts faster now when using configuration files with very large substitutions (for instance, very large lists of IP networks).When running a query, table valued functions run once. Previously, remote and mysql table valued functions performed the same query twice to retrieve the table structure from a remote server.The MkDocs documentation generator is used.When you try to delete a table column that DEFAULT/MATERIALIZED expressions of other columns depend on, an exception is thrown (zhang2014).Added the ability to parse an empty line in text formats as the number 0 for Float data types. This feature was previously available but was lost in release 1.1.54342.Enum values can be used in min, max, sum and some other functions. In these cases, it uses the corresponding numeric values. This feature was previously available but was lost in the release 1.1.54337.Added max_expanded_ast_elements to restrict the size of the AST after recursively expanding aliases. Bug Fixes:​ Fixed cases when unnecessary columns were removed from subqueries in error, or not removed from subqueries containing UNION ALL.Fixed a bug in merges for ReplacingMergeTree tables.Fixed synchronous insertions in Distributed tables (insert_distributed_sync = 1).Fixed segfault for certain uses of FULL and RIGHT JOIN with duplicate columns in subqueries.Fixed segfault for certain uses of replace_running_query and KILL QUERY.Fixed the order of the source and last_exception columns in the system.dictionaries table.Fixed a bug when the DROP DATABASE query did not delete the file with metadata.Fixed the DROP DATABASE query for Dictionary databases.Fixed the low precision of uniqHLL12 and uniqCombined functions for cardinalities greater than 100 million items (Alex Bocharov).Fixed the calculation of implicit default values when necessary to simultaneously calculate default explicit expressions in INSERT queries (zhang2014).Fixed a rare case when a query to a MergeTree table couldn’t finish (chenxing-xc).Fixed a crash that occurred when running a CHECK query for Distributed tables if all shards are local (chenxing.xc).Fixed a slight performance regression with functions that use regular expressions.Fixed a performance regression when creating multidimensional arrays from complex expressions.Fixed a bug that could cause an extra FORMAT section to appear in an .sql file with metadata.Fixed a bug that caused the max_table_size_to_drop limit to apply when trying to delete a MATERIALIZED VIEW looking at an explicitly specified table.Fixed incompatibility with old clients (old clients were sometimes sent data with the DateTime('timezone') type, which they do not understand).Fixed a bug when reading Nested column elements of structures that were added using ALTER but that are empty for the old partitions, when the conditions for these columns moved to PREWHERE.Fixed a bug when filtering tables by virtual _table columns in queries to Merge tables.Fixed a bug when using ALIAS columns in Distributed tables.Fixed a bug that made dynamic compilation impossible for queries with aggregate functions from the quantile family.Fixed a race condition in the query execution pipeline that occurred in very rare cases when using Merge tables with a large number of tables, and when using GLOBAL subqueries.Fixed a crash when passing arrays of different sizes to an arrayReduce function when using aggregate functions from multiple arguments.Prohibited the use of queries with UNION ALL in a MATERIALIZED VIEW.Fixed an error during initialization of the part_log system table when the server starts (by default, part_log is disabled). Backward Incompatible Changes:​ Removed the distributed_ddl_allow_replicated_alter option. This behavior is enabled by default.Removed the strict_insert_defaults setting. If you were using this functionality, write to feedback@clickhouse.com.Removed the UnsortedMergeTree engine. "},{"title":"ClickHouse Release 1.1.54343, 2018-02-05​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-1-1-54343-2018-02-05","content":"Added macros support for defining cluster names in distributed DDL queries and constructors of Distributed tables: CREATE TABLE distr ON CLUSTER '{cluster}' (...) ENGINE = Distributed('{cluster}', 'db', 'table').Now queries like SELECT ... FROM table WHERE expr IN (subquery) are processed using the table index.Improved processing of duplicates when inserting to Replicated tables, so they no longer slow down execution of the replication queue. "},{"title":"ClickHouse Release 1.1.54342, 2018-01-22​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-1-1-54342-2018-01-22","content":"This release contains bug fixes for the previous release 1.1.54337: Fixed a regression in 1.1.54337: if the default user has readonly access, then the server refuses to start up with the message Cannot create database in readonly mode.Fixed a regression in 1.1.54337: on systems with systemd, logs are always written to syslog regardless of the configuration; the watchdog script still uses init.d.Fixed a regression in 1.1.54337: wrong default configuration in the Docker image.Fixed nondeterministic behavior of GraphiteMergeTree (you can see it in log messages Data after merge is not byte-identical to the data on another replicas).Fixed a bug that may lead to inconsistent merges after OPTIMIZE query to Replicated tables (you may see it in log messages Part ... intersects the previous part).Buffer tables now work correctly when MATERIALIZED columns are present in the destination table (by zhang2014).Fixed a bug in implementation of NULL. "},{"title":"ClickHouse Release 1.1.54337, 2018-01-18​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#clickhouse-release-1-1-54337-2018-01-18","content":"New Features:​ Added support for storage of multi-dimensional arrays and tuples (Tuple data type) in tables.Support for table functions for DESCRIBE and INSERT queries. Added support for subqueries in DESCRIBE. Examples: DESC TABLE remote('host', default.hits); DESC TABLE (SELECT 1); INSERT INTO TABLE FUNCTION remote('host', default.hits). Support for INSERT INTO TABLE in addition to INSERT INTO.Improved support for time zones. The DateTime data type can be annotated with the timezone that is used for parsing and formatting in text formats. Example: DateTime('Asia/Istanbul'). When timezones are specified in functions for DateTime arguments, the return type will track the timezone, and the value will be displayed as expected.Added the functions toTimeZone, timeDiff, toQuarter, toRelativeQuarterNum. The toRelativeHour/Minute/Second functions can take a value of type Date as an argument. The now function name is case-sensitive.Added the toStartOfFifteenMinutes function (Kirill Shvakov).Added the clickhouse format tool for formatting queries.Added the format_schema_path configuration parameter (Marek Vavruşa). It is used for specifying a schema in Cap'n Proto format. Schema files can be located only in the specified directory.Added support for config substitutions (incl and conf.d) for configuration of external dictionaries and models (Pavel Yakunin).Added a column with documentation for the system.settings table (Kirill Shvakov).Added the system.parts_columns table with information about column sizes in each data part of MergeTree tables.Added the system.models table with information about loaded CatBoost machine learning models.Added the mysql and odbc table function and corresponding MySQL and ODBC table engines for accessing remote databases. This functionality is in the beta stage.Added the possibility to pass an argument of type AggregateFunction for the groupArray aggregate function (so you can create an array of states of some aggregate function).Removed restrictions on various combinations of aggregate function combinators. For example, you can use avgForEachIf as well as avgIfForEach aggregate functions, which have different behaviors.The -ForEach aggregate function combinator is extended for the case of aggregate functions of multiple arguments.Added support for aggregate functions of Nullable arguments even for cases when the function returns a non-Nullable result (added with the contribution of Silviu Caragea). Example: groupArray, groupUniqArray, topK.Added the max_client_network_bandwidth for clickhouse-client (Kirill Shvakov).Users with the readonly = 2 setting are allowed to work with TEMPORARY tables (CREATE, DROP, INSERT…) (Kirill Shvakov).Added support for using multiple consumers with the Kafka engine. Extended configuration options for Kafka (Marek Vavruša).Added the intExp3 and intExp4 functions.Added the sumKahan aggregate function.Added the to * Number* OrNull functions, where * Number* is a numeric type.Added support for WITH clauses for an INSERT SELECT query (author: zhang2014).Added settings: http_connection_timeout, http_send_timeout, http_receive_timeout. In particular, these settings are used for downloading data parts for replication. Changing these settings allows for faster failover if the network is overloaded.Added support for ALTER for tables of type Null (Anastasiya Tsarkova).The reinterpretAsString function is extended for all data types that are stored contiguously in memory.Added the --silent option for the clickhouse-local tool. It suppresses printing query execution info in stderr.Added support for reading values of type Date from text in a format where the month and/or day of the month is specified using a single digit instead of two digits (Amos Bird). Performance Optimizations:​ Improved performance of aggregate functions min, max, any, anyLast, anyHeavy, argMin, argMax from string arguments.Improved performance of the functions isInfinite, isFinite, isNaN, roundToExp2.Improved performance of parsing and formatting Date and DateTime type values in text format.Improved performance and precision of parsing floating point numbers.Lowered memory usage for JOIN in the case when the left and right parts have columns with identical names that are not contained in USING .Improved performance of aggregate functions varSamp, varPop, stddevSamp, stddevPop, covarSamp, covarPop, corr by reducing computational stability. The old functions are available under the names varSampStable, varPopStable, stddevSampStable, stddevPopStable, covarSampStable, covarPopStable, corrStable. Bug Fixes:​ Fixed data deduplication after running a DROP or DETACH PARTITION query. In the previous version, dropping a partition and inserting the same data again was not working because inserted blocks were considered duplicates.Fixed a bug that could lead to incorrect interpretation of the WHERE clause for CREATE MATERIALIZED VIEW queries with POPULATE .Fixed a bug in using the root_path parameter in the zookeeper_servers configuration.Fixed unexpected results of passing the Date argument to toStartOfDay .Fixed the addMonths and subtractMonths functions and the arithmetic for INTERVAL n MONTH in cases when the result has the previous year.Added missing support for the UUID data type for DISTINCT , JOIN , and uniq aggregate functions and external dictionaries (Evgeniy Ivanov). Support for UUID is still incomplete.Fixed SummingMergeTree behavior in cases when the rows summed to zero.Various fixes for the Kafka engine (Marek Vavruša).Fixed incorrect behavior of the Join table engine (Amos Bird).Fixed incorrect allocator behavior under FreeBSD and OS X.The extractAll function now supports empty matches.Fixed an error that blocked usage of libressl instead of openssl .Fixed the CREATE TABLE AS SELECT query from temporary tables.Fixed non-atomicity of updating the replication queue. This could lead to replicas being out of sync until the server restarts.Fixed possible overflow in gcd , lcm and modulo (% operator) (Maks Skorokhod).-preprocessed files are now created after changing umask (umask can be changed in the config).Fixed a bug in the background check of parts (MergeTreePartChecker ) when using a custom partition key.Fixed parsing of tuples (values of the Tuple data type) in text formats.Improved error messages about incompatible types passed to multiIf , array and some other functions.Redesigned support for Nullable types. Fixed bugs that may lead to a server crash. Fixed almost all other bugs related to NULL support: incorrect type conversions in INSERT SELECT, insufficient support for Nullable in HAVING and PREWHERE, join_use_nulls mode, Nullable types as arguments of OR operator, etc.Fixed various bugs related to internal semantics of data types. Examples: unnecessary summing of Enum type fields in SummingMergeTree ; alignment of Enum types in Pretty formats, etc.Stricter checks for allowed combinations of composite columns.Fixed the overflow when specifying a very large parameter for the FixedString data type.Fixed a bug in the topK aggregate function in a generic case.Added the missing check for equality of array sizes in arguments of n-ary variants of aggregate functions with an -Array combinator.Fixed a bug in --pager for clickhouse-client (author: ks1322).Fixed the precision of the exp10 function.Fixed the behavior of the visitParamExtract function for better compliance with documentation.Fixed the crash when incorrect data types are specified.Fixed the behavior of DISTINCT in the case when all columns are constants.Fixed query formatting in the case of using the tupleElement function with a complex constant expression as the tuple element index.Fixed a bug in Dictionary tables for range_hashed dictionaries.Fixed a bug that leads to excessive rows in the result of FULL and RIGHT JOIN (Amos Bird).Fixed a server crash when creating and removing temporary files in config.d directories during config reload.Fixed the SYSTEM DROP DNS CACHE query: the cache was flushed but addresses of cluster nodes were not updated.Fixed the behavior of MATERIALIZED VIEW after executing DETACH TABLE for the table under the view (Marek Vavruša). Build Improvements:​ The pbuilder tool is used for builds. The build process is almost completely independent of the build host environment.A single build is used for different OS versions. Packages and binaries have been made compatible with a wide range of Linux systems.Added the clickhouse-test package. It can be used to run functional tests.The source tarball can now be published to the repository. It can be used to reproduce the build without using GitHub.Added limited integration with Travis CI. Due to limits on build time in Travis, only the debug build is tested and a limited subset of tests are run.Added support for Cap'n'Proto in the default build.Changed the format of documentation sources from Restricted Text to Markdown.Added support for systemd (Vladimir Smirnov). It is disabled by default due to incompatibility with some OS images and can be enabled manually.For dynamic code generation, clang and lld are embedded into the clickhouse binary. They can also be invoked as clickhouse clang and clickhouse lld .Removed usage of GNU extensions from the code. Enabled the -Wextra option. When building with clang the default is libc++ instead of libstdc++.Extracted clickhouse_parsers and clickhouse_common_io libraries to speed up builds of various tools. Backward Incompatible Changes:​ The format for marks in Log type tables that contain Nullable columns was changed in a backward incompatible way. If you have these tables, you should convert them to the TinyLog type before starting up the new server version. To do this, replace ENGINE = Log with ENGINE = TinyLog in the corresponding .sql file in the metadata directory. If your table does not have Nullable columns or if the type of your table is not Log, then you do not need to do anything.Removed the experimental_allow_extended_storage_definition_syntax setting. Now this feature is enabled by default.The runningIncome function was renamed to runningDifferenceStartingWithFirstvalue to avoid confusion.Removed the FROM ARRAY JOIN arr syntax when ARRAY JOIN is specified directly after FROM with no table (Amos Bird).Removed the BlockTabSeparated format that was used solely for demonstration purposes.Changed the state format for aggregate functions varSamp, varPop, stddevSamp, stddevPop, covarSamp, covarPop, corr. If you have stored states of these aggregate functions in tables (using the AggregateFunction data type or materialized views with corresponding states), please write to feedback@clickhouse.com.In previous server versions there was an undocumented feature: if an aggregate function depends on parameters, you can still specify it without parameters in the AggregateFunction data type. Example: AggregateFunction(quantiles, UInt64) instead of AggregateFunction(quantiles(0.5, 0.9), UInt64). This feature was lost. Although it was undocumented, we plan to support it again in future releases.Enum data types cannot be used in min/max aggregate functions. This ability will be returned in the next release. Please Note When Upgrading:​ When doing a rolling update on a cluster, at the point when some of the replicas are running the old version of ClickHouse and some are running the new version, replication is temporarily stopped and the message unknown parameter 'shard' appears in the log. Replication will continue after all replicas of the cluster are updated.If different versions of ClickHouse are running on the cluster servers, it is possible that distributed queries using the following functions will have incorrect results: varSamp, varPop, stddevSamp, stddevPop, covarSamp, covarPop, corr. You should update all cluster nodes. "},{"title":"Changelog for 2017​","type":1,"pageTitle":"2018","url":"docs/en/whats-new/changelog/2018#changelog-for-2017","content":""},{"title":"sqlite","type":0,"sectionRef":"#","url":"docs/en/sql-reference/table-functions/sqlite","content":"","keywords":""},{"title":"sqlite​","type":1,"pageTitle":"sqlite","url":"docs/en/sql-reference/table-functions/sqlite#sqlite","content":"Allows to perform queries on a data stored in an SQLite database. Syntax  sqlite('db_path', 'table_name')  Arguments db_path — Path to a file with an SQLite database. String.table_name — Name of a table in the SQLite database. String. Returned value A table object with the same columns as in the original SQLite table. Example Query: SELECT * FROM sqlite('sqlite.db', 'table1') ORDER BY col2;  Result: ┌─col1──┬─col2─┐ │ line1 │ 1 │ │ line2 │ 2 │ │ line3 │ 3 │ └───────┴──────┘  See Also SQLite table engine "},{"title":"Security Changelog","type":0,"sectionRef":"#","url":"docs/en/whats-new/security-changelog","content":"","keywords":"clickhouse security changelog"},{"title":"Fixed in ClickHouse 21.4.3.21, 2021-04-12​","type":1,"pageTitle":"Security Changelog","url":"docs/en/whats-new/security-changelog#fixed-in-clickhouse-release-21-4-3-21-2021-04-12","content":""},{"title":"CVE-2021-25263​","type":1,"pageTitle":"Security Changelog","url":"docs/en/whats-new/security-changelog#cve-2021-25263","content":"An attacker that has CREATE DICTIONARY privilege, can read arbitary file outside permitted directory. Fix has been pushed to versions 20.8.18.32-lts, 21.1.9.41-stable, 21.2.9.41-stable, 21.3.6.55-lts, 21.4.3.21-stable and later. Credits: Vyacheslav Egoshin "},{"title":"Fixed in ClickHouse Release 19.14.3.3, 2019-09-10​","type":1,"pageTitle":"Security Changelog","url":"docs/en/whats-new/security-changelog#fixed-in-clickhouse-release-19-14-3-3-2019-09-10","content":""},{"title":"CVE-2019-15024​","type":1,"pageTitle":"Security Changelog","url":"docs/en/whats-new/security-changelog#cve-2019-15024","content":"Аn attacker that has write access to ZooKeeper and who can run a custom server available from the network where ClickHouse runs, can create a custom-built malicious server that will act as a ClickHouse replica and register it in ZooKeeper. When another replica will fetch data part from the malicious replica, it can force clickhouse-server to write to arbitrary path on filesystem. Credits: Eldar Zaitov of Yandex Information Security Team "},{"title":"CVE-2019-16535​","type":1,"pageTitle":"Security Changelog","url":"docs/en/whats-new/security-changelog#cve-2019-16535","content":"Аn OOB read, OOB write and integer underflow in decompression algorithms can be used to achieve RCE or DoS via native protocol. Credits: Eldar Zaitov of Yandex Information Security Team "},{"title":"CVE-2019-16536​","type":1,"pageTitle":"Security Changelog","url":"docs/en/whats-new/security-changelog#cve-2019-16536","content":"Stack overflow leading to DoS can be triggered by a malicious authenticated client. Credits: Eldar Zaitov of Yandex Information Security Team "},{"title":"Fixed in ClickHouse Release 19.13.6.1, 2019-09-20​","type":1,"pageTitle":"Security Changelog","url":"docs/en/whats-new/security-changelog#fixed-in-clickhouse-release-19-13-6-1-2019-09-20","content":""},{"title":"CVE-2019-18657​","type":1,"pageTitle":"Security Changelog","url":"docs/en/whats-new/security-changelog#cve-2019-18657","content":"Table function url had the vulnerability allowed the attacker to inject arbitrary HTTP headers in the request. Credits: Nikita Tikhomirov "},{"title":"Fixed in ClickHouse Release 18.12.13, 2018-09-10​","type":1,"pageTitle":"Security Changelog","url":"docs/en/whats-new/security-changelog#fixed-in-clickhouse-release-18-12-13-2018-09-10","content":""},{"title":"CVE-2018-14672​","type":1,"pageTitle":"Security Changelog","url":"docs/en/whats-new/security-changelog#cve-2018-14672","content":"Functions for loading CatBoost models allowed path traversal and reading arbitrary files through error messages. Credits: Andrey Krasichkov of Yandex Information Security Team "},{"title":"Fixed in ClickHouse Release 18.10.3, 2018-08-13​","type":1,"pageTitle":"Security Changelog","url":"docs/en/whats-new/security-changelog#fixed-in-clickhouse-release-18-10-3-2018-08-13","content":""},{"title":"CVE-2018-14671​","type":1,"pageTitle":"Security Changelog","url":"docs/en/whats-new/security-changelog#cve-2018-14671","content":"unixODBC allowed loading arbitrary shared objects from the file system which led to a Remote Code Execution vulnerability. Credits: Andrey Krasichkov and Evgeny Sidorov of Yandex Information Security Team "},{"title":"Fixed in ClickHouse Release 1.1.54388, 2018-06-28​","type":1,"pageTitle":"Security Changelog","url":"docs/en/whats-new/security-changelog#fixed-in-clickhouse-release-1-1-54388-2018-06-28","content":""},{"title":"CVE-2018-14668​","type":1,"pageTitle":"Security Changelog","url":"docs/en/whats-new/security-changelog#cve-2018-14668","content":"“remote” table function allowed arbitrary symbols in “user”, “password” and “default_database” fields which led to Cross Protocol Request Forgery Attacks. Credits: Andrey Krasichkov of Yandex Information Security Team "},{"title":"Fixed in ClickHouse Release 1.1.54390, 2018-07-06​","type":1,"pageTitle":"Security Changelog","url":"docs/en/whats-new/security-changelog#fixed-in-clickhouse-release-1-1-54390-2018-07-06","content":""},{"title":"CVE-2018-14669​","type":1,"pageTitle":"Security Changelog","url":"docs/en/whats-new/security-changelog#cve-2018-14669","content":"ClickHouse MySQL client had “LOAD DATA LOCAL INFILE” functionality enabled that allowed a malicious MySQL database read arbitrary files from the connected ClickHouse server. Credits: Andrey Krasichkov and Evgeny Sidorov of Yandex Information Security Team "},{"title":"Fixed in ClickHouse Release 1.1.54131, 2017-01-10​","type":1,"pageTitle":"Security Changelog","url":"docs/en/whats-new/security-changelog#fixed-in-clickhouse-release-1-1-54131-2017-01-10","content":""},{"title":"CVE-2018-14670​","type":1,"pageTitle":"Security Changelog","url":"docs/en/whats-new/security-changelog#cve-2018-14670","content":"Incorrect configuration in deb package could lead to the unauthorized use of the database. Credits: the UK’s National Cyber Security Centre (NCSC) "},{"title":"Roadmap","type":0,"sectionRef":"#","url":"docs/en/whats-new/roadmap","content":"Roadmap The roadmap for the year 2022 is published for open discussion here.","keywords":""},{"title":"ClickHouse Changelog","type":0,"sectionRef":"#","url":"docs/en/whats-new/changelog/","content":"","keywords":"clickhouse changelog"},{"title":"Table of Contents​","type":1,"pageTitle":"ClickHouse Changelog","url":"docs/en/whats-new/changelog/#table-of-contents","content":"ClickHouse release v22.3-lts, 2022-03-17 ClickHouse release v22.2, 2022-02-17 ClickHouse release v22.1, 2022-01-18 Changelog for 2021  "},{"title":" ClickHouse release v22.3-lts, 2022-03-17​","type":1,"pageTitle":"ClickHouse Changelog","url":"docs/en/whats-new/changelog/#-clickhouse-release-v223-lts-2022-03-17","content":"Backward Incompatible Change​ Make arrayCompact function behave as other higher-order functions: perform compaction not of lambda function results but on the original array. If you're using nontrivial lambda functions in arrayCompact you may restore old behaviour by wrapping arrayCompact arguments into arrayMap. Closes #34010 #18535 #14778. #34795 (Alexandre Snarskii).Change implementation specific behavior on overflow of function toDatetime. It will be saturated to the nearest min/max supported instant of datetime instead of wraparound. This change is highlighted as &quot;backward incompatible&quot; because someone may unintentionally rely on the old behavior. #32898 (HaiBo Li).Make function cast(value, 'IPv4'), cast(value, 'IPv6') behave same as toIPv4, toIPv6 functions. Changed behavior of incorrect IP address passed into functions toIPv4, toIPv6, now if invalid IP address passes into this functions exception will be raised, before this function return default value. Added functions IPv4StringToNumOrDefault, IPv4StringToNumOrNull, IPv6StringToNumOrDefault, IPv6StringOrNull toIPv4OrDefault, toIPv4OrNull, toIPv6OrDefault, toIPv6OrNull. Functions IPv4StringToNumOrDefault , toIPv4OrDefault , toIPv6OrDefault should be used if previous logic relied on IPv4StringToNum, toIPv4, toIPv6 returning default value for invalid address. Added setting cast_ipv4_ipv6_default_on_conversion_error, if this setting enabled, then IP address conversion functions will behave as before. Closes #22825. Closes #5799. Closes #35156. #35240 (Maksim Kita). New Feature​ Support for caching data locally for remote filesystems. It can be enabled for s3 disks. Closes #28961. #33717 (Kseniia Sumarokova). In the meantime, we enabled the test suite on s3 filesystem and no more known issues exist, so it is started to be production ready.Add new table function hive. It can be used as follows hive('&lt;hive metastore url&gt;', '&lt;hive database&gt;', '&lt;hive table name&gt;', '&lt;columns definition&gt;', '&lt;partition columns&gt;') for example SELECT * FROM hive('thrift://hivetest:9083', 'test', 'demo', 'id Nullable(String), score Nullable(Int32), day Nullable(String)', 'day'). #34946 (lgbo).Support authentication of users connected via SSL by their X.509 certificate. #31484 (eungenue).Support schema inference for inserting into table functions file/hdfs/s3/url. #34732 (Kruglov Pavel).Now you can read system.zookeeper table without restrictions on path or using like expression. This reads can generate quite heavy load for zookeeper so to enable this ability you have to enable setting allow_unrestricted_reads_from_keeper. #34609 (Sergei Trifonov).Display CPU and memory metrics in clickhouse-local. Close #34545. #34605 (李扬).Implement startsWith and endsWith function for arrays, closes #33982. #34368 (usurai).Add three functions for Map data type: 1. mapReplace(map1, map2) - replaces values for keys in map1 with the values of the corresponding keys in map2; adds keys from map2 that don't exist in map1. 2. mapFilter 3. mapMap. mapFilter and mapMap are higher order functions, accepting two arguments, the first argument is a lambda function with k, v pair as arguments, the second argument is a column of type Map. #33698 (hexiaoting).Allow getting default user and password for clickhouse-client from the CLICKHOUSE_USER and CLICKHOUSE_PASSWORD environment variables. Close #34538. #34947 (DR). Experimental Feature​ New data type Object(&lt;schema_format&gt;), which supports storing of semi-structured data (for now JSON only). Data is written to such types as string. Then all paths are extracted according to format of semi-structured data and written as separate columns in most optimal types, that can store all their values. Those columns can be queried by names that match paths in source data. E.g data.key1.key2 or with cast operator data.key1.key2::Int64.Add database_replicated_allow_only_replicated_engine setting. When enabled, it only allowed to only create Replicated tables or tables with stateless engines in Replicated databases. #35214 (Nikolai Kochetov). Note that Replicated database is still an experimental feature. Performance Improvement​ Improve performance of insertion into MergeTree tables by optimizing sorting. Up to 2x improvement is observed on realistic benchmarks. #34750 (Maksim Kita).Columns pruning when reading Parquet, ORC and Arrow files from URL and S3. Closes #34163. #34849 (Kseniia Sumarokova).Columns pruning when reading Parquet, ORC and Arrow files from Hive. #34954 (lgbo).A bunch of performance optimizations from a performance superhero. Improve performance of processing queries with large IN section. Improve performance of direct dictionary if its source is ClickHouse. Improve performance of detectCharset , detectLanguageUnknown functions. #34888 (Maksim Kita).Improve performance of any aggregate function by using more batching. #34760 (Raúl Marín).Multiple improvements for performance of clickhouse-keeper: less locking #35010 (zhanglistar), lower memory usage by streaming reading and writing of snapshot instead of full copy. #34584 (zhanglistar), optimizing compaction of log store in the RAFT implementation. #34534 (zhanglistar), versioning of the internal data structure #34486 (zhanglistar). Improvement​ Allow asynchronous inserts to table functions. Fixes #34864. #34866 (Anton Popov).Implicit type casting of the key argument for functions dictGetHierarchy, dictIsIn, dictGetChildren, dictGetDescendants. Closes #34970. #35027 (Maksim Kita).EXPLAIN AST query can output AST in form of a graph in Graphviz format: EXPLAIN AST graph = 1 SELECT * FROM system.parts. #35173 (李扬).When large files were written with s3 table function or table engine, the content type on the files was mistakenly set to application/xml due to a bug in the AWS SDK. This closes #33964. #34433 (Alexey Milovidov).Change restrictive row policies a bit to make them an easier alternative to permissive policies in easy cases. If for a particular table only restrictive policies exist (without permissive policies) users will be able to see some rows. Also SHOW CREATE ROW POLICY will always show AS permissive or AS restrictive in row policy's definition. #34596 (Vitaly Baranov).Improve schema inference with globs in File/S3/HDFS/URL engines. Try to use the next path for schema inference in case of error. #34465 (Kruglov Pavel).Play UI now correctly detects the preferred light/dark theme from the OS. #35068 (peledni).Added date_time_input_format = 'best_effort_us'. Closes #34799. #34982 (WenYao).A new settings called allow_plaintext_password and allow_no_password are added in server configuration which turn on/off authentication types that can be potentially insecure in some environments. They are allowed by default. #34738 (Heena Bansal).Support for DateTime64 data type in Arrow format, closes #8280 and closes #28574. #34561 (李扬).Reload remote_url_allow_hosts (filtering of outgoing connections) on config update. #35294 (Nikolai Kochetov).Support --testmode parameter for clickhouse-local. This parameter enables interpretation of test hints that we use in functional tests. #35264 (Kseniia Sumarokova).Add distributed_depth to query log. It is like a more detailed variant of is_initial_query #35207 (李扬).Respect remote_url_allow_hosts for MySQL and PostgreSQL table functions. #35191 (Heena Bansal).Added disk_name field to system.part_log. #35178 (Artyom Yurkov).Do not retry non-rertiable errors when querying remote URLs. Closes #35161. #35172 (Kseniia Sumarokova).Support distributed INSERT SELECT queries (the setting parallel_distributed_insert_select) table function view(). #35132 (Azat Khuzhin).More precise memory tracking during INSERT into Buffer with AggregateFunction. #35072 (Azat Khuzhin).Avoid division by zero in Query Profiler if Linux kernel has a bug. Closes #34787. #35032 (Alexey Milovidov).Add more sanity checks for keeper configuration: now mixing of localhost and non-local servers is not allowed, also add checks for same value of internal raft port and keeper client port. #35004 (alesapin).Currently, if the user changes the settings of the system tables there will be tons of logs and ClickHouse will rename the tables every minute. This fixes #34929. #34949 (Nikita Mikhaylov).Use connection pool for Hive metastore client. #34940 (lgbo).Ignore per-column TTL in CREATE TABLE AS if new table engine does not support it (i.e. if the engine is not of MergeTree family). #34938 (Azat Khuzhin).Allow LowCardinality strings for ngrambf_v1/tokenbf_v1 indexes. Closes #21865. #34911 (Lars Hiller Eidnes).Allow opening empty sqlite db if the file doesn't exist. Closes #33367. #34907 (Kseniia Sumarokova).Implement memory statistics for FreeBSD - this is required for max_server_memory_usage to work correctly. #34902 (Alexandre Snarskii).In previous versions the progress bar in clickhouse-client can jump forward near 50% for no reason. This closes #34324. #34801 (Alexey Milovidov).Now ALTER TABLE DROP COLUMN columnX queries for MergeTree table engines will work instantly when columnX is an ALIAS column. Fixes #34660. #34786 (alesapin).Show hints when user mistyped the name of a data skipping index. Closes #29698. #34764 (flynn).Support remote()/cluster() table functions for parallel_distributed_insert_select. #34728 (Azat Khuzhin).Do not reset logging that configured via --log-file/--errorlog-file command line options in case of empty configuration in the config file. #34718 (Amos Bird).Extract schema only once on table creation and prevent reading from local files/external sources to extract schema on each server startup. #34684 (Kruglov Pavel).Allow specifying argument names for executable UDFs. This is necessary for formats where argument name is part of serialization, like Native, JSONEachRow. Closes #34604. #34653 (Maksim Kita).MaterializedMySQL (experimental feature) now supports materialized_mysql_tables_list (a comma-separated list of MySQL database tables, which will be replicated by the MaterializedMySQL database engine. Default value: empty list — means all the tables will be replicated), mentioned at #32977. #34487 (zzsmdfj).Improve OpenTelemetry span logs for INSERT operation on distributed table. #34480 (Frank Chen).Make the znode ctime and mtime consistent between servers in ClickHouse Keeper. #33441 (小路). Build/Testing/Packaging Improvement​ Package repository is migrated to JFrog Artifactory (Mikhail f. Shiryaev).Randomize some settings in functional tests, so more possible combinations of settings will be tested. This is yet another fuzzing method to ensure better test coverage. This closes #32268. #34092 (Kruglov Pavel).Drop PVS-Studio from our CI. #34680 (Mikhail f. Shiryaev).Add an ability to build stripped binaries with CMake. In previous versions it was performed by dh-tools. #35196 (alesapin).Smaller &quot;fat-free&quot; clickhouse-keeper build. #35031 (alesapin).Use @robot-clickhouse as an author and committer for PRs like https://github.com/ClickHouse/ClickHouse/pull/34685. #34793 (Mikhail f. Shiryaev).Limit DWARF version for debug info by 4 max, because our internal stack symbolizer cannot parse DWARF version 5. This makes sense if you compile ClickHouse with clang-15. #34777 (Alexey Milovidov).Remove clickhouse-test debian package as unneeded complication. CI use tests from repository and standalone testing via deb package is no longer supported. #34606 (Ilya Yatsishin). Bug Fix (user-visible misbehaviour in official stable or prestable release)​ A fix for HDFS integration: When the inner buffer size is too small, NEED_MORE_INPUT in HadoopSnappyDecoder will run multi times (&gt;=3) for one compressed block. This makes the input data be copied into the wrong place in HadoopSnappyDecoder::buffer. #35116 (lgbo).Ignore obsolete grants in ATTACH GRANT statements. This PR fixes #34815. #34855 (Vitaly Baranov).Fix segfault in Postgres database when getting create table query if database was created using named collections. Closes #35312. #35313 (Kseniia Sumarokova).Fix partial merge join duplicate rows bug, close #31009. #35311 (Vladimir C).Fix possible Assertion 'position() != working_buffer.end()' failed while using bzip2 compression with small max_read_buffer_size setting value. The bug was found in https://github.com/ClickHouse/ClickHouse/pull/35047. #35300 (Kruglov Pavel). While using lz4 compression with a small max_read_buffer_size setting value. #35296 (Kruglov Pavel). While using lzma compression with small max_read_buffer_size setting value. #35295 (Kruglov Pavel). While using brotli compression with a small max_read_buffer_size setting value. The bug was found in https://github.com/ClickHouse/ClickHouse/pull/35047. #35281 (Kruglov Pavel).Fix possible segfault in JSONEachRow schema inference. #35291 (Kruglov Pavel).Fix CHECK TABLE query in case when sparse columns are enabled in table. #35274 (Anton Popov).Avoid std::terminate in case of exception in reading from remote VFS. #35257 (Azat Khuzhin).Fix reading port from config, close #34776. #35193 (Vladimir C).Fix error in query with WITH TOTALS in case if HAVING returned empty result. This fixes #33711. #35186 (Amos Bird).Fix a corner case of replaceRegexpAll, close #35117. #35182 (Vladimir C).Schema inference didn't work properly on case of INSERT INTO FUNCTION s3(...) FROM ..., it tried to read schema from s3 file instead of from select query. #35176 (Kruglov Pavel).Fix MaterializedPostgreSQL (experimental feature) table overrides for partition by, etc. Closes #35048. #35162 (Kseniia Sumarokova).Fix MaterializedPostgreSQL (experimental feature) adding new table to replication (ATTACH TABLE) after manually removing (DETACH TABLE). Closes #33800. Closes #34922. Closes #34315. #35158 (Kseniia Sumarokova).Fix partition pruning error when non-monotonic function is used with IN operator. This fixes #35136. #35146 (Amos Bird).Fixed slightly incorrect translation of YAML configs to XML. #35135 (Miel Donkers).Fix optimize_skip_unused_shards_rewrite_in for signed columns and negative values. #35134 (Azat Khuzhin).The update_lag external dictionary configuration option was unusable showing the error message Unexpected key `update_lag` in dictionary source configuration. #35089 (Jason Chu).Avoid possible deadlock on server shutdown. #35081 (Azat Khuzhin).Fix missing alias after function is optimized to a subcolumn when setting optimize_functions_to_subcolumns is enabled. Closes #33798. #35079 (qieqieplus).Fix reading from system.asynchronous_inserts table if there exists asynchronous insert into table function. #35050 (Anton Popov).Fix possible exception Reading for MergeTree family tables must be done with last position boundary (relevant to operation on remote VFS). Closes #34979. #35001 (Kseniia Sumarokova).Fix unexpected result when use -State type aggregate function in window frame. #34999 (metahys).Fix possible segfault in FileLog (experimental feature). Closes #30749. #34996 (Kseniia Sumarokova).Fix possible rare error Cannot push block to port which already has data. #34993 (Nikolai Kochetov).Fix wrong schema inference for unquoted dates in CSV. Closes #34768. #34961 (Kruglov Pavel).Integration with Hive: Fix unexpected result when use in in where in hive query. #34945 (lgbo).Avoid busy polling in ClickHouse Keeper while searching for changelog files to delete. #34931 (Azat Khuzhin).Fix DateTime64 conversion from PostgreSQL. Closes #33364. #34910 (Kseniia Sumarokova).Fix possible &quot;Part directory doesn't exist&quot; during INSERT into MergeTree table backed by VFS over s3. #34876 (Azat Khuzhin).Support DDLs like CREATE USER to be executed on cross replicated cluster. #34860 (Jianmei Zhang).Fix bugs for multiple columns group by in WindowView (experimental feature). #34859 (vxider).Fix possible failures in S2 functions when queries contain const columns. #34745 (Bharat Nallan).Fix bug for H3 funcs containing const columns which cause queries to fail. #34743 (Bharat Nallan).Fix No such file or directory with enabled fsync_part_directory and vertical merge. #34739 (Azat Khuzhin).Fix serialization/printing for system queries RELOAD MODEL, RELOAD FUNCTION, RESTART DISK when used ON CLUSTER. Closes #34514. #34696 (Maksim Kita).Fix allow_experimental_projection_optimization with enable_global_with_statement (before it may lead to Stack size too large error in case of multiple expressions in WITH clause, and also it executes scalar subqueries again and again, so not it will be more optimal). #34650 (Azat Khuzhin).Stop to select part for mutate when the other replica has already updated the transaction log for ReplatedMergeTree engine. #34633 (Jianmei Zhang).Fix incorrect result of trivial count query when part movement feature is used #34089. #34385 (nvartolomei).Fix inconsistency of max_query_size limitation in distributed subqueries. #34078 (Chao Ma). "},{"title":" ClickHouse release v22.2, 2022-02-17​","type":1,"pageTitle":"ClickHouse Changelog","url":"docs/en/whats-new/changelog/#-clickhouse-release-v222-2022-02-17","content":"Upgrade Notes​ Applying data skipping indexes for queries with FINAL may produce incorrect result. In this release we disabled data skipping indexes by default for queries with FINAL (a new setting use_skip_indexes_if_final is introduced and disabled by default). #34243 (Azat Khuzhin). New Feature​ Projections are production ready. Set allow_experimental_projection_optimization by default and deprecate this setting. #34456 (Nikolai Kochetov).An option to create a new files on insert for File/S3/HDFS engines. Allow to overwrite a file in HDFS. Throw an exception in attempt to overwrite a file in S3 by default. Throw an exception in attempt to append data to file in formats that have a suffix (and thus don't support appends, like Parquet, ORC). Closes #31640 Closes #31622 Closes #23862 Closes #15022 Closes #16674. #33302 (Kruglov Pavel).Add a setting that allows a user to provide own deduplication semantic in MergeTree/ReplicatedMergeTree If provided, it's used instead of data digest to generate block ID. So, for example, by providing a unique value for the setting in each INSERT statement, the user can avoid the same inserted data being deduplicated. This closes: #7461. #32304 (Igor Nikonov).Add support of DEFAULT keyword for INSERT statements. Closes #6331. #33141 (Andrii Buriachevskyi).EPHEMERAL column specifier is added to CREATE TABLE query. Closes #9436. #34424 (yakov-olkhovskiy).Support IF EXISTS clause for TTL expr TO [DISK|VOLUME] [IF EXISTS] 'xxx' feature. Parts will be moved to disk or volume only if it exists on replica, so MOVE TTL rules will be able to behave differently on replicas according to the existing storage policies. Resolves #34455. #34504 (Anton Popov).Allow set default table engine and to create tables without specifying ENGINE. #34187 (Ilya Yatsishin).Add table function format(format_name, data). #34125 (Kruglov Pavel).Detect format in clickhouse-local by file name even in the case when it is passed to stdin. #33829 (Kruglov Pavel).Add schema inference for values table function. Closes #33811. #34017 (Kruglov Pavel).Dynamic reload of server TLS certificates on config reload. Closes #15764. #15765 (johnskopis). #31257 (Filatenkov Artur).Now ReplicatedMergeTree can recover data when some of its disks are broken. #13544 (Amos Bird).Fault-tolerant connections in clickhouse-client: clickhouse-client ... --host host1 --host host2 --port port2 --host host3 --port port --host host4. #34490 (Kruglov Pavel). #33824 (Filippov Denis).Add DEGREES and RADIANS functions for MySQL compatibility. #33769 (Bharat Nallan).Add h3ToCenterChild function. #33313 (Bharat Nallan). Add new h3 miscellaneous functions: edgeLengthKm,exactEdgeLengthKm,exactEdgeLengthM,exactEdgeLengthRads,numHexagons. #33621 (Bharat Nallan).Add function bitSlice to extract bit subsequences from String/FixedString. #33360 (RogerYK).Implemented meanZTest aggregate function. #33354 (achimbab).Add confidence intervals to T-tests aggregate functions. #33260 (achimbab).Add function addressToLineWithInlines. Close #26211. #33467 (SuperDJY).Added #! and # as a recognised start of a single line comment. Closes #34138. #34230 (Aaron Katz). Experimental Feature​ Functions for text classification: language and charset detection. See #23271. #33314 (Nikolay Degterinsky).Add memory overcommit to MemoryTracker. Added guaranteed settings for memory limits which represent soft memory limits. In case when hard memory limit is reached, MemoryTracker tries to cancel the most overcommited query. New setting memory_usage_overcommit_max_wait_microseconds specifies how long queries may wait another query to stop. Closes #28375. #31182 (Dmitry Novik).Enable stream to table join in WindowView. #33729 (vxider).Support SET, YEAR, TIME and GEOMETRY data types in MaterializedMySQL (experimental feature). Fixes #18091, #21536, #26361. #33429 (zzsmdfj).Fix various issues when projection is enabled by default. Each issue is described in separate commit. This is for #33678 . This fixes #34273. #34305 (Amos Bird). Performance Improvement​ Support optimize_read_in_order if prefix of sorting key is already sorted. E.g. if we have sorting key ORDER BY (a, b) in table and query with WHERE a = const ORDER BY b clauses, now it will be applied reading in order of sorting key instead of full sort. #32748 (Anton Popov).Improve performance of partitioned insert into table functions URL, S3, File, HDFS. Closes #34348. #34510 (Maksim Kita).Multiple performance improvements of clickhouse-keeper. #34484 #34587 (zhanglistar).FlatDictionary improve performance of dictionary data load. #33871 (Maksim Kita).Improve performance of mapPopulateSeries function. Closes #33944. #34318 (Maksim Kita)._file and _path virtual columns (in file-like table engines) are made LowCardinality - it will make queries for multiple files faster. Closes #34300. #34317 (flynn).Speed up loading of data parts. It was not parallelized before: the setting part_loading_threads did not have effect. See #4699. #34310 (alexey-milovidov).Improve performance of LineAsString format. This closes #34303. #34306 (alexey-milovidov).Optimize quantilesExact{Low,High} to use nth_element instead of sort. #34287 (Danila Kutenin).Slightly improve performance of Regexp format. #34202 (alexey-milovidov).Minor improvement for analysis of scalar subqueries. #34128 (Federico Rodriguez).Make ORDER BY tuple almost as fast as ORDER BY columns. We have special optimizations for multiple column ORDER BY: https://github.com/ClickHouse/ClickHouse/pull/10831 . It's beneficial to also apply to tuple columns. #34060 (Amos Bird).Rework and reintroduce the scalar subqueries cache to Materialized Views execution. #33958 (Raúl Marín).Slightly improve performance of ORDER BY by adding x86-64 AVX-512 support for memcmpSmall functions to accelerate memory comparison. It works only if you compile ClickHouse by yourself. #33706 (hanqf-git).Improve range_hashed dictionary performance if for key there are a lot of intervals. Fixes #23821. #33516 (Maksim Kita).For inserts and merges into S3, write files in parallel whenever possible (TODO: check if it's merged). #33291 (Nikolai Kochetov).Improve clickhouse-keeper performance and fix several memory leaks in NuRaft library. #33329 (alesapin). Improvement​ Support asynchronous inserts in clickhouse-client for queries with inlined data. #34267 (Anton Popov).Functions dictGet, dictHas implicitly cast key argument to dictionary key structure, if they are different. #33672 (Maksim Kita).Improvements for range_hashed dictionaries. Improve performance of load time if there are multiple attributes. Allow to create a dictionary without attributes. Added option to specify strategy when intervals start and end have Nullable type convert_null_range_bound_to_open by default is true. Closes #29791. Allow to specify Float, Decimal, DateTime64, Int128, Int256, UInt128, UInt256 as range types. RangeHashedDictionary added support for range values that extend Int64 type. Closes #28322. Added option range_lookup_strategy to specify range lookup type min, max by default is min . Closes #21647. Fixed allocated bytes calculations. Fixed type name in system.dictionaries in case of ComplexKeyHashedDictionary. #33927 (Maksim Kita).flat, hashed, hashed_array dictionaries now support creating with empty attributes, with support of reading the keys and using dictHas. Fixes #33820. #33918 (Maksim Kita).Added support for DateTime64 data type in dictionaries. #33914 (Maksim Kita).Allow to write s3(url, access_key_id, secret_access_key) (autodetect of data format and table structure, but with explicit credentials). #34503 (Kruglov Pavel).Added sending of the output format back to client like it's done in HTTP protocol as suggested in #34362. Closes #34362. #34499 (Vitaly Baranov).Send ProfileEvents statistics in case of INSERT SELECT query (to display query metrics in clickhouse-client for this type of queries). #34498 (Dmitry Novik).Recognize .jsonl extension for JSONEachRow format. #34496 (Kruglov Pavel).Improve schema inference in clickhouse-local. Allow to write just clickhouse-local -q &quot;select * from table&quot; &lt; data.format. #34495 (Kruglov Pavel).Privileges CREATE/ALTER/DROP ROW POLICY now can be granted on a table or on database.* as well as globally *.*. #34489 (Vitaly Baranov).Allow to export arbitrary large files to s3. Add two new settings: s3_upload_part_size_multiply_factor and s3_upload_part_size_multiply_parts_count_threshold. Now each time s3_upload_part_size_multiply_parts_count_threshold uploaded to S3 from a single query s3_min_upload_part_size multiplied by s3_upload_part_size_multiply_factor. Fixes #34244. #34422 (alesapin).Allow to skip not found (404) URLs for globs when using URL storage / table function. Also closes #34359. #34392 (Kseniia Sumarokova).Default input and output formats for clickhouse-local that can be overriden by --input-format and --output-format. Close #30631. #34352 (李扬).Add options for clickhouse-format. Which close #30528 - max_query_size - max_parser_depth. #34349 (李扬).Better handling of pre-inputs before client start. This is for #34308. #34336 (Amos Bird).REGEXP_MATCHES and REGEXP_REPLACE function aliases for compatibility with PostgreSQL. Close #30885. #34334 (李扬).Some servers expect a User-Agent header in their HTTP requests. A User-Agent header entry has been added to HTTP requests of the form: User-Agent: ClickHouse/VERSION_STRING. #34330 (Saad Ur Rahman).Cancel merges before acquiring table lock for TRUNCATE query to avoid DEADLOCK_AVOIDED error in some cases. Fixes #34302. #34304 (tavplubix).Change severity of the &quot;Cancelled merging parts&quot; message in logs, because it's not an error. This closes #34148. #34232 (alexey-milovidov).Add ability to compose PostgreSQL-style cast operator :: with expressions using [] and . operators (array and tuple indexing). #34229 (Nikolay Degterinsky).Recognize YYYYMMDD-hhmmss format in parseDateTimeBestEffort function. This closes #34206. #34208 (alexey-milovidov).Allow carriage return in the middle of the line while parsing by Regexp format. This closes #34200. #34205 (alexey-milovidov).Allow to parse dictionary's PRIMARY KEY as PRIMARY KEY (id, value); previously supported only PRIMARY KEY id, value. Closes #34135. #34141 (Maksim Kita).An optional argument for splitByChar to limit the number of resulting elements. close #34081. #34140 (李扬).Improving the experience of multiple line editing for clickhouse-client. This is a follow-up of #31123. #34114 (Amos Bird).Add UUID suport in MsgPack input/output format. #34065 (Kruglov Pavel).Tracing context (for OpenTelemetry) is now propagated from GRPC client metadata (this change is relevant for GRPC client-server protocol). #34064 (andremarianiello).Supports all types of SYSTEM queries with ON CLUSTER clause. #34005 (小路).Improve memory accounting for queries that are using less than max_untracker_memory. #34001 (Azat Khuzhin).Fixed UTF-8 string case-insensitive search when lowercase and uppercase characters are represented by different number of bytes. Example is ẞ and ß. This closes #7334. #33992 (Harry Lee).Detect format and schema from stdin in clickhouse-local. #33960 (Kruglov Pavel).Correctly handle the case of misconfiguration when multiple disks are using the same path on the filesystem. #29072. #33905 (zhongyuankai).Try every resolved IP address while getting S3 proxy. S3 proxies are rarely used, mostly in Yandex Cloud. #33862 (Nikolai Kochetov).Support EXPLAIN AST CREATE FUNCTION query EXPLAIN AST CREATE FUNCTION mycast AS (n) -&gt; cast(n as String) will return EXPLAIN AST CREATE FUNCTION mycast AS n -&gt; CAST(n, 'String'). #33819 (李扬).Added support for cast from Map(Key, Value) to Array(Tuple(Key, Value)). #33794 (Maksim Kita).Add some improvements and fixes for Bool data type. Fixes #33244. #33737 (Kruglov Pavel).Parse and store OpenTelemetry trace-id in big-endian order. #33723 (Frank Chen).Improvement for fromUnixTimestamp64 family functions.. They now accept any integer value that can be converted to Int64. This closes: #14648. #33505 (Andrey Zvonov).Reimplement _shard_num from constants (see #7624) with shardNum() function (seee #27020), to avoid possible issues (like those that had been found in #16947). #33392 (Azat Khuzhin).Enable binary arithmetic (plus, minus, multiply, division, least, greatest) between Decimal and Float. #33355 (flynn).Respect cgroups limits in max_threads autodetection. #33342 (JaySon).Add new clickhouse-keeper setting min_session_timeout_ms. Now clickhouse-keeper will determine client session timeout according to min_session_timeout_ms and session_timeout_ms settings. #33288 (JackyWoo).Added UUID data type support for functions hex and bin. #32170 (Frank Chen).Fix reading of subcolumns with dots in their names. In particular fixed reading of Nested columns, if their element names contain dots (e.g Nested(`keys.name` String, `keys.id` UInt64, values UInt64)). #34228 (Anton Popov).Fixes parallel_view_processing = 0 not working when inserting into a table using VALUES. - Fixes view_duration_ms in the query_views_log not being set correctly for materialized views. #34067 (Raúl Marín).Fix parsing tables structure from ZooKeeper: now metadata from ZooKeeper compared with local metadata in canonical form. It helps when canonical function names can change between ClickHouse versions. #33933 (sunny).Properly escape some characters for interaction with LDAP. #33401 (IlyaTsoi). Build/Testing/Packaging Improvement​ Remove unbundled build support. #33690 (Azat Khuzhin).Ensure that tests don't depend on the result of non-stable sorting of equal elements. Added equal items ranges randomization in debug after sort to prevent issues when we rely on equal items sort order. #34393 (Maksim Kita).Add verbosity to a style check. #34289 (Mikhail f. Shiryaev).Remove clickhouse-test debian package because it's obsolete. #33948 (Ilya Yatsishin).Multiple improvements for build system to remove the possibility of occasionally using packages from the OS and to enforce hermetic builds. #33695 (Amos Bird). Bug Fix (user-visible misbehaviour in official stable or prestable release)​ Fixed the assertion in case of using allow_experimental_parallel_reading_from_replicas with max_parallel_replicas equals to 1. This fixes #34525. #34613 (Nikita Mikhaylov).Fix rare bug while reading of empty arrays, which could lead to Data compressed with different methods error. It can reproduce if you have mostly empty arrays, but not always. And reading is performed in backward direction with ORDER BY ... DESC. This error is extremely unlikely to happen. #34327 (Anton Popov).Fix wrong result of round/roundBankers if integer values of small types are rounded. Closes #33267. #34562 (李扬).Sometimes query cancellation did not work immediately when we were reading multiple files from s3 or HDFS. Fixes #34301 Relates to #34397. #34539 (Dmitry Novik).Fix exception Chunk should have AggregatedChunkInfo in MergingAggregatedTransform (in case of optimize_aggregation_in_order = 1 and distributed_aggregation_memory_efficient = 0). Fixes #34526. #34532 (Anton Popov).Fix comparison between integers and floats in index analysis. Previously it could lead to skipping some granules for reading by mistake. Fixes #34493. #34528 (Anton Popov).Fix compression support in URL engine. #34524 (Frank Chen).Fix possible error 'file_size: Operation not supported' in files' schema autodetection. #34479 (Kruglov Pavel).Fixes possible race with table deletion. #34416 (Kseniia Sumarokova).Fix possible error Cannot convert column Function to mask in short circuit function evaluation. Closes #34171. #34415 (Kruglov Pavel).Fix potential crash when doing schema inference from url source. Closes #34147. #34405 (Kruglov Pavel).For UDFs access permissions were checked for database level instead of global level as it should be. Closes #34281. #34404 (Maksim Kita).Fix wrong engine syntax in result of SHOW CREATE DATABASE query for databases with engine Memory. This closes #34335. #34345 (alexey-milovidov).Fixed a couple of extremely rare race conditions that might lead to broken state of replication queue and &quot;intersecting parts&quot; error. #34297 (tavplubix).Fix progress bar width. It was incorrectly rounded to integer number of characters. #34275 (alexey-milovidov).Fix current_user/current_address client information fields for inter-server communication (before this patch current_user/current_address will be preserved from the previous query). #34263 (Azat Khuzhin).Fix memory leak in case of some Exception during query processing with optimize_aggregation_in_order=1. #34234 (Azat Khuzhin).Fix metric Query, which shows the number of executing queries. In last several releases it was always 0. #34224 (Anton Popov).Fix schema inference for table runction s3. #34186 (Kruglov Pavel).Fix rare and benign race condition in HDFS, S3 and URL storage engines which can lead to additional connections. #34172 (alesapin).Fix bug which can rarely lead to error &quot;Cannot read all data&quot; while reading LowCardinality columns of MergeTree table engines family which stores data on remote file system like S3 (virtual filesystem over s3 is an experimental feature that is not ready for production). #34139 (alesapin).Fix inserts to distributed tables in case of a change of native protocol. The last change was in the version 22.1, so there may be some failures of inserts to distributed tables after upgrade to that version. #34132 (Anton Popov).Fix possible data race in File table engine that was introduced in #33960. Closes #34111. #34113 (Kruglov Pavel).Fixed minor race condition that might cause &quot;intersecting parts&quot; error in extremely rare cases after ZooKeeper connection loss. #34096 (tavplubix).Fix asynchronous inserts with Native format. #34068 (Anton Popov).Fix bug which lead to inability for server to start when both replicated access storage and keeper (embedded in clickhouse-server) are used. Introduced two settings for keeper socket timeout instead of settings from default user: keeper_server.socket_receive_timeout_sec and keeper_server.socket_send_timeout_sec. Fixes #33973. #33988 (alesapin).Fix segfault while parsing ORC file with corrupted footer. Closes #33797. #33984 (Kruglov Pavel).Fix parsing IPv6 from query parameter (prepared statements) and fix IPv6 to string conversion. Closes #33928. #33971 (Kruglov Pavel).Fix crash while reading of nested tuples. Fixes #33838. #33956 (Anton Popov).Fix usage of functions array and tuple with literal arguments in distributed queries. Previously it could lead to Not found columns exception. #33938 (Anton Popov).Aggregate function combinator -If did not correctly process Nullable filter argument. This closes #27073. #33920 (alexey-milovidov).Fix potential race condition when doing remote disk read (virtual filesystem over s3 is an experimental feature that is not ready for production). #33912 (Amos Bird).Fix crash if SQL UDF is created with lambda with non identifier arguments. Closes #33866. #33868 (Maksim Kita).Fix usage of sparse columns (which can be enabled by experimental setting ratio_of_defaults_for_sparse_serialization). #33849 (Anton Popov).Fixed replica is not readonly logical error on SYSTEM RESTORE REPLICA query when replica is actually readonly. Fixes #33806. #33847 (tavplubix).Fix memory leak in clickhouse-keeper in case of compression is used (default). #33840 (Azat Khuzhin).Fix index analysis with no common types available. #33833 (Amos Bird).Fix schema inference for JSONEachRow and JSONCompactEachRow. #33830 (Kruglov Pavel).Fix usage of external dictionaries with redis source and large number of keys. #33804 (Anton Popov).Fix bug in client that led to 'Connection reset by peer' in server. Closes #33309. #33790 (Kruglov Pavel).Fix parsing query INSERT INTO ... VALUES SETTINGS ... (...), ... #33776 (Kruglov Pavel).Fix bug of check table when creating data part with wide format and projection. #33774 (李扬).Fix tiny race between count() and INSERT/merges/... in MergeTree (it is possible to return incorrect number of rows for SELECT with optimize_trivial_count_query). #33753 (Azat Khuzhin).Throw exception when directory listing request has failed in storage HDFS. #33724 (LiuNeng).Fix mutation when table contains projections. This fixes #33010. This fixes #33275. #33679 (Amos Bird).Correctly determine current database if CREATE TEMPORARY TABLE AS SELECT is queried inside a named HTTP session. This is a very rare use case. This closes #8340. #33676 (alexey-milovidov).Allow some queries with sorting, LIMIT BY, ARRAY JOIN and lambda functions. This closes #7462. #33675 (alexey-milovidov).Fix bug in &quot;zero copy replication&quot; (a feature that is under development and should not be used in production) which lead to data duplication in case of TTL move. Fixes #33643. #33642 (alesapin).Fix Chunk should have AggregatedChunkInfo in GroupingAggregatedTransform (in case of optimize_aggregation_in_order = 1). #33637 (Azat Khuzhin).Fix error Bad cast from type ... to DB::DataTypeArray which may happen when table has Nested column with dots in name, and default value is generated for it (e.g. during insert, when column is not listed). Continuation of #28762. #33588 (Alexey Pavlenko).Export into lz4 files has been fixed. Closes #31421. #31862 (Kruglov Pavel).Fix potential crash if group_by_overflow_mode was set to any (approximate GROUP BY) and aggregation was performed by single column of type LowCardinality. #34506 (DR).Fix inserting to temporary tables via gRPC client-server protocol. Fixes #34347, issue #2. #34364 (Vitaly Baranov).Fix issue #19429. #34225 (Vitaly Baranov).Fix issue #18206. #33977 (Vitaly Baranov).This PR allows using multiple LDAP storages in the same list of user directories. It worked earlier but was broken because LDAP tests are disabled (they are part of the testflows tests). #33574 (Vitaly Baranov). "},{"title":" ClickHouse release v22.1, 2022-01-18​","type":1,"pageTitle":"ClickHouse Changelog","url":"docs/en/whats-new/changelog/#-clickhouse-release-v221-2022-01-18","content":"Upgrade Notes​ The functions left and right were previously implemented in parser and now full-featured. Distributed queries with left or right functions without aliases may throw exception if cluster contains different versions of clickhouse-server. If you are upgrading your cluster and encounter this error, you should finish upgrading your cluster to ensure all nodes have the same version. Also you can add aliases (AS something) to the columns in your queries to avoid this issue. #33407 (alexey-milovidov).Resource usage by scalar subqueries is fully accounted since this version. With this change, rows read in scalar subqueries are now reported in the query_log. If the scalar subquery is cached (repeated or called for several rows) the rows read are only counted once. This change allows KILLing queries and reporting progress while they are executing scalar subqueries. #32271 (Raúl Marín). New Feature​ Implement data schema inference for input formats. Allow to skip structure (or write just auto) in table functions file, url, s3, hdfs and in parameters of clickhouse-local . Allow to skip structure in create query for table engines File, HDFS, S3, URL, Merge, Buffer, Distributed and ReplicatedMergeTree (if we add new replicas). #32455 (Kruglov Pavel).Detect format by file extension in file/hdfs/s3/url table functions and HDFS/S3/URL table engines and also for SELECT INTO OUTFILE and INSERT FROM INFILE #33565 (Kruglov Pavel). Close #30918. #33443 (OnePiece).A tool for collecting diagnostics data if you need support. #33175 (Alexander Burmak).Automatic cluster discovery via Zoo/Keeper. It allows to add replicas to the cluster without changing configuration on every server. #31442 (vdimir).Implement hive table engine to access apache hive from clickhouse. This implements: #29245. #31104 (taiyang-li).Add aggregate functions cramersV, cramersVBiasCorrected, theilsU and contingency. These functions calculate dependency (measure of association) between categorical values. All these functions are using cross-tab (histogram on pairs) for implementation. You can imagine it like a correlation coefficient but for any discrete values (not necessary numbers). #33366 (alexey-milovidov). Initial implementation by Vanyok-All-is-OK and antikvist.Added table function hdfsCluster which allows processing files from HDFS in parallel from many nodes in a specified cluster, similarly to s3Cluster. #32400 (Zhichang Yu).Adding support for disks backed by Azure Blob Storage, in a similar way it has been done for disks backed by AWS S3. #31505 (Jakub Kuklis).Allow COMMENT in CREATE VIEW (for all VIEW kinds). #31062 (Vasily Nemkov).Dynamically reinitialize listening ports and protocols when configuration changes. #30549 (Kevin Michel).Added left, right, leftUTF8, rightUTF8 functions. Fix error in implementation of substringUTF8 function with negative offset (offset from the end of string). #33407 (alexey-milovidov).Add new functions for H3 coordinate system: h3HexAreaKm2, h3CellAreaM2, h3CellAreaRads2. #33479 (Bharat Nallan).Add MONTHNAME function. #33436 (usurai).Added function arrayLast. Closes #33390. #33415 Added function arrayLastIndex. #33465 (Maksim Kita).Add function decodeURLFormComponent slightly different to decodeURLComponent. Close #10298. #33451 (SuperDJY).Allow to split GraphiteMergeTree rollup rules for plain/tagged metrics (optional rule_type field). #33494 (Michail Safronov). Performance Improvement​ Support moving conditions to PREWHERE (setting optimize_move_to_prewhere) for tables of Merge engine if its all underlying tables supports PREWHERE. #33300 (Anton Popov).More efficient handling of globs for URL storage. Now you can easily query million URLs in parallel with retries. Closes #32866. #32907 (Kseniia Sumarokova).Avoid exponential backtracking in parser. This closes #20158. #33481 (alexey-milovidov).Abuse of untuple function was leading to exponential complexity of query analysis (found by fuzzer). This closes #33297. #33445 (alexey-milovidov).Reduce allocated memory for dictionaries with string attributes. #33466 (Maksim Kita).Slight performance improvement of reinterpret function. #32587 (alexey-milovidov).Non significant change. In extremely rare cases when data part is lost on every replica, after merging of some data parts, the subsequent queries may skip less amount of partitions during partition pruning. This hardly affects anything. #32220 (Azat Khuzhin).Improve clickhouse-keeper writing performance by optimization the size calculation logic. #32366 (zhanglistar).Optimize single part projection materialization. This closes #31669. #31885 (Amos Bird).Improve query performance of system tables. #33312 (OnePiece).Optimize selecting of MergeTree parts that can be moved between volumes. #33225 (OnePiece).Fix sparse_hashed dict performance with sequential keys (wrong hash function). #32536 (Azat Khuzhin). Experimental Feature​ Parallel reading from multiple replicas within a shard during distributed query without using sample key. To enable this, set allow_experimental_parallel_reading_from_replicas = 1 and max_parallel_replicas to any number. This closes #26748. #29279 (Nikita Mikhaylov).Implemented sparse serialization. It can reduce usage of disk space and improve performance of some queries for columns, which contain a lot of default (zero) values. It can be enabled by setting ratio_for_sparse_serialization. Sparse serialization will be chosen dynamically for column, if it has ratio of number of default values to number of all values above that threshold. Serialization (default or sparse) will be fixed for every column in part, but may varies between parts. #22535 (Anton Popov).Add &quot;TABLE OVERRIDE&quot; feature for customizing MaterializedMySQL table schemas. #32325 (Stig Bakken).Add EXPLAIN TABLE OVERRIDE query. #32836 (Stig Bakken).Support TABLE OVERRIDE clause for MaterializedPostgreSQL. RFC: #31480. #32749 (Kseniia Sumarokova).Change ZooKeeper path for zero-copy marks for shared data. Note that &quot;zero-copy replication&quot; is non-production feature (in early stages of development) that you shouldn't use anyway. But in case if you have used it, let you keep in mind this change. #32061 (ianton-ru).Events clause support for WINDOW VIEW watch query. #32607 (vxider).Fix ACL with explicit digit hash in clickhouse-keeper: now the behavior consistent with ZooKeeper and generated digest is always accepted. #33249 (小路). #33246.Fix unexpected projection removal when detaching parts. #32067 (Amos Bird). Improvement​ Now date time conversion functions that generates time before 1970-01-01 00:00:00 will be saturated to zero instead of overflow. #29953 (Amos Bird). It also fixes a bug in index analysis if date truncation function would yield result before the Unix epoch.Always display resource usage (total CPU usage, total RAM usage and max RAM usage per host) in client. #33271 (alexey-milovidov).Improve Bool type serialization and deserialization, check the range of values. #32984 (Kruglov Pavel).If an invalid setting is defined using the SET query or using the query parameters in the HTTP request, error message will contain suggestions that are similar to the invalid setting string (if any exists). #32946 (Antonio Andelic).Support hints for mistyped setting names for clickhouse-client and clickhouse-local. Closes #32237. #32841 (凌涛).Allow to use virtual columns in Materialized Views. Close #11210. #33482 (OnePiece).Add config to disable IPv6 in clickhouse-keeper if needed. This close #33381. #33450 (Wu Xueyang).Add more info to system.build_options about current git revision. #33431 (taiyang-li).clickhouse-local: track memory under --max_memory_usage_in_client option. #33341 (Azat Khuzhin).Allow negative intervals in function intervalLengthSum. Their length will be added as well. This closes #33323. #33335 (alexey-milovidov).LineAsString can be used as output format. This closes #30919. #33331 (Sergei Trifonov).Support &lt;secure/&gt; in cluster configuration, as an alternative form of &lt;secure&gt;1&lt;/secure&gt;. Close #33270. #33330 (SuperDJY).Pressing Ctrl+C twice will terminate clickhouse-benchmark immediately without waiting for in-flight queries. This closes #32586. #33303 (alexey-milovidov).Support Unix timestamp with milliseconds in parseDateTimeBestEffort function. #33276 (Ben).Allow to cancel query while reading data from external table in the formats: Arrow / Parquet / ORC - it failed to be cancelled it case of big files and setting input_format_allow_seeks as false. Closes #29678. #33238 (Kseniia Sumarokova).If table engine supports SETTINGS clause, allow to pass the settings as key-value or via config. Add this support for MySQL. #33231 (Kseniia Sumarokova).Correctly prevent Nullable primary keys if necessary. This is for #32780. #33218 (Amos Bird).Add retry for PostgreSQL connections in case nothing has been fetched yet. Closes #33199. #33209 (Kseniia Sumarokova).Validate config keys for external dictionaries. #33095. #33130 (Kseniia Sumarokova).Send profile info inside clickhouse-local. Closes #33093. #33097 (Kseniia Sumarokova).Short circuit evaluation: support for function throwIf. Closes #32969. #32973 (Maksim Kita).(This only happens in unofficial builds). Fixed segfault when inserting data into compressed Decimal, String, FixedString and Array columns. This closes #32939. #32940 (N. Kolotov).Added support for specifying subquery as SQL user defined function. Example: CREATE FUNCTION test AS () -&gt; (SELECT 1). Closes #30755. #32758 (Maksim Kita).Improve gRPC compression support for #28671. #32747 (Vitaly Baranov).Flush all In-Memory data parts when WAL is not enabled while shutdown server or detaching table. #32742 (nauta).Allow to control connection timeouts for MySQL (previously was supported only for dictionary source). Closes #16669. Previously default connect_timeout was rather small, now it is configurable. #32734 (Kseniia Sumarokova).Support authSource option for storage MongoDB. Closes #32594. #32702 (Kseniia Sumarokova).Support Date32 type in genarateRandom table function. #32643 (nauta).Add settings max_concurrent_select_queries and max_concurrent_insert_queries for control concurrent queries by query kind. Close #3575. #32609 (SuperDJY).Improve handling nested structures with missing columns while reading data in Protobuf format. Follow-up to https://github.com/ClickHouse/ClickHouse/pull/31988. #32531 (Vitaly Baranov).Allow empty credentials for MongoDB engine. Closes #26267. #32460 (Kseniia Sumarokova).Disable some optimizations for window functions that may lead to exceptions. Closes #31535. Closes #31620. #32453 (Kseniia Sumarokova).Allows to connect to MongoDB 5.0. Closes #31483,. #32416 (Kseniia Sumarokova).Enable comparison between Decimal and Float. Closes #22626. #31966 (flynn).Added settings command_read_timeout, command_write_timeout for StorageExecutable, StorageExecutablePool, ExecutableDictionary, ExecutablePoolDictionary, ExecutableUserDefinedFunctions. Setting command_read_timeout controls timeout for reading data from command stdout in milliseconds. Setting command_write_timeout timeout for writing data to command stdin in milliseconds. Added settings command_termination_timeout for ExecutableUserDefinedFunction, ExecutableDictionary, StorageExecutable. Added setting execute_direct for ExecutableUserDefinedFunction, by default true. Added setting execute_direct for ExecutableDictionary, ExecutablePoolDictionary, by default false. #30957 (Maksim Kita).Bitmap aggregate functions will give correct result for out of range argument instead of wraparound. #33127 (DR).Fix parsing incorrect queries with FROM INFILE statement. #33521 (Kruglov Pavel).Don't allow to write into S3 if path contains globs. #33142 (Kruglov Pavel).--echo option was not used by clickhouse-client in batch mode with single query. #32843 (N. Kolotov).Use --database option for clickhouse-local. #32797 (Kseniia Sumarokova).Fix surprisingly bad code in SQL ordinary function file. Now it supports symlinks. #32640 (alexey-milovidov).Updating modification_time for data part in system.parts after part movement #32964. #32965 (save-my-heart).Potential issue, cannot be exploited: integer overflow may happen in array resize. #33024 (varadarajkumar). Build/Testing/Packaging Improvement​ Add packages, functional tests and Docker builds for AArch64 (ARM) version of ClickHouse. #32911 (Mikhail f. Shiryaev). #32415Prepare ClickHouse to be built with musl-libc. It is not enabled by default. #33134 (alexey-milovidov).Make installation script working on FreeBSD. This closes #33384. #33418 (alexey-milovidov).Add actionlint for GitHub Actions workflows and verify workflow files via act --list to check the correct workflow syntax. #33612 (Mikhail f. Shiryaev).Add more tests for the nullable primary key feature. Add more tests with different types and merge tree kinds, plus randomly generated data. #33228 (Amos Bird).Add a simple tool to visualize flaky tests in web browser. #33185 (alexey-milovidov).Enable hermetic build for shared builds. This is mainly for developers. #32968 (Amos Bird).Update libc++ and libc++abi to the latest. #32484 (Raúl Marín).Added integration test for external .NET client (ClickHouse.Client). #23230 (Oleg V. Kozlyuk).Inject git information into clickhouse binary file. So we can get source code revision easily from clickhouse binary file. #33124 (taiyang-li).Remove obsolete code from ConfigProcessor. Yandex specific code is not used anymore. The code contained one minor defect. This defect was reported by Mallik Hassan in #33032. This closes #33032. #33026 (alexey-milovidov). Bug Fix (user-visible misbehavior in official stable or prestable release)​ Several fixes for format parsing. This is relevant if clickhouse-server is open for write access to adversary. Specifically crafted input data for Native format may lead to reading uninitialized memory or crash. This is relevant if clickhouse-server is open for write access to adversary. #33050 (Heena Bansal). Fixed Apache Avro Union type index out of boundary issue in Apache Avro binary format. #33022 (Harry Lee). Fix null pointer dereference in LowCardinality data when deserializing LowCardinality data in the Native format. #33021 (Harry Lee).ClickHouse Keeper handler will correctly remove operation when response sent. #32988 (JackyWoo).Potential off-by-one miscalculation of quotas: quota limit was not reached, but the limit was exceeded. This fixes #31174. #31656 (sunny).Fixed CASTing from String to IPv4 or IPv6 and back. Fixed error message in case of failed conversion. #29224 (Dmitry Novik) #27914 (Vasily Nemkov).Fixed an exception like Unknown aggregate function nothing during an execution on a remote server. This fixes #16689. #26074 (hexiaoting).Fix wrong database for JOIN without explicit database in distributed queries (Fixes: #10471). #33611 (Azat Khuzhin).Fix segfault in Apache Avro format that appears after the second insert into file. #33566 (Kruglov Pavel).Fix segfault in Apache Arrow format if schema contains Dictionary type. Closes #33507. #33529 (Kruglov Pavel).Out of band offset and limit settings may be applied incorrectly for views. Close #33289 #33518 (hexiaoting).Fix an exception Block structure mismatch which may happen during insertion into table with default nested LowCardinality column. Fixes #33028. #33504 (Nikolai Kochetov).Fix dictionary expressions for range_hashed range min and range max attributes when created using DDL. Closes #30809. #33478 (Maksim Kita).Fix possible use-after-free for INSERT into Materialized View with concurrent DROP (Azat Khuzhin).Do not try to read pass EOF (to workaround for a bug in the Linux kernel), this bug can be reproduced on kernels (3.14..5.9), and requires index_granularity_bytes=0 (i.e. turn off adaptive index granularity). #33372 (Azat Khuzhin).The commands SYSTEM SUSPEND and SYSTEM ... THREAD FUZZER missed access control. It is fixed. Author: Kevin Michel. #33333 (alexey-milovidov).Fix when COMMENT for dictionaries does not appear in system.tables, system.dictionaries. Allow to modify the comment for Dictionary engine. Closes #33251. #33261 (Maksim Kita).Add asynchronous inserts (with enabled setting async_insert) to query log. Previously such queries didn't appear in the query log. #33239 (Anton Popov).Fix sending WHERE 1 = 0 expressions for external databases query. Closes #33152. #33214 (Kseniia Sumarokova).Fix DDL validation for MaterializedPostgreSQL. Fix setting materialized_postgresql_allow_automatic_update. Closes #29535. #33200 (Kseniia Sumarokova). Make sure unused replication slots are always removed. Found in #26952. #33187 (Kseniia Sumarokova). Fix MaterializedPostreSQL detach/attach (removing / adding to replication) tables with non-default schema. Found in #29535. #33179 (Kseniia Sumarokova). Fix DROP MaterializedPostgreSQL database. #33468 (Kseniia Sumarokova).The metric StorageBufferBytes sometimes was miscalculated. #33159 (xuyatian).Fix error Invalid version for SerializationLowCardinality key column in case of reading from LowCardinality column with local_filesystem_read_prefetch or remote_filesystem_read_prefetch enabled. #33046 (Nikolai Kochetov).Fix s3 table function reading empty file. Closes #33008. #33037 (Kseniia Sumarokova).Fix Context leak in case of cancel_http_readonly_queries_on_client_close (i.e. leaking of external tables that had been uploaded the the server and other resources). #32982 (Azat Khuzhin).Fix wrong tuple output in CSV format in case of custom csv delimiter. #32981 (Kruglov Pavel).Fix HDFS URL check that didn't allow using HA namenode address. Bug was introduced in https://github.com/ClickHouse/ClickHouse/pull/31042. #32976 (Kruglov Pavel).Fix throwing exception like positional argument out of bounds for non-positional arguments. Closes #31173#event-5789668239. #32961 (Kseniia Sumarokova).Fix UB in case of unexpected EOF during filling a set from HTTP query (i.e. if the client interrupted in the middle, i.e. timeout 0.15s curl -Ss -F 's=@t.csv;' 'http://127.0.0.1:8123/?s_structure=key+Int&amp;query=SELECT+dummy+IN+s' and with large enough t.csv). #32955 (Azat Khuzhin).Fix a regression in replaceRegexpAll function. The function worked incorrectly when matched substring was empty. This closes #32777. This closes #30245. #32945 (alexey-milovidov).Fix ORC format stripe reading. #32929 (kreuzerkrieg).topKWeightedState failed for some input types. #32487. #32914 (vdimir).Fix exception Single chunk is expected from view inner query (LOGICAL_ERROR) in materialized view. Fixes #31419. #32862 (Nikolai Kochetov).Fix optimization with lazy seek for async reads from remote filesystems. Closes #32803. #32835 (Kseniia Sumarokova).MergeTree table engine might silently skip some mutations if there are too many running mutations or in case of high memory consumption, it's fixed. Fixes #17882. #32814 (tavplubix).Avoid reusing the scalar subquery cache when processing MV blocks. This fixes a bug when the scalar query reference the source table but it means that all subscalar queries in the MV definition will be calculated for each block. #32811 (Raúl Marín).Server might fail to start if database with MySQL engine cannot connect to MySQL server, it's fixed. Fixes #14441. #32802 (tavplubix).Fix crash when used fuzzBits function, close #32737. #32755 (SuperDJY).Fix error Column is not under aggregate function in case of MV with GROUP BY (list of columns) (which is pared as GROUP BY tuple(...)) over Kafka/RabbitMQ. Fixes #32668 and #32744. #32751 (Nikolai Kochetov).Fix ALTER TABLE ... MATERIALIZE TTL query with TTL ... DELETE WHERE ... and TTL ... GROUP BY ... modes. #32695 (Anton Popov).Fix optimize_read_in_order optimization in case when table engine is Distributed or Merge and its underlying MergeTree tables have monotonous function in prefix of sorting key. #32670 (Anton Popov).Fix LOGICAL_ERROR exception when the target of a materialized view is a JOIN or a SET table. #32669 (Raúl Marín).Inserting into S3 with multipart upload to Google Cloud Storage may trigger abort. #32504. #32649 (vdimir).Fix possible exception at RabbitMQ storage startup by delaying channel creation. #32584 (Kseniia Sumarokova).Fix table lifetime (i.e. possible use-after-free) in case of parallel DROP TABLE and INSERT. #32572 (Azat Khuzhin).Fix async inserts with formats CustomSeparated, Template, Regexp, MsgPack and JSONAsString. Previousely the async inserts with these formats didn't read any data. #32530 (Kruglov Pavel).Fix groupBitmapAnd function on distributed table. #32529 (minhthucdao).Fix crash in JOIN found by fuzzer, close #32458. #32508 (vdimir).Proper handling of the case with Apache Arrow column duplication. #32507 (Dmitriy Mokhnatkin).Fix issue with ambiguous query formatting in distributed queries that led to errors when some table columns were named ALL or DISTINCT. This closes #32391. #32490 (alexey-milovidov).Fix failures in queries that are trying to use skipping indices, which are not materialized yet. Fixes #32292 and #30343. #32359 (Anton Popov).Fix broken select query when there are more than 2 row policies on same column, begin at second queries on the same session. #31606. #32291 (SuperDJY).Fix fractional unix timestamp conversion to DateTime64, fractional part was reversed for negative unix timestamps (before 1970-01-01). #32240 (Ben).Some entries of replication queue might hang for temporary_directories_lifetime (1 day by default) with Directory tmp_merge_&lt;part_name&gt; or Part ... (state Deleting) already exists, but it will be deleted soon or similar error. It's fixed. Fixes #29616. #32201 (tavplubix).Fix parsing of APPLY lambda column transformer which could lead to client/server crash. #32138 (Kruglov Pavel).Fix base64Encode adding trailing bytes on small strings. #31797 (Kevin Michel).Fix possible crash (or incorrect result) in case of LowCardinality arguments of window function. Fixes #31114. #31888 (Nikolai Kochetov).Fix hang up with command DROP TABLE system.query_log sync. #33293 (zhanghuajie). "},{"title":"Changelog for 2021​","type":1,"pageTitle":"ClickHouse Changelog","url":"docs/en/whats-new/changelog/#changelog-for-2021","content":""},{"title":"What Does “ClickHouse” Mean?","type":0,"sectionRef":"#","url":"docs/faq/general/dbms-naming","content":"What Does “ClickHouse” Mean? It’s a combination of “Clickstream” and “Data wareHouse”. It comes from the original use case at Yandex.Metrica, where ClickHouse was supposed to keep records of all clicks by people from all over the Internet, and it still does the job. You can read more about this use case on ClickHouse history page. This two-part meaning has two consequences: The only correct way to write ClickHouse is with capital H.If you need to abbreviate it, use CH. For some historical reasons, abbreviating as CK is also popular in China, mostly because one of the first talks about ClickHouse in Chinese used this form. info Many years after ClickHouse got its name, this approach of combining two words that are meaningful on their own has been highlighted as the best way to name a database in a research by Andy Pavlo, an Associate Professor of Databases at Carnegie Mellon University. ClickHouse shared his “best database name of all time” award with Postgres.","keywords":""},{"title":"Why Not Use Something Like MapReduce?","type":0,"sectionRef":"#","url":"docs/faq/general/mapreduce","content":"Why Not Use Something Like MapReduce? We can refer to systems like MapReduce as distributed computing systems in which the reduce operation is based on distributed sorting. The most common open-source solution in this class is Apache Hadoop. These systems aren’t appropriate for online queries due to their high latency. In other words, they can’t be used as the back-end for a web interface. These types of systems aren’t useful for real-time data updates. Distributed sorting isn’t the best way to perform reduce operations if the result of the operation and all the intermediate results (if there are any) are located in the RAM of a single server, which is usually the case for online queries. In such a case, a hash table is an optimal way to perform reduce operations. A common approach to optimizing map-reduce tasks is pre-aggregation (partial reduce) using a hash table in RAM. The user performs this optimization manually. Distributed sorting is one of the main causes of reduced performance when running simple map-reduce tasks. Most MapReduce implementations allow you to execute arbitrary code on a cluster. But a declarative query language is better suited to OLAP to run experiments quickly. For example, Hadoop has Hive and Pig. Also consider Cloudera Impala or Shark (outdated) for Spark, as well as Spark SQL, Presto, and Apache Drill. Performance when running such tasks is highly sub-optimal compared to specialized systems, but relatively high latency makes it unrealistic to use these systems as the backend for a web interface.","keywords":""},{"title":"ClickHouse FAQ","type":0,"sectionRef":"#","url":"docs/faq/","content":"ClickHouse FAQ We have four categories of frequently asked questions: General Use Cases Operations Integration Original article","keywords":""},{"title":"What Is a Columnar Database?","type":0,"sectionRef":"#","url":"docs/faq/general/columnar-database","content":"What Is a Columnar Database? A columnar database stores data of each column independently. This allows to read data from disks only for those columns that are used in any given query. The cost is that operations that affect whole rows become proportionally more expensive. The synonym for a columnar database is a column-oriented database management system. ClickHouse is a typical example of such a system. Key columnar database advantages are: Queries that use only a few columns out of many.Aggregating queries against large volumes of data.Column-wise data compression. Here is the illustration of the difference between traditional row-oriented systems and columnar databases when building reports: Traditional row-oriented Columnar A columnar database is a preferred choice for analytical applications because it allows to have many columns in a table just in case, but do not pay the cost for unused columns on read query execution time. Column-oriented databases are designed for big data processing because and data warehousing, they often natively scale using distributed clusters of low-cost hardware to increase throughput. ClickHouse does it with combination of distributed and replicated tables.","keywords":""},{"title":"What Does “Не тормозит” Mean?","type":0,"sectionRef":"#","url":"docs/faq/general/ne-tormozit","content":"What Does “Не тормозит” Mean? This question usually arises when people see official ClickHouse t-shirts. They have large words “ClickHouse не тормозит” on the front. Before ClickHouse became open-source, it has been developed as an in-house storage system by the largest Russian IT company, Yandex. That’s why it initially got its slogan in Russian, which is “не тормозит” (pronounced as “ne tormozit”). After the open-source release we first produced some of those t-shirts for events in Russia and it was a no-brainer to use the slogan as-is. One of the following batches of those t-shirts was supposed to be given away on events outside of Russia and we tried to make the English version of the slogan. Unfortunately, the Russian language is kind of elegant in terms of expressing stuff and there was a restriction of limited space on a t-shirt, so we failed to come up with good enough translation (most options appeared to be either long or inaccurate) and decided to keep the slogan in Russian even on t-shirts produced for international events. It appeared to be a great decision because people all over the world get positively surprised and curious when they see it. So, what does it mean? Here are some ways to translate “не тормозит”: If you translate it literally, it’d be something like “ClickHouse does not press the brake pedal”.If you’d want to express it as close to how it sounds to a Russian person with IT background, it’d be something like “If your larger system lags, it’s not because it uses ClickHouse”.Shorter, but not so precise versions could be “ClickHouse is not slow”, “ClickHouse does not lag” or just “ClickHouse is fast”. If you haven’t seen one of those t-shirts in person, you can check them out online in many ClickHouse-related videos. For example, this one: P.S. These t-shirts are not for sale, they are given away for free on most ClickHouse Meetups, usually for best questions or other forms of active participation.","keywords":""},{"title":"Why ClickHouse Is So Fast?","type":0,"sectionRef":"#","url":"docs/faq/general/why-clickhouse-is-so-fast","content":"Why ClickHouse Is So Fast? It was designed to be fast. Query execution performance has always been a top priority during the development process, but other important characteristics like user-friendliness, scalability, and security were also considered so ClickHouse could become a real production system. ClickHouse was initially built as a prototype to do just a single task well: to filter and aggregate data as fast as possible. That’s what needs to be done to build a typical analytical report and that’s what a typical GROUP BY query does. ClickHouse team has made several high-level decisions that combined made achieving this task possible: Column-oriented storage : Source data often contain hundreds or even thousands of columns, while a report can use just a few of them. The system needs to avoid reading unnecessary columns, or most expensive disk read operations would be wasted. Indexes : ClickHouse keeps data structures in memory that allows reading not only used columns but only necessary row ranges of those columns. Data compression : Storing different values of the same column together often leads to better compression ratios (compared to row-oriented systems) because in real data column often has the same or not so many different values for neighboring rows. In addition to general-purpose compression, ClickHouse supports specialized codecs that can make data even more compact. Vectorized query execution : ClickHouse not only stores data in columns but also processes data in columns. It leads to better CPU cache utilization and allows for SIMD CPU instructions usage. Scalability : ClickHouse can leverage all available CPU cores and disks to execute even a single query. Not only on a single server but all CPU cores and disks of a cluster as well. But many other database management systems use similar techniques. What really makes ClickHouse stand out is attention to low-level details. Most programming languages provide implementations for most common algorithms and data structures, but they tend to be too generic to be effective. Every task can be considered as a landscape with various characteristics, instead of just throwing in random implementation. For example, if you need a hash table, here are some key questions to consider: Which hash function to choose?Collision resolution algorithm: open addressing vs chaining?Memory layout: one array for keys and values or separate arrays? Will it store small or large values?Fill factor: when and how to resize? How to move values around on resize?Will values be removed and which algorithm will work better if they will?Will we need fast probing with bitmaps, inline placement of string keys, support for non-movable values, prefetch, and batching? Hash table is a key data structure for GROUP BY implementation and ClickHouse automatically chooses one of 30+ variations for each specific query. The same goes for algorithms, for example, in sorting you might consider: What will be sorted: an array of numbers, tuples, strings, or structures?Is all data available completely in RAM?Do we need a stable sort?Do we need a full sort? Maybe partial sort or n-th element will suffice?How to implement comparisons?Are we sorting data that has already been partially sorted? Algorithms that they rely on characteristics of data they are working with can often do better than their generic counterparts. If it is not really known in advance, the system can try various implementations and choose the one that works best in runtime. For example, see an article on how LZ4 decompression is implemented in ClickHouse. Last but not least, the ClickHouse team always monitors the Internet on people claiming that they came up with the best implementation, algorithm, or data structure to do something and tries it out. Those claims mostly appear to be false, but from time to time you’ll indeed find a gem. Tips for building your own high-performance software Keep in mind low-level details when designing your system.Design based on hardware capabilities.Choose data structures and abstractions based on the needs of the task.Provide specializations for special cases.Try new, “best” algorithms, that you read about yesterday.Choose an algorithm in runtime based on statistics.Benchmark on real datasets.Test for performance regressions in CI.Measure and observe everything.","keywords":""},{"title":"What Is OLAP?","type":0,"sectionRef":"#","url":"docs/faq/general/olap","content":"","keywords":""},{"title":"OLAP from the Business Perspective​","type":1,"pageTitle":"What Is OLAP?","url":"docs/faq/general/olap#olap-from-the-business-perspective","content":"In recent years, business people started to realize the value of data. Companies who make their decisions blindly, more often than not fail to keep up with the competition. The data-driven approach of successful companies forces them to collect all data that might be remotely useful for making business decisions and need mechanisms to timely analyze them. Here’s where OLAP database management systems (DBMS) come in. In a business sense, OLAP allows companies to continuously plan, analyze, and report operational activities, thus maximizing efficiency, reducing expenses, and ultimately conquering the market share. It could be done either in an in-house system or outsourced to SaaS providers like web/mobile analytics services, CRM services, etc. OLAP is the technology behind many BI applications (Business Intelligence). ClickHouse is an OLAP database management system that is pretty often used as a backend for those SaaS solutions for analyzing domain-specific data. However, some businesses are still reluctant to share their data with third-party providers and an in-house data warehouse scenario is also viable. "},{"title":"OLAP from the Technical Perspective​","type":1,"pageTitle":"What Is OLAP?","url":"docs/faq/general/olap#olap-from-the-technical-perspective","content":"All database management systems could be classified into two groups: OLAP (Online Analytical Processing) and OLTP (Online Transactional Processing). Former focuses on building reports, each based on large volumes of historical data, but doing it not so frequently. While the latter usually handle a continuous stream of transactions, constantly modifying the current state of data. In practice OLAP and OLTP are not categories, it’s more like a spectrum. Most real systems usually focus on one of them but provide some solutions or workarounds if the opposite kind of workload is also desired. This situation often forces businesses to operate multiple storage systems integrated, which might be not so big deal but having more systems make it more expensive to maintain. So the trend of recent years is HTAP (Hybrid Transactional/Analytical Processing) when both kinds of the workload are handled equally well by a single database management system. Even if a DBMS started as a pure OLAP or pure OLTP, they are forced to move towards that HTAP direction to keep up with their competition. And ClickHouse is no exception, initially, it has been designed as fast-as-possible OLAP system and it still does not have full-fledged transaction support, but some features like consistent read/writes and mutations for updating/deleting data had to be added. The fundamental trade-off between OLAP and OLTP systems remains: To build analytical reports efficiently it’s crucial to be able to read columns separately, thus most OLAP databases are columnar,While storing columns separately increases costs of operations on rows, like append or in-place modification, proportionally to the number of columns (which can be huge if the systems try to collect all details of an event just in case). Thus, most OLTP systems store data arranged by rows. "},{"title":"Questions About Integrating ClickHouse and Other Systems","type":0,"sectionRef":"#","url":"docs/faq/integration/","content":"Questions About Integrating ClickHouse and Other Systems How do I export data from ClickHouse to a file?How to import JSON into ClickHouse?How do I connect Kafka to ClickHouse?Can I connect my Java application to ClickHouse?Can ClickHouse read tables from MySQL?Can ClickHouse read tables from PostgreSQLWhat if I have a problem with encodings when connecting to Oracle via ODBC? Don’t see what you're looking for? Check out our other FAQ categories and also browse the many helpful articles found here in the documentation. Original article","keywords":"clickhouse faq questions integrations"},{"title":"How Do I Export Data from ClickHouse to a File?","type":0,"sectionRef":"#","url":"docs/faq/integration/file-export","content":"","keywords":""},{"title":"Using INTO OUTFILE Clause​","type":1,"pageTitle":"How Do I Export Data from ClickHouse to a File?","url":"docs/faq/integration/file-export#using-into-outfile-clause","content":"Add an INTO OUTFILE clause to your query. For example: SELECT * FROM table INTO OUTFILE 'file'  By default, ClickHouse uses the TabSeparated format for output data. To select the data format, use the FORMAT clause. For example: SELECT * FROM table INTO OUTFILE 'file' FORMAT CSV  "},{"title":"Using a File-Engine Table​","type":1,"pageTitle":"How Do I Export Data from ClickHouse to a File?","url":"docs/faq/integration/file-export#using-a-file-engine-table","content":"See File table engine. "},{"title":"Using Command-Line Redirection​","type":1,"pageTitle":"How Do I Export Data from ClickHouse to a File?","url":"docs/faq/integration/file-export#using-command-line-redirection","content":"$ clickhouse-client --query &quot;SELECT * from table&quot; --format FormatName &gt; result.txt  See clickhouse-client. "},{"title":"General Questions About ClickHouse","type":0,"sectionRef":"#","url":"docs/faq/general/","content":"General Questions About ClickHouse What is ClickHouse?Why ClickHouse is so fast?Who is using ClickHouse?What does “ClickHouse” mean?What does “Не тормозит” mean?What is OLAP?What is a columnar database?Why not use something like MapReduce?How do I contribute code to ClickHouse? Don’t see what you're looking for? Check out our other FAQ categories and also browse the many helpful articles found here in the documentation. Original article","keywords":"clickhouse faq questions what is"},{"title":"How to Import JSON Into ClickHouse?","type":0,"sectionRef":"#","url":"docs/faq/integration/json-import","content":"","keywords":""},{"title":"Examples​","type":1,"pageTitle":"How to Import JSON Into ClickHouse?","url":"docs/faq/integration/json-import#examples","content":"Using HTTP interface: $ echo '{&quot;foo&quot;:&quot;bar&quot;}' | curl 'http://localhost:8123/?query=INSERT%20INTO%20test%20FORMAT%20JSONEachRow' --data-binary @-  Using CLI interface: $ echo '{&quot;foo&quot;:&quot;bar&quot;}' | clickhouse-client --query=&quot;INSERT INTO test FORMAT JSONEachRow&quot;  Instead of inserting data manually, you might consider to use one of client libraries instead. "},{"title":"Useful Settings​","type":1,"pageTitle":"How to Import JSON Into ClickHouse?","url":"docs/faq/integration/json-import#useful-settings","content":"input_format_skip_unknown_fields allows to insert JSON even if there were additional fields not present in table schema (by discarding them).input_format_import_nested_json allows to insert nested JSON objects into columns of Nested type. note Settings are specified as GET parameters for the HTTP interface or as additional command-line arguments prefixed with -- for the CLI interface. "},{"title":"What If I Have a Problem with Encodings When Using Oracle Via ODBC?","type":0,"sectionRef":"#","url":"docs/faq/integration/oracle-odbc","content":"What If I Have a Problem with Encodings When Using Oracle Via ODBC? If you use Oracle as a source of ClickHouse external dictionaries via Oracle ODBC driver, you need to set the correct value for the NLS_LANG environment variable in /etc/default/clickhouse. For more information, see the Oracle NLS_LANG FAQ. Example NLS_LANG=RUSSIAN_RUSSIA.UTF8 ","keywords":""},{"title":"Does ClickHouse support multi-region replication?","type":0,"sectionRef":"#","url":"docs/faq/operations/multi-region-replication","content":"Does ClickHouse support multi-region replication? The short answer is &quot;yes&quot;. However, we recommend keeping latency between all regions/datacenters in two-digit range, otherwise write performance will suffer as it goes through distributed consensus protocol. For example, replication between US coasts will likely work fine, but between the US and Europe won't. Configuration-wise there's no difference compared to single-region replication, simply use hosts that are located in different locations for replicas. For more information, see full article on data replication.","keywords":""},{"title":"Is It Possible to Delete Old Records from a ClickHouse Table?","type":0,"sectionRef":"#","url":"docs/faq/operations/delete-old-data","content":"","keywords":""},{"title":"TTL​","type":1,"pageTitle":"Is It Possible to Delete Old Records from a ClickHouse Table?","url":"docs/faq/operations/delete-old-data#ttl","content":"ClickHouse allows to automatically drop values when some condition happens. This condition is configured as an expression based on any columns, usually just static offset for any timestamp column. The key advantage of this approach is that it does not need any external system to trigger, once TTL is configured, data removal happens automatically in background. note TTL can also be used to move data not only to /dev/null, but also between different storage systems, like from SSD to HDD. More details on configuring TTL. "},{"title":"ALTER DELETE​","type":1,"pageTitle":"Is It Possible to Delete Old Records from a ClickHouse Table?","url":"docs/faq/operations/delete-old-data#alter-delete","content":"ClickHouse does not have real-time point deletes like in OLTP databases. The closest thing to them are mutations. They are issued as ALTER ... DELETE or ALTER ... UPDATE queries to distinguish from normal DELETE or UPDATE as they are asynchronous batch operations, not immediate modifications. The rest of syntax after ALTER TABLE prefix is similar. ALTER DELETE can be issued to flexibly remove old data. If you need to do it regularly, the main downside will be the need to have an external system to submit the query. There are also some performance considerations since mutation rewrite complete parts even there’s only a single row to be deleted. This is the most common approach to make your system based on ClickHouse GDPR-compliant. More details on mutations. "},{"title":"DROP PARTITION​","type":1,"pageTitle":"Is It Possible to Delete Old Records from a ClickHouse Table?","url":"docs/faq/operations/delete-old-data#drop-partition","content":"ALTER TABLE ... DROP PARTITION provides a cost-efficient way to drop a whole partition. It’s not that flexible and needs proper partitioning scheme configured on table creation, but still covers most common cases. Like mutations need to be executed from an external system for regular use. More details on manipulating partitions. "},{"title":"TRUNCATE​","type":1,"pageTitle":"Is It Possible to Delete Old Records from a ClickHouse Table?","url":"docs/faq/operations/delete-old-data#truncate","content":"It’s rather radical to drop all data from a table, but in some cases it might be exactly what you need. More details on table truncation. "},{"title":"Who Is Using ClickHouse?","type":0,"sectionRef":"#","url":"docs/faq/general/who-is-using-clickhouse","content":"Who Is Using ClickHouse? Being an open-source product makes this question not so straightforward to answer. You do not have to tell anyone if you want to start using ClickHouse, you just go grab source code or pre-compiled packages. There’s no contract to sign and the Apache 2.0 license allows for unconstrained software distribution. Also, the technology stack is often in a grey zone of what’s covered by an NDA. Some companies consider technologies they use as a competitive advantage even if they are open-source and do not allow employees to share any details publicly. Some see some PR risks and allow employees to share implementation details only with their PR department approval. So how to tell who is using ClickHouse? One way is to ask around. If it’s not in writing, people are much more willing to share what technologies are used in their companies, what the use cases are, what kind of hardware is used, data volumes, etc. We’re talking with users regularly on ClickHouse Meetups all over the world and have heard stories about 1000+ companies that use ClickHouse. Unfortunately, that’s not reproducible and we try to treat such stories as if they were told under NDA to avoid any potential troubles. But you can come to any of our future meetups and talk with other users on your own. There are multiple ways how meetups are announced, for example, you can subscribe to our Twitter. The second way is to look for companies publicly saying that they use ClickHouse. It’s more substantial because there’s usually some hard evidence like a blog post, talk video recording, slide deck, etc. We collect the collection of links to such evidence on our Adopters page. Feel free to contribute the story of your employer or just some links you’ve stumbled upon (but try not to violate your NDA in the process). You can find names of very large companies in the adopters list, like Bloomberg, Cisco, China Telecom, Tencent, or Uber, but with the first approach, we found that there are many more. For example, if you take the list of largest IT companies by Forbes (2020) over half of them are using ClickHouse in some way. Also, it would be unfair not to mention Yandex, the company which initially open-sourced ClickHouse in 2016 and happens to be one of the largest IT companies in Europe.","keywords":""},{"title":"Which ClickHouse Version to Use in Production?","type":0,"sectionRef":"#","url":"docs/faq/operations/production","content":"","keywords":""},{"title":"Which ClickHouse Version Do You Recommend?​","type":1,"pageTitle":"Which ClickHouse Version to Use in Production?","url":"docs/faq/operations/production#which-clickhouse-version-do-you-recommend","content":"It’s tempting to hire consultants or trust some known experts to get rid of responsibility for your production environment. You install some specific ClickHouse version that someone else recommended, now if there’s some issue with it - it’s not your fault, it’s someone else’s. This line of reasoning is a big trap. No external person knows better what’s going on in your company’s production environment. So how to properly choose which ClickHouse version to upgrade to? Or how to choose your first ClickHouse version? First of all, you need to invest in setting up a realistic pre-production environment. In an ideal world, it could be a completely identical shadow copy, but that’s usually expensive. Here’re some key points to get reasonable fidelity in a pre-production environment with not so high costs: Pre-production environment needs to run an as close set of queries as you intend to run in production: Don’t make it read-only with some frozen data.Don’t make it write-only with just copying data without building some typical reports.Don’t wipe it clean instead of applying schema migrations. Use a sample of real production data and queries. Try to choose a sample that’s still representative and makes SELECT queries return reasonable results. Use obfuscation if your data is sensitive and internal policies do not allow it to leave the production environment.Make sure that pre-production is covered by your monitoring and alerting software the same way as your production environment does.If your production spans across multiple datacenters or regions, make your pre-production does the same.If your production uses complex features like replication, distributed table, cascading materialize views, make sure they are configured similarly in pre-production.There’s a trade-off on using the roughly same number of servers or VMs in pre-production as in production, but of smaller size, or much less of them, but of the same size. The first option might catch extra network-related issues, while the latter is easier to manage. The second area to invest in is automated testing infrastructure. Don’t assume that if some kind of query has executed successfully once, it’ll continue to do so forever. It’s ok to have some unit tests where ClickHouse is mocked but make sure your product has a reasonable set of automated tests that are run against real ClickHouse and check that all important use cases are still working as expected. Extra step forward could be contributing those automated tests to ClickHouse’s open-source test infrastructure that’s continuously used in its day-to-day development. It definitely will take some additional time and effort to learn how to run it and then how to adapt your tests to this framework, but it’ll pay off by ensuring that ClickHouse releases are already tested against them when they are announced stable, instead of repeatedly losing time on reporting the issue after the fact and then waiting for a bugfix to be implemented, backported and released. Some companies even have such test contributions to infrastructure by its use as an internal policy, most notably it’s called Beyonce’s Rule at Google. When you have your pre-production environment and testing infrastructure in place, choosing the best version is straightforward: Routinely run your automated tests against new ClickHouse releases. You can do it even for ClickHouse releases that are marked as testing, but going forward to the next steps with them is not recommended.Deploy the ClickHouse release that passed the tests to pre-production and check that all processes are running as expected.Report any issues you discovered to ClickHouse GitHub Issues.If there were no major issues, it should be safe to start deploying ClickHouse release to your production environment. Investing in gradual release automation that implements an approach similar to canary releases or green-blue deployments might further reduce the risk of issues in production. As you might have noticed, there’s nothing specific to ClickHouse in the approach described above, people do that for any piece of infrastructure they rely on if they take their production environment seriously. "},{"title":"How to Choose Between ClickHouse Releases?​","type":1,"pageTitle":"Which ClickHouse Version to Use in Production?","url":"docs/faq/operations/production#how-to-choose-between-clickhouse-releases","content":"If you look into contents of ClickHouse package repository, you’ll see four kinds of packages: testingprestablestablelts (long-term support) As was mentioned earlier, testing is good mostly to notice issues early, running them in production is not recommended because each of them is not tested as thoroughly as other kinds of packages. prestable is a release candidate which generally looks promising and is likely to become announced as stable soon. You can try them out in pre-production and report issues if you see any. For production use, there are two key options: stable and lts. Here is some guidance on how to choose between them: stable is the kind of package we recommend by default. They are released roughly monthly (and thus provide new features with reasonable delay) and three latest stable releases are supported in terms of diagnostics and backporting of bugfixes.lts are released twice a year and are supported for a year after their initial release. You might prefer them over stable in the following cases: Your company has some internal policies that do not allow for frequent upgrades or using non-LTS software.You are using ClickHouse in some secondary products that either does not require any complex ClickHouse features and do not have enough resources to keep it updated. Many teams who initially thought that lts is the way to go, often switch to stable anyway because of some recent feature that’s important for their product. warning One more thing to keep in mind when upgrading ClickHouse: we’re always keeping eye on compatibility across releases, but sometimes it’s not reasonable to keep and some minor details might change. So make sure you check the changelog before upgrading to see if there are any notes about backward-incompatible changes. "},{"title":"Question About Operating ClickHouse Servers and Clusters","type":0,"sectionRef":"#","url":"docs/faq/operations/","content":"Question About Operating ClickHouse Servers and Clusters Which ClickHouse version should I use in production?Is it possible to delete old records from a ClickHouse table?How do I configure ClickHouse Keeper?Can ClickHouse integrate with LDAP?How do I configure users, roles and permissions in ClickHouse?Can you update or delete rows in ClickHouse?Does ClickHouse support multi-region replication? Don’t see what you're looking for? Check out our other FAQ categories and also browse the many helpful articles found here in the documentation. Original article","keywords":""},{"title":"Questions About ClickHouse Use Cases","type":0,"sectionRef":"#","url":"docs/faq/use-cases/","content":"Questions About ClickHouse Use Cases Can I use ClickHouse as a time-series database?Can I use ClickHouse as a key-value storage? Don’t see what you're looking for? Check out our other FAQ categories and also browse the many helpful articles found here in the documentation. Original article","keywords":""},{"title":"Can I Use ClickHouse As a Time-Series Database?","type":0,"sectionRef":"#","url":"docs/faq/use-cases/time-series","content":"Can I Use ClickHouse As a Time-Series Database? ClickHouse is a generic data storage solution for OLAP workloads, while there are many specialized time-series database management systems. Nevertheless, ClickHouse’s focus on query execution speed allows it to outperform specialized systems in many cases. There are many independent benchmarks on this topic out there, so we’re not going to conduct one here. Instead, let’s focus on ClickHouse features that are important to use if that’s your use case. First of all, there are specialized codecs which make typical time-series. Either common algorithms like DoubleDelta and Gorilla or specific to ClickHouse like T64. Second, time-series queries often hit only recent data, like one day or one week old. It makes sense to use servers that have both fast nVME/SSD drives and high-capacity HDD drives. ClickHouse TTL feature allows to configure keeping fresh hot data on fast drives and gradually move it to slower drives as it ages. Rollup or removal of even older data is also possible if your requirements demand it. Even though it’s against ClickHouse philosophy of storing and processing raw data, you can use materialized views to fit into even tighter latency or costs requirements.","keywords":""},{"title":"Can I Use ClickHouse As a Key-Value Storage?","type":0,"sectionRef":"#","url":"docs/faq/use-cases/key-value","content":"Can I Use ClickHouse As a Key-Value Storage? The short answer is “no”. The key-value workload is among top positions in the list of cases when NOT{.text-danger} to use ClickHouse. It’s an OLAP system after all, while there are many excellent key-value storage systems out there. However, there might be situations where it still makes sense to use ClickHouse for key-value-like queries. Usually, it’s some low-budget products where the main workload is analytical in nature and fits ClickHouse well, but there’s also some secondary process that needs a key-value pattern with not so high request throughput and without strict latency requirements. If you had an unlimited budget, you would have installed a secondary key-value database for thus secondary workload, but in reality, there’s an additional cost of maintaining one more storage system (monitoring, backups, etc.) which might be desirable to avoid. If you decide to go against recommendations and run some key-value-like queries against ClickHouse, here’re some tips: The key reason why point queries are expensive in ClickHouse is its sparse primary index of main MergeTree table engine family. This index can’t point to each specific row of data, instead, it points to each N-th and the system has to scan from the neighboring N-th row to the desired one, reading excessive data along the way. In a key-value scenario, it might be useful to reduce the value of N with the index_granularity setting.ClickHouse keeps each column in a separate set of files, so to assemble one complete row it needs to go through each of those files. Their count increases linearly with the number of columns, so in the key-value scenario, it might be worth to avoid using many columns and put all your payload in a single String column encoded in some serialization format like JSON, Protobuf or whatever makes sense.There’s an alternative approach that uses Join table engine instead of normal MergeTree tables and joinGet function to retrieve the data. It can provide better query performance but might have some usability and reliability issues. Here’s an usage example.","keywords":""},{"title":"Applying a Catboost Model in ClickHouse","type":0,"sectionRef":"#","url":"docs/guides/developer/apply-catboost-model","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Applying a Catboost Model in ClickHouse","url":"docs/guides/developer/apply-catboost-model#prerequisites","content":"If you do not have the Docker yet, install it. note Docker is a software platform that allows you to create containers that isolate a CatBoost and ClickHouse installation from the rest of the system. Before applying a CatBoost model: 1. Pull the Docker image from the registry: $ docker pull yandex/tutorial-catboost-clickhouse  This Docker image contains everything you need to run CatBoost and ClickHouse: code, runtime, libraries, environment variables, and configuration files. 2. Make sure the Docker image has been successfully pulled: $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE yandex/tutorial-catboost-clickhouse latest 622e4d17945b 22 hours ago 1.37GB  3. Start a Docker container based on this image: $ docker run -it -p 8888:8888 yandex/tutorial-catboost-clickhouse  "},{"title":"1. Create a Table​","type":1,"pageTitle":"Applying a Catboost Model in ClickHouse","url":"docs/guides/developer/apply-catboost-model#create-table","content":"To create a ClickHouse table for the training sample: 1. Start ClickHouse console client in the interactive mode: $ clickhouse client  note The ClickHouse server is already running inside the Docker container. 2. Create the table using the command: :) CREATE TABLE amazon_train ( date Date MATERIALIZED today(), ACTION UInt8, RESOURCE UInt32, MGR_ID UInt32, ROLE_ROLLUP_1 UInt32, ROLE_ROLLUP_2 UInt32, ROLE_DEPTNAME UInt32, ROLE_TITLE UInt32, ROLE_FAMILY_DESC UInt32, ROLE_FAMILY UInt32, ROLE_CODE UInt32 ) ENGINE = MergeTree ORDER BY date  3. Exit from ClickHouse console client: :) exit  "},{"title":"2. Insert the Data to the Table​","type":1,"pageTitle":"Applying a Catboost Model in ClickHouse","url":"docs/guides/developer/apply-catboost-model#insert-data-to-table","content":"To insert the data: 1. Run the following command: $ clickhouse client --host 127.0.0.1 --query 'INSERT INTO amazon_train FORMAT CSVWithNames' &lt; ~/amazon/train.csv  2. Start ClickHouse console client in the interactive mode: $ clickhouse client  3. Make sure the data has been uploaded: :) SELECT count() FROM amazon_train SELECT count() FROM amazon_train +-count()-+ | 65538 | +-------+  "},{"title":"3. Integrate CatBoost into ClickHouse​","type":1,"pageTitle":"Applying a Catboost Model in ClickHouse","url":"docs/guides/developer/apply-catboost-model#integrate-catboost-into-clickhouse","content":"note Optional step. The Docker image contains everything you need to run CatBoost and ClickHouse. To integrate CatBoost into ClickHouse: 1. Build the evaluation library. The fastest way to evaluate a CatBoost model is compile libcatboostmodel.&lt;so|dll|dylib&gt; library. For more information about how to build the library, see CatBoost documentation. 2. Create a new directory anywhere and with any name, for example, data and put the created library in it. The Docker image already contains the library data/libcatboostmodel.so. 3. Create a new directory for config model anywhere and with any name, for example, models. 4. Create a model configuration file with any name, for example, models/amazon_model.xml. 5. Describe the model configuration: &lt;models&gt; &lt;model&gt; &lt;!-- Model type. Now catboost only. --&gt; &lt;type&gt;catboost&lt;/type&gt; &lt;!-- Model name. --&gt; &lt;name&gt;amazon&lt;/name&gt; &lt;!-- Path to trained model. --&gt; &lt;path&gt;/home/catboost/tutorial/catboost_model.bin&lt;/path&gt; &lt;!-- Update interval. --&gt; &lt;lifetime&gt;0&lt;/lifetime&gt; &lt;/model&gt; &lt;/models&gt;  6. Add the path to CatBoost and the model configuration to the ClickHouse configuration: &lt;!-- File etc/clickhouse-server/config.d/models_config.xml. --&gt; &lt;catboost_dynamic_library_path&gt;/home/catboost/data/libcatboostmodel.so&lt;/catboost_dynamic_library_path&gt; &lt;models_config&gt;/home/catboost/models/*_model.xml&lt;/models_config&gt;  note You can change path to the CatBoost model configuration later without restarting server. "},{"title":"4. Run the Model Inference from SQL​","type":1,"pageTitle":"Applying a Catboost Model in ClickHouse","url":"docs/guides/developer/apply-catboost-model#run-model-inference","content":"For test model run the ClickHouse client $ clickhouse client. Let’s make sure that the model is working: :) SELECT modelEvaluate('amazon', RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTNAME, ROLE_TITLE, ROLE_FAMILY_DESC, ROLE_FAMILY, ROLE_CODE) &gt; 0 AS prediction, ACTION AS target FROM amazon_train LIMIT 10  note Function modelEvaluate returns tuple with per-class raw predictions for multiclass models. Let’s predict the probability: :) SELECT modelEvaluate('amazon', RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTNAME, ROLE_TITLE, ROLE_FAMILY_DESC, ROLE_FAMILY, ROLE_CODE) AS prediction, 1. / (1 + exp(-prediction)) AS probability, ACTION AS target FROM amazon_train LIMIT 10  note More info about exp() function. Let’s calculate LogLoss on the sample: :) SELECT -avg(tg * log(prob) + (1 - tg) * log(1 - prob)) AS logloss FROM ( SELECT modelEvaluate('amazon', RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTNAME, ROLE_TITLE, ROLE_FAMILY_DESC, ROLE_FAMILY, ROLE_CODE) AS prediction, 1. / (1. + exp(-prediction)) AS prob, ACTION AS tg FROM amazon_train )  note More info about avg() and log() functions. Original article "},{"title":"How do I contribute code to ClickHouse?","type":0,"sectionRef":"#","url":"docs/faq/general/how-do-i-contribute-code-to-clickhouse","content":"How do I contribute code to ClickHouse? ClickHouse is an open-source project developed on GitHub. As customary, contribution instructions are published in CONTRIBUTING.md file in the root of the source code repository. If you want to suggest a substantial change to ClickHouse, consider opening a GitHub issue explaining what you want to do, to discuss it with maintainers and community first. Examples of such RFC issues. If your contributions are security related, please check out our security policy too.","keywords":""},{"title":"Updating and Deleting ClickHouse Data","type":0,"sectionRef":"#","url":"docs/guides/developer/mutations","content":"","keywords":"update delete mutation"},{"title":"Updating Existing Data​","type":1,"pageTitle":"Updating and Deleting ClickHouse Data","url":"docs/guides/developer/mutations#updating-existing-data","content":"From ClickHouse client, enter your update ALTER TABLE command in this form: ALTER TABLE [&lt;database&gt;.]&lt;table&gt; UPDATE &lt;column&gt; = &lt;expression&gt; WHERE &lt;filter_expr&gt;  &lt;expression&gt; is the new value for the column where the &lt;filter_expr&gt; is satisfied. The &lt;expression&gt; must be the same datatype as the column or be convertable to the same datatype using the CAST operator. The &lt;filter_expr&gt; should return a UInt8 (zero or non-zero) value for each row of the data. Multiple UPDATE &lt;column&gt; statements can be combined in a single ALTER TABLE command separated by commas. Examples: A mutation like this allows updating replacing visitor_ids with new ones using a dictionary lookup: ALTER TABLE website.clicks UPDATE visitor_id = getDict('visitors', 'new_visitor_id', visitor_id) WHERE visit_date &lt; '2022-01-01' Modifying multiple values in one command can be more efficient than multiple commands: ALTER TABLE website.clicks UPDATE url = substring(url, position(url, '://') + 3), visitor_id = new_visit_id WHERE visit_date &lt; '2022-01-01' Mutations can be exectuted ON CLUSTER for sharded tables: ALTER TABLE clicks ON CLUSTER main_cluster UPDATE click_count = click_count / 2 WHERE visitor_id ILIKE '%robot%'  Note It is not possible to update columns that are part of the primary or sorting key. "},{"title":"Deleting Data​","type":1,"pageTitle":"Updating and Deleting ClickHouse Data","url":"docs/guides/developer/mutations#deleting-data","content":"From ClickHouse client, enter your delete ALTER TABLE command in this form: ALTER TABLE [&lt;database&gt;.]&lt;table&gt; DELETE WHERE &lt;filter_expr&gt;  Again &lt;filter_expr&gt; should return a UInt8 value for each row of data. Examples Delete any records where a column is in an array of values: ALTER TABLE website.clicks DELETE WHERE visitor_id in (253, 1002, 4277) What does this query alter? ALTER TABLE clicks ON CLUSTER main_cluster WHERE visit_date &lt; '2022-01-02 15:00:00' AND page_id = '573'  Note To delete all of the data in a table, it is more efficient to use the command TRUNCATE TABLE [&lt;database].]&lt;table&gt; command. This command can also be executed ON CLUSTER. "},{"title":"2019","type":0,"sectionRef":"#","url":"docs/en/whats-new/changelog/2019","content":"","keywords":""},{"title":"ClickHouse Release 19.17​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-v19-17","content":""},{"title":"ClickHouse Release 19.17.6.36, 2019-12-27​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-v19-17-6-36-2019-12-27","content":"Bug Fix​ Fixed potential buffer overflow in decompress. Malicious user can pass fabricated compressed data that could cause read after buffer. This issue was found by Eldar Zaitov from Yandex information security team. #8404 (alexey-milovidov)Fixed possible server crash (std::terminate) when the server cannot send or write data in JSON or XML format with values of String data type (that require UTF-8 validation) or when compressing result data with Brotli algorithm or in some other rare cases. #8384 (alexey-milovidov)Fixed dictionaries with source from a clickhouse VIEW, now reading such dictionaries does not cause the error There is no query. #8351 (Nikolai Kochetov)Fixed checking if a client host is allowed by host_regexp specified in users.xml. #8241, #8342 (Vitaly Baranov)RENAME TABLE for a distributed table now renames the folder containing inserted data before sending to shards. This fixes an issue with successive renames tableA-&gt;tableB, tableC-&gt;tableA. #8306 (tavplubix)range_hashed external dictionaries created by DDL queries now allow ranges of arbitrary numeric types. #8275 (alesapin)Fixed INSERT INTO table SELECT ... FROM mysql(...) table function. #8234 (tavplubix)Fixed segfault in INSERT INTO TABLE FUNCTION file() while inserting into a file which does not exist. Now in this case file would be created and then insert would be processed. #8177 (Olga Khvostikova)Fixed bitmapAnd error when intersecting an aggregated bitmap and a scalar bitmap. #8082 (Yue Huang)Fixed segfault when EXISTS query was used without TABLE or DICTIONARY qualifier, just like EXISTS t. #8213 (alexey-milovidov)Fixed return type for functions rand and randConstant in case of nullable argument. Now functions always return UInt32 and never Nullable(UInt32). #8204 (Nikolai Kochetov)Fixed DROP DICTIONARY IF EXISTS db.dict, now it does not throw exception if db does not exist. #8185 (Vitaly Baranov)If a table wasn’t completely dropped because of server crash, the server will try to restore and load it #8176 (tavplubix)Fixed a trivial count query for a distributed table if there are more than two shard local table. #8164 (小路)Fixed bug that lead to a data race in DB::BlockStreamProfileInfo::calculateRowsBeforeLimit() #8143 (Alexander Kazakov)Fixed ALTER table MOVE part executed immediately after merging the specified part, which could cause moving a part which the specified part merged into. Now it correctly moves the specified part. #8104 (Vladimir Chebotarev)Expressions for dictionaries can be specified as strings now. This is useful for calculation of attributes while extracting data from non-ClickHouse sources because it allows to use non-ClickHouse syntax for those expressions. #8098 (alesapin)Fixed a very rare race in clickhouse-copier because of an overflow in ZXid. #8088 (Ding Xiang Fei)Fixed the bug when after the query failed (due to “Too many simultaneous queries” for example) it would not read external tables info, and the next request would interpret this info as the beginning of the next query causing an error like Unknown packet from client. #8084 (Azat Khuzhin)Avoid null dereference after “Unknown packet X from server” #8071 (Azat Khuzhin)Restore support of all ICU locales, add the ability to apply collations for constant expressions and add language name to system.collations table. #8051 (alesapin)Number of streams for read from StorageFile and StorageHDFS is now limited, to avoid exceeding the memory limit. #7981 (alesapin)Fixed CHECK TABLE query for *MergeTree tables without key. #7979 (alesapin)Removed the mutation number from a part name in case there were no mutations. This removing improved the compatibility with older versions. #8250 (alesapin)Fixed the bug that mutations are skipped for some attached parts due to their data_version are larger than the table mutation version. #7812 (Zhichang Yu)Allow starting the server with redundant copies of parts after moving them to another device. #7810 (Vladimir Chebotarev)Fixed the error “Sizes of columns does not match” that might appear when using aggregate function columns. #7790 (Boris Granveaud)Now an exception will be thrown in case of using WITH TIES alongside LIMIT BY. And now it’s possible to use TOP with LIMIT BY. #7637 (Nikita Mikhaylov)Fix dictionary reload if it has invalidate_query, which stopped updates and some exception on previous update tries. #8029 (alesapin) "},{"title":"ClickHouse Release 19.17.4.11, 2019-11-22​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-v19-17-4-11-2019-11-22","content":"Backward Incompatible Change​ Using column instead of AST to store scalar subquery results for better performance. Setting enable_scalar_subquery_optimization was added in 19.17 and it was enabled by default. It leads to errors like this during upgrade to 19.17.2 or 19.17.3 from previous versions. This setting was disabled by default in 19.17.4, to make possible upgrading from 19.16 and older versions without errors. #7392 (Amos Bird) New Feature​ Add the ability to create dictionaries with DDL queries. #7360 (alesapin)Make bloom_filter type of index supporting LowCardinality and Nullable #7363 #7561 (Nikolai Kochetov)Add function isValidJSON to check that passed string is a valid json. #5910 #7293 (Vdimir)Implement arrayCompact function #7328 (Memo)Created function hex for Decimal numbers. It works like hex(reinterpretAsString()), but does not delete last zero bytes. #7355 (Mikhail Korotov)Add arrayFill and arrayReverseFill functions, which replace elements by other elements in front/back of them in the array. #7380 (hcz)Add CRC32IEEE()/CRC64() support #7480 (Azat Khuzhin)Implement char function similar to one in mysql #7486 (sundyli)Add bitmapTransform function. It transforms an array of values in a bitmap to another array of values, the result is a new bitmap #7598 (Zhichang Yu)Implemented javaHashUTF16LE() function #7651 (achimbab)Add _shard_num virtual column for the Distributed engine #7624 (Azat Khuzhin) Experimental Feature​ Support for processors (new query execution pipeline) in MergeTree. #7181 (Nikolai Kochetov) Bug Fix​ Fix incorrect float parsing in Values #7817 #7870 (tavplubix)Fix rare deadlock which can happen when trace_log is enabled. #7838 (filimonov)Prevent message duplication when producing Kafka table has any MVs selecting from it #7265 (Ivan)Support for Array(LowCardinality(Nullable(String))) in IN. Resolves #7364 #7366 (achimbab)Add handling of SQL_TINYINT and SQL_BIGINT, and fix handling of SQL_FLOAT data source types in ODBC Bridge. #7491 (Denis Glazachev)Fix aggregation (avg and quantiles) over empty decimal columns #7431 (Andrey Konyaev)Fix INSERT into Distributed with MATERIALIZED columns #7377 (Azat Khuzhin)Make MOVE PARTITION work if some parts of partition are already on destination disk or volume #7434 (Vladimir Chebotarev)Fixed bug with hardlinks failing to be created during mutations in ReplicatedMergeTree in multi-disk configurations. #7558 (Vladimir Chebotarev)Fixed a bug with a mutation on a MergeTree when whole part remains unchanged and best space is being found on another disk #7602 (Vladimir Chebotarev)Fixed bug with keep_free_space_ratio not being read from disks configuration #7645 (Vladimir Chebotarev)Fix bug with table contains only Tuple columns or columns with complex paths. Fixes 7541. #7545 (alesapin)Do not account memory for Buffer engine in max_memory_usage limit #7552 (Azat Khuzhin)Fix final mark usage in MergeTree tables ordered by tuple(). In rare cases it could lead to Can't adjust last granule error while select. #7639 (Anton Popov)Fix bug in mutations that have predicate with actions that require context (for example functions for json), which may lead to crashes or strange exceptions. #7664 (alesapin)Fix mismatch of database and table names escaping in data/ and shadow/ directories #7575 (Alexander Burmak)Support duplicated keys in RIGHT|FULL JOINs, e.g. ON t.x = u.x AND t.x = u.y. Fix crash in this case. #7586 (Artem Zuikov)Fix Not found column &lt;expression&gt; in block when joining on expression with RIGHT or FULL JOIN. #7641 (Artem Zuikov)One more attempt to fix infinite loop in PrettySpace format #7591 (Olga Khvostikova)Fix bug in concat function when all arguments were FixedString of the same size. #7635 (alesapin)Fixed exception in case of using 1 argument while defining S3, URL and HDFS storages. #7618 (Vladimir Chebotarev)Fix scope of the InterpreterSelectQuery for views with query #7601 (Azat Khuzhin) Improvement​ Nullable columns recognized and NULL-values handled correctly by ODBC-bridge #7402 (Vasily Nemkov)Write current batch for distributed send atomically #7600 (Azat Khuzhin)Throw an exception if we cannot detect table for column name in query. #7358 (Artem Zuikov)Add merge_max_block_size setting to MergeTreeSettings #7412 (Artem Zuikov)Queries with HAVING and without GROUP BY assume group by constant. So, SELECT 1 HAVING 1 now returns a result. #7496 (Amos Bird)Support parsing (X,) as tuple similar to python. #7501, #7562 (Amos Bird)Make range function behaviors almost like pythonic one. #7518 (sundyli)Add constraints columns to table system.settings #7553 (Vitaly Baranov)Better Null format for tcp handler, so that it’s possible to use select ignore(&lt;expression&gt;) from table format Null for perf measure via clickhouse-client #7606 (Amos Bird)Queries like CREATE TABLE ... AS (SELECT (1, 2)) are parsed correctly #7542 (hcz) Performance Improvement​ The performance of aggregation over short string keys is improved. #6243 (Alexander Kuzmenkov, Amos Bird)Run another pass of syntax/expression analysis to get potential optimizations after constant predicates are folded. #7497 (Amos Bird)Use storage meta info to evaluate trivial SELECT count() FROM table; #7510 (Amos Bird, alexey-milovidov)Vectorize processing arrayReduce similar to Aggregator addBatch. #7608 (Amos Bird)Minor improvements in performance of Kafka consumption #7475 (Ivan) Build/Testing/Packaging Improvement​ Add support for cross-compiling to the CPU architecture AARCH64. Refactor packager script. #7370 #7539 (Ivan)Unpack darwin-x86_64 and linux-aarch64 toolchains into mounted Docker volume when building packages #7534 (Ivan)Update Docker Image for Binary Packager #7474 (Ivan)Fixed compile errors on MacOS Catalina #7585 (Ernest Poletaev)Some refactoring in query analysis logic: split complex class into several simple ones. #7454 (Artem Zuikov)Fix build without submodules #7295 (proller)Better add_globs in CMake files #7418 (Amos Bird)Remove hardcoded paths in unwind target #7460 (Konstantin Podshumok)Allow to use mysql format without ssl #7524 (proller) Other​ Added ANTLR4 grammar for ClickHouse SQL dialect #7595 #7596 (alexey-milovidov) "},{"title":"ClickHouse Release 19.16​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-v19-16","content":"ClickHouse Release 19.16.14.65, 2020-03-25​ Fixed up a bug in batched calculations of ternary logical OPs on multiple arguments (more than 10). #8718 (Alexander Kazakov) This bugfix was backported to version 19.16 by a special request from Altinity. ClickHouse Release 19.16.14.65, 2020-03-05​ Fix distributed subqueries incompatibility with older CH versions. Fixes #7851(tabplubix)When executing CREATE query, fold constant expressions in storage engine arguments. Replace empty database name with current database. Fixes #6508, #3492. Also fix check for local address in ClickHouseDictionarySource.#9262 (tabplubix)Now background merges in *MergeTree table engines family preserve storage policy volume order more accurately.#8549 (Vladimir Chebotarev)Prevent losing data in Kafka in rare cases when exception happens after reading suffix but before commit. Fixes #9378. Related: #7175#9507 (filimonov)Fix bug leading to server termination when trying to use / drop Kafka table created with wrong parameters. Fixes #9494. Incorporates #9507.#9513 (filimonov)Allow using MaterializedView with subqueries above Kafka tables.#8197 (filimonov) New Feature​ Add deduplicate_blocks_in_dependent_materialized_views option to control the behaviour of idempotent inserts into tables with materialized views. This new feature was added to the bugfix release by a special request from Altinity.#9070 (urykhy) "},{"title":"ClickHouse Release 19.16.2.2, 2019-10-30​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-v19-16-2-2-2019-10-30","content":"Backward Incompatible Change​ Add missing arity validation for count/counIf.#7095#7298 (Vdimir)Remove legacy asterisk_left_columns_only setting (it was disabled by default).#7335 (Artem Zuikov)Format strings for Template data format are now specified in files.#7118(tavplubix) New Feature​ Introduce uniqCombined64() to calculate cardinality greater than UINT_MAX.#7213,#7222 (Azat Khuzhin)Support Bloom filter indexes on Array columns.#6984(achimbab)Add a function getMacro(name) that returns String with the value of corresponding &lt;macros&gt;from server configuration. #7240(alexey-milovidov)Set two configuration options for a dictionary based on an HTTP source: credentials andhttp-headers. #7092 (Guillaume Tassery)Add a new ProfileEvent Merge that counts the number of launched background merges.#7093 (Mikhail Korotov)Add fullHostName function that returns a fully qualified domain name.#7263#7291 (sundyli)Add function arraySplit and arrayReverseSplit which split an array by “cut off” conditions. They are useful in time sequence handling.#7294 (hcz)Add new functions that return the Array of all matched indices in multiMatch family of functions.#7299 (Danila Kutenin)Add a new database engine Lazy that is optimized for storing a large number of small -Log tables. #7171 (Nikita Vasilev)Add aggregate functions groupBitmapAnd, -Or, -Xor for bitmap columns. #7109 (Zhichang Yu)Add aggregate function combinators -OrNull and -OrDefault, which return null or default values when there is nothing to aggregate.#7331(hcz)Introduce CustomSeparated data format that supports custom escaping and delimiter rules. #7118(tavplubix)Support Redis as source of external dictionary. #4361 #6962 (comunodi, Anton Popov) Bug Fix​ Fix wrong query result if it has WHERE IN (SELECT ...) section and optimize_read_in_order is used. #7371 (Anton Popov)Disabled MariaDB authentication plugin, which depends on files outside of project.#7140 (Yuriy Baranov)Fix exception Cannot convert column ... because it is constant but values of constants are different in source and result which could rarely happen when functions now(), today(),yesterday(), randConstant() are used.#7156 (Nikolai Kochetov)Fixed issue of using HTTP keep alive timeout instead of TCP keep alive timeout.#7351 (Vasily Nemkov)Fixed a segmentation fault in groupBitmapOr (issue #7109).#7289 (Zhichang Yu)For materialized views the commit for Kafka is called after all data were written.#7175 (Ivan)Fixed wrong duration_ms value in system.part_log table. It was ten times off.#7172 (Vladimir Chebotarev)A quick fix to resolve crash in LIVE VIEW table and re-enabling all LIVE VIEW tests.#7201(vzakaznikov)Serialize NULL values correctly in min/max indexes of MergeTree parts.#7234 (Alexander Kuzmenkov)Don’t put virtual columns to .sql metadata when table is created as CREATE TABLE AS.#7183 (Ivan)Fix segmentation fault in ATTACH PART query.#7185(alesapin)Fix wrong result for some queries given by the optimization of empty IN subqueries and empty INNER/RIGHT JOIN. #7284 (Nikolai Kochetov)Fixing AddressSanitizer error in the LIVE VIEW getHeader() method.#7271(vzakaznikov) Improvement​ Add a message in case of queue_wait_max_ms wait takes place.#7390 (Azat Khuzhin)Made setting s3_min_upload_part_size table-level.#7059 (Vladimir Chebotarev)Check TTL in StorageFactory. #7304(sundyli)Squash left-hand blocks in partial merge join (optimization).#7122 (Artem Zuikov)Do not allow non-deterministic functions in mutations of Replicated table engines, because this can introduce inconsistencies between replicas.#7247 (Alexander Kazakov)Disable memory tracker while converting exception stack trace to string. It can prevent the loss of error messages of type Memory limit exceeded on server, which caused the Attempt to read after eof exception on client. #7264(Nikolai Kochetov)Miscellaneous format improvements. Resolves#6033,#2633,#6611,#6742#7215(tavplubix)ClickHouse ignores values on the right side of IN operator that are not convertible to the left side type. Make it work properly for compound types – Array and Tuple.#7283 (Alexander Kuzmenkov)Support missing inequalities for ASOF JOIN. It’s possible to join less-or-equal variant and strict greater and less variants for ASOF column in ON syntax.#7282 (Artem Zuikov)Optimize partial merge join. #7070(Artem Zuikov)Do not use more than 98K of memory in uniqCombined functions.#7236,#7270 (Azat Khuzhin)Flush parts of right-hand joining table on disk in PartialMergeJoin (if there is not enough memory). Load data back when needed. #7186(Artem Zuikov) Performance Improvement​ Speed up joinGet with const arguments by avoiding data duplication.#7359 (Amos Bird)Return early if the subquery is empty.#7007 (小路)Optimize parsing of SQL expression in Values.#6781(tavplubix) Build/Testing/Packaging Improvement​ Disable some contribs for cross-compilation to Mac OS.#7101 (Ivan)Add missing linking with PocoXML for clickhouse_common_io.#7200 (Azat Khuzhin)Accept multiple test filter arguments in clickhouse-test.#7226 (Alexander Kuzmenkov)Enable musl and jemalloc for ARM. #7300(Amos Bird)Added --client-option parameter to clickhouse-test to pass additional parameters to client.#7277 (Nikolai Kochetov)Preserve existing configs on rpm package upgrade.#7103(filimonov)Fix errors detected by PVS. #7153 (Artem Zuikov)Fix build for Darwin. #7149(Ivan)glibc 2.29 compatibility. #7142 (Amos Bird)Make sure dh_clean does not touch potential source files.#7205 (Amos Bird)Attempt to avoid conflict when updating from altinity rpm - it has config file packaged separately in clickhouse-server-common. #7073(filimonov)Optimize some header files for faster rebuilds.#7212,#7231 (Alexander Kuzmenkov)Add performance tests for Date and DateTime. #7332 (Vasily Nemkov)Fix some tests that contained non-deterministic mutations.#7132 (Alexander Kazakov)Add build with MemorySanitizer to CI. #7066(Alexander Kuzmenkov)Avoid use of uninitialized values in MetricsTransmitter.#7158 (Azat Khuzhin)Fix some issues in Fields found by MemorySanitizer.#7135,#7179 (Alexander Kuzmenkov), #7376(Amos Bird)Fix undefined behavior in murmurhash32. #7388 (Amos Bird)Fix undefined behavior in StoragesInfoStream. #7384(tavplubix)Fixed constant expressions folding for external database engines (MySQL, ODBC, JDBC). In previous versions it wasn’t working for multiple constant expressions and was not working at all for Date, DateTime and UUID. This fixes #7245#7252(alexey-milovidov)Fixing ThreadSanitizer data race error in the LIVE VIEW when accessing no_users_thread variable.#7353(vzakaznikov)Get rid of malloc symbols in libcommon#7134,#7065 (Amos Bird)Add global flag ENABLE_LIBRARIES for disabling all libraries.#7063(proller) Code Cleanup​ Generalize configuration repository to prepare for DDL for Dictionaries. #7155(alesapin)Parser for dictionaries DDL without any semantic.#7209(alesapin)Split ParserCreateQuery into different smaller parsers.#7253(alesapin)Small refactoring and renaming near external dictionaries.#7111(alesapin)Refactor some code to prepare for role-based access control. #7235 (Vitaly Baranov)Some improvements in DatabaseOrdinary code.#7086 (Nikita Vasilev)Do not use iterators in find() and emplace() methods of hash tables.#7026 (Alexander Kuzmenkov)Fix getMultipleValuesFromConfig in case when parameter root is not empty. #7374(Mikhail Korotov)Remove some copy-paste (TemporaryFile and TemporaryFileStream)#7166 (Artem Zuikov)Improved code readability a little bit (MergeTreeData::getActiveContainingPart).#7361 (Vladimir Chebotarev)Wait for all scheduled jobs, which are using local objects, if ThreadPool::schedule(...) throws an exception. Rename ThreadPool::schedule(...) to ThreadPool::scheduleOrThrowOnError(...) and fix comments to make obvious that it may throw.#7350(tavplubix) "},{"title":"ClickHouse Release 19.15​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-15","content":""},{"title":"ClickHouse Release 19.15.4.10, 2019-10-31​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-15-4-10-2019-10-31","content":"Bug Fix​ Added handling of SQL_TINYINT and SQL_BIGINT, and fix handling of SQL_FLOAT data source types in ODBC Bridge.#7491 (Denis Glazachev)Allowed to have some parts on destination disk or volume in MOVE PARTITION.#7434 (Vladimir Chebotarev)Fixed NULL-values in nullable columns through ODBC-bridge.#7402 (Vasily Nemkov)Fixed INSERT into Distributed non local node with MATERIALIZED columns.#7377 (Azat Khuzhin)Fixed function getMultipleValuesFromConfig.#7374 (Mikhail Korotov)Fixed issue of using HTTP keep alive timeout instead of TCP keep alive timeout.#7351 (Vasily Nemkov)Wait for all jobs to finish on exception (fixes rare segfaults).#7350 (tavplubix)Don’t push to MVs when inserting into Kafka table.#7265 (Ivan)Disable memory tracker for exception stack.#7264 (Nikolai Kochetov)Fixed bad code in transforming query for external database.#7252 (alexey-milovidov)Avoid use of uninitialized values in MetricsTransmitter.#7158 (Azat Khuzhin)Added example config with macros for tests (alexey-milovidov) "},{"title":"ClickHouse Release 19.15.3.6, 2019-10-09​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-15-3-6-2019-10-09","content":"Bug Fix​ Fixed bad_variant in hashed dictionary. (alesapin)Fixed up bug with segmentation fault in ATTACH PART query. (alesapin)Fixed time calculation in MergeTreeData. (Vladimir Chebotarev)Commit to Kafka explicitly after the writing is finalized.#7175 (Ivan)Serialize NULL values correctly in min/max indexes of MergeTree parts.#7234 (Alexander Kuzmenkov) "},{"title":"ClickHouse Release 19.15.2.2, 2019-10-01​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-15-2-2-2019-10-01","content":"New Feature​ Tiered storage: support to use multiple storage volumes for tables with MergeTree engine. It’s possible to store fresh data on SSD and automatically move old data to HDD. (example). #4918 (Igr) #6489 (alesapin)Add table function input for reading incoming data in INSERT SELECT query. #5450 (palasonic1) #6832 (Anton Popov)Add a sparse_hashed dictionary layout, that is functionally equivalent to the hashed layout, but is more memory efficient. It uses about twice as less memory at the cost of slower value retrieval. #6894 (Azat Khuzhin)Implement ability to define list of users for access to dictionaries. Only current connected database using. #6907 (Guillaume Tassery)Add LIMIT option to SHOW query. #6944 (Philipp Malkovsky)Add bitmapSubsetLimit(bitmap, range_start, limit) function, that returns subset of the smallest limit values in set that is no smaller than range_start. #6957 (Zhichang Yu)Add bitmapMin and bitmapMax functions. #6970 (Zhichang Yu)Add function repeat related to issue-6648 #6999 (flynn) Experimental Feature​ Implement (in memory) Merge Join variant that does not change current pipeline. Result is partially sorted by merge key. Set partial_merge_join = 1 to use this feature. The Merge Join is still in development. #6940 (Artem Zuikov)Add S3 engine and table function. It is still in development (no authentication support yet). #5596 (Vladimir Chebotarev) Improvement​ Every message read from Kafka is inserted atomically. This resolves almost all known issues with Kafka engine. #6950 (Ivan)Improvements for failover of Distributed queries. Shorten recovery time, also it is now configurable and can be seen in system.clusters. #6399 (Vasily Nemkov)Support numeric values for Enums directly in IN section. #6766 #6941 (dimarub2000)Support (optional, disabled by default) redirects on URL storage. #6914 (maqroll)Add information message when client with an older version connects to a server. #6893 (Philipp Malkovsky)Remove maximum backoff sleep time limit for sending data in Distributed tables #6895 (Azat Khuzhin)Add ability to send profile events (counters) with cumulative values to graphite. It can be enabled under &lt;events_cumulative&gt; in server config.xml. #6969 (Azat Khuzhin)Add automatically cast type T to LowCardinality(T) while inserting data in column of type LowCardinality(T) in Native format via HTTP. #6891 (Nikolai Kochetov)Add ability to use function hex without using reinterpretAsString for Float32, Float64. #7024 (Mikhail Korotov) Build/Testing/Packaging Improvement​ Add gdb-index to clickhouse binary with debug info. It will speed up startup time of gdb. #6947 (alesapin)Speed up deb packaging with patched dpkg-deb which uses pigz. #6960 (alesapin)Set enable_fuzzing = 1 to enable libfuzzer instrumentation of all the project code. #7042 (kyprizel)Add split build smoke test in CI. #7061 (alesapin)Add build with MemorySanitizer to CI. #7066 (Alexander Kuzmenkov)Replace libsparsehash with sparsehash-c11 #6965 (Azat Khuzhin) Bug Fix​ Fixed performance degradation of index analysis on complex keys on large tables. This fixes #6924. #7075 (alexey-milovidov)Fix logical error causing segfaults when selecting from Kafka empty topic. #6909 (Ivan)Fix too early MySQL connection close in MySQLBlockInputStream.cpp. #6882 (Clément Rodriguez)Returned support for very old Linux kernels (fix #6841) #6853 (alexey-milovidov)Fix possible data loss in insert select query in case of empty block in input stream. #6834 #6862 #6911 (Nikolai Kochetov)Fix for function АrrayEnumerateUniqRanked with empty arrays in params #6928 (proller)Fix complex queries with array joins and global subqueries. #6934 (Ivan)Fix Unknown identifier error in ORDER BY and GROUP BY with multiple JOINs #7022 (Artem Zuikov)Fixed MSan warning while executing function with LowCardinality argument. #7062 (Nikolai Kochetov) Backward Incompatible Change​ Changed serialization format of bitmap* aggregate function states to improve performance. Serialized states of bitmap* from previous versions cannot be read. #6908 (Zhichang Yu) "},{"title":"ClickHouse Release 19.14​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-14","content":""},{"title":"ClickHouse Release 19.14.7.15, 2019-10-02​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-14-7-15-2019-10-02","content":"Bug Fix​ This release also contains all bug fixes from 19.11.12.69.Fixed compatibility for distributed queries between 19.14 and earlier versions. This fixes #7068. #7069 (alexey-milovidov) "},{"title":"ClickHouse Release 19.14.6.12, 2019-09-19​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-14-6-12-2019-09-19","content":"Bug Fix​ Fix for function АrrayEnumerateUniqRanked with empty arrays in params. #6928 (proller)Fixed subquery name in queries with ARRAY JOIN and GLOBAL IN subquery with alias. Use subquery alias for external table name if it is specified. #6934 (Ivan) Build/Testing/Packaging Improvement​ Fix flapping test 00715_fetch_merged_or_mutated_part_zookeeper by rewriting it to a shell scripts because it needs to wait for mutations to apply. #6977 (Alexander Kazakov)Fixed UBSan and MemSan failure in function groupUniqArray with emtpy array argument. It was caused by placing of empty PaddedPODArray into hash table zero cell because constructor for zero cell value was not called. #6937 (Amos Bird) "},{"title":"ClickHouse Release 19.14.3.3, 2019-09-10​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-14-3-3-2019-09-10","content":"New Feature​ WITH FILL modifier for ORDER BY. (continuation of #5069) #6610 (Anton Popov)WITH TIES modifier for LIMIT. (continuation of #5069) #6610 (Anton Popov)Parse unquoted NULL literal as NULL (if setting format_csv_unquoted_null_literal_as_null=1). Initialize null fields with default values if data type of this field is not nullable (if setting input_format_null_as_default=1). #5990 #6055 (tavplubix)Support for wildcards in paths of table functions file and hdfs. If the path contains wildcards, the table will be readonly. Example of usage: select * from hdfs('hdfs://hdfs1:9000/some_dir/another_dir/*/file{0..9}{0..9}') and select * from file('some_dir/{some_file,another_file,yet_another}.tsv', 'TSV', 'value UInt32'). #6092 (Olga Khvostikova)New system.metric_log table which stores values of system.events and system.metrics with specified time interval. #6363 #6467 (Nikita Mikhaylov) #6530 (alexey-milovidov)Allow to write ClickHouse text logs to system.text_log table. #6037 #6103 (Nikita Mikhaylov) #6164 (alexey-milovidov)Show private symbols in stack traces (this is done via parsing symbol tables of ELF files). Added information about file and line number in stack traces if debug info is present. Speedup symbol name lookup with indexing symbols present in program. Added new SQL functions for introspection: demangle and addressToLine. Renamed function symbolizeAddress to addressToSymbol for consistency. Function addressToSymbol will return mangled name for performance reasons and you have to apply demangle. Added setting allow_introspection_functions which is turned off by default. #6201 (alexey-milovidov)Table function values (the name is case-insensitive). It allows to read from VALUES list proposed in #5984. Example: SELECT * FROM VALUES('a UInt64, s String', (1, 'one'), (2, 'two'), (3, 'three')). #6217. #6209 (dimarub2000)Added an ability to alter storage settings. Syntax: ALTER TABLE &lt;table&gt; MODIFY SETTING &lt;setting&gt; = &lt;value&gt;. #6366 #6669 #6685 (alesapin)Support for removing of detached parts. Syntax: ALTER TABLE &lt;table_name&gt; DROP DETACHED PART '&lt;part_id&gt;'. #6158 (tavplubix)Table constraints. Allows to add constraint to table definition which will be checked at insert. #5273 (Gleb Novikov) #6652 (alexey-milovidov)Suppport for cascaded materialized views. #6324 (Amos Bird)Turn on query profiler by default to sample every query execution thread once a second. #6283 (alexey-milovidov)Input format ORC. #6454 #6703 (akonyaev90)Added two new functions: sigmoid and tanh (that are useful for machine learning applications). #6254 (alexey-milovidov)Function hasToken(haystack, token), hasTokenCaseInsensitive(haystack, token) to check if given token is in haystack. Token is a maximal length substring between two non alphanumeric ASCII characters (or boundaries of haystack). Token must be a constant string. Supported by tokenbf_v1 index specialization. #6596, #6662 (Vasily Nemkov)New function neighbor(value, offset[, default_value]). Allows to reach prev/next value within column in a block of data. #5925 (Alex Krash) 6685365ab8c5b74f9650492c88a012596eb1b0c6 341e2e4587a18065c2da1ca888c73389f48ce36c Alexey MilovidovCreated a function currentUser(), returning login of authorized user. Added alias user() for compatibility with MySQL. #6470 (Alex Krash)New aggregate functions quantilesExactInclusive and quantilesExactExclusive which were proposed in #5885. #6477 (dimarub2000)Function bitmapRange(bitmap, range_begin, range_end) which returns new set with specified range (not include the range_end). #6314 (Zhichang Yu)Function geohashesInBox(longitude_min, latitude_min, longitude_max, latitude_max, precision) which creates array of precision-long strings of geohash-boxes covering provided area. #6127 (Vasily Nemkov)Implement support for INSERT query with Kafka tables. #6012 (Ivan)Added support for _partition and _timestamp virtual columns to Kafka engine. #6400 (Ivan)Possibility to remove sensitive data from query_log, server logs, process list with regexp-based rules. #5710 (filimonov) Experimental Feature​ Input and output data format Template. It allows to specify custom format string for input and output. #4354 #6727 (tavplubix)Implementation of LIVE VIEW tables that were originally proposed in #2898, prepared in #3925, and then updated in #5541. See #5541 for detailed description. #5541 (vzakaznikov) #6425 (Nikolai Kochetov) #6656 (vzakaznikov) Note that LIVE VIEW feature may be removed in next versions. Bug Fix​ This release also contains all bug fixes from 19.13 and 19.11.Fix segmentation fault when the table has skip indices and vertical merge happens. #6723 (alesapin)Fix per-column TTL with non-trivial column defaults. Previously in case of force TTL merge with OPTIMIZE ... FINAL query, expired values was replaced by type defaults instead of user-specified column defaults. #6796 (Anton Popov)Fix Kafka messages duplication problem on normal server restart. #6597 (Ivan)Fixed infinite loop when reading Kafka messages. Do not pause/resume consumer on subscription at all - otherwise it may get paused indefinitely in some scenarios. #6354 (Ivan)Fix Key expression contains comparison between inconvertible types exception in bitmapContains function. #6136 #6146 #6156 (dimarub2000)Fix segfault with enabled optimize_skip_unused_shards and missing sharding key. #6384 (Anton Popov)Fixed wrong code in mutations that may lead to memory corruption. Fixed segfault with read of address 0x14c0 that may happed due to concurrent DROP TABLE and SELECT from system.parts or system.parts_columns. Fixed race condition in preparation of mutation queries. Fixed deadlock caused by OPTIMIZE of Replicated tables and concurrent modification operations like ALTERs. #6514 (alexey-milovidov)Removed extra verbose logging in MySQL interface #6389 (alexey-milovidov)Return the ability to parse boolean settings from ‘true’ and ‘false’ in the configuration file. #6278 (alesapin)Fix crash in quantile and median function over Nullable(Decimal128). #6378 (Artem Zuikov)Fixed possible incomplete result returned by SELECT query with WHERE condition on primary key contained conversion to Float type. It was caused by incorrect checking of monotonicity in toFloat function. #6248 #6374 (dimarub2000)Check max_expanded_ast_elements setting for mutations. Clear mutations after TRUNCATE TABLE. #6205 (Winter Zhang)Fix JOIN results for key columns when used with join_use_nulls. Attach Nulls instead of columns defaults. #6249 (Artem Zuikov)Fix for skip indices with vertical merge and alter. Fix for Bad size of marks file exception. #6594 #6713 (alesapin)Fix rare crash in ALTER MODIFY COLUMN and vertical merge when one of merged/altered parts is empty (0 rows) #6746 #6780 (alesapin)Fixed bug in conversion of LowCardinality types in AggregateFunctionFactory. This fixes #6257. #6281 (Nikolai Kochetov)Fix wrong behavior and possible segfaults in topK and topKWeighted aggregated functions. #6404 (Anton Popov)Fixed unsafe code around getIdentifier function. #6401 #6409 (alexey-milovidov)Fixed bug in MySQL wire protocol (is used while connecting to ClickHouse form MySQL client). Caused by heap buffer overflow in PacketPayloadWriteBuffer. #6212 (Yuriy Baranov)Fixed memory leak in bitmapSubsetInRange function. #6819 (Zhichang Yu)Fix rare bug when mutation executed after granularity change. #6816 (alesapin)Allow protobuf message with all fields by default. #6132 (Vitaly Baranov)Resolve a bug with nullIf function when we send a NULL argument on the second argument. #6446 (Guillaume Tassery)Fix rare bug with wrong memory allocation/deallocation in complex key cache dictionaries with string fields which leads to infinite memory consumption (looks like memory leak). Bug reproduces when string size was a power of two starting from eight (8, 16, 32, etc). #6447 (alesapin)Fixed Gorilla encoding on small sequences which caused exception Cannot write after end of buffer. #6398 #6444 (Vasily Nemkov)Allow to use not nullable types in JOINs with join_use_nulls enabled. #6705 (Artem Zuikov)Disable Poco::AbstractConfiguration substitutions in query in clickhouse-client. #6706 (alexey-milovidov)Avoid deadlock in REPLACE PARTITION. #6677 (alexey-milovidov)Using arrayReduce for constant arguments may lead to segfault. #6242 #6326 (alexey-milovidov)Fix inconsistent parts which can appear if replica was restored after DROP PARTITION. #6522 #6523 (tavplubix)Fixed hang in JSONExtractRaw function. #6195 #6198 (alexey-milovidov)Fix bug with incorrect skip indices serialization and aggregation with adaptive granularity. #6594. #6748 (alesapin)Fix WITH ROLLUP and WITH CUBE modifiers of GROUP BY with two-level aggregation. #6225 (Anton Popov)Fix bug with writing secondary indices marks with adaptive granularity. #6126 (alesapin)Fix initialization order while server startup. Since StorageMergeTree::background_task_handle is initialized in startup() the MergeTreeBlockOutputStream::write() may try to use it before initialization. Just check if it is initialized. #6080 (Ivan)Clearing the data buffer from the previous read operation that was completed with an error. #6026 (Nikolay)Fix bug with enabling adaptive granularity when creating a new replica for Replicated*MergeTree table. #6394 #6452 (alesapin)Fixed possible crash during server startup in case of exception happened in libunwind during exception at access to uninitialized ThreadStatus structure. #6456 (Nikita Mikhaylov)Fix crash in yandexConsistentHash function. Found by fuzz test. #6304 #6305 (alexey-milovidov)Fixed the possibility of hanging queries when server is overloaded and global thread pool becomes near full. This have higher chance to happen on clusters with large number of shards (hundreds), because distributed queries allocate a thread per connection to each shard. For example, this issue may reproduce if a cluster of 330 shards is processing 30 concurrent distributed queries. This issue affects all versions starting from 19.2. #6301 (alexey-milovidov)Fixed logic of arrayEnumerateUniqRanked function. #6423 (alexey-milovidov)Fix segfault when decoding symbol table. #6603 (Amos Bird)Fixed irrelevant exception in cast of LowCardinality(Nullable) to not-Nullable column in case if it does not contain Nulls (e.g. in query like SELECT CAST(CAST('Hello' AS LowCardinality(Nullable(String))) AS String). #6094 #6119 (Nikolai Kochetov)Removed extra quoting of description in system.settings table. #6696 #6699 (alexey-milovidov)Avoid possible deadlock in TRUNCATE of Replicated table. #6695 (alexey-milovidov)Fix reading in order of sorting key. #6189 (Anton Popov)Fix ALTER TABLE ... UPDATE query for tables with enable_mixed_granularity_parts=1. #6543 (alesapin)Fix bug opened by #4405 (since 19.4.0). Reproduces in queries to Distributed tables over MergeTree tables when we does not query any columns (SELECT 1). #6236 (alesapin)Fixed overflow in integer division of signed type to unsigned type. The behaviour was exactly as in C or C++ language (integer promotion rules) that may be surprising. Please note that the overflow is still possible when dividing large signed number to large unsigned number or vice-versa (but that case is less usual). The issue existed in all server versions. #6214 #6233 (alexey-milovidov)Limit maximum sleep time for throttling when max_execution_speed or max_execution_speed_bytes is set. Fixed false errors like Estimated query execution time (inf seconds) is too long. #5547 #6232 (alexey-milovidov)Fixed issues about using MATERIALIZED columns and aliases in MaterializedView. #448 #3484 #3450 #2878 #2285 #3796 (Amos Bird) #6316 (alexey-milovidov)Fix FormatFactory behaviour for input streams which are not implemented as processor. #6495 (Nikolai Kochetov)Fixed typo. #6631 (Alex Ryndin)Typo in the error message ( is -&gt; are ). #6839 (Denis Zhuravlev)Fixed error while parsing of columns list from string if type contained a comma (this issue was relevant for File, URL, HDFS storages) #6217. #6209 (dimarub2000) Security Fix​ This release also contains all bug security fixes from 19.13 and 19.11.Fixed the possibility of a fabricated query to cause server crash due to stack overflow in SQL parser. Fixed the possibility of stack overflow in Merge and Distributed tables, materialized views and conditions for row-level security that involve subqueries. #6433 (alexey-milovidov) Improvement​ Correct implementation of ternary logic for AND/OR. #6048 (Alexander Kazakov)Now values and rows with expired TTL will be removed after OPTIMIZE ... FINAL query from old parts without TTL infos or with outdated TTL infos, e.g. after ALTER ... MODIFY TTL query. Added queries SYSTEM STOP/START TTL MERGES to disallow/allow assign merges with TTL and filter expired values in all merges. #6274 (Anton Popov)Possibility to change the location of ClickHouse history file for client using CLICKHOUSE_HISTORY_FILE env. #6840 (filimonov)Remove dry_run flag from InterpreterSelectQuery. … #6375 (Nikolai Kochetov)Support ASOF JOIN with ON section. #6211 (Artem Zuikov)Better support of skip indexes for mutations and replication. Support for MATERIALIZE/CLEAR INDEX ... IN PARTITION query. UPDATE x = x recalculates all indices that use column x. #5053 (Nikita Vasilev)Allow to ATTACH live views (for example, at the server startup) regardless to allow_experimental_live_view setting. #6754 (alexey-milovidov)For stack traces gathered by query profiler, do not include stack frames generated by the query profiler itself. #6250 (alexey-milovidov)Now table functions values, file, url, hdfs have support for ALIAS columns. #6255 (alexey-milovidov)Throw an exception if config.d file does not have the corresponding root element as the config file. #6123 (dimarub2000)Print extra info in exception message for no space left on device. #6182, #6252 #6352 (tavplubix)When determining shards of a Distributed table to be covered by a read query (for optimize_skip_unused_shards = 1) ClickHouse now checks conditions from both prewhere and where clauses of select statement. #6521 (Alexander Kazakov)Enabled SIMDJSON for machines without AVX2 but with SSE 4.2 and PCLMUL instruction set. #6285 #6320 (alexey-milovidov)ClickHouse can work on filesystems without O_DIRECT support (such as ZFS and BtrFS) without additional tuning. #4449 #6730 (alexey-milovidov)Support push down predicate for final subquery. #6120 (TCeason) #6162 (alexey-milovidov)Better JOIN ON keys extraction #6131 (Artem Zuikov)Upated SIMDJSON. #6285. #6306 (alexey-milovidov)Optimize selecting of smallest column for SELECT count() query. #6344 (Amos Bird)Added strict parameter in windowFunnel(). When the strict is set, the windowFunnel() applies conditions only for the unique values. #6548 (achimbab)Safer interface of mysqlxx::Pool. #6150 (avasiliev)Options line size when executing with --help option now corresponds with terminal size. #6590 (dimarub2000)Disable “read in order” optimization for aggregation without keys. #6599 (Anton Popov)HTTP status code for INCORRECT_DATA and TYPE_MISMATCH error codes was changed from default 500 Internal Server Error to 400 Bad Request. #6271 (Alexander Rodin)Move Join object from ExpressionAction into AnalyzedJoin. ExpressionAnalyzer and ExpressionAction do not know about Join class anymore. Its logic is hidden by AnalyzedJoin iface. #6801 (Artem Zuikov)Fixed possible deadlock of distributed queries when one of shards is localhost but the query is sent via network connection. #6759 (alexey-milovidov)Changed semantic of multiple tables RENAME to avoid possible deadlocks. #6757. #6756 (alexey-milovidov)Rewritten MySQL compatibility server to prevent loading full packet payload in memory. Decreased memory consumption for each connection to approximately 2 * DBMS_DEFAULT_BUFFER_SIZE (read/write buffers). #5811 (Yuriy Baranov)Move AST alias interpreting logic out of parser that does not have to know anything about query semantics. #6108 (Artem Zuikov)Slightly more safe parsing of NamesAndTypesList. #6408. #6410 (alexey-milovidov)clickhouse-copier: Allow use where_condition from config with partition_key alias in query for checking partition existence (Earlier it was used only in reading data queries). #6577 (proller)Added optional message argument in throwIf. (#5772) #6329 (Vdimir)Server exception got while sending insertion data is now being processed in client as well. #5891 #6711 (dimarub2000)Added a metric DistributedFilesToInsert that shows the total number of files in filesystem that are selected to send to remote servers by Distributed tables. The number is summed across all shards. #6600 (alexey-milovidov)Move most of JOINs prepare logic from ExpressionAction/ExpressionAnalyzer to AnalyzedJoin. #6785 (Artem Zuikov)Fix TSan warning ‘lock-order-inversion’. #6740 (Vasily Nemkov)Better information messages about lack of Linux capabilities. Logging fatal errors with “fatal” level, that will make it easier to find in system.text_log. #6441 (alexey-milovidov)When enable dumping temporary data to the disk to restrict memory usage during GROUP BY, ORDER BY, it didn’t check the free disk space. The fix add a new setting min_free_disk_space, when the free disk space it smaller then the threshold, the query will stop and throw ErrorCodes::NOT_ENOUGH_SPACE. #6678 (Weiqing Xu) #6691 (alexey-milovidov)Removed recursive rwlock by thread. It makes no sense, because threads are reused between queries. SELECT query may acquire a lock in one thread, hold a lock from another thread and exit from first thread. In the same time, first thread can be reused by DROP query. This will lead to false “Attempt to acquire exclusive lock recursively” messages. #6771 (alexey-milovidov)Split ExpressionAnalyzer.appendJoin(). Prepare a place in ExpressionAnalyzer for MergeJoin. #6524 (Artem Zuikov)Added mysql_native_password authentication plugin to MySQL compatibility server. #6194 (Yuriy Baranov)Less number of clock_gettime calls; fixed ABI compatibility between debug/release in Allocator (insignificant issue). #6197 (alexey-milovidov)Move collectUsedColumns from ExpressionAnalyzer to SyntaxAnalyzer. SyntaxAnalyzer makes required_source_columns itself now. #6416 (Artem Zuikov)Add setting joined_subquery_requires_alias to require aliases for subselects and table functions in FROM that more than one table is present (i.e. queries with JOINs). #6733 (Artem Zuikov)Extract GetAggregatesVisitor class from ExpressionAnalyzer. #6458 (Artem Zuikov)system.query_log: change data type of type column to Enum. #6265 (Nikita Mikhaylov)Static linking of sha256_password authentication plugin. #6512 (Yuriy Baranov)Avoid extra dependency for the setting compile to work. In previous versions, the user may get error like cannot open crti.o, unable to find library -lc etc. #6309 (alexey-milovidov)More validation of the input that may come from malicious replica. #6303 (alexey-milovidov)Now clickhouse-obfuscator file is available in clickhouse-client package. In previous versions it was available as clickhouse obfuscator (with whitespace). #5816 #6609 (dimarub2000)Fixed deadlock when we have at least two queries that read at least two tables in different order and another query that performs DDL operation on one of tables. Fixed another very rare deadlock. #6764 (alexey-milovidov)Added os_thread_ids column to system.processes and system.query_log for better debugging possibilities. #6763 (alexey-milovidov)A workaround for PHP mysqlnd extension bugs which occur when sha256_password is used as a default authentication plugin (described in #6031). #6113 (Yuriy Baranov)Remove unneeded place with changed nullability columns. #6693 (Artem Zuikov)Set default value of queue_max_wait_ms to zero, because current value (five seconds) makes no sense. There are rare circumstances when this settings has any use. Added settings replace_running_query_max_wait_ms, kafka_max_wait_ms and connection_pool_max_wait_ms for disambiguation. #6692 (alexey-milovidov)Extract SelectQueryExpressionAnalyzer from ExpressionAnalyzer. Keep the last one for non-select queries. #6499 (Artem Zuikov)Removed duplicating input and output formats. #6239 (Nikolai Kochetov)Allow user to override poll_interval and idle_connection_timeout settings on connection. #6230 (alexey-milovidov)MergeTree now has an additional option ttl_only_drop_parts (disabled by default) to avoid partial pruning of parts, so that they dropped completely when all the rows in a part are expired. #6191 (Sergi Vladykin)Type checks for set index functions. Throw exception if function got a wrong type. This fixes fuzz test with UBSan. #6511 (Nikita Vasilev) Performance Improvement​ Optimize queries with ORDER BY expressions clause, where expressions have coinciding prefix with sorting key in MergeTree tables. This optimization is controlled by optimize_read_in_order setting. #6054 #6629 (Anton Popov)Allow to use multiple threads during parts loading and removal. #6372 #6074 #6438 (alexey-milovidov)Implemented batch variant of updating aggregate function states. It may lead to performance benefits. #6435 (alexey-milovidov)Using FastOps library for functions exp, log, sigmoid, tanh. FastOps is a fast vector math library from Michael Parakhin (Yandex CTO). Improved performance of exp and log functions more than 6 times. The functions exp and log from Float32 argument will return Float32 (in previous versions they always return Float64). Now exp(nan) may return inf. The result of exp and log functions may be not the nearest machine representable number to the true answer. #6254 (alexey-milovidov) Using Danila Kutenin variant to make fastops working #6317 (alexey-milovidov)Disable consecutive key optimization for UInt8/16. #6298 #6701 (akuzm)Improved performance of simdjson library by getting rid of dynamic allocation in ParsedJson::Iterator. #6479 (Vitaly Baranov)Pre-fault pages when allocating memory with mmap(). #6667 (akuzm)Fix performance bug in Decimal comparison. #6380 (Artem Zuikov) Build/Testing/Packaging Improvement​ Remove Compiler (runtime template instantiation) because we’ve win over it’s performance. #6646 (alexey-milovidov)Added performance test to show degradation of performance in gcc-9 in more isolated way. #6302 (alexey-milovidov)Added table function numbers_mt, which is multithreaded version of numbers. Updated performance tests with hash functions. #6554 (Nikolai Kochetov)Comparison mode in clickhouse-benchmark #6220 #6343 (dimarub2000)Best effort for printing stack traces. Also added SIGPROF as a debugging signal to print stack trace of a running thread. #6529 (alexey-milovidov)Every function in its own file, part 10. #6321 (alexey-milovidov)Remove doubled const TABLE_IS_READ_ONLY. #6566 (filimonov)Formatting changes for StringHashMap PR #5417. #6700 (akuzm)Better subquery for join creation in ExpressionAnalyzer. #6824 (Artem Zuikov)Remove a redundant condition (found by PVS Studio). #6775 (akuzm)Separate the hash table interface for ReverseIndex. #6672 (akuzm)Refactoring of settings. #6689 (alesapin)Add comments for set index functions. #6319 (Nikita Vasilev)Increase OOM score in debug version on Linux. #6152 (akuzm)HDFS HA now work in debug build. #6650 (Weiqing Xu)Added a test to transform_query_for_external_database. #6388 (alexey-milovidov)Add test for multiple materialized views for Kafka table. #6509 (Ivan)Make a better build scheme. #6500 (Ivan)Fixed test_external_dictionaries integration in case it was executed under non root user. #6507 (Nikolai Kochetov)The bug reproduces when total size of written packets exceeds DBMS_DEFAULT_BUFFER_SIZE. #6204 (Yuriy Baranov)Added a test for RENAME table race condition #6752 (alexey-milovidov)Avoid data race on Settings in KILL QUERY. #6753 (alexey-milovidov)Add integration test for handling errors by a cache dictionary. #6755 (Vitaly Baranov)Disable parsing of ELF object files on Mac OS, because it makes no sense. #6578 (alexey-milovidov)Attempt to make changelog generator better. #6327 (alexey-milovidov)Adding -Wshadow switch to the GCC. #6325 (kreuzerkrieg)Removed obsolete code for mimalloc support. #6715 (alexey-milovidov)zlib-ng determines x86 capabilities and saves this info to global variables. This is done in defalteInit call, which may be made by different threads simultaneously. To avoid multithreaded writes, do it on library startup. #6141 (akuzm)Regression test for a bug which in join which was fixed in #5192. #6147 (Bakhtiyor Ruziev)Fixed MSan report. #6144 (alexey-milovidov)Fix flapping TTL test. #6782 (Anton Popov)Fixed false data race in MergeTreeDataPart::is_frozen field. #6583 (alexey-milovidov)Fixed timeouts in fuzz test. In previous version, it managed to find false hangup in query SELECT * FROM numbers_mt(gccMurmurHash('')). #6582 (alexey-milovidov)Added debug checks to static_cast of columns. #6581 (alexey-milovidov)Support for Oracle Linux in official RPM packages. #6356 #6585 (alexey-milovidov)Changed json perftests from once to loop type. #6536 (Nikolai Kochetov)odbc-bridge.cpp defines main() so it should not be included in clickhouse-lib. #6538 (Orivej Desh)Test for crash in FULL|RIGHT JOIN with nulls in right table’s keys. #6362 (Artem Zuikov)Added a test for the limit on expansion of aliases just in case. #6442 (alexey-milovidov)Switched from boost::filesystem to std::filesystem where appropriate. #6253 #6385 (alexey-milovidov)Added RPM packages to website. #6251 (alexey-milovidov)Add a test for fixed Unknown identifier exception in IN section. #6708 (Artem Zuikov)Simplify shared_ptr_helper because people facing difficulties understanding it. #6675 (alexey-milovidov)Added performance tests for fixed Gorilla and DoubleDelta codec. #6179 (Vasily Nemkov)Split the integration test test_dictionaries into 4 separate tests. #6776 (Vitaly Baranov)Fix PVS-Studio warning in PipelineExecutor. #6777 (Nikolai Kochetov)Allow to use library dictionary source with ASan. #6482 (alexey-milovidov)Added option to generate changelog from a list of PRs. #6350 (alexey-milovidov)Lock the TinyLog storage when reading. #6226 (akuzm)Check for broken symlinks in CI. #6634 (alexey-milovidov)Increase timeout for “stack overflow” test because it may take a long time in debug build. #6637 (alexey-milovidov)Added a check for double whitespaces. #6643 (alexey-milovidov)Fix new/delete memory tracking when build with sanitizers. Tracking is not clear. It only prevents memory limit exceptions in tests. #6450 (Artem Zuikov)Enable back the check of undefined symbols while linking. #6453 (Ivan)Avoid rebuilding hyperscan every day. #6307 (alexey-milovidov)Fixed UBSan report in ProtobufWriter. #6163 (alexey-milovidov)Don’t allow to use query profiler with sanitizers because it is not compatible. #6769 (alexey-milovidov)Add test for reloading a dictionary after fail by timer. #6114 (Vitaly Baranov)Fix inconsistency in PipelineExecutor::prepareProcessor argument type. #6494 (Nikolai Kochetov)Added a test for bad URIs. #6493 (alexey-milovidov)Added more checks to CAST function. This should get more information about segmentation fault in fuzzy test. #6346 (Nikolai Kochetov)Added gcc-9 support to docker/builder container that builds image locally. #6333 (Gleb Novikov)Test for primary key with LowCardinality(String). #5044 #6219 (dimarub2000)Fixed tests affected by slow stack traces printing. #6315 (alexey-milovidov)Add a test case for crash in groupUniqArray fixed in #6029. #4402 #6129 (akuzm)Fixed indices mutations tests. #6645 (Nikita Vasilev)In performance test, do not read query log for queries we didn’t run. #6427 (akuzm)Materialized view now could be created with any low cardinality types regardless to the setting about suspicious low cardinality types. #6428 (Olga Khvostikova)Updated tests for send_logs_level setting. #6207 (Nikolai Kochetov)Fix build under gcc-8.2. #6196 (Max Akhmedov)Fix build with internal libc++. #6724 (Ivan)Fix shared build with rdkafka library #6101 (Ivan)Fixes for Mac OS build (incomplete). #6390 (alexey-milovidov) #6429 (alex-zaitsev)Fix “splitted” build. #6618 (alexey-milovidov)Other build fixes: #6186 (Amos Bird) #6486 #6348 (vxider) #6744 (Ivan) #6016 #6421 #6491 (proller) Backward Incompatible Change​ Removed rarely used table function catBoostPool and storage CatBoostPool. If you have used this table function, please write email to clickhouse-feedback@yandex-team.com. Note that CatBoost integration remains and will be supported. #6279 (alexey-milovidov)Disable ANY RIGHT JOIN and ANY FULL JOIN by default. Set any_join_distinct_right_table_keys setting to enable them. #5126 #6351 (Artem Zuikov) "},{"title":"ClickHouse Release 19.13​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-13","content":""},{"title":"ClickHouse Release 19.13.6.51, 2019-10-02​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-13-6-51-2019-10-02","content":"Bug Fix​ This release also contains all bug fixes from 19.11.12.69. "},{"title":"ClickHouse Release 19.13.5.44, 2019-09-20​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-13-5-44-2019-09-20","content":"Bug Fix​ This release also contains all bug fixes from 19.14.6.12.Fixed possible inconsistent state of table while executing DROP query for replicated table while zookeeper is not accessible. #6045 #6413 (Nikita Mikhaylov)Fix for data race in StorageMerge #6717 (alexey-milovidov)Fix bug introduced in query profiler which leads to endless recv from socket. #6386 (alesapin)Fix excessive CPU usage while executing JSONExtractRaw function over a boolean value. #6208 (Vitaly Baranov)Fixes the regression while pushing to materialized view. #6415 (Ivan)Table function url had the vulnerability allowed the attacker to inject arbitrary HTTP headers in the request. This issue was found by Nikita Tikhomirov. #6466 (alexey-milovidov)Fix useless AST check in Set index. #6510 #6651 (Nikita Vasilev)Fixed parsing of AggregateFunction values embedded in query. #6575 #6773 (Zhichang Yu)Fixed wrong behaviour of trim functions family. #6647 (alexey-milovidov) "},{"title":"ClickHouse Release 19.13.4.32, 2019-09-10​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-13-4-32-2019-09-10","content":"Bug Fix​ This release also contains all bug security fixes from 19.11.9.52 and 19.11.10.54.Fixed data race in system.parts table and ALTER query. #6245 #6513 (alexey-milovidov)Fixed mismatched header in streams happened in case of reading from empty distributed table with sample and prewhere. #6167 (Lixiang Qian) #6823 (Nikolai Kochetov)Fixed crash when using IN clause with a subquery with a tuple. #6125 #6550 (tavplubix)Fix case with same column names in GLOBAL JOIN ON section. #6181 (Artem Zuikov)Fix crash when casting types to Decimal that do not support it. Throw exception instead. #6297 (Artem Zuikov)Fixed crash in extractAll() function. #6644 (Artem Zuikov)Query transformation for MySQL, ODBC, JDBC table functions now works properly for SELECT WHERE queries with multiple AND expressions. #6381 #6676 (dimarub2000)Added previous declaration checks for MySQL 8 integration. #6569 (Rafael David Tinoco) Security Fix​ Fix two vulnerabilities in codecs in decompression phase (malicious user can fabricate compressed data that will lead to buffer overflow in decompression). #6670 (Artem Zuikov) "},{"title":"ClickHouse Release 19.13.3.26, 2019-08-22​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-13-3-26-2019-08-22","content":"Bug Fix​ Fix ALTER TABLE ... UPDATE query for tables with enable_mixed_granularity_parts=1. #6543 (alesapin)Fix NPE when using IN clause with a subquery with a tuple. #6125 #6550 (tavplubix)Fixed an issue that if a stale replica becomes alive, it may still have data parts that were removed by DROP PARTITION. #6522 #6523 (tavplubix)Fixed issue with parsing CSV #6426 #6559 (tavplubix)Fixed data race in system.parts table and ALTER query. This fixes #6245. #6513 (alexey-milovidov)Fixed wrong code in mutations that may lead to memory corruption. Fixed segfault with read of address 0x14c0 that may happed due to concurrent DROP TABLE and SELECT from system.parts or system.parts_columns. Fixed race condition in preparation of mutation queries. Fixed deadlock caused by OPTIMIZE of Replicated tables and concurrent modification operations like ALTERs. #6514 (alexey-milovidov)Fixed possible data loss after ALTER DELETE query on table with skipping index. #6224 #6282 (Nikita Vasilev) Security Fix​ If the attacker has write access to ZooKeeper and is able to run custom server available from the network where ClickHouse run, it can create custom-built malicious server that will act as ClickHouse replica and register it in ZooKeeper. When another replica will fetch data part from malicious replica, it can force clickhouse-server to write to arbitrary path on filesystem. Found by Eldar Zaitov, information security team at Yandex. #6247 (alexey-milovidov) "},{"title":"ClickHouse Release 19.13.2.19, 2019-08-14​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-13-2-19-2019-08-14","content":"New Feature​ Sampling profiler on query level. Example. #4247 (laplab) #6124 (alexey-milovidov) #6250 #6283 #6386Allow to specify a list of columns with COLUMNS('regexp') expression that works like a more sophisticated variant of * asterisk. #5951 (mfridental), (alexey-milovidov)CREATE TABLE AS table_function() is now possible #6057 (dimarub2000)Adam optimizer for stochastic gradient descent is used by default in stochasticLinearRegression() and stochasticLogisticRegression() aggregate functions, because it shows good quality without almost any tuning. #6000 (Quid37)Added functions for working with the сustom week number #5212 (Andy Yang)RENAME queries now work with all storages. #5953 (Ivan)Now client receive logs from server with any desired level by setting send_logs_level regardless to the log level specified in server settings. #5964 (Nikita Mikhaylov) Backward Incompatible Change​ The setting input_format_defaults_for_omitted_fields is enabled by default. Inserts in Distributed tables need this setting to be the same on cluster (you need to set it before rolling update). It enables calculation of complex default expressions for omitted fields in JSONEachRow and CSV* formats. It should be the expected behavior but may lead to negligible performance difference. #6043 (Artem Zuikov), #5625 (akuzm) Experimental Features​ New query processing pipeline. Use experimental_use_processors=1 option to enable it. Use for your own trouble. #4914 (Nikolai Kochetov) Bug Fix​ Kafka integration has been fixed in this version.Fixed DoubleDelta encoding of Int64 for large DoubleDelta values, improved DoubleDelta encoding for random data for Int32. #5998 (Vasily Nemkov)Fixed overestimation of max_rows_to_read if the setting merge_tree_uniform_read_distribution is set to 0. #6019 (alexey-milovidov) Improvement​ Throws an exception if config.d file does not have the corresponding root element as the config file #6123 (dimarub2000) Performance Improvement​ Optimize count(). Now it uses the smallest column (if possible). #6028 (Amos Bird) Build/Testing/Packaging Improvement​ Report memory usage in performance tests. #5899 (akuzm)Fix build with external libcxx #6010 (Ivan)Fix shared build with rdkafka library #6101 (Ivan) "},{"title":"ClickHouse Release 19.11​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-11","content":""},{"title":"ClickHouse Release 19.11.13.74, 2019-11-01​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-11-13-74-2019-11-01","content":"Bug Fix​ Fixed rare crash in ALTER MODIFY COLUMN and vertical merge when one of merged/altered parts is empty (0 rows). #6780 (alesapin)Manual update of SIMDJSON. This fixes possible flooding of stderr files with bogus json diagnostic messages. #7548 (Alexander Kazakov)Fixed bug with mrk file extension for mutations (alesapin) "},{"title":"ClickHouse Release 19.11.12.69, 2019-10-02​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-11-12-69-2019-10-02","content":"Bug Fix​ Fixed performance degradation of index analysis on complex keys on large tables. This fixes #6924. #7075 (alexey-milovidov)Avoid rare SIGSEGV while sending data in tables with Distributed engine (Failed to send batch: file with index XXXXX is absent). #7032 (Azat Khuzhin)Fix Unknown identifier with multiple joins. This fixes #5254. #7022 (Artem Zuikov) "},{"title":"ClickHouse Release 19.11.11.57, 2019-09-13​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-11-11-57-2019-09-13","content":"Fix logical error causing segfaults when selecting from Kafka empty topic. #6902 #6909 (Ivan)Fix for function АrrayEnumerateUniqRanked with empty arrays in params. #6928 (proller) "},{"title":"ClickHouse Release 19.11.10.54, 2019-09-10​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-11-10-54-2019-09-10","content":"Bug Fix​ Do store offsets for Kafka messages manually to be able to commit them all at once for all partitions. Fixes potential duplication in “one consumer - many partitions” scenario. #6872 (Ivan) "},{"title":"ClickHouse Release 19.11.9.52, 2019-09-6​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-11-9-52-2019-09-6","content":"Improve error handling in cache dictionaries. #6737 (Vitaly Baranov)Fixed bug in function arrayEnumerateUniqRanked. #6779 (proller)Fix JSONExtract function while extracting a Tuple from JSON. #6718 (Vitaly Baranov)Fixed possible data loss after ALTER DELETE query on table with skipping index. #6224 #6282 (Nikita Vasilev)Fixed performance test. #6392 (alexey-milovidov)Parquet: Fix reading boolean columns. #6579 (alexey-milovidov)Fixed wrong behaviour of nullIf function for constant arguments. #6518 (Guillaume Tassery) #6580 (alexey-milovidov)Fix Kafka messages duplication problem on normal server restart. #6597 (Ivan)Fixed an issue when long ALTER UPDATE or ALTER DELETE may prevent regular merges to run. Prevent mutations from executing if there is no enough free threads available. #6502 #6617 (tavplubix)Fixed error with processing “timezone” in server configuration file. #6709 (alexey-milovidov)Fix kafka tests. #6805 (Ivan) Security Fix​ If the attacker has write access to ZooKeeper and is able to run custom server available from the network where ClickHouse runs, it can create custom-built malicious server that will act as ClickHouse replica and register it in ZooKeeper. When another replica will fetch data part from malicious replica, it can force clickhouse-server to write to arbitrary path on filesystem. Found by Eldar Zaitov, information security team at Yandex. #6247 (alexey-milovidov) "},{"title":"ClickHouse Release 19.11.8.46, 2019-08-22​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-11-8-46-2019-08-22","content":"Bug Fix​ Fix ALTER TABLE ... UPDATE query for tables with enable_mixed_granularity_parts=1. #6543 (alesapin)Fix NPE when using IN clause with a subquery with a tuple. #6125 #6550 (tavplubix)Fixed an issue that if a stale replica becomes alive, it may still have data parts that were removed by DROP PARTITION. #6522 #6523 (tavplubix)Fixed issue with parsing CSV #6426 #6559 (tavplubix)Fixed data race in system.parts table and ALTER query. This fixes #6245. #6513 (alexey-milovidov)Fixed wrong code in mutations that may lead to memory corruption. Fixed segfault with read of address 0x14c0 that may happed due to concurrent DROP TABLE and SELECT from system.parts or system.parts_columns. Fixed race condition in preparation of mutation queries. Fixed deadlock caused by OPTIMIZE of Replicated tables and concurrent modification operations like ALTERs. #6514 (alexey-milovidov) "},{"title":"ClickHouse Release 19.11.7.40, 2019-08-14​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-11-7-40-2019-08-14","content":"Bug Fix​ Kafka integration has been fixed in this version.Fix segfault when using arrayReduce for constant arguments. #6326 (alexey-milovidov)Fixed toFloat() monotonicity. #6374 (dimarub2000)Fix segfault with enabled optimize_skip_unused_shards and missing sharding key. #6384 (CurtizJ)Fixed logic of arrayEnumerateUniqRanked function. #6423 (alexey-milovidov)Removed extra verbose logging from MySQL handler. #6389 (alexey-milovidov)Fix wrong behavior and possible segfaults in topK and topKWeighted aggregated functions. #6404 (CurtizJ)Do not expose virtual columns in system.columns table. This is required for backward compatibility. #6406 (alexey-milovidov)Fix bug with memory allocation for string fields in complex key cache dictionary. #6447 (alesapin)Fix bug with enabling adaptive granularity when creating new replica for Replicated*MergeTree table. #6452 (alesapin)Fix infinite loop when reading Kafka messages. #6354 (abyss7)Fixed the possibility of a fabricated query to cause server crash due to stack overflow in SQL parser and possibility of stack overflow in Merge and Distributed tables #6433 (alexey-milovidov)Fixed Gorilla encoding error on small sequences. #6444 (Enmk) Improvement​ Allow user to override poll_interval and idle_connection_timeout settings on connection. #6230 (alexey-milovidov) "},{"title":"ClickHouse Release 19.11.5.28, 2019-08-05​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-11-5-28-2019-08-05","content":"Bug Fix​ Fixed the possibility of hanging queries when server is overloaded. #6301 (alexey-milovidov)Fix FPE in yandexConsistentHash function. This fixes #6304. #6126 (alexey-milovidov)Fixed bug in conversion of LowCardinality types in AggregateFunctionFactory. This fixes #6257. #6281 (Nikolai Kochetov)Fix parsing of bool settings from true and false strings in configuration files. #6278 (alesapin)Fix rare bug with incompatible stream headers in queries to Distributed table over MergeTree table when part of WHERE moves to PREWHERE. #6236 (alesapin)Fixed overflow in integer division of signed type to unsigned type. This fixes #6214. #6233 (alexey-milovidov) Backward Incompatible Change​ Kafka still broken. "},{"title":"ClickHouse Release 19.11.4.24, 2019-08-01​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-11-4-24-2019-08-01","content":"Bug Fix​ Fix bug with writing secondary indices marks with adaptive granularity. #6126 (alesapin)Fix WITH ROLLUP and WITH CUBE modifiers of GROUP BY with two-level aggregation. #6225 (Anton Popov)Fixed hang in JSONExtractRaw function. Fixed #6195 #6198 (alexey-milovidov)Fix segfault in ExternalLoader::reloadOutdated(). #6082 (Vitaly Baranov)Fixed the case when server may close listening sockets but not shutdown and continue serving remaining queries. You may end up with two running clickhouse-server processes. Sometimes, the server may return an error bad_function_call for remaining queries. #6231 (alexey-milovidov)Fixed useless and incorrect condition on update field for initial loading of external dictionaries via ODBC, MySQL, ClickHouse and HTTP. This fixes #6069 #6083 (alexey-milovidov)Fixed irrelevant exception in cast of LowCardinality(Nullable) to not-Nullable column in case if it does not contain Nulls (e.g. in query like SELECT CAST(CAST('Hello' AS LowCardinality(Nullable(String))) AS String). #6094 #6119 (Nikolai Kochetov)Fix non-deterministic result of “uniq” aggregate function in extreme rare cases. The bug was present in all ClickHouse versions. #6058 (alexey-milovidov)Segfault when we set a little bit too high CIDR on the function IPv6CIDRToRange. #6068 (Guillaume Tassery)Fixed small memory leak when server throw many exceptions from many different contexts. #6144 (alexey-milovidov)Fix the situation when consumer got paused before subscription and not resumed afterwards. #6075 (Ivan) Note that Kafka is broken in this version.Clearing the Kafka data buffer from the previous read operation that was completed with an error #6026 (Nikolay) Note that Kafka is broken in this version.Since StorageMergeTree::background_task_handle is initialized in startup() the MergeTreeBlockOutputStream::write() may try to use it before initialization. Just check if it is initialized. #6080 (Ivan) Build/Testing/Packaging Improvement​ Added official rpm packages. #5740 (proller) (alesapin)Add an ability to build .rpm and .tgz packages with packager script. #5769 (alesapin)Fixes for “Arcadia” build system. #6223 (proller) Backward Incompatible Change​ Kafka is broken in this version. "},{"title":"ClickHouse Release 19.11.3.11, 2019-07-18​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-11-3-11-2019-07-18","content":"New Feature​ Added support for prepared statements. #5331 (Alexander) #5630 (alexey-milovidov)DoubleDelta and Gorilla column codecs #5600 (Vasily Nemkov)Added os_thread_priority setting that allows to control the “nice” value of query processing threads that is used by OS to adjust dynamic scheduling priority. It requires CAP_SYS_NICE capabilities to work. This implements #5858 #5909 (alexey-milovidov)Implement _topic, _offset, _key columns for Kafka engine #5382 (Ivan) Note that Kafka is broken in this version.Add aggregate function combinator -Resample #5590 (hcz)Aggregate functions groupArrayMovingSum(win_size)(x) and groupArrayMovingAvg(win_size)(x), which calculate moving sum/avg with or without window-size limitation. #5595 (inv2004)Add synonim arrayFlatten \\&lt;-&gt; flatten #5764 (hcz)Intergate H3 function geoToH3 from Uber. #4724 (Remen Ivan) #5805 (alexey-milovidov) Bug Fix​ Implement DNS cache with asynchronous update. Separate thread resolves all hosts and updates DNS cache with period (setting dns_cache_update_period). It should help, when ip of hosts changes frequently. #5857 (Anton Popov)Fix segfault in Delta codec which affects columns with values less than 32 bits size. The bug led to random memory corruption. #5786 (alesapin)Fix segfault in TTL merge with non-physical columns in block. #5819 (Anton Popov)Fix rare bug in checking of part with LowCardinality column. Previously checkDataPart always fails for part with LowCardinality column. #5832 (alesapin)Avoid hanging connections when server thread pool is full. It is important for connections from remote table function or connections to a shard without replicas when there is long connection timeout. This fixes #5878 #5881 (alexey-milovidov)Support for constant arguments to evalMLModel function. This fixes #5817 #5820 (alexey-milovidov)Fixed the issue when ClickHouse determines default time zone as UCT instead of UTC. This fixes #5804. #5828 (alexey-milovidov)Fixed buffer underflow in visitParamExtractRaw. This fixes #5901 #5902 (alexey-milovidov)Now distributed DROP/ALTER/TRUNCATE/OPTIMIZE ON CLUSTER queries will be executed directly on leader replica. #5757 (alesapin)Fix coalesce for ColumnConst with ColumnNullable + related changes. #5755 (Artem Zuikov)Fix the ReadBufferFromKafkaConsumer so that it keeps reading new messages after commit() even if it was stalled before #5852 (Ivan)Fix FULL and RIGHT JOIN results when joining on Nullable keys in right table. #5859 (Artem Zuikov)Possible fix of infinite sleeping of low-priority queries. #5842 (alexey-milovidov)Fix race condition, which cause that some queries may not appear in query_log after SYSTEM FLUSH LOGS query. #5456 #5685 (Anton Popov)Fixed heap-use-after-free ASan warning in ClusterCopier caused by watch which try to use already removed copier object. #5871 (Nikolai Kochetov)Fixed wrong StringRef pointer returned by some implementations of IColumn::deserializeAndInsertFromArena. This bug affected only unit-tests. #5973 (Nikolai Kochetov)Prevent source and intermediate array join columns of masking same name columns. #5941 (Artem Zuikov)Fix insert and select query to MySQL engine with MySQL style identifier quoting. #5704 (Winter Zhang)Now CHECK TABLE query can work with MergeTree engine family. It returns check status and message if any for each part (or file in case of simplier engines). Also, fix bug in fetch of a broken part. #5865 (alesapin)Fix SPLIT_SHARED_LIBRARIES runtime #5793 (Danila Kutenin)Fixed time zone initialization when /etc/localtime is a relative symlink like ../usr/share/zoneinfo/Asia/Istanbul #5922 (alexey-milovidov)clickhouse-copier: Fix use-after free on shutdown #5752 (proller)Updated simdjson. Fixed the issue that some invalid JSONs with zero bytes successfully parse. #5938 (alexey-milovidov)Fix shutdown of SystemLogs #5802 (Anton Popov)Fix hanging when condition in invalidate_query depends on a dictionary. #6011 (Vitaly Baranov) Improvement​ Allow unresolvable addresses in cluster configuration. They will be considered unavailable and tried to resolve at every connection attempt. This is especially useful for Kubernetes. This fixes #5714 #5924 (alexey-milovidov)Close idle TCP connections (with one hour timeout by default). This is especially important for large clusters with multiple distributed tables on every server, because every server can possibly keep a connection pool to every other server, and after peak query concurrency, connections will stall. This fixes #5879 #5880 (alexey-milovidov)Better quality of topK function. Changed the SavingSpace set behavior to remove the last element if the new element have a bigger weight. #5833 #5850 (Guillaume Tassery)URL functions to work with domains now can work for incomplete URLs without scheme #5725 (alesapin)Checksums added to the system.parts_columns table. #5874 (Nikita Mikhaylov)Added Enum data type as a synonim for Enum8 or Enum16. #5886 (dimarub2000)Full bit transpose variant for T64 codec. Could lead to better compression with zstd. #5742 (Artem Zuikov)Condition on startsWith function now can uses primary key. This fixes #5310 and #5882 #5919 (dimarub2000)Allow to use clickhouse-copier with cross-replication cluster topology by permitting empty database name. #5745 (nvartolomei)Use UTC as default timezone on a system without tzdata (e.g. bare Docker container). Before this patch, error message Could not determine local time zone was printed and server or client refused to start. #5827 (alexey-milovidov)Returned back support for floating point argument in function quantileTiming for backward compatibility. #5911 (alexey-milovidov)Show which table is missing column in error messages. #5768 (Ivan)Disallow run query with same query_id by various users #5430 (proller)More robust code for sending metrics to Graphite. It will work even during long multiple RENAME TABLE operation. #5875 (alexey-milovidov)More informative error messages will be displayed when ThreadPool cannot schedule a task for execution. This fixes #5305 #5801 (alexey-milovidov)Inverting ngramSearch to be more intuitive #5807 (Danila Kutenin)Add user parsing in HDFS engine builder #5946 (akonyaev90)Update default value of max_ast_elements parameter #5933 (Artem Konovalov)Added a notion of obsolete settings. The obsolete setting allow_experimental_low_cardinality_type can be used with no effect. 0f15c01c6802f7ce1a1494c12c846be8c98944cd Alexey Milovidov Performance Improvement​ Increase number of streams to SELECT from Merge table for more uniform distribution of threads. Added setting max_streams_multiplier_for_merge_tables. This fixes #5797 #5915 (alexey-milovidov) Build/Testing/Packaging Improvement​ Add a backward compatibility test for client-server interaction with different versions of clickhouse. #5868 (alesapin)Test coverage information in every commit and pull request. #5896 (alesapin)Cooperate with address sanitizer to support our custom allocators (Arena and ArenaWithFreeLists) for better debugging of “use-after-free” errors. #5728 (akuzm)Switch to LLVM libunwind implementation for C++ exception handling and for stack traces printing #4828 (Nikita Lapkov)Add two more warnings from -Weverything #5923 (alexey-milovidov)Allow to build ClickHouse with Memory Sanitizer. #3949 (alexey-milovidov)Fixed ubsan report about bitTest function in fuzz test. #5943 (alexey-milovidov)Docker: added possibility to init a ClickHouse instance which requires authentication. #5727 (Korviakov Andrey)Update librdkafka to version 1.1.0 #5872 (Ivan)Add global timeout for integration tests and disable some of them in tests code. #5741 (alesapin)Fix some ThreadSanitizer failures. #5854 (akuzm)The --no-undefined option forces the linker to check all external names for existence while linking. It’s very useful to track real dependencies between libraries in the split build mode. #5855 (Ivan)Added performance test for #5797 #5914 (alexey-milovidov)Fixed compatibility with gcc-7. #5840 (alexey-milovidov)Added support for gcc-9. This fixes #5717 #5774 (alexey-milovidov)Fixed error when libunwind can be linked incorrectly. #5948 (alexey-milovidov)Fixed a few warnings found by PVS-Studio. #5921 (alexey-milovidov)Added initial support for clang-tidy static analyzer. #5806 (alexey-milovidov)Convert BSD/Linux endian macros( ‘be64toh’ and ‘htobe64’) to the Mac OS X equivalents #5785 (Fu Chen)Improved integration tests guide. #5796 (Vladimir Chebotarev)Fixing build at macosx + gcc9 #5822 (filimonov)Fix a hard-to-spot typo: aggreAGte -&gt; aggregate. #5753 (akuzm)Fix freebsd build #5760 (proller)Add link to experimental YouTube channel to website #5845 (Ivan Blinkov)CMake: add option for coverage flags: WITH_COVERAGE #5776 (proller)Fix initial size of some inline PODArray’s. #5787 (akuzm)clickhouse-server.postinst: fix os detection for centos 6 #5788 (proller)Added Arch linux package generation. #5719 (Vladimir Chebotarev)Split Common/config.h by libs (dbms) #5715 (proller)Fixes for “Arcadia” build platform #5795 (proller)Fixes for unconventional build (gcc9, no submodules) #5792 (proller)Require explicit type in unalignedStore because it was proven to be bug-prone #5791 (akuzm)Fixes MacOS build #5830 (filimonov)Performance test concerning the new JIT feature with bigger dataset, as requested here #5263 #5887 (Guillaume Tassery)Run stateful tests in stress test 12693e568722f11e19859742f56428455501fd2a (alesapin) Backward Incompatible Change​ Kafka is broken in this version.Enable adaptive_index_granularity = 10MB by default for new MergeTree tables. If you created new MergeTree tables on version 19.11+, downgrade to versions prior to 19.6 will be impossible. #5628 (alesapin)Removed obsolete undocumented embedded dictionaries that were used by Yandex.Metrica. The functions OSIn, SEIn, OSToRoot, SEToRoot, OSHierarchy, SEHierarchy are no longer available. If you are using these functions, write email to clickhouse-feedback@yandex-team.com. Note: at the last moment we decided to keep these functions for a while. #5780 (alexey-milovidov) "},{"title":"ClickHouse Release 19.10​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-10","content":""},{"title":"ClickHouse Release 19.10.1.5, 2019-07-12​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-10-1-5-2019-07-12","content":"New Feature​ Add new column codec: T64. Made for (U)IntX/EnumX/Data(Time)/DecimalX columns. It should be good for columns with constant or small range values. Codec itself allows enlarge or shrink data type without re-compression. #5557 (Artem Zuikov)Add database engine MySQL that allow to view all the tables in remote MySQL server #5599 (Winter Zhang)bitmapContains implementation. It’s 2x faster than bitmapHasAny if the second bitmap contains one element. #5535 (Zhichang Yu)Support for crc32 function (with behaviour exactly as in MySQL or PHP). Do not use it if you need a hash function. #5661 (Remen Ivan)Implemented SYSTEM START/STOP DISTRIBUTED SENDS queries to control asynchronous inserts into Distributed tables. #4935 (Winter Zhang) Bug Fix​ Ignore query execution limits and max parts size for merge limits while executing mutations. #5659 (Anton Popov)Fix bug which may lead to deduplication of normal blocks (extremely rare) and insertion of duplicate blocks (more often). #5549 (alesapin)Fix of function arrayEnumerateUniqRanked for arguments with empty arrays #5559 (proller)Don’t subscribe to Kafka topics without intent to poll any messages. #5698 (Ivan)Make setting join_use_nulls get no effect for types that cannot be inside Nullable #5700 (Olga Khvostikova)Fixed Incorrect size of index granularity errors #5720 (coraxster)Fix Float to Decimal convert overflow #5607 (coraxster)Flush buffer when WriteBufferFromHDFS’s destructor is called. This fixes writing into HDFS. #5684 (Xindong Peng) Improvement​ Treat empty cells in CSV as default values when the setting input_format_defaults_for_omitted_fields is enabled. #5625 (akuzm)Non-blocking loading of external dictionaries. #5567 (Vitaly Baranov)Network timeouts can be dynamically changed for already established connections according to the settings. #4558 (Konstantin Podshumok)Using “public_suffix_list” for functions firstSignificantSubdomain, cutToFirstSignificantSubdomain. It’s using a perfect hash table generated by gperf with a list generated from the file: https://publicsuffix.org/list/public_suffix_list.dat. (for example, now we recognize the domain ac.uk as non-significant). #5030 (Guillaume Tassery)Adopted IPv6 data type in system tables; unified client info columns in system.processes and system.query_log #5640 (alexey-milovidov)Using sessions for connections with MySQL compatibility protocol. #5476 #5646 (Yuriy Baranov)Support more ALTER queries ON CLUSTER. #5593 #5613 (sundyli)Support &lt;logger&gt; section in clickhouse-local config file. #5540 (proller)Allow run query with remote table function in clickhouse-local #5627 (proller) Performance Improvement​ Add the possibility to write the final mark at the end of MergeTree columns. It allows to avoid useless reads for keys that are out of table data range. It is enabled only if adaptive index granularity is in use. #5624 (alesapin)Improved performance of MergeTree tables on very slow filesystems by reducing number of stat syscalls. #5648 (alexey-milovidov)Fixed performance degradation in reading from MergeTree tables that was introduced in version 19.6. Fixes #5631. #5633 (alexey-milovidov) Build/Testing/Packaging Improvement​ Implemented TestKeeper as an implementation of ZooKeeper interface used for testing #5643 (alexey-milovidov) (levushkin aleksej)From now on .sql tests can be run isolated by server, in parallel, with random database. It allows to run them faster, add new tests with custom server configurations, and be sure that different tests does not affect each other. #5554 (Ivan)Remove &lt;name&gt; and &lt;metrics&gt; from performance tests #5672 (Olga Khvostikova)Fixed “select_format” performance test for Pretty formats #5642 (alexey-milovidov) "},{"title":"ClickHouse Release 19.9​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-9","content":""},{"title":"ClickHouse Release 19.9.3.31, 2019-07-05​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-9-3-31-2019-07-05","content":"Bug Fix​ Fix segfault in Delta codec which affects columns with values less than 32 bits size. The bug led to random memory corruption. #5786 (alesapin)Fix rare bug in checking of part with LowCardinality column. #5832 (alesapin)Fix segfault in TTL merge with non-physical columns in block. #5819 (Anton Popov)Fix potential infinite sleeping of low-priority queries. #5842 (alexey-milovidov)Fix how ClickHouse determines default time zone as UCT instead of UTC. #5828 (alexey-milovidov)Fix bug about executing distributed DROP/ALTER/TRUNCATE/OPTIMIZE ON CLUSTER queries on follower replica before leader replica. Now they will be executed directly on leader replica. #5757 (alesapin)Fix race condition, which cause that some queries may not appear in query_log instantly after SYSTEM FLUSH LOGS query. #5685 (Anton Popov)Added missing support for constant arguments to evalMLModel function. #5820 (alexey-milovidov) "},{"title":"ClickHouse Release 19.9.2.4, 2019-06-24​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-9-2-4-2019-06-24","content":"New Feature​ Print information about frozen parts in system.parts table. #5471 (proller)Ask client password on clickhouse-client start on tty if not set in arguments #5092 (proller)Implement dictGet and dictGetOrDefault functions for Decimal types. #5394 (Artem Zuikov) Improvement​ Debian init: Add service stop timeout #5522 (proller)Add setting forbidden by default to create table with suspicious types for LowCardinality #5448 (Olga Khvostikova)Regression functions return model weights when not used as State in function evalMLMethod. #5411 (Quid37)Rename and improve regression methods. #5492 (Quid37)Clearer interfaces of string searchers. #5586 (Danila Kutenin) Bug Fix​ Fix potential data loss in Kafka #5445 (Ivan)Fix potential infinite loop in PrettySpace format when called with zero columns #5560 (Olga Khvostikova)Fixed UInt32 overflow bug in linear models. Allow eval ML model for non-const model argument. #5516 (Nikolai Kochetov)ALTER TABLE ... DROP INDEX IF EXISTS ... should not raise an exception if provided index does not exist #5524 (Gleb Novikov)Fix segfault with bitmapHasAny in scalar subquery #5528 (Zhichang Yu)Fixed error when replication connection pool does not retry to resolve host, even when DNS cache was dropped. #5534 (alesapin)Fixed ALTER ... MODIFY TTL on ReplicatedMergeTree. #5539 (Anton Popov)Fix INSERT into Distributed table with MATERIALIZED column #5429 (Azat Khuzhin)Fix bad alloc when truncate Join storage #5437 (TCeason)In recent versions of package tzdata some of files are symlinks now. The current mechanism for detecting default timezone gets broken and gives wrong names for some timezones. Now at least we force the timezone name to the contents of TZ if provided. #5443 (Ivan)Fix some extremely rare cases with MultiVolnitsky searcher when the constant needles in sum are at least 16KB long. The algorithm missed or overwrote the previous results which can lead to the incorrect result of multiSearchAny. #5588 (Danila Kutenin)Fix the issue when settings for ExternalData requests couldn’t use ClickHouse settings. Also, for now, settings date_time_input_format and low_cardinality_allow_in_native_format cannot be used because of the ambiguity of names (in external data it can be interpreted as table format and in the query it can be a setting). #5455 (Danila Kutenin)Fix bug when parts were removed only from FS without dropping them from Zookeeper. #5520 (alesapin)Remove debug logging from MySQL protocol #5478 (alexey-milovidov)Skip ZNONODE during DDL query processing #5489 (Azat Khuzhin)Fix mix UNION ALL result column type. There were cases with inconsistent data and column types of resulting columns. #5503 (Artem Zuikov)Throw an exception on wrong integers in dictGetT functions instead of crash. #5446 (Artem Zuikov)Fix wrong element_count and load_factor for hashed dictionary in system.dictionaries table. #5440 (Azat Khuzhin) Build/Testing/Packaging Improvement​ Fixed build without Brotli HTTP compression support (ENABLE_BROTLI=OFF cmake variable). #5521 (Anton Yuzhaninov)Include roaring.h as roaring/roaring.h #5523 (Orivej Desh)Fix gcc9 warnings in hyperscan (#line directive is evil!) #5546 (Danila Kutenin)Fix all warnings when compiling with gcc-9. Fix some contrib issues. Fix gcc9 ICE and submit it to bugzilla. #5498 (Danila Kutenin)Fixed linking with lld #5477 (alexey-milovidov)Remove unused specializations in dictionaries #5452 (Artem Zuikov)Improvement performance tests for formatting and parsing tables for different types of files #5497 (Olga Khvostikova)Fixes for parallel test run #5506 (proller)Docker: use configs from clickhouse-test #5531 (proller)Fix compile for FreeBSD #5447 (proller)Upgrade boost to 1.70 #5570 (proller)Fix build clickhouse as submodule #5574 (proller)Improve JSONExtract performance tests #5444 (Vitaly Baranov) "},{"title":"ClickHouse Release 19.8​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-8","content":""},{"title":"ClickHouse Release 19.8.3.8, 2019-06-11​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-8-3-8-2019-06-11","content":"New Features​ Added functions to work with JSON #4686 (hcz) #5124. (Vitaly Baranov)Add a function basename, with a similar behaviour to a basename function, which exists in a lot of languages (os.path.basename in python, basename in PHP, etc…). Work with both an UNIX-like path or a Windows path. #5136 (Guillaume Tassery)Added LIMIT n, m BY or LIMIT m OFFSET n BY syntax to set offset of n for LIMIT BY clause. #5138 (Anton Popov)Added new data type SimpleAggregateFunction, which allows to have columns with light aggregation in an AggregatingMergeTree. This can only be used with simple functions like any, anyLast, sum, min, max. #4629 (Boris Granveaud)Added support for non-constant arguments in function ngramDistance #5198 (Danila Kutenin)Added functions skewPop, skewSamp, kurtPop and kurtSamp to compute for sequence skewness, sample skewness, kurtosis and sample kurtosis respectively. #5200 (hcz)Support rename operation for MaterializeView storage. #5209 (Guillaume Tassery)Added server which allows connecting to ClickHouse using MySQL client. #4715 (Yuriy Baranov)Add toDecimal*OrZero and toDecimal*OrNull functions. #5291 (Artem Zuikov)Support Decimal types in functions: quantile, quantiles, median, quantileExactWeighted, quantilesExactWeighted, medianExactWeighted. #5304 (Artem Zuikov)Added toValidUTF8 function, which replaces all invalid UTF-8 characters by replacement character � (U+FFFD). #5322 (Danila Kutenin)Added format function. Formatting constant pattern (simplified Python format pattern) with the strings listed in the arguments. #5330 (Danila Kutenin)Added system.detached_parts table containing information about detached parts of MergeTree tables. #5353 (akuzm)Added ngramSearch function to calculate the non-symmetric difference between needle and haystack. #5418#5422 (Danila Kutenin)Implementation of basic machine learning methods (stochastic linear regression and logistic regression) using aggregate functions interface. Has different strategies for updating model weights (simple gradient descent, momentum method, Nesterov method). Also supports mini-batches of custom size. #4943 (Quid37)Implementation of geohashEncode and geohashDecode functions. #5003 (Vasily Nemkov)Added aggregate function timeSeriesGroupSum, which can aggregate different time series that sample timestamp not alignment. It will use linear interpolation between two sample timestamp and then sum time-series together. Added aggregate function timeSeriesGroupRateSum, which calculates the rate of time-series and then sum rates together. #4542 (Yangkuan Liu)Added functions IPv4CIDRtoIPv4Range and IPv6CIDRtoIPv6Range to calculate the lower and higher bounds for an IP in the subnet using a CIDR. #5095 (Guillaume Tassery)Add a X-ClickHouse-Summary header when we send a query using HTTP with enabled setting send_progress_in_http_headers. Return the usual information of X-ClickHouse-Progress, with additional information like how many rows and bytes were inserted in the query. #5116 (Guillaume Tassery) Improvements​ Added max_parts_in_total setting for MergeTree family of tables (default: 100 000) that prevents unsafe specification of partition key #5166. #5171 (alexey-milovidov)clickhouse-obfuscator: derive seed for individual columns by combining initial seed with column name, not column position. This is intended to transform datasets with multiple related tables, so that tables will remain JOINable after transformation. #5178 (alexey-milovidov)Added functions JSONExtractRaw, JSONExtractKeyAndValues. Renamed functions jsonExtract&lt;type&gt; to JSONExtract&lt;type&gt;. When something goes wrong these functions return the correspondent values, not NULL. Modified function JSONExtract, now it gets the return type from its last parameter and does not inject nullables. Implemented fallback to RapidJSON in case AVX2 instructions are not available. Simdjson library updated to a new version. #5235 (Vitaly Baranov)Now if and multiIf functions do not rely on the condition’s Nullable, but rely on the branches for sql compatibility. #5238 (Jian Wu)In predicate now generates Null result from Null input like the Equal function. #5152 (Jian Wu)Check the time limit every (flush_interval / poll_timeout) number of rows from Kafka. This allows to break the reading from Kafka consumer more frequently and to check the time limits for the top-level streams #5249 (Ivan)Link rdkafka with bundled SASL. It should allow to use SASL SCRAM authentication #5253 (Ivan)Batched version of RowRefList for ALL JOINS. #5267 (Artem Zuikov)clickhouse-server: more informative listen error messages. #5268 (proller)Support dictionaries in clickhouse-copier for functions in &lt;sharding_key&gt; #5270 (proller)Add new setting kafka_commit_every_batch to regulate Kafka committing policy. It allows to set commit mode: after every batch of messages is handled, or after the whole block is written to the storage. It’s a trade-off between losing some messages or reading them twice in some extreme situations. #5308 (Ivan)Make windowFunnel support other Unsigned Integer Types. #5320 (sundyli)Allow to shadow virtual column _table in Merge engine. #5325 (Ivan)Make sequenceMatch aggregate functions support other unsigned Integer types #5339 (sundyli)Better error messages if checksum mismatch is most likely caused by hardware failures. #5355 (alexey-milovidov)Check that underlying tables support sampling for StorageMerge #5366 (Ivan)Сlose MySQL connections after their usage in external dictionaries. It is related to issue #893. #5395 (Clément Rodriguez)Improvements of MySQL Wire Protocol. Changed name of format to MySQLWire. Using RAII for calling RSA_free. Disabling SSL if context cannot be created. #5419 (Yuriy Baranov)clickhouse-client: allow to run with unaccessable history file (read-only, no disk space, file is directory, …). #5431 (proller)Respect query settings in asynchronous INSERTs into Distributed tables. #4936 (TCeason)Renamed functions leastSqr to simpleLinearRegression, LinearRegression to linearRegression, LogisticRegression to logisticRegression. #5391 (Nikolai Kochetov) Performance Improvements​ Parallelize processing of parts of non-replicated MergeTree tables in ALTER MODIFY query. #4639 (Ivan Kush)Optimizations in regular expressions extraction. #5193 #5191 (Danila Kutenin)Do not add right join key column to join result if it’s used only in join on section. #5260 (Artem Zuikov)Freeze the Kafka buffer after first empty response. It avoids multiple invokations of ReadBuffer::next() for empty result in some row-parsing streams. #5283 (Ivan)concat function optimization for multiple arguments. #5357 (Danila Kutenin)Query optimisation. Allow push down IN statement while rewriting commа/cross join into inner one. #5396 (Artem Zuikov)Upgrade our LZ4 implementation with reference one to have faster decompression. #5070 (Danila Kutenin)Implemented MSD radix sort (based on kxsort), and partial sorting. #5129 (Evgenii Pravda) Bug Fixes​ Fix push require columns with join #5192 (Winter Zhang)Fixed bug, when ClickHouse is run by systemd, the command sudo service clickhouse-server forcerestart was not working as expected. #5204 (proller)Fix http error codes in DataPartsExchange (interserver http server on 9009 port always returned code 200, even on errors). #5216 (proller)Fix SimpleAggregateFunction for String longer than MAX_SMALL_STRING_SIZE #5311 (Azat Khuzhin)Fix error for Decimal to Nullable(Decimal) conversion in IN. Support other Decimal to Decimal conversions (including different scales). #5350 (Artem Zuikov)Fixed FPU clobbering in simdjson library that lead to wrong calculation of uniqHLL and uniqCombined aggregate function and math functions such as log. #5354 (alexey-milovidov)Fixed handling mixed const/nonconst cases in JSON functions. #5435 (Vitaly Baranov)Fix retention function. Now all conditions that satisfy in a row of data are added to the data state. #5119 (小路)Fix result type for quantileExact with Decimals. #5304 (Artem Zuikov) Documentation​ Translate documentation for CollapsingMergeTree to chinese. #5168 (张风啸)Translate some documentation about table engines to chinese.#5134#5328(never lee) Build/Testing/Packaging Improvements​ Fix some sanitizer reports that show probable use-after-free.#5139 #5143 #5393 (Ivan)Move performance tests out of separate directories for convenience. #5158 (alexey-milovidov)Fix incorrect performance tests. #5255 (alesapin)Added a tool to calculate checksums caused by bit flips to debug hardware issues. #5334 (alexey-milovidov)Make runner script more usable. #5340#5360 (filimonov)Add small instruction how to write performance tests. #5408 (alesapin)Add ability to make substitutions in create, fill and drop query in performance tests #5367 (Olga Khvostikova) "},{"title":"ClickHouse Release 19.7​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-7","content":""},{"title":"ClickHouse Release 19.7.5.29, 2019-07-05​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-7-5-29-2019-07-05","content":"Bug Fix​ Fix performance regression in some queries with JOIN. #5192 (Winter Zhang) "},{"title":"ClickHouse Release 19.7.5.27, 2019-06-09​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-7-5-27-2019-06-09","content":"New Features​ Added bitmap related functions bitmapHasAny and bitmapHasAll analogous to hasAny and hasAll functions for arrays. #5279 (Sergi Vladykin) Bug Fixes​ Fix segfault on minmax INDEX with Null value. #5246 (Nikita Vasilev)Mark all input columns in LIMIT BY as required output. It fixes ‘Not found column’ error in some distributed queries. #5407 (Constantin S. Pan)Fix “Column ‘0’ already exists” error in SELECT .. PREWHERE on column with DEFAULT #5397 (proller)Fix ALTER MODIFY TTL query on ReplicatedMergeTree. #5539 (Anton Popov)Don’t crash the server when Kafka consumers have failed to start. #5285 (Ivan)Fixed bitmap functions produce wrong result. #5359 (Andy Yang)Fix element_count for hashed dictionary (do not include duplicates) #5440 (Azat Khuzhin)Use contents of environment variable TZ as the name for timezone. It helps to correctly detect default timezone in some cases.#5443 (Ivan)Do not try to convert integers in dictGetT functions, because it does not work correctly. Throw an exception instead. #5446 (Artem Zuikov)Fix settings in ExternalData HTTP request. #5455 (Danila Kutenin)Fix bug when parts were removed only from FS without dropping them from Zookeeper. #5520 (alesapin)Fix segmentation fault in bitmapHasAny function. #5528 (Zhichang Yu)Fixed error when replication connection pool does not retry to resolve host, even when DNS cache was dropped. #5534 (alesapin)Fixed DROP INDEX IF EXISTS query. Now ALTER TABLE ... DROP INDEX IF EXISTS ... query does not raise an exception if provided index does not exist. #5524 (Gleb Novikov)Fix union all supertype column. There were cases with inconsistent data and column types of resulting columns. #5503 (Artem Zuikov)Skip ZNONODE during DDL query processing. Before if another node removes the znode in task queue, the one that did not process it, but already get list of children, will terminate the DDLWorker thread. #5489 (Azat Khuzhin)Fix INSERT into Distributed() table with MATERIALIZED column. #5429 (Azat Khuzhin) "},{"title":"ClickHouse Release 19.7.3.9, 2019-05-30​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-7-3-9-2019-05-30","content":"New Features​ Allow to limit the range of a setting that can be specified by user. These constraints can be set up in user settings profile.#4931 (Vitaly Baranov)Add a second version of the function groupUniqArray with an optionalmax_size parameter that limits the size of the resulting array. This behavior is similar to groupArray(max_size)(x) function.#5026 (Guillaume Tassery)For TSVWithNames/CSVWithNames input file formats, column order can now be determined from file header. This is controlled byinput_format_with_names_use_header parameter.#5081(Alexander) Bug Fixes​ Crash with uncompressed_cache + JOIN during merge (#5197)#5133 (Danila Kutenin)Segmentation fault on a clickhouse-client query to system tables. #5066#5127(Ivan)Data loss on heavy load via KafkaEngine (#4736)#5080(Ivan)Fixed very rare data race condition that could happen when executing a query with UNION ALL involving at least two SELECTs from system.columns, system.tables, system.parts, system.parts_tables or tables of Merge family and performing ALTER of columns of the related tables concurrently. #5189 (alexey-milovidov) Performance Improvements​ Use radix sort for sorting by single numeric column in ORDER BY withoutLIMIT. #5106,#4439(Evgenii Pravda,alexey-milovidov) Documentation​ Translate documentation for some table engines to Chinese.#5107,#5094,#5087(张风啸),#5068 (never lee) Build/Testing/Packaging Improvements​ Print UTF-8 characters properly in clickhouse-test.#5084(alexey-milovidov)Add command line parameter for clickhouse-client to always load suggestion data. #5102(alexey-milovidov)Resolve some of PVS-Studio warnings.#5082(alexey-milovidov)Update LZ4 #5040 (Danila Kutenin)Add gperf to build requirements for upcoming pull request #5030.#5110(proller) "},{"title":"ClickHouse Release 19.6​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-6","content":""},{"title":"ClickHouse Release 19.6.3.18, 2019-06-13​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-6-3-18-2019-06-13","content":"Bug Fixes​ Fixed IN condition pushdown for queries from table functions mysql and odbc and corresponding table engines. This fixes #3540 and #2384. #5313 (alexey-milovidov)Fix deadlock in Zookeeper. #5297 (github1youlc)Allow quoted decimals in CSV. #5284 (Artem ZuikovDisallow conversion from float Inf/NaN into Decimals (throw exception). #5282 (Artem Zuikov)Fix data race in rename query. #5247 (Winter Zhang)Temporarily disable LFAlloc. Usage of LFAlloc might lead to a lot of MAP_FAILED in allocating UncompressedCache and in a result to crashes of queries at high loaded servers. cfdba93(Danila Kutenin) "},{"title":"ClickHouse Release 19.6.2.11, 2019-05-13​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-6-2-11-2019-05-13","content":"New Features​ TTL expressions for columns and tables. #4212 (Anton Popov)Added support for brotli compression for HTTP responses (Accept-Encoding: br) #4388 (Mikhail)Added new function isValidUTF8 for checking whether a set of bytes is correctly utf-8 encoded. #4934 (Danila Kutenin)Add new load balancing policy first_or_random which sends queries to the first specified host and if it’s inaccessible send queries to random hosts of shard. Useful for cross-replication topology setups. #5012 (nvartolomei) Experimental Features​ Add setting index_granularity_bytes (adaptive index granularity) for MergeTree* tables family. #4826 (alesapin) Improvements​ Added support for non-constant and negative size and length arguments for function substringUTF8. #4989 (alexey-milovidov)Disable push-down to right table in left join, left table in right join, and both tables in full join. This fixes wrong JOIN results in some cases. #4846 (Ivan)clickhouse-copier: auto upload task configuration from --task-file option #4876 (proller)Added typos handler for storage factory and table functions factory. #4891 (Danila Kutenin)Support asterisks and qualified asterisks for multiple joins without subqueries #4898 (Artem Zuikov)Make missing column error message more user friendly. #4915 (Artem Zuikov) Performance Improvements​ Significant speedup of ASOF JOIN #4924 (Martijn Bakker) Backward Incompatible Changes​ HTTP header Query-Id was renamed to X-ClickHouse-Query-Id for consistency. #4972 (Mikhail) Bug Fixes​ Fixed potential null pointer dereference in clickhouse-copier. #4900 (proller)Fixed error on query with JOIN + ARRAY JOIN #4938 (Artem Zuikov)Fixed hanging on start of the server when a dictionary depends on another dictionary via a database with engine=Dictionary. #4962 (Vitaly Baranov)Partially fix distributed_product_mode = local. It’s possible to allow columns of local tables in where/having/order by/… via table aliases. Throw exception if table does not have alias. There’s not possible to access to the columns without table aliases yet. #4986 (Artem Zuikov)Fix potentially wrong result for SELECT DISTINCT with JOIN #5001 (Artem Zuikov)Fixed very rare data race condition that could happen when executing a query with UNION ALL involving at least two SELECTs from system.columns, system.tables, system.parts, system.parts_tables or tables of Merge family and performing ALTER of columns of the related tables concurrently. #5189 (alexey-milovidov) Build/Testing/Packaging Improvements​ Fixed test failures when running clickhouse-server on different host #4713 (Vasily Nemkov)clickhouse-test: Disable color control sequences in non tty environment. #4937 (alesapin)clickhouse-test: Allow use any test database (remove test. qualification where it possible) #5008 (proller)Fix ubsan errors #5037 (Vitaly Baranov)Yandex LFAlloc was added to ClickHouse to allocate MarkCache and UncompressedCache data in different ways to catch segfaults more reliable #4995 (Danila Kutenin)Python util to help with backports and changelogs. #4949 (Ivan) "},{"title":"ClickHouse Release 19.5​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-5","content":""},{"title":"ClickHouse Release 19.5.4.22, 2019-05-13​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-5-4-22-2019-05-13","content":"Bug Fixes​ Fixed possible crash in bitmap* functions #5220 #5228 (Andy Yang)Fixed very rare data race condition that could happen when executing a query with UNION ALL involving at least two SELECTs from system.columns, system.tables, system.parts, system.parts_tables or tables of Merge family and performing ALTER of columns of the related tables concurrently. #5189 (alexey-milovidov)Fixed error Set for IN is not created yet in case of using single LowCardinality column in the left part of IN. This error happened if LowCardinality column was the part of primary key. #5031 #5154 (Nikolai Kochetov)Modification of retention function: If a row satisfies both the first and NTH condition, only the first satisfied condition is added to the data state. Now all conditions that satisfy in a row of data are added to the data state. #5119 (小路) "},{"title":"ClickHouse Release 19.5.3.8, 2019-04-18​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-5-3-8-2019-04-18","content":"Bug Fixes​ Fixed type of setting max_partitions_per_insert_block from boolean to UInt64. #5028 (Mohammad Hossein Sekhavat) "},{"title":"ClickHouse Release 19.5.2.6, 2019-04-15​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-5-2-6-2019-04-15","content":"New Features​ Hyperscan multiple regular expression matching was added (functions multiMatchAny, multiMatchAnyIndex, multiFuzzyMatchAny, multiFuzzyMatchAnyIndex). #4780, #4841 (Danila Kutenin)multiSearchFirstPosition function was added. #4780 (Danila Kutenin)Implement the predefined expression filter per row for tables. #4792 (Ivan)A new type of data skipping indices based on bloom filters (can be used for equal, in and like functions). #4499 (Nikita Vasilev)Added ASOF JOIN which allows to run queries that join to the most recent value known. #4774 #4867 #4863 #4875 (Martijn Bakker, Artem Zuikov)Rewrite multiple COMMA JOIN to CROSS JOIN. Then rewrite them to INNER JOIN if possible. #4661 (Artem Zuikov) Improvement​ topK and topKWeighted now supports custom loadFactor (fixes issue #4252). #4634 (Kirill Danshin)Allow to use parallel_replicas_count &gt; 1 even for tables without sampling (the setting is simply ignored for them). In previous versions it was lead to exception. #4637 (Alexey Elymanov)Support for CREATE OR REPLACE VIEW. Allow to create a view or set a new definition in a single statement. #4654 (Boris Granveaud)Buffer table engine now supports PREWHERE. #4671 (Yangkuan Liu)Add ability to start replicated table without metadata in zookeeper in readonly mode. #4691 (alesapin)Fixed flicker of progress bar in clickhouse-client. The issue was most noticeable when using FORMAT Null with streaming queries. #4811 (alexey-milovidov)Allow to disable functions with hyperscan library on per user basis to limit potentially excessive and uncontrolled resource usage. #4816 (alexey-milovidov)Add version number logging in all errors. #4824 (proller)Added restriction to the multiMatch functions which requires string size to fit into unsigned int. Also added the number of arguments limit to the multiSearch functions. #4834 (Danila Kutenin)Improved usage of scratch space and error handling in Hyperscan. #4866 (Danila Kutenin)Fill system.graphite_detentions from a table config of *GraphiteMergeTree engine tables. #4584 (Mikhail f. Shiryaev)Rename trigramDistance function to ngramDistance and add more functions with CaseInsensitive and UTF. #4602 (Danila Kutenin)Improved data skipping indices calculation. #4640 (Nikita Vasilev)Keep ordinary, DEFAULT, MATERIALIZED and ALIAS columns in a single list (fixes issue #2867). #4707 (Alex Zatelepin) Bug Fix​ Avoid std::terminate in case of memory allocation failure. Now std::bad_alloc exception is thrown as expected. #4665 (alexey-milovidov)Fixes capnproto reading from buffer. Sometimes files wasn’t loaded successfully by HTTP. #4674 (Vladislav)Fix error Unknown log entry type: 0 after OPTIMIZE TABLE FINAL query. #4683 (Amos Bird)Wrong arguments to hasAny or hasAll functions may lead to segfault. #4698 (alexey-milovidov)Deadlock may happen while executing DROP DATABASE dictionary query. #4701 (alexey-milovidov)Fix undefined behavior in median and quantile functions. #4702 (hcz)Fix compression level detection when network_compression_method in lowercase. Broken in v19.1. #4706 (proller)Fixed ignorance of &lt;timezone&gt;UTC&lt;/timezone&gt; setting (fixes issue #4658). #4718 (proller)Fix histogram function behaviour with Distributed tables. #4741 (olegkv)Fixed tsan report destroy of a locked mutex. #4742 (alexey-milovidov)Fixed TSan report on shutdown due to race condition in system logs usage. Fixed potential use-after-free on shutdown when part_log is enabled. #4758 (alexey-milovidov)Fix recheck parts in ReplicatedMergeTreeAlterThread in case of error. #4772 (Nikolai Kochetov)Arithmetic operations on intermediate aggregate function states were not working for constant arguments (such as subquery results). #4776 (alexey-milovidov)Always backquote column names in metadata. Otherwise it’s impossible to create a table with column named index (server won’t restart due to malformed ATTACH query in metadata). #4782 (alexey-milovidov)Fix crash in ALTER ... MODIFY ORDER BY on Distributed table. #4790 (TCeason)Fix segfault in JOIN ON with enabled enable_optimize_predicate_expression. #4794 (Winter Zhang)Fix bug with adding an extraneous row after consuming a protobuf message from Kafka. #4808 (Vitaly Baranov)Fix crash of JOIN on not-nullable vs nullable column. Fix NULLs in right keys in ANY JOIN + join_use_nulls. #4815 (Artem Zuikov)Fix segmentation fault in clickhouse-copier. #4835 (proller)Fixed race condition in SELECT from system.tables if the table is renamed or altered concurrently. #4836 (alexey-milovidov)Fixed data race when fetching data part that is already obsolete. #4839 (alexey-milovidov)Fixed rare data race that can happen during RENAME table of MergeTree family. #4844 (alexey-milovidov)Fixed segmentation fault in function arrayIntersect. Segmentation fault could happen if function was called with mixed constant and ordinary arguments. #4847 (Lixiang Qian)Fixed reading from Array(LowCardinality) column in rare case when column contained a long sequence of empty arrays. #4850 (Nikolai Kochetov)Fix crash in FULL/RIGHT JOIN when we joining on nullable vs not nullable. #4855 (Artem Zuikov)Fix No message received exception while fetching parts between replicas. #4856 (alesapin)Fixed arrayIntersect function wrong result in case of several repeated values in single array. #4871 (Nikolai Kochetov)Fix a race condition during concurrent ALTER COLUMN queries that could lead to a server crash (fixes issue #3421). #4592 (Alex Zatelepin)Fix incorrect result in FULL/RIGHT JOIN with const column. #4723 (Artem Zuikov)Fix duplicates in GLOBAL JOIN with asterisk. #4705 (Artem Zuikov)Fix parameter deduction in ALTER MODIFY of column CODEC when column type is not specified. #4883 (alesapin)Functions cutQueryStringAndFragment() and queryStringAndFragment() now works correctly when URL contains a fragment and no query. #4894 (Vitaly Baranov)Fix rare bug when setting min_bytes_to_use_direct_io is greater than zero, which occures when thread have to seek backward in column file. #4897 (alesapin)Fix wrong argument types for aggregate functions with LowCardinality arguments (fixes issue #4919). #4922 (Nikolai Kochetov)Fix wrong name qualification in GLOBAL JOIN. #4969 (Artem Zuikov)Fix function toISOWeek result for year 1970. #4988 (alexey-milovidov)Fix DROP, TRUNCATE and OPTIMIZE queries duplication, when executed on ON CLUSTER for ReplicatedMergeTree* tables family. #4991 (alesapin) Backward Incompatible Change​ Rename setting insert_sample_with_metadata to setting input_format_defaults_for_omitted_fields. #4771 (Artem Zuikov)Added setting max_partitions_per_insert_block (with value 100 by default). If inserted block contains larger number of partitions, an exception is thrown. Set it to 0 if you want to remove the limit (not recommended). #4845 (alexey-milovidov)Multi-search functions were renamed (multiPosition to multiSearchAllPositions, multiSearch to multiSearchAny, firstMatch to multiSearchFirstIndex). #4780 (Danila Kutenin) Performance Improvement​ Optimize Volnitsky searcher by inlining, giving about 5-10% search improvement for queries with many needles or many similar bigrams. #4862 (Danila Kutenin)Fix performance issue when setting use_uncompressed_cache is greater than zero, which appeared when all read data contained in cache. #4913 (alesapin) Build/Testing/Packaging Improvement​ Hardening debug build: more granular memory mappings and ASLR; add memory protection for mark cache and index. This allows to find more memory stomping bugs in case when ASan and MSan cannot do it. #4632 (alexey-milovidov)Add support for cmake variables ENABLE_PROTOBUF, ENABLE_PARQUET and ENABLE_BROTLI which allows to enable/disable the above features (same as we can do for librdkafka, mysql, etc). #4669 (Silviu Caragea)Add ability to print process list and stacktraces of all threads if some queries are hung after test run. #4675 (alesapin)Add retries on Connection loss error in clickhouse-test. #4682 (alesapin)Add freebsd build with vagrant and build with thread sanitizer to packager script. #4712 #4748 (alesapin)Now user asked for password for user 'default' during installation. #4725 (proller)Suppress warning in rdkafka library. #4740 (alexey-milovidov)Allow ability to build without ssl. #4750 (proller)Add a way to launch clickhouse-server image from a custom user. #4753 (Mikhail f. Shiryaev)Upgrade contrib boost to 1.69. #4793 (proller)Disable usage of mremap when compiled with Thread Sanitizer. Surprisingly enough, TSan does not intercept mremap (though it does intercept mmap, munmap) that leads to false positives. Fixed TSan report in stateful tests. #4859 (alexey-milovidov)Add test checking using format schema via HTTP interface. #4864 (Vitaly Baranov) "},{"title":"ClickHouse Release 19.4​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-4","content":""},{"title":"ClickHouse Release 19.4.4.33, 2019-04-17​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-4-4-33-2019-04-17","content":"Bug Fixes​ Avoid std::terminate in case of memory allocation failure. Now std::bad_alloc exception is thrown as expected. #4665 (alexey-milovidov)Fixes capnproto reading from buffer. Sometimes files wasn’t loaded successfully by HTTP. #4674 (Vladislav)Fix error Unknown log entry type: 0 after OPTIMIZE TABLE FINAL query. #4683 (Amos Bird)Wrong arguments to hasAny or hasAll functions may lead to segfault. #4698 (alexey-milovidov)Deadlock may happen while executing DROP DATABASE dictionary query. #4701 (alexey-milovidov)Fix undefined behavior in median and quantile functions. #4702 (hcz)Fix compression level detection when network_compression_method in lowercase. Broken in v19.1. #4706 (proller)Fixed ignorance of &lt;timezone&gt;UTC&lt;/timezone&gt; setting (fixes issue #4658). #4718 (proller)Fix histogram function behaviour with Distributed tables. #4741 (olegkv)Fixed tsan report destroy of a locked mutex. #4742 (alexey-milovidov)Fixed TSan report on shutdown due to race condition in system logs usage. Fixed potential use-after-free on shutdown when part_log is enabled. #4758 (alexey-milovidov)Fix recheck parts in ReplicatedMergeTreeAlterThread in case of error. #4772 (Nikolai Kochetov)Arithmetic operations on intermediate aggregate function states were not working for constant arguments (such as subquery results). #4776 (alexey-milovidov)Always backquote column names in metadata. Otherwise it’s impossible to create a table with column named index (server won’t restart due to malformed ATTACH query in metadata). #4782 (alexey-milovidov)Fix crash in ALTER ... MODIFY ORDER BY on Distributed table. #4790 (TCeason)Fix segfault in JOIN ON with enabled enable_optimize_predicate_expression. #4794 (Winter Zhang)Fix bug with adding an extraneous row after consuming a protobuf message from Kafka. #4808 (Vitaly Baranov)Fix segmentation fault in clickhouse-copier. #4835 (proller)Fixed race condition in SELECT from system.tables if the table is renamed or altered concurrently. #4836 (alexey-milovidov)Fixed data race when fetching data part that is already obsolete. #4839 (alexey-milovidov)Fixed rare data race that can happen during RENAME table of MergeTree family. #4844 (alexey-milovidov)Fixed segmentation fault in function arrayIntersect. Segmentation fault could happen if function was called with mixed constant and ordinary arguments. #4847 (Lixiang Qian)Fixed reading from Array(LowCardinality) column in rare case when column contained a long sequence of empty arrays. #4850 (Nikolai Kochetov)Fix No message received exception while fetching parts between replicas. #4856 (alesapin)Fixed arrayIntersect function wrong result in case of several repeated values in single array. #4871 (Nikolai Kochetov)Fix a race condition during concurrent ALTER COLUMN queries that could lead to a server crash (fixes issue #3421). #4592 (Alex Zatelepin)Fix parameter deduction in ALTER MODIFY of column CODEC when column type is not specified. #4883 (alesapin)Functions cutQueryStringAndFragment() and queryStringAndFragment() now works correctly when URL contains a fragment and no query. #4894 (Vitaly Baranov)Fix rare bug when setting min_bytes_to_use_direct_io is greater than zero, which occures when thread have to seek backward in column file. #4897 (alesapin)Fix wrong argument types for aggregate functions with LowCardinality arguments (fixes issue #4919). #4922 (Nikolai Kochetov)Fix function toISOWeek result for year 1970. #4988 (alexey-milovidov)Fix DROP, TRUNCATE and OPTIMIZE queries duplication, when executed on ON CLUSTER for ReplicatedMergeTree* tables family. #4991 (alesapin) Improvements​ Keep ordinary, DEFAULT, MATERIALIZED and ALIAS columns in a single list (fixes issue #2867). #4707 (Alex Zatelepin) "},{"title":"ClickHouse Release 19.4.3.11, 2019-04-02​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-4-3-11-2019-04-02","content":"Bug Fixes​ Fix crash in FULL/RIGHT JOIN when we joining on nullable vs not nullable. #4855 (Artem Zuikov)Fix segmentation fault in clickhouse-copier. #4835 (proller) Build/Testing/Packaging Improvement​ Add a way to launch clickhouse-server image from a custom user. #4753 (Mikhail f. Shiryaev) "},{"title":"ClickHouse Release 19.4.2.7, 2019-03-30​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-4-2-7-2019-03-30","content":"Bug Fixes​ Fixed reading from Array(LowCardinality) column in rare case when column contained a long sequence of empty arrays. #4850 (Nikolai Kochetov) "},{"title":"ClickHouse Release 19.4.1.3, 2019-03-19​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-4-1-3-2019-03-19","content":"Bug Fixes​ Fixed remote queries which contain both LIMIT BY and LIMIT. Previously, if LIMIT BY and LIMIT were used for remote query, LIMIT could happen before LIMIT BY, which led to too filtered result. #4708 (Constantin S. Pan) "},{"title":"ClickHouse Release 19.4.0.49, 2019-03-09​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-4-0-49-2019-03-09","content":"New Features​ Added full support for Protobuf format (input and output, nested data structures). #4174 #4493 (Vitaly Baranov)Added bitmap functions with Roaring Bitmaps. #4207 (Andy Yang) #4568 (Vitaly Baranov)Parquet format support. #4448 (proller)N-gram distance was added for fuzzy string comparison. It is similar to q-gram metrics in R language. #4466 (Danila Kutenin)Combine rules for graphite rollup from dedicated aggregation and retention patterns. #4426 (Mikhail f. Shiryaev)Added max_execution_speed and max_execution_speed_bytes to limit resource usage. Added min_execution_speed_bytes setting to complement the min_execution_speed. #4430 (Winter Zhang)Implemented function flatten. #4555 #4409 (alexey-milovidov, kzon)Added functions arrayEnumerateDenseRanked and arrayEnumerateUniqRanked (it’s like arrayEnumerateUniq but allows to fine tune array depth to look inside multidimensional arrays). #4475 (proller) #4601 (alexey-milovidov)Multiple JOINS with some restrictions: no asterisks, no complex aliases in ON/WHERE/GROUP BY/… #4462 (Artem Zuikov) Bug Fixes​ This release also contains all bug fixes from 19.3 and 19.1.Fixed bug in data skipping indices: order of granules after INSERT was incorrect. #4407 (Nikita Vasilev)Fixed set index for Nullable and LowCardinality columns. Before it, set index with Nullable or LowCardinality column led to error Data type must be deserialized with multiple streams while selecting. #4594 (Nikolai Kochetov)Correctly set update_time on full executable dictionary update. #4551 (Tema Novikov)Fix broken progress bar in 19.3. #4627 (filimonov)Fixed inconsistent values of MemoryTracker when memory region was shrinked, in certain cases. #4619 (alexey-milovidov)Fixed undefined behaviour in ThreadPool. #4612 (alexey-milovidov)Fixed a very rare crash with the message mutex lock failed: Invalid argument that could happen when a MergeTree table was dropped concurrently with a SELECT. #4608 (Alex Zatelepin)ODBC driver compatibility with LowCardinality data type. #4381 (proller)FreeBSD: Fixup for AIOcontextPool: Found io_event with unknown id 0 error. #4438 (urgordeadbeef)system.part_log table was created regardless to configuration. #4483 (alexey-milovidov)Fix undefined behaviour in dictIsIn function for cache dictionaries. #4515 (alesapin)Fixed a deadlock when a SELECT query locks the same table multiple times (e.g. from different threads or when executing multiple subqueries) and there is a concurrent DDL query. #4535 (Alex Zatelepin)Disable compile_expressions by default until we get own llvm contrib and can test it with clang and asan. #4579 (alesapin)Prevent std::terminate when invalidate_query for clickhouse external dictionary source has returned wrong resultset (empty or more than one row or more than one column). Fixed issue when the invalidate_query was performed every five seconds regardless to the lifetime. #4583 (alexey-milovidov)Avoid deadlock when the invalidate_query for a dictionary with clickhouse source was involving system.dictionaries table or Dictionaries database (rare case). #4599 (alexey-milovidov)Fixes for CROSS JOIN with empty WHERE. #4598 (Artem Zuikov)Fixed segfault in function “replicate” when constant argument is passed. #4603 (alexey-milovidov)Fix lambda function with predicate optimizer. #4408 (Winter Zhang)Multiple JOINs multiple fixes. #4595 (Artem Zuikov) Improvements​ Support aliases in JOIN ON section for right table columns. #4412 (Artem Zuikov)Result of multiple JOINs need correct result names to be used in subselects. Replace flat aliases with source names in result. #4474 (Artem Zuikov)Improve push-down logic for joined statements. #4387 (Ivan) Performance Improvements​ Improved heuristics of “move to PREWHERE” optimization. #4405 (alexey-milovidov)Use proper lookup tables that uses HashTable’s API for 8-bit and 16-bit keys. #4536 (Amos Bird)Improved performance of string comparison. #4564 (alexey-milovidov)Cleanup distributed DDL queue in a separate thread so that it does not slow down the main loop that processes distributed DDL tasks. #4502 (Alex Zatelepin)When min_bytes_to_use_direct_io is set to 1, not every file was opened with O_DIRECT mode because the data size to read was sometimes underestimated by the size of one compressed block. #4526 (alexey-milovidov) Build/Testing/Packaging Improvement​ Added support for clang-9 #4604 (alexey-milovidov)Fix wrong __asm__ instructions (again) #4621 (Konstantin Podshumok)Add ability to specify settings for clickhouse-performance-test from command line. #4437 (alesapin)Add dictionaries tests to integration tests. #4477 (alesapin)Added queries from the benchmark on the website to automated performance tests. #4496 (alexey-milovidov)xxhash.h does not exist in external lz4 because it is an implementation detail and its symbols are namespaced with XXH_NAMESPACE macro. When lz4 is external, xxHash has to be external too, and the dependents have to link to it. #4495 (Orivej Desh)Fixed a case when quantileTiming aggregate function can be called with negative or floating point argument (this fixes fuzz test with undefined behaviour sanitizer). #4506 (alexey-milovidov)Spelling error correction. #4531 (sdk2)Fix compilation on Mac. #4371 (Vitaly Baranov)Build fixes for FreeBSD and various unusual build configurations. #4444 (proller) "},{"title":"ClickHouse Release 19.3​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-3","content":""},{"title":"ClickHouse Release 19.3.9.1, 2019-04-02​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-3-9-1-2019-04-02","content":"Bug Fixes​ Fix crash in FULL/RIGHT JOIN when we joining on nullable vs not nullable. #4855 (Artem Zuikov)Fix segmentation fault in clickhouse-copier. #4835 (proller)Fixed reading from Array(LowCardinality) column in rare case when column contained a long sequence of empty arrays. #4850 (Nikolai Kochetov) Build/Testing/Packaging Improvement​ Add a way to launch clickhouse-server image from a custom user #4753 (Mikhail f. Shiryaev) "},{"title":"ClickHouse Release 19.3.7, 2019-03-12​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-3-7-2019-03-12","content":"Bug Fixes​ Fixed error in #3920. This error manifests itself as random cache corruption (messages Unknown codec family code, Cannot seek through file) and segfaults. This bug first appeared in version 19.1 and is present in versions up to 19.1.10 and 19.3.6. #4623 (alexey-milovidov) "},{"title":"ClickHouse Release 19.3.6, 2019-03-02​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-3-6-2019-03-02","content":"Bug Fixes​ When there are more than 1000 threads in a thread pool, std::terminate may happen on thread exit. Azat Khuzhin #4485 #4505 (alexey-milovidov)Now it’s possible to create ReplicatedMergeTree* tables with comments on columns without defaults and tables with columns codecs without comments and defaults. Also fix comparison of codecs. #4523 (alesapin)Fixed crash on JOIN with array or tuple. #4552 (Artem Zuikov)Fixed crash in clickhouse-copier with the message ThreadStatus not created. #4540 (Artem Zuikov)Fixed hangup on server shutdown if distributed DDLs were used. #4472 (Alex Zatelepin)Incorrect column numbers were printed in error message about text format parsing for columns with number greater than 10. #4484 (alexey-milovidov) Build/Testing/Packaging Improvements​ Fixed build with AVX enabled. #4527 (alexey-milovidov)Enable extended accounting and IO accounting based on good known version instead of kernel under which it is compiled. #4541 (nvartolomei)Allow to skip setting of core_dump.size_limit, warning instead of throw if limit set fail. #4473 (proller)Removed the inline tags of void readBinary(...) in Field.cpp. Also merged redundant namespace DB blocks. #4530 (hcz) "},{"title":"ClickHouse Release 19.3.5, 2019-02-21​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-3-5-2019-02-21","content":"Bug Fixes​ Fixed bug with large http insert queries processing. #4454 (alesapin)Fixed backward incompatibility with old versions due to wrong implementation of send_logs_level setting. #4445 (alexey-milovidov)Fixed backward incompatibility of table function remote introduced with column comments. #4446 (alexey-milovidov) "},{"title":"ClickHouse Release 19.3.4, 2019-02-16​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-3-4-2019-02-16","content":"Improvements​ Table index size is not accounted for memory limits when doing ATTACH TABLE query. Avoided the possibility that a table cannot be attached after being detached. #4396 (alexey-milovidov)Slightly raised up the limit on max string and array size received from ZooKeeper. It allows to continue to work with increased size of CLIENT_JVMFLAGS=-Djute.maxbuffer=... on ZooKeeper. #4398 (alexey-milovidov)Allow to repair abandoned replica even if it already has huge number of nodes in its queue. #4399 (alexey-milovidov)Add one required argument to SET index (max stored rows number). #4386 (Nikita Vasilev) Bug Fixes​ Fixed WITH ROLLUP result for group by single LowCardinality key. #4384 (Nikolai Kochetov)Fixed bug in the set index (dropping a granule if it contains more than max_rows rows). #4386 (Nikita Vasilev)A lot of FreeBSD build fixes. #4397 (proller)Fixed aliases substitution in queries with subquery containing same alias (issue #4110). #4351 (Artem Zuikov) Build/Testing/Packaging Improvements​ Add ability to run clickhouse-server for stateless tests in docker image. #4347 (Vasily Nemkov) "},{"title":"ClickHouse Release 19.3.3, 2019-02-13​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-3-3-2019-02-13","content":"New Features​ Added the KILL MUTATION statement that allows removing mutations that are for some reasons stuck. Added latest_failed_part, latest_fail_time, latest_fail_reason fields to the system.mutations table for easier troubleshooting. #4287 (Alex Zatelepin)Added aggregate function entropy which computes Shannon entropy. #4238 (Quid37)Added ability to send queries INSERT INTO tbl VALUES (.... to server without splitting on query and data parts. #4301 (alesapin)Generic implementation of arrayWithConstant function was added. #4322 (alexey-milovidov)Implemented NOT BETWEEN comparison operator. #4228 (Dmitry Naumov)Implement sumMapFiltered in order to be able to limit the number of keys for which values will be summed by sumMap. #4129 (Léo Ercolanelli)Added support of Nullable types in mysql table function. #4198 (Emmanuel Donin de Rosière)Support for arbitrary constant expressions in LIMIT clause. #4246 (k3box)Added topKWeighted aggregate function that takes additional argument with (unsigned integer) weight. #4245 (Andrew Golman)StorageJoin now supports join_any_take_last_row setting that allows overwriting existing values of the same key. #3973 (Amos BirdAdded function toStartOfInterval. #4304 (Vitaly Baranov)Added RowBinaryWithNamesAndTypes format. #4200 (Oleg V. Kozlyuk)Added IPv4 and IPv6 data types. More effective implementations of IPv* functions. #3669 (Vasily Nemkov)Added function toStartOfTenMinutes(). #4298 (Vitaly Baranov)Added Protobuf output format. #4005 #4158 (Vitaly Baranov)Added brotli support for HTTP interface for data import (INSERTs). #4235 (Mikhail)Added hints while user make typo in function name or type in command line client. #4239 (Danila Kutenin)Added Query-Id to Server’s HTTP Response header. #4231 (Mikhail) Experimental Features​ Added minmax and set data skipping indices for MergeTree table engines family. #4143 (Nikita Vasilev)Added conversion of CROSS JOIN to INNER JOIN if possible. #4221 #4266 (Artem Zuikov) Bug Fixes​ Fixed Not found column for duplicate columns in JOIN ON section. #4279 (Artem Zuikov)Make START REPLICATED SENDS command start replicated sends. #4229 (nvartolomei)Fixed aggregate functions execution with Array(LowCardinality) arguments. #4055 (KochetovNicolai)Fixed wrong behaviour when doing INSERT ... SELECT ... FROM file(...) query and file has CSVWithNames or TSVWIthNames format and the first data row is missing. #4297 (alexey-milovidov)Fixed crash on dictionary reload if dictionary not available. This bug was appeared in 19.1.6. #4188 (proller)Fixed ALL JOIN with duplicates in right table. #4184 (Artem Zuikov)Fixed segmentation fault with use_uncompressed_cache=1 and exception with wrong uncompressed size. This bug was appeared in 19.1.6. #4186 (alesapin)Fixed compile_expressions bug with comparison of big (more than int16) dates. #4341 (alesapin)Fixed infinite loop when selecting from table function numbers(0). #4280 (alexey-milovidov)Temporarily disable predicate optimization for ORDER BY. #3890 (Winter Zhang)Fixed Illegal instruction error when using base64 functions on old CPUs. This error has been reproduced only when ClickHouse was compiled with gcc-8. #4275 (alexey-milovidov)Fixed No message received error when interacting with PostgreSQL ODBC Driver through TLS connection. Also fixes segfault when using MySQL ODBC Driver. #4170 (alexey-milovidov)Fixed incorrect result when Date and DateTime arguments are used in branches of conditional operator (function if). Added generic case for function if. #4243 (alexey-milovidov)ClickHouse dictionaries now load within clickhouse process. #4166 (alexey-milovidov)Fixed deadlock when SELECT from a table with File engine was retried after No such file or directory error. #4161 (alexey-milovidov)Fixed race condition when selecting from system.tables may give table does not exist error. #4313 (alexey-milovidov)clickhouse-client can segfault on exit while loading data for command line suggestions if it was run in interactive mode. #4317 (alexey-milovidov)Fixed a bug when the execution of mutations containing IN operators was producing incorrect results. #4099 (Alex Zatelepin)Fixed error: if there is a database with Dictionary engine, all dictionaries forced to load at server startup, and if there is a dictionary with ClickHouse source from localhost, the dictionary cannot load. #4255 (alexey-milovidov)Fixed error when system logs are tried to create again at server shutdown. #4254 (alexey-milovidov)Correctly return the right type and properly handle locks in joinGet function. #4153 (Amos Bird)Added sumMapWithOverflow function. #4151 (Léo Ercolanelli)Fixed segfault with allow_experimental_multiple_joins_emulation. 52de2c (Artem Zuikov)Fixed bug with incorrect Date and DateTime comparison. #4237 (valexey)Fixed fuzz test under undefined behavior sanitizer: added parameter type check for quantile*Weighted family of functions. #4145 (alexey-milovidov)Fixed rare race condition when removing of old data parts can fail with File not found error. #4378 (alexey-milovidov)Fix install package with missing /etc/clickhouse-server/config.xml. #4343 (proller) Build/Testing/Packaging Improvements​ Debian package: correct /etc/clickhouse-server/preprocessed link according to config. #4205 (proller)Various build fixes for FreeBSD. #4225 (proller)Added ability to create, fill and drop tables in perftest. #4220 (alesapin)Added a script to check for duplicate includes. #4326 (alexey-milovidov)Added ability to run queries by index in performance test. #4264 (alesapin)Package with debug symbols is suggested to be installed. #4274 (alexey-milovidov)Refactoring of performance-test. Better logging and signals handling. #4171 (alesapin)Added docs to anonymized Yandex.Metrika datasets. #4164 (alesapin)Аdded tool for converting an old month-partitioned part to the custom-partitioned format. #4195 (Alex Zatelepin)Added docs about two datasets in s3. #4144 (alesapin)Added script which creates changelog from pull requests description. #4169 #4173 (KochetovNicolai) (KochetovNicolai)Added puppet module for ClickHouse. #4182 (Maxim Fedotov)Added docs for a group of undocumented functions. #4168 (Winter Zhang)ARM build fixes. #4210#4306 #4291 (proller) (proller)Dictionary tests now able to run from ctest. #4189 (proller)Now /etc/ssl is used as default directory with SSL certificates. #4167 (alexey-milovidov)Added checking SSE and AVX instruction at start. #4234 (Igr)Init script will wait server until start. #4281 (proller) Backward Incompatible Changes​ Removed allow_experimental_low_cardinality_type setting. LowCardinality data types are production ready. #4323 (alexey-milovidov)Reduce mark cache size and uncompressed cache size accordingly to available memory amount. #4240 (Lopatin KonstantinAdded keyword INDEX in CREATE TABLE query. A column with name index must be quoted with backticks or double quotes: `index`. #4143 (Nikita Vasilev)sumMap now promote result type instead of overflow. The old sumMap behavior can be obtained by using sumMapWithOverflow function. #4151 (Léo Ercolanelli) Performance Improvements​ std::sort replaced by pdqsort for queries without LIMIT. #4236 (Evgenii Pravda)Now server reuse threads from global thread pool. This affects performance in some corner cases. #4150 (alexey-milovidov) Improvements​ Implemented AIO support for FreeBSD. #4305 (urgordeadbeef)SELECT * FROM a JOIN b USING a, b now return a and b columns only from the left table. #4141 (Artem Zuikov)Allow -C option of client to work as -c option. #4232 (syominsergey)Now option --password used without value requires password from stdin. #4230 (BSD_Conqueror)Added highlighting of unescaped metacharacters in string literals that contain LIKE expressions or regexps. #4327 (alexey-milovidov)Added cancelling of HTTP read only queries if client socket goes away. #4213 (nvartolomei)Now server reports progress to keep client connections alive. #4215 (Ivan)Slightly better message with reason for OPTIMIZE query with optimize_throw_if_noop setting enabled. #4294 (alexey-milovidov)Added support of --version option for clickhouse server. #4251 (Lopatin Konstantin)Added --help/-h option to clickhouse-server. #4233 (Yuriy Baranov)Added support for scalar subqueries with aggregate function state result. #4348 (Nikolai Kochetov)Improved server shutdown time and ALTERs waiting time. #4372 (alexey-milovidov)Added info about the replicated_can_become_leader setting to system.replicas and add logging if the replica won’t try to become leader. #4379 (Alex Zatelepin) "},{"title":"ClickHouse Release 19.1​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-1","content":""},{"title":"ClickHouse Release 19.1.14, 2019-03-14​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-1-14-2019-03-14","content":"Fixed error Column ... queried more than once that may happen if the setting asterisk_left_columns_only is set to 1 in case of using GLOBAL JOIN with SELECT * (rare case). The issue does not exist in 19.3 and newer. 6bac7d8d (Artem Zuikov) "},{"title":"ClickHouse Release 19.1.13, 2019-03-12​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-1-13-2019-03-12","content":"This release contains exactly the same set of patches as 19.3.7. "},{"title":"ClickHouse Release 19.1.10, 2019-03-03​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-1-10-2019-03-03","content":"This release contains exactly the same set of patches as 19.3.6. "},{"title":"ClickHouse Release 19.1​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-1-1","content":""},{"title":"ClickHouse Release 19.1.9, 2019-02-21​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-1-9-2019-02-21","content":"Bug Fixes​ Fixed backward incompatibility with old versions due to wrong implementation of send_logs_level setting. #4445 (alexey-milovidov)Fixed backward incompatibility of table function remote introduced with column comments. #4446 (alexey-milovidov) "},{"title":"ClickHouse Release 19.1.8, 2019-02-16​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-1-8-2019-02-16","content":"Bug Fixes​ Fix install package with missing /etc/clickhouse-server/config.xml. #4343 (proller) "},{"title":"ClickHouse Release 19.1​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-1-2","content":""},{"title":"ClickHouse Release 19.1.7, 2019-02-15​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-1-7-2019-02-15","content":"Bug Fixes​ Correctly return the right type and properly handle locks in joinGet function. #4153 (Amos Bird)Fixed error when system logs are tried to create again at server shutdown. #4254 (alexey-milovidov)Fixed error: if there is a database with Dictionary engine, all dictionaries forced to load at server startup, and if there is a dictionary with ClickHouse source from localhost, the dictionary cannot load. #4255 (alexey-milovidov)Fixed a bug when the execution of mutations containing IN operators was producing incorrect results. #4099 (Alex Zatelepin)clickhouse-client can segfault on exit while loading data for command line suggestions if it was run in interactive mode. #4317 (alexey-milovidov)Fixed race condition when selecting from system.tables may give table does not exist error. #4313 (alexey-milovidov)Fixed deadlock when SELECT from a table with File engine was retried after No such file or directory error. #4161 (alexey-milovidov)Fixed an issue: local ClickHouse dictionaries are loaded via TCP, but should load within process. #4166 (alexey-milovidov)Fixed No message received error when interacting with PostgreSQL ODBC Driver through TLS connection. Also fixes segfault when using MySQL ODBC Driver. #4170 (alexey-milovidov)Temporarily disable predicate optimization for ORDER BY. #3890 (Winter Zhang)Fixed infinite loop when selecting from table function numbers(0). #4280 (alexey-milovidov)Fixed compile_expressions bug with comparison of big (more than int16) dates. #4341 (alesapin)Fixed segmentation fault with uncompressed_cache=1 and exception with wrong uncompressed size. #4186 (alesapin)Fixed ALL JOIN with duplicates in right table. #4184 (Artem Zuikov)Fixed wrong behaviour when doing INSERT ... SELECT ... FROM file(...) query and file has CSVWithNames or TSVWIthNames format and the first data row is missing. #4297 (alexey-milovidov)Fixed aggregate functions execution with Array(LowCardinality) arguments. #4055 (KochetovNicolai)Debian package: correct /etc/clickhouse-server/preprocessed link according to config. #4205 (proller)Fixed fuzz test under undefined behavior sanitizer: added parameter type check for quantile*Weighted family of functions. #4145 (alexey-milovidov)Make START REPLICATED SENDS command start replicated sends. #4229 (nvartolomei)Fixed Not found column for duplicate columns in JOIN ON section. #4279 (Artem Zuikov)Now /etc/ssl is used as default directory with SSL certificates. #4167 (alexey-milovidov)Fixed crash on dictionary reload if dictionary not available. #4188 (proller)Fixed bug with incorrect Date and DateTime comparison. #4237 (valexey)Fixed incorrect result when Date and DateTime arguments are used in branches of conditional operator (function if). Added generic case for function if. #4243 (alexey-milovidov) "},{"title":"ClickHouse Release 19.1.6, 2019-01-24​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#clickhouse-release-19-1-6-2019-01-24","content":"New Features​ Custom per column compression codecs for tables. #3899 #4111 (alesapin, Winter Zhang, Anatoly)Added compression codec Delta. #4052 (alesapin)Allow to ALTER compression codecs. #4054 (alesapin)Added functions left, right, trim, ltrim, rtrim, timestampadd, timestampsub for SQL standard compatibility. #3826 (Ivan Blinkov)Support for write in HDFS tables and hdfs table function. #4084 (alesapin)Added functions to search for multiple constant strings from big haystack: multiPosition, multiSearch ,firstMatch also with -UTF8, -CaseInsensitive, and -CaseInsensitiveUTF8 variants. #4053 (Danila Kutenin)Pruning of unused shards if SELECT query filters by sharding key (setting optimize_skip_unused_shards). #3851 (Gleb Kanterov, Ivan)Allow Kafka engine to ignore some number of parsing errors per block. #4094 (Ivan)Added support for CatBoost multiclass models evaluation. Function modelEvaluate returns tuple with per-class raw predictions for multiclass models. libcatboostmodel.so should be built with #607. #3959 (KochetovNicolai)Added functions filesystemAvailable, filesystemFree, filesystemCapacity. #4097 (Boris Granveaud)Added hashing functions xxHash64 and xxHash32. #3905 (filimonov)Added gccMurmurHash hashing function (GCC flavoured Murmur hash) which uses the same hash seed as gcc #4000 (sundyli)Added hashing functions javaHash, hiveHash. #3811 (shangshujie365)Added table function remoteSecure. Function works as remote, but uses secure connection. #4088 (proller) Experimental Features​ Added multiple JOINs emulation (allow_experimental_multiple_joins_emulation setting). #3946 (Artem Zuikov) Bug Fixes​ Make compiled_expression_cache_size setting limited by default to lower memory consumption. #4041 (alesapin)Fix a bug that led to hangups in threads that perform ALTERs of Replicated tables and in the thread that updates configuration from ZooKeeper. #2947 #3891 #3934 (Alex Zatelepin)Fixed a race condition when executing a distributed ALTER task. The race condition led to more than one replica trying to execute the task and all replicas except one failing with a ZooKeeper error. #3904 (Alex Zatelepin)Fix a bug when from_zk config elements weren’t refreshed after a request to ZooKeeper timed out. #2947 #3947 (Alex Zatelepin)Fix bug with wrong prefix for IPv4 subnet masks. #3945 (alesapin)Fixed crash (std::terminate) in rare cases when a new thread cannot be created due to exhausted resources. #3956 (alexey-milovidov)Fix bug when in remote table function execution when wrong restrictions were used for in getStructureOfRemoteTable. #4009 (alesapin)Fix a leak of netlink sockets. They were placed in a pool where they were never deleted and new sockets were created at the start of a new thread when all current sockets were in use. #4017 (Alex Zatelepin)Fix bug with closing /proc/self/fd directory earlier than all fds were read from /proc after forking odbc-bridge subprocess. #4120 (alesapin)Fixed String to UInt monotonic conversion in case of usage String in primary key. #3870 (Winter Zhang)Fixed error in calculation of integer conversion function monotonicity. #3921 (alexey-milovidov)Fixed segfault in arrayEnumerateUniq, arrayEnumerateDense functions in case of some invalid arguments. #3909 (alexey-milovidov)Fix UB in StorageMerge. #3910 (Amos Bird)Fixed segfault in functions addDays, subtractDays. #3913 (alexey-milovidov)Fixed error: functions round, floor, trunc, ceil may return bogus result when executed on integer argument and large negative scale. #3914 (alexey-milovidov)Fixed a bug induced by ‘kill query sync’ which leads to a core dump. #3916 (muVulDeePecker)Fix bug with long delay after empty replication queue. #3928 #3932 (alesapin)Fixed excessive memory usage in case of inserting into table with LowCardinality primary key. #3955 (KochetovNicolai)Fixed LowCardinality serialization for Native format in case of empty arrays. #3907 #4011 (KochetovNicolai)Fixed incorrect result while using distinct by single LowCardinality numeric column. #3895 #4012 (KochetovNicolai)Fixed specialized aggregation with LowCardinality key (in case when compile setting is enabled). #3886 (KochetovNicolai)Fix user and password forwarding for replicated tables queries. #3957 (alesapin) (小路)Fixed very rare race condition that can happen when listing tables in Dictionary database while reloading dictionaries. #3970 (alexey-milovidov)Fixed incorrect result when HAVING was used with ROLLUP or CUBE. #3756 #3837 (Sam Chou)Fixed column aliases for query with JOIN ON syntax and distributed tables. #3980 (Winter Zhang)Fixed error in internal implementation of quantileTDigest (found by Artem Vakhrushev). This error never happens in ClickHouse and was relevant only for those who use ClickHouse codebase as a library directly. #3935 (alexey-milovidov) Improvements​ Support for IF NOT EXISTS in ALTER TABLE ADD COLUMN statements along with IF EXISTS in DROP/MODIFY/CLEAR/COMMENT COLUMN. #3900 (Boris Granveaud)Function parseDateTimeBestEffort: support for formats DD.MM.YYYY, DD.MM.YY, DD-MM-YYYY, DD-Mon-YYYY, DD/Month/YYYY and similar. #3922 (alexey-milovidov)CapnProtoInputStream now support jagged structures. #4063 (Odin Hultgren Van Der Horst)Usability improvement: added a check that server process is started from the data directory’s owner. Do not allow to start server from root if the data belongs to non-root user. #3785 (sergey-v-galtsev)Better logic of checking required columns during analysis of queries with JOINs. #3930 (Artem Zuikov)Decreased the number of connections in case of large number of Distributed tables in a single server. #3726 (Winter Zhang)Supported totals row for WITH TOTALS query for ODBC driver. #3836 (Maksim Koritckiy)Allowed to use Enums as integers inside if function. #3875 (Ivan)Added low_cardinality_allow_in_native_format setting. If disabled, do not use LowCadrinality type in Native format. #3879 (KochetovNicolai)Removed some redundant objects from compiled expressions cache to lower memory usage. #4042 (alesapin)Add check that SET send_logs_level = 'value' query accept appropriate value. #3873 (Sabyanin Maxim)Fixed data type check in type conversion functions. #3896 (Winter Zhang) Performance Improvements​ Add a MergeTree setting use_minimalistic_part_header_in_zookeeper. If enabled, Replicated tables will store compact part metadata in a single part znode. This can dramatically reduce ZooKeeper snapshot size (especially if the tables have a lot of columns). Note that after enabling this setting you will not be able to downgrade to a version that does not support it. #3960 (Alex Zatelepin)Add an DFA-based implementation for functions sequenceMatch and sequenceCount in case pattern does not contain time. #4004 (Léo Ercolanelli)Performance improvement for integer numbers serialization. #3968 (Amos Bird)Zero left padding PODArray so that -1 element is always valid and zeroed. It’s used for branchless calculation of offsets. #3920 (Amos Bird)Reverted jemalloc version which lead to performance degradation. #4018 (alexey-milovidov) Backward Incompatible Changes​ Removed undocumented feature ALTER MODIFY PRIMARY KEY because it was superseded by the ALTER MODIFY ORDER BY command. #3887 (Alex Zatelepin)Removed function shardByHash. #3833 (alexey-milovidov)Forbid using scalar subqueries with result of type AggregateFunction. #3865 (Ivan) Build/Testing/Packaging Improvements​ Added support for PowerPC (ppc64le) build. #4132 (Danila Kutenin)Stateful functional tests are run on public available dataset. #3969 (alexey-milovidov)Fixed error when the server cannot start with the bash: /usr/bin/clickhouse-extract-from-config: Operation not permitted message within Docker or systemd-nspawn. #4136 (alexey-milovidov)Updated rdkafka library to v1.0.0-RC5. Used cppkafka instead of raw C interface. #4025 (Ivan)Updated mariadb-client library. Fixed one of issues found by UBSan. #3924 (alexey-milovidov)Some fixes for UBSan builds. #3926 #3021 #3948 (alexey-milovidov)Added per-commit runs of tests with UBSan build.Added per-commit runs of PVS-Studio static analyzer.Fixed bugs found by PVS-Studio. #4013 (alexey-milovidov)Fixed glibc compatibility issues. #4100 (alexey-milovidov)Move Docker images to 18.10 and add compatibility file for glibc &gt;= 2.28 #3965 (alesapin)Add env variable if user do not want to chown directories in server Docker image. #3967 (alesapin)Enabled most of the warnings from -Weverything in clang. Enabled -Wpedantic. #3986 (alexey-milovidov)Added a few more warnings that are available only in clang 8. #3993 (alexey-milovidov)Link to libLLVM rather than to individual LLVM libs when using shared linking. #3989 (Orivej Desh)Added sanitizer variables for test images. #4072 (alesapin)clickhouse-server debian package will recommend libcap2-bin package to use setcap tool for setting capabilities. This is optional. #4093 (alexey-milovidov)Improved compilation time, fixed includes. #3898 (proller)Added performance tests for hash functions. #3918 (filimonov)Fixed cyclic library dependences. #3958 (proller)Improved compilation with low available memory. #4030 (proller)Added test script to reproduce performance degradation in jemalloc. #4036 (alexey-milovidov)Fixed misspells in comments and string literals under dbms. #4122 (maiha)Fixed typos in comments. #4089 (Evgenii Pravda) "},{"title":"Changelog for 2018​","type":1,"pageTitle":"2019","url":"docs/en/whats-new/changelog/2019#changelog-for-2018","content":""},{"title":"Understanding ClickHouse Data Skipping Indexes","type":0,"sectionRef":"#","url":"docs/guides/improving-query-performance/skipping-indexes","content":"","keywords":""},{"title":"Introduction to Skipping Indexes​","type":1,"pageTitle":"Understanding ClickHouse Data Skipping Indexes","url":"docs/guides/improving-query-performance/skipping-indexes#introduction-to-skipping-indexes","content":"Many factors affect ClickHouse query performance. The critical element in most scenarios is whether ClickHouse can use the primary key when evaluating the query WHERE clause condition. Accordingly, selecting a primary key that applies to the most common query patterns is essential for effective table design. Neverthelss, no matter how carefully tuned the primary key, there will inevitably be query use cases that can not efficiently use it. Users commonly rely on ClickHouse for time series type data, but they often wish to analyze that same data according to other business dimensions, such as customer id, website URL, or product number. In that case, query performance can be considerably worse because a full scan of each column value may be required to apply the WHERE clause condition. While ClickHouse is still relatively fast in those circumstances, evaluating millions or billions of individual values will cause &quot;non-indexed&quot; queries to execute much more slowly than those based on the primary key. In a traditional relational database, one approach to this problem is to attach one or more &quot;secondary&quot; indexes to a table. This is a b-tree structure that permits the database to find all matching rows on disk in O(log(n)) time instead of O(n) time (a table scan), where n is the number of rows. However, this type of secondary index will not work for ClickHouse (or other column-oriented databases) because there are no individual rows on the disk to add to the index. Instead, ClickHouse provides a different type of index, which in specific circumstances can significantly improve query speed. These structures are labeled &quot;Skip&quot; indexes because they enable ClickHouse to skip reading significant chunks of data that are guaranteed to have no matching values. "},{"title":"Basic Operation​","type":1,"pageTitle":"Understanding ClickHouse Data Skipping Indexes","url":"docs/guides/improving-query-performance/skipping-indexes#basic-operation","content":"Users can only employ Data Skipping Indexes on the MergeTree family of tables. Each data skipping has four primary arguments: Index name. The index name is used to create the index file in each partition. Also, it is required as a parameter when dropping or materializing the index.Index expression. The index expression is used to calculate the set of values stored in the index. It can be a combination of columns, simple operators, and/or a subset of functions determined by the index type.TYPE. The type of index controls the calculation that determines if it is possible to skip reading and evaluating each index block.GRANULARITY. Each indexed block consists of GRANULARITY granules. For example, if the granularity of the primary table index is 8192 rows, and the index granularity is 4, each indexed &quot;block&quot; will be 32768 rows. When a user creates a data skipping index, there will be two additional files in each data part directory for the table. skpidx{index_name}.idx, which contains the ordered expression values)skpidx{index_name}.mrk2, which contains the corresponding offsets into the associated data column files. If some portion of the WHERE clause filtering condition matches the skip index expression when executing a query and reading the relevant column files, ClickHouse will use the index file data to determine whether each relevant block of data must be processed or can be bypassed (assuming that the block has not already been excluded by applying the primary key). To use a very simplified example, consider the following table loaded with predictable data. CREATE TABLE skip_table ( my_key UInt64, my_value UInt64 ) ENGINE MergeTree primary key my_key SETTINGS index_granularity=8192; INSERT INTO skip_table SELECT number, intDiv(number,4096) FROM numbers(100000000);  When executing a simple query that does not use the primary key, all 100 million entries in the my_valuecolumn are scanned: SELECT * FROM skip_table WHERE my_value IN (125, 700) ┌─my_key─┬─my_value─┐ │ 512000 │ 125 │ │ 512001 │ 125 │ │ ... | ... | └────────┴──────────┘ 8192 rows in set. Elapsed: 0.079 sec. Processed 100.00 million rows, 800.10 MB (1.26 billion rows/s., 10.10 GB/s.  Now add a very basic skip index: ALTER TABLE skip_table ADD INDEX vix my_value TYPE set(100) GRANULARITY 2;  Normally skip indexes are only applied on newly inserted data, so just adding the index won't affect the above query. To index already existing data, use this statement: ALTER TABLE skip_table MATERIALIZE INDEX vix;  Rerun the query with the newly created index: SELECT * FROM skip_table WHERE my_value IN (125, 700) ┌─my_key─┬─my_value─┐ │ 512000 │ 125 │ │ 512001 │ 125 │ │ ... | ... | └────────┴──────────┘ 8192 rows in set. Elapsed: 0.051 sec. Processed 32.77 thousand rows, 360.45 KB (643.75 thousand rows/s., 7.08 MB/s.)  Instead of processing 100 million rows of 800 megabytes, ClickHouse has only read and analyzed 32768 rows of 360 kilobytes -- four granules of 8192 rows each. In a more visual form, this is how the 4096 rows with a my_value of 125 were read and selected, and how the following rows were skipped without reading from disk:  Users can access detailed information about skip index usage by enabling the trace when executing queries. From clickhouse-client, set the send_logs_level: SET send_logs_level='trace';  This will provide useful debugging information when trying to tune query SQL and table indexes. From the above above example, the debug log shows that the skip index dropped all but two granules: &lt;Debug&gt; default.skip_table (933d4b2c-8cea-4bf9-8c93-c56e900eefd1) (SelectExecutor): Index `vix` has dropped 6102/6104 granules.  "},{"title":"Skip Index Types​","type":1,"pageTitle":"Understanding ClickHouse Data Skipping Indexes","url":"docs/guides/improving-query-performance/skipping-indexes#skip-index-types","content":"minmax​ This lightweight index type requires no parameters. It stores the minimum and maximum values of the index expression for each block (if the expression is a tuple, it separately stores the values for each member of the element of the tuple). This type is ideal for columns that tend to be loosely sorted by value. This index type is usually the least expensive to apply during query processing. This type of index only works correctly with a scalar or tuple expression -- the index will never be applied to expressions that return an array or map data type. set​ This lightweight index type accepts a single parameter of the max_size of the value set per block (0 permits an unlimited number of discrete values). This set contains all values in the block (or is empty if the number of values exceeds the max_size). This index type works well with columns with low cardinality within each set of granules (essentially, &quot;clumped together&quot;) but higher cardinality overall. The cost, performance, and effectiveness of this index is dependent on the cardinality within blocks. If each block contains a large number of unique values, either evaluating the query condition against a large index set will be very expensive, or the index will not be applied because the index is empty due to exceeding max_size. Bloom Filter Types​ A Bloom filter is a data structure that allows space-efficient testing of set membership at the cost of a slight chance of false positives. A false positive is not a significant concern in the case of skip indexes because the only disadvantage is reading a few unnecessary blocks. However, the potential for false positives does mean that the indexed expression should be expected to be true, otherwise valid data may be skipped. Because Bloom filters can more efficiently handle testing for a large number of discrete values, they can be appropriate for conditional expressions that produce more values to test. In particular, a Bloom filter index can be applied to arrays, where every value of the array is tested, and to maps, by converting either the keys or values to an array using the mapKeys or mapValues function. There are three Data Skipping Index types based on Bloom filters: The basic bloom_filter which takes a single optional parameter of the allowed &quot;false positive&quot; rate between 0 and 1 (if unspecified, .025 is used). The specialized tokenbf_v1. It takes three parameters, all related to tuning the bloom filter used: (1) the size of the filter in bytes (larger filters have fewer false positives, at some cost in storage), (2) number of hash functions applied (again, more hash filters reduce false positives), and (3) the seed for the bloom filter hash functions. See the calculator here for more detail on how these parameters affect bloom filter functionality. This index works only with String, FixedString, and Map datatypes. The input expression is split into character sequences separated by non-alphanumeric characters. For example, a column value of This is a candidate for a &quot;full text&quot; search will contain the tokens This is a candidate for full text search. It is intended for use in LIKE, EQUALS, IN, hasToken() and similar searches for words and other values within longer strings. For example, one possible use might be searching for a small number of class names or line numbers in a column of free form application log lines. The specialized ngrambf_v1. This index functions the same as the token index. It takes one additional parameter before the Bloom filter settings, the size of the ngrams to index. An ngram is a character string of length n of any characters, so the string A short string with an ngram size of 4 would be indexed as A sh`` sho, shor, hort, ort s, or st, r str, stri, trin, ring. This index can also be useful for text searches, particularly languages without word breaks, such as Chinese. "},{"title":"Skip Index Functions​","type":1,"pageTitle":"Understanding ClickHouse Data Skipping Indexes","url":"docs/guides/improving-query-performance/skipping-indexes#skip-index-functions","content":"The core purpose of data-skipping indexes is to limit the amount of data analyzed by popular queries. Given the analytic nature of ClickHouse data, the pattern of those queries in most cases includes functional expressions. Accordingly, skip indexes must interact correctly with common functions to be efficient. This can happen either when: • data is inserted and the index is defined as a functional expression (with the result of the expression stored in the index files), or • the query is processed and the expression is applied to the stored index values to determine whether to exclude the block. Each type of skip index works on a subset of available ClickHouse functions appropriate to the index implementation listedhere. In general, set indexes and Bloom filter based indexes (another type of set index) are both unordered and therefore do not work with ranges. In contrast, minmax indexes work particularly well with ranges since determining whether ranges intersect is very fast. The efficacy of partial match functions LIKE, startsWith, endsWith, and hasToken depend on the index type used, the index expression, and the particular shape of the data. "},{"title":"Skip Index Settings​","type":1,"pageTitle":"Understanding ClickHouse Data Skipping Indexes","url":"docs/guides/improving-query-performance/skipping-indexes#skip-index-settings","content":"There are two available settings that apply to skip indexes. use_skip_indexes (0 or 1, default 1). Not all queries can efficiently use skip indexes. If a particular filtering condition is likely to include most granules, applying the data skipping index incurs an unnecessary, and sometimes significant, cost. Set the value to 0 for queries that are unlikely to benefit from any skip indexes.force_data_skipping_indexes (comma separated list of index names). This setting can be used to prevent some kinds of inefficient queries. In circumstances where querying a table is too expensive unless a skip index is used, using this setting with one or more index names will return an exception for any query that does not use the listed index. This would prevent poorly written queries from consuming server resources. "},{"title":"Skip Best Practices​","type":1,"pageTitle":"Understanding ClickHouse Data Skipping Indexes","url":"docs/guides/improving-query-performance/skipping-indexes#skip-best-practices","content":"Skip indexes are not intuitive, especially for users accustomed to secondary row-based indexes from the RDMS realm or inverted indexes from document stores. To get any benefit, applying a ClickHouse data skipping index must avoid enough granule reads to offset the cost of calculating the index. Critically, if a value occurs even once in an indexed block, it means the entire block must be read into memory and evaluated, and the index cost has been needlessly incurred. Consider the following data distribution:  Assume the primary/order by key is timestamp, and there is an index on visitor_id. Consider the following query: SELECT timestamp, url FROM table WHERE visitor_id = 1001 A traditional secondary index would be very advantageous with this kind of data distribution. Instead of reading all 32678 rows to find the 5 rows with the requested visitor_id, the secondary index would include just five row locations, and only those five rows would be read from disk. The exact opposite is true for a ClickHouse data skipping index. All 32678 values in the visitor_id column will be tested regardless of the type of skip index. Accordingly, the natural impulse to try to speed up ClickHouse queries by simply adding an index to key columns is often incorrect. This advanced functionality should only be used after investigating other alternatives, such as modifying the primary key (see How to Pick a Primary Key), using projections, or using materialized views. Even when a data skipping index is appropriate, careful tuning both the index and the table will often be necessary. In most cases a useful skip index requires a strong correlation between the primary key and the targeted, non-primary column/expression. If there is no correlation (as in the above diagram), the chances of the filtering condition being met by at least one of the rows in the block of several thousand values is high and few blocks will be skipped. In constrast, if a range of values for the primary key (like time of day) is strongly associated with the values in the potential index column (such as television viewer ages), then a minmax type of index is likely to be beneficial. Note that it may be possible to increase this correlation when inserting data, either by including additional columns in the sorting/ORDER BY key, or batching inserts in a way that values associated with the primary key are grouped on insert. For example, all of the events for a particular site_id could be grouped and inserted together by the ingest process, even if the primary key is a timestamp containing events from a large number of sites. This will result in many granules that contains only a few site ids, so many blocks could be skipped when searching by a specific site_id value. Another good candidate for a skip index is for high cardinality expressions where any one value is relatively sparse in the data. One example might be an observability platform that tracks error codes in API requests. Certain error codes, while rare in the data, might be particularly important for searches. A set skip index on the error_code column would allow bypassing the vast majority of blocks that don't contain errors and therefore significantly improve error focused queries. Finally, the key best practice is to test, test, test. Again, unlike b-tree secondary indexes or inverted indexes for searching documents, data skipping index behavior is not easily predictable. Adding them to a table incurs a meangingful cost both on data ingest and on queries that for any number of reasons don't benefit from the index. They should always be tested on real world type of data, and testing should include variations of the type, granularity size and other parameters. Testing will often reveal patterns and pitfalls that aren't obvious from thought experiments alone. "},{"title":"Full-text search with ClickHouse and Quickwit","type":0,"sectionRef":"#","url":"docs/guides/developer/full-text-search","content":"","keywords":""},{"title":"Installing ClickHouse​","type":1,"pageTitle":"Full-text search with ClickHouse and Quickwit","url":"docs/guides/developer/full-text-search#installing-clickhouse","content":"The first step is to install Quickwit and ClickHouse if you don’t have them installed already, follow the instruction below to install ClickHouse: sudo apt-get install apt-transport-https ca-certificates dirmngr sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv E0C56BD4 echo &quot;deb https://repo.clickhouse.com/deb/stable/ main/&quot; | sudo tee \\ /etc/apt/sources.list.d/clickhouse.list sudo apt-get update sudo apt-get install -y clickhouse-server clickhouse-client sudo service clickhouse-server start  "},{"title":"Installing Quickwit​","type":1,"pageTitle":"Full-text search with ClickHouse and Quickwit","url":"docs/guides/developer/full-text-search#installing-quickwit","content":"Quickwit is relying on two external libraries to work correctly. You will need to instal them before installing Quickwit # Quickwit depends on the following external libraries to work correctly sudo apt-get -y update sudo apt-get -y install libpq-dev libssl-dev  Once these two libraries are installed you can go ahaead and installed Quickwit: curl -L https://install.quickwit.io | sh # Quickwit detects the config from CLI args or the QW_CONFIG env variable. # Let's set QW_CONFIG to the default config. cd quickwit-v*/ export QW_CONFIG=./config/quickwit.yaml  You can test that Quickwit has been properly installed by running the following command: ./quickwit --version  Now that both ClickHouse and Quickwit are installed and run all we have to do is add some data to both of them. "},{"title":"Indexing Data in QuickWit​","type":1,"pageTitle":"Full-text search with ClickHouse and Quickwit","url":"docs/guides/developer/full-text-search#indexing-data-in-quickwit","content":"The first thing we need to do is provide a data schema for the data we are going to use. We are going to use a subset of the data provided by GitHub. You can find the original data here, the dataset we are going to use has slightly been modified in order to be more practical to use. curl -o gh-archive-index-config.yaml https://datasets-documentation.s3.eu-west-3.amazonaws.com/full-text-search/gh-archive-index-config.yaml ./quickwit index create --index-config gh-archive-index-config.yaml  Now that the data schema is defined, let’s download and index some data in Quickwit: wget https://datasets-documentation.s3.eu-west-3.amazonaws.com/full-text-search/gh-archive-2021-12-text-only.json.gz gunzip -c gh-archive-2021-12-text-only.json.gz | ./quickwit index ingest --index gh-archive  You can search through your data within Quickwit: ./quickwit index search --index gh-archive --query &quot;clickhouse&quot;  But we want to use it in conjunction with ClickHouse, so in order to do so, we will need to create a searcher. ./quickwit service run searcher  This command will start an HTTP server with a REST API. We are now ready to fetch some ids with the search stream endpoint. Let's start by streaming them on a simple query and with a CSV output format. curl &quot;http://0.0.0.0:7280/api/v1/gh-archive/search/stream?query=clickhouse&amp;outputFormat=csv&amp;fastField=id&quot;  In the remaining of this guide we will be using the ClickHouse binary output format to speed up queries using ClickHouse. "},{"title":"Adding Data to ClickHouse​","type":1,"pageTitle":"Full-text search with ClickHouse and Quickwit","url":"docs/guides/developer/full-text-search#adding-data-to-clickhouse","content":"First thing first, we need to connect to the ClickHouse database. Let’s use theclickhouse-client to do it. clickhouse-client –password &lt;PASSWORD&gt;  The first thing we need to do is to create a database: CREATE DATABASE &quot;github&quot;; USE github;  Now we need to create the table that’s going to store our data: CREATE TABLE github.github_events ( `id` UInt64, `event_type` Enum('CommitCommentEvent' = 1, 'CreateEvent' = 2, 'DeleteEvent' = 3, 'ForkEvent' = 4, 'GollumEvent' = 5, 'IssueCommentEvent' = 6, 'IssuesEvent' = 7, 'MemberEvent' = 8, 'PublicEvent' = 9, 'PullRequestEvent' = 10, 'PullRequestReviewCommentEvent' = 11, 'PushEvent' = 12, 'ReleaseEvent' = 13, 'SponsorshipEvent' = 14, 'WatchEvent' = 15, 'GistEvent' = 16, 'FollowEvent' = 17, 'DownloadEvent' = 18, 'PullRequestReviewEvent' = 19, 'ForkApplyEvent' = 20, 'Event' = 21, 'TeamAddEvent' = 22), `actor_login` LowCardinality(String), `repo_name` LowCardinality(String), `created_at` Int64, `action` Enum('none' = 0, 'created' = 1, 'added' = 2, 'edited' = 3, 'deleted' = 4, 'opened' = 5, 'closed' = 6, 'reopened' = 7, 'assigned' = 8, 'unassigned' = 9, 'labeled' = 10, 'unlabeled' = 11, 'review_requested' = 12, 'review_request_removed' = 13, 'synchronize' = 14, 'started' = 15, 'published' = 16, 'update' = 17, 'create' = 18, 'fork' = 19, 'merged' = 20), `comment_id` UInt64, `body` String, `ref` LowCardinality(String), `number` UInt32, `title` String, `labels` Array(LowCardinality(String)), `additions` UInt32, `deletions` UInt32, `commit_id` String ) ENGINE = MergeTree ORDER BY (event_type, repo_name, created_at)  We are going to add some data to ClickHouse. It’s the same dataset as the one we have indexed in Quickwit but this time it does not include text fields since they are indexed already. wget https://datasets-documentation.s3.eu-west-3.amazonaws.com/full-text-search/gh-archive-2021-12.json.gz gunzip -c gh-archive-2021-12.json.gz | clickhouse-client --query=&quot;INSERT INTO github.github_events FORMAT JSONEachRow&quot; --password &lt;PASSWORD&gt;  Now that the data is in ClickHouse we can query them using the clikchouse-client. SELECT repo_name, count() AS stars FROM github.github_events WHERE event_type = 'WatchEvent' GROUP BY repo_name ORDER BY stars DESC LIMIT 5  "},{"title":"Full-text search within ClickHouse​","type":1,"pageTitle":"Full-text search with ClickHouse and Quickwit","url":"docs/guides/developer/full-text-search#full-text-search-within-clickhouse","content":"So we have data both in ClickHouse and in QuickWit all there is to do now is to query them! The url function allows you to fetch ids using the Quickwit search stream: SELECT count(*) FROM url('http://127.0.0.1:7280/api/v1/gh-archive/search/stream?query=clickhouse&amp;fastField=id&amp;outputFormat=clickHouseRowBinary', RowBinary, 'id UInt64')  In this query above we are counting the number of ID returned by the ClickHouse query executed in QuickWit. As you can see below, it’s returning the following: ┌─count()─┐ │ 2012 │ └─────────┘ 1 rows in set. Elapsed: 0.010 sec. Processed 2.01 thousand rows, 16.10 KB (210.35 thousand rows/s., 1.68 MB/s.)  We can search multiple tokens by separating them with a + : SELECT count(*) FROM url('http://127.0.0.1:7280/api/v1/gh-archive/search/stream?query=clickhouse+cloki&amp;fastField=id&amp;outputFormat=clickHouseRowBinary', RowBinary, 'id UInt64')  In the query above we are searching for documents containing the wordsClickHouse AND cloki. Now we can tweak the query around to search forClickHouse OR cloki: SELECT count(*) FROM url('http://127.0.0.1:7280/api/v1/gh-archive/search/stream?query=clickhouse+OR+cloki&amp;fastField=id&amp;outputFormat=clickHouseRowBinary', RowBinary, 'id UInt64')  So the full text search is working, now let’s combine it with some GROUP BYthat would be done on the ClickHouse side. Here we want to know how many rows match the words: ClickHouse, cloki or quickwit and in which GitHub repository there located in. SELECT count(*) AS count, repo_name AS repo FROM github.github_events WHERE id IN ( SELECT id FROM url('http://127.0.0.1:7280/api/v1/gh-archive/search/stream?query=cloki+OR+clickhouse+OR+quickwit&amp;fastField=id&amp;outputFormat=clickHouseRowBinary', RowBinary, 'id UInt64') ) GROUP BY repo ORDER BY count DESC  And as you can see below, it is fast: ┌─count─┬─repo──────────────────────────────────────────────┐ │ 874 │ ClickHouse/ClickHouse │ │ 112 │ traceon/ClickHouse │ │ 112 │ quickwit-inc/quickwit │ │ 110 │ PostHog/posthog │ │ 73 │ PostHog/charts-clickhouse │ │ 64 │ datafuselabs/databend │ │ 54 │ airbytehq/airbyte │ │ 53 │ ClickHouse/clickhouse-jdbc │ │ 37 │ getsentry/snuba │ │ 37 │ PostHog/posthog.com │ … … │ 1 │ antrea-io/antrea │ │ 1 │ python/typeshed │ │ 1 │ Sunt-ing/database-system-readings │ │ 1 │ duckdb/duckdb │ │ 1 │ open-botech/ClickHouse │ └───────┴───────────────────────────────────────────────────┘ 195 rows in set. Elapsed: 0.518 sec. Processed 45.43 million rows, 396.87 MB (87.77 million rows/s., 766.79 MB/s.)  The query is really fast, returning the result in 0.5 second. "},{"title":"Conclusion​","type":1,"pageTitle":"Full-text search with ClickHouse and Quickwit","url":"docs/guides/developer/full-text-search#conclusion","content":"Using Quickwit within ClickHouse gives a lot of flexibility in how you work with your data, especially when your data contains textual information and you need to be able to search through them very quickly. You can find more information on how to use Quickwit directly on their documentation. "},{"title":"Configuring ClickHouse to use LDAP for authentication and role mapping","type":0,"sectionRef":"#","url":"docs/guides/sre/configuring-ldap","content":"","keywords":""},{"title":"1. Configure LDAP connection settings in ClickHouse​","type":1,"pageTitle":"Configuring ClickHouse to use LDAP for authentication and role mapping","url":"docs/guides/sre/configuring-ldap#1-configure-ldap-connection-settings-in-clickhouse","content":"Test your connection to this public LDAP server: $ ldapsearch -x -b dc=example,dc=com -H ldap://ldap.forumsys.com The reply will be something like this: # extended LDIF # # LDAPv3 # base &lt;dc=example,dc=com&gt; with scope subtree # filter: (objectclass=*) # requesting: ALL # # example.com dn: dc=example,dc=com objectClass: top objectClass: dcObject objectClass: organization o: example.com dc: example ... Edit the config.xml file and add the following to configure LDAP: &lt;ldap_servers&gt; &lt;test_ldap_server&gt; &lt;host&gt;ldap.forumsys.com&lt;/host&gt; &lt;port&gt;389&lt;/port&gt; &lt;bind_dn&gt;uid={user_name},dc=example,dc=com&lt;/bind_dn&gt; &lt;enable_tls&gt;no&lt;/enable_tls&gt; &lt;tls_require_cert&gt;never&lt;/tls_require_cert&gt; &lt;/test_ldap_server&gt; &lt;/ldap_servers&gt; :::note The &lt;test_ldap_server&gt; tags is an arbitrary label to identify a particular LDAP server. ::: These are the basic settings used above: Parameter\tDescription\tExamplehost\thostname or IP of LDAP server\tldap.forumsys.com port\tdirectory port for LDAP server\t389 bind_dn\ttemplate path to users\tuid={user_name},dc=example,dc=com enable_tls\twhether to use secure ldap\tno tls_require_cert\twhether to require certificate for connection\tnever :::note In this example, since the public server uses 389 and does not use a secure port, we disable TLS for demonstration purposes. ::: note View the LDAP doc page for more details on the LDAP settings. Add the &lt;ldap&gt; section to &lt;user_directories&gt; section to configure the user role mapping. This section defines when a user is authenticated and what role the user will receive. In this basic example, any user authenticating to LDAP will receive the scientists_role which will be defined at a later step in ClickHouse. The section should look similar to this: &lt;user_directories&gt; &lt;users_xml&gt; &lt;path&gt;users.xml&lt;/path&gt; &lt;/users_xml&gt; &lt;local_directory&gt; &lt;path&gt;/var/lib/clickhouse/access/&lt;/path&gt; &lt;/local_directory&gt; &lt;ldap&gt; &lt;server&gt;test_ldap_server&lt;/server&gt; &lt;roles&gt; &lt;scientists_role /&gt; &lt;/roles&gt; &lt;role_mapping&gt; &lt;base_dn&gt;dc=example,dc=com&lt;/base_dn&gt; &lt;search_filter&gt;(&amp;amp;(objectClass=groupOfUniqueNames)(uniqueMember={bind_dn}))&lt;/search_filter&gt; &lt;attribute&gt;cn&lt;/attribute&gt; &lt;/role_mapping&gt; &lt;/ldap&gt; &lt;/user_directories&gt; These are the basic settings used above: Parameter\tDescription\tExampleserver\tlabel defined in the prior ldap_servers section\ttest_ldap_server roles\tname of the roles defined in ClickHouse the users will be mapped to\tscientists_role base_dn\tbase path to start search for groups with user\tdc=example,dc=com search_filter\tldap search filter to identify groups to select for mapping users\t(&amp;(objectClass=groupOfUniqueNames)(uniqueMember={bind_dn})) attribute\twhich attribute name should value be returned from\tcn Restart your ClickHouse server to apply the settings. "},{"title":"2. Configure ClickHouse database roles and permissions​","type":1,"pageTitle":"Configuring ClickHouse to use LDAP for authentication and role mapping","url":"docs/guides/sre/configuring-ldap#2-configure-clickhouse-database-roles-and-permissions","content":":::note The procedures in this section assumes that SQL Access Control and Account Management in ClickHouse has been enabled. To enable, view the [SQL Users and Roles guide](/docs/testing/docs/guides/sre/users-and-roles). :::  Create a role in clickhouse with the same name used in the role mapping section of the config.xml file CREATE ROLE scientists_role; Grant needed privileges to the role. The following statement grants admin privileges to any user able to authenticate through LDAP: GRANT ALL ON *.* TO scientists_role;  "},{"title":"3. Test the LDAP configuration​","type":1,"pageTitle":"Configuring ClickHouse to use LDAP for authentication and role mapping","url":"docs/guides/sre/configuring-ldap#3-test-the-ldap-configuration","content":"Login using the ClickHouse client $ clickhouse-client --user einstein --password password ClickHouse client version 22.2.2.1. Connecting to localhost:9000 as user einstein. Connected to ClickHouse server version 22.2.2 revision 54455. chnode1 :) note Use the ldapsearch command in step 1 to view all of the users available in the directory and for all of the users the password is password Test that the user was mapped correctly to the scientists_role role and has admin permissions SHOW DATABASES Query id: 93b785ff-1482-4eda-95b0-b2d68b2c5e0f ┌─name───────────────┐ │ INFORMATION_SCHEMA │ │ db1_mysql │ │ db2 │ │ db3 │ │ db4_mysql │ │ db5_merge │ │ default │ │ information_schema │ │ system │ └────────────────────┘ 9 rows in set. Elapsed: 0.004 sec.  "},{"title":"Summary​","type":1,"pageTitle":"Configuring ClickHouse to use LDAP for authentication and role mapping","url":"docs/guides/sre/configuring-ldap#summary","content":"This article demostrated the basics of configuring ClickHouse to authenticate to an LDAP server and also to map to a role. There are also options for configuring individual users in ClickHouse but having those users be authenticated by LDAP without configuring automated role mapping. The LDAP module can also be used to connect to Active Directory. "},{"title":"Configuring ClickHouse Keeper","type":0,"sectionRef":"#","url":"docs/guides/sre/clickhouse-keeper","content":"","keywords":""},{"title":"1. Configure Nodes with Keeper settings​","type":1,"pageTitle":"Configuring ClickHouse Keeper","url":"docs/guides/sre/clickhouse-keeper#1-configure-nodes-with-keeper-settings","content":"Install 3 ClickHouse instances on 3 hosts (chnode1, chnode2, chnode3). (View the Quick Start for details on installing ClickHouse.) On each node, add the following entry to allow external communication through the network interface. &lt;listen_host&gt;0.0.0.0&lt;/listen_host&gt; Add the following ClickHouse Keeper configuration to all three servers updating the &lt;server_id&gt; setting for each server; for chnode1 would be 1, chnode2 would be 2, etc. &lt;keeper_server&gt; &lt;tcp_port&gt;9181&lt;/tcp_port&gt; &lt;server_id&gt;1&lt;/server_id&gt; &lt;log_storage_path&gt;/var/lib/clickhouse/coordination/log&lt;/log_storage_path&gt; &lt;snapshot_storage_path&gt;/var/lib/clickhouse/coordination/snapshots&lt;/snapshot_storage_path&gt; &lt;coordination_settings&gt; &lt;operation_timeout_ms&gt;10000&lt;/operation_timeout_ms&gt; &lt;session_timeout_ms&gt;30000&lt;/session_timeout_ms&gt; &lt;raft_logs_level&gt;warning&lt;/raft_logs_level&gt; &lt;/coordination_settings&gt; &lt;raft_configuration&gt; &lt;server&gt; &lt;id&gt;1&lt;/id&gt; &lt;hostname&gt;chnode1.domain.com&lt;/hostname&gt; &lt;port&gt;9444&lt;/port&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;2&lt;/id&gt; &lt;hostname&gt;chnode2.domain.com&lt;/hostname&gt; &lt;port&gt;9444&lt;/port&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;3&lt;/id&gt; &lt;hostname&gt;chnode3.domain.com&lt;/hostname&gt; &lt;port&gt;9444&lt;/port&gt; &lt;/server&gt; &lt;/raft_configuration&gt; &lt;/keeper_server&gt; These are the basic settings used above: Parameter\tDescription\tExampletcp_port\tport to be used by clients of keeper\t9181 default equivalent of 2181 as in zookeeper server_id\tidentifier for each CLickHouse Keeper server used in raft configuration\t1 coordination_settings\tsection to parameters such as timeouts\ttimeouts: 10000, log level: trace server\tdefinition of server participating\tlist of each server definition raft_configuration\tsettings for each server in the keeper cluster\tserver and settings for each id\tnumeric id of the server for keeper services\t1 hostname\thostname, IP or FQDN of each server in the keeper cluster\tchnode1.domain.com port\tport to listen on for interserver keeper connections\t9444 note View the ClickHouse Keeper docs page for details on all the available parameters. Enable the Zookeeper component. It will use the ClickHouse Keeper engine: &lt;zookeeper&gt; &lt;node&gt; &lt;host&gt;chnode1.domain.com&lt;/host&gt; &lt;port&gt;9181&lt;/port&gt; &lt;/node&gt; &lt;node&gt; &lt;host&gt;chnode2.domain.com&lt;/host&gt; &lt;port&gt;9181&lt;/port&gt; &lt;/node&gt; &lt;node&gt; &lt;host&gt;chnode3.domain.com&lt;/host&gt; &lt;port&gt;9181&lt;/port&gt; &lt;/node&gt; &lt;/zookeeper&gt; These are the basic settings used above: Parameter\tDescription\tExamplenode\tlist of nodes for ClickHouse Keeper connections\tsettings entry for each server host\thostname, IP or FQDN of each ClickHouse keepr node\tchnode1.domain.com port\tClickHouse Keeper client port\t9181 Restart ClickHouse and verify that each Keeper instance is running. Execute the following command on each server. The ruok command returns imok if Keeper is running and healthy: # echo ruok | nc localhost 9181; echo imok The system database has a table named zookeeper that contains the details of your ClickHouse Keeper instances. Let's view the table: SELECT * FROM system.zookeeper WHERE path IN ('/', '/clickhouse') The table looks like: ┌─name───────┬─value─┬─czxid─┬─mzxid─┬───────────────ctime─┬───────────────mtime─┬─version─┬─cversion─┬─aversion─┬─ephemeralOwner─┬─dataLength─┬─numChildren─┬─pzxid─┬─path────────┐ │ clickhouse │ │ 124 │ 124 │ 2022-03-07 00:49:34 │ 2022-03-07 00:49:34 │ 0 │ 2 │ 0 │ 0 │ 0 │ 2 │ 5693 │ / │ │ task_queue │ │ 125 │ 125 │ 2022-03-07 00:49:34 │ 2022-03-07 00:49:34 │ 0 │ 1 │ 0 │ 0 │ 0 │ 1 │ 126 │ /clickhouse │ │ tables │ │ 5693 │ 5693 │ 2022-03-07 00:49:34 │ 2022-03-07 00:49:34 │ 0 │ 3 │ 0 │ 0 │ 0 │ 3 │ 6461 │ /clickhouse │ └────────────┴───────┴───────┴───────┴─────────────────────┴─────────────────────┴─────────┴──────────┴──────────┴────────────────┴────────────┴─────────────┴───────┴─────────────┘  "},{"title":"2. Configure a cluster in ClickHouse​","type":1,"pageTitle":"Configuring ClickHouse Keeper","url":"docs/guides/sre/clickhouse-keeper#2--configure-a-cluster-in-clickhouse","content":"Let's configure a simple cluster with 2 shards and only one replica on 2 of the nodes. The third node will be used to achieve a quorum for the requirement in ClickHouse Keeper. Update the configuration on chnode1 and chnode2. The following cluster defines 1 shard on each node for a total of 2 shards with no replication. In this example, some of the data will be on node and some will be on the other node: &lt;cluster_2S_1R&gt; &lt;shard&gt; &lt;replica&gt; &lt;host&gt;chnode1.domain.com&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;ClickHouse123!&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;shard&gt; &lt;replica&gt; &lt;host&gt;chnode2.domain.com&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;ClickHouse123!&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;/cluster_2S_1R&gt; Parameter\tDescription\tExampleshard\tlist of replicas on the cluster definition\tlist of replicas for each shard replica\tlist of settings for each replica\tsettings entries for each replica host\thostname, IP or FQDN of server that will host a replica shard\tchnode1.domain.com port\tport used to communicate using the native tcp protocol\t9000 user\tusername that will be used to authenticate to the cluster instances\tdefault password\tpassword for the user define to allow connections to cluster instances\tClickHouse123! Restart ClickHouse and verify the cluster was created: SHOW clusters; You should see your cluster: ┌─cluster───────┐ │ cluster_1S_2R │ └───────────────┘  "},{"title":"3. Create and test distributed table​","type":1,"pageTitle":"Configuring ClickHouse Keeper","url":"docs/guides/sre/clickhouse-keeper#3-create-and-test-distributed-table","content":"Create a new database on the new cluster using ClickHouse client on chnode1. The ON CLUSTER clause automatically creates the database on both nodes. CREATE DATABASE db1 ON CLUSTER 'cluster_2S_1R'; Create a new table on the db1 database. Once again, ON CLUSTER creates the table on both nodes. CREATE TABLE db1.table1 on cluster 'cluster_2S_1R' ( `id` UInt64, `column1` String ) ENGINE = MergeTree ORDER BY column1 On the chnode1 node, add a couple of rows: INSERT INTO db1.table1 (id, column1) VALUES (1, 'abc'), (2, 'def') Add a couple of rows on the chnode2 node: INSERT INTO db1.table1 (id, column1) VALUES (3, 'ghi'), (4, 'jkl') Notice that running a SELECT statement on each node only shows the data on that node. For example, on chnode1: SELECT * FROM db1.table1 Query id: 7ef1edbc-df25-462b-a9d4-3fe6f9cb0b6d ┌─id─┬─column1─┐ │ 1 │ abc │ │ 2 │ def │ └────┴─────────┘ 2 rows in set. Elapsed: 0.006 sec. On chnode2: SELECT * FROM db1.table1 Query id: c43763cc-c69c-4bcc-afbe-50e764adfcbf ┌─id─┬─column1─┐ │ 3 │ ghi │ │ 4 │ jkl │ └────┴─────────┘ You can create a Distributed table to represent the data on the two shards. Tables with the Distributed table engine do not store any data of their own, but allow distributed query processing on multiple servers. Reads hit all the shards, and writes can be distributed across the shards. Run the following query on chnode1: CREATE TABLE db1.dist_table ( id UInt64, column1 String ) ENGINE = Distributed(cluster_2S_1R,db1,table1) Notice querying dist_table returns all four rows of data from the two shards: SELECT * FROM db1.dist_table Query id: 495bffa0-f849-4a0c-aeea-d7115a54747a ┌─id─┬─column1─┐ │ 1 │ abc │ │ 2 │ def │ └────┴─────────┘ ┌─id─┬─column1─┐ │ 3 │ ghi │ │ 4 │ jkl │ └────┴─────────┘ 4 rows in set. Elapsed: 0.018 sec.  "},{"title":"Summary​","type":1,"pageTitle":"Configuring ClickHouse Keeper","url":"docs/guides/sre/clickhouse-keeper#summary","content":"This guide demostrated how to setup a cluster using ClickHouse Keeper. With ClickHouse Keeper, you can configure clusters and define distributed tables that can be replicated across shards. "},{"title":"2021","type":0,"sectionRef":"#","url":"docs/en/whats-new/changelog/2021","content":"","keywords":"clickhouse changelog"},{"title":"ClickHouse release v21.12, 2021-12-15​","type":1,"pageTitle":"2021","url":"docs/en/whats-new/changelog/2021#clickhouse-release-v2112-2021-12-15","content":"Backward Incompatible Change​ A fix for a feature that previously had unwanted behaviour. Do not allow direct select for Kafka/RabbitMQ/FileLog. Can be enabled by setting stream_like_engine_allow_direct_select. Direct select will be not allowed even if enabled by setting, in case there is an attached materialized view. For Kafka and RabbitMQ direct selectm if allowed, will not commit massages by default. To enable commits with direct select, user must use storage level setting kafka{rabbitmq}_commit_on_select=1 (default 0). #31053 (Kseniia Sumarokova).A slight change in behaviour of a new function. Return unquoted string in JSON_VALUE. Closes #27965. #31008 (Kseniia Sumarokova).Setting rename. Add custom null representation support for TSV/CSV input formats. Fix deserialing Nullable(String) in TSV/CSV/JSONCompactStringsEachRow/JSONStringsEachRow input formats. Rename output_format_csv_null_representation and output_format_tsv_null_representation to format_csv_null_representation and format_tsv_null_representation accordingly. #30497 (Kruglov Pavel).Further deprecation of already unused code. This is relevant only for users of ClickHouse versions older than 20.6. A &quot;leader election&quot; mechanism is removed from ReplicatedMergeTree, because multiple leaders are supported since 20.6. If you are upgrading from an older version and some replica with an old version is a leader, then server will fail to start after upgrade. Stop replicas with old version to make new version start. After that it will not be possible to downgrade to version older than 20.6. #32140 (tavplubix). New Feature​ Implemented more of the ZooKeeper Four Letter Words commands in clickhouse-keeper: https://zookeeper.apache.org/doc/r3.4.8/zookeeperAdmin.html#sc_zkCommands. #28981 (JackyWoo). Now clickhouse-keeper is feature complete.Support for Bool data type. #31072 (kevin wan).Support for PARTITION BY in File, URL, HDFS storages and with INSERT INTO table function. Closes #30273. #30690 (Kseniia Sumarokova).Added CONSTRAINT ... ASSUME ... (without checking during INSERT). Added query transformation to CNF (https://github.com/ClickHouse/ClickHouse/issues/11749) for more convenient optimization. Added simple query rewriting using constraints (only simple matching now, will be improved to support &lt;,=,&gt;... later). Added ability to replace heavy columns with light columns if it's possible. #18787 (Nikita Vasilev).Basic access authentication for http/url functions. #31648 (michael1589).Support INTERVAL type in STEP clause for WITH FILL modifier. #30927 (Anton Popov).Add support for parallel reading from multiple files and support globs in FROM INFILE clause. #30135 (Filatenkov Artur).Add support for Identifier table and database query parameters. Closes #27226. #28668 (Nikolay Degterinsky).TLDR: Major improvements of completeness and consistency of text formats. Refactor formats TSV, TSVRaw, CSV and JSONCompactEachRow, JSONCompactStringsEachRow, remove code duplication, add base interface for formats with -WithNames and -WithNamesAndTypes suffixes. Add formats CSVWithNamesAndTypes, TSVRawWithNames, TSVRawWithNamesAndTypes, JSONCompactEachRowWIthNames, JSONCompactStringsEachRowWIthNames, RowBinaryWithNames. Support parallel parsing for formats TSVWithNamesAndTypes, TSVRaw(WithNames/WIthNamesAndTypes), CSVWithNamesAndTypes, JSONCompactEachRow(WithNames/WIthNamesAndTypes), JSONCompactStringsEachRow(WithNames/WIthNamesAndTypes). Support columns mapping and types checking for RowBinaryWithNamesAndTypes format. Add setting input_format_with_types_use_header which specify if we should check that types written in &lt;format_name&gt;WIthNamesAndTypes format matches with table structure. Add setting input_format_csv_empty_as_default and use it in CSV format instead of input_format_defaults_for_omitted_fields (because this setting should not control csv_empty_as_default). Fix usage of setting input_format_defaults_for_omitted_fields (it was used only as csv_empty_as_default, but it should control calculation of default expressions for omitted fields). Fix Nullable input/output in TSVRaw format, make this format fully compatible with inserting into TSV. Fix inserting NULLs in LowCardinality(Nullable) when input_format_null_as_default is enabled (previously default values was inserted instead of actual NULLs). Fix strings deserialization in JSONStringsEachRow/JSONCompactStringsEachRow formats (strings were parsed just until first '\\n' or '\\t'). Add ability to use Raw escaping rule in Template input format. Add diagnostic info for JSONCompactEachRow(WithNames/WIthNamesAndTypes) input format. Fix bug with parallel parsing of -WithNames formats in case when setting min_chunk_bytes_for_parallel_parsing is less than bytes in a single row. #30178 (Kruglov Pavel). Allow to print/parse names and types of colums in CustomSeparated input/output format. Add formats CustomSeparatedWithNames/WithNamesAndTypes similar to TSVWithNames/WithNamesAndTypes. #31434 (Kruglov Pavel).Aliyun OSS Storage support. #31286 (cfcz48).Exposes all settings of the global thread pool in the configuration file. #31285 (Tomáš Hromada).Introduced window functions exponentialTimeDecayedSum, exponentialTimeDecayedMax, exponentialTimeDecayedCount and exponentialTimeDecayedAvg which are more effective than exponentialMovingAverage for bigger windows. Also more use-cases were covered. #29799 (Vladimir Chebotarev).Add option to compress logs before writing them to a file using LZ4. Closes #23860. #29219 (Nikolay Degterinsky).Support JOIN ON 1 = 1 that have CROSS JOIN semantic. This closes #25578. #25894 (Vladimir C).Add Map combinator for Map type. - Rename old sum-, min-, max- Map for mapped arrays to sum-, min-, max- MappedArrays. #24539 (Ildus Kurbangaliev).Make reading from HTTP retriable. Closes #29696. #29894 (Kseniia Sumarokova). Experimental Feature​ WINDOW VIEW to enable stream processing in ClickHouse. #8331 (vxider).Drop support for using Ordinary databases with MaterializedMySQL. #31292 (Stig Bakken).Implement the commands BACKUP and RESTORE for the Log family. This feature is under development. #30688 (Vitaly Baranov). Performance Improvement​ Reduce memory usage when reading with s3 / url / hdfs formats Parquet, ORC, Arrow (controlled by setting input_format_allow_seeks, enabled by default). Also add setting remote_read_min_bytes_for_seek to control seeks. Closes #10461. Closes #16857. #30936 (Kseniia Sumarokova).Add optimizations for constant conditions in JOIN ON, ref #26928. #27021 (Vladimir C).Support parallel formatting for all text formats, except JSONEachRowWithProgress and PrettyCompactMonoBlock. #31489 (Kruglov Pavel).Speed up count over nullable columns. #31806 (Raúl Marín).Speed up avg and sumCount aggregate functions. #31694 (Raúl Marín).Improve performance of JSON and XML output formats. #31673 (alexey-milovidov).Improve performance of syncing data to block device. This closes #31181. #31229 (zhanglistar).Fixing query performance issue in LiveView tables. Fixes #30831. #31006 (vzakaznikov).Speed up query parsing. #31949 (Raúl Marín).Allow to split GraphiteMergeTree rollup rules for plain/tagged metrics (optional rule_type field). #25122 (Michail Safronov).Remove excessive DESC TABLE requests for remote() (in case of remote('127.1', system.one) (i.e. identifier as the db.table instead of string) there was excessive DESC TABLE request). #32019 (Azat Khuzhin).Optimize function tupleElement to reading of subcolumn with enabled setting optimize_functions_to_subcolumns. #31261 (Anton Popov).Optimize function mapContains to reading of subcolumn key with enabled settings optimize_functions_to_subcolumns. #31218 (Anton Popov).Add settings merge_tree_min_rows_for_concurrent_read_for_remote_filesystem and merge_tree_min_bytes_for_concurrent_read_for_remote_filesystem. #30970 (Kseniia Sumarokova).Skipping mutations of different partitions in StorageMergeTree. #21326 (Vladimir Chebotarev). Improvement​ Do not allow to drop a table or dictionary if some tables or dictionaries depend on it. #30977 (tavplubix).Allow versioning of aggregate function states. Now we can introduce backward compatible changes in serialization format of aggregate function states. Closes #12552. #24820 (Kseniia Sumarokova).Support PostgreSQL style ALTER MODIFY COLUMN syntax. #32003 (SuperDJY).Added update_field support for RangeHashedDictionary, ComplexKeyRangeHashedDictionary. #32185 (Maksim Kita).The murmurHash3_128 and sipHash128 functions now accept an arbitrary number of arguments. This closes #28774. #28965 (小路).Support default expression for HDFS storage and optimize fetching when source is column oriented. #32256 (李扬).Improve the operation name of an opentelemetry span. #32234 (Frank Chen).Use Content-Type: application/x-ndjson (http://ndjson.org/) for output format JSONEachRow. #32223 (Dmitriy Dorofeev).Improve skipping unknown fields with quoted escaping rule in Template/CustomSeparated formats. Previously you could skip only quoted strings, now you can skip values with any type. #32204 (Kruglov Pavel).Now clickhouse-keeper refuses to start or apply configuration changes when they contain duplicated IDs or endpoints. Fixes #31339. #32121 (alesapin).Set Content-Type in HTTP packets issued from URL engine. #32113 (Frank Chen).Return Content-Type as 'application/json' for JSONEachRow format if output_format_json_array_of_rows is enabled. #32112 (Frank Chen).Allow to parse + before Float32/Float64 values. #32079 (Kruglov Pavel).Allow a user configured hdfs_replication parameter for DiskHDFS and StorageHDFS. Closes #32039. #32049 (leosunli).Added ClickHouse exception and exception_code fields to opentelemetry span log. #32040 (Frank Chen).Improve opentelemetry span log duration - it was is zero at the query level if there is a query exception. #32038 (Frank Chen).Fix the issue that LowCardinality of Int256 cannot be created. #31832 (alexey-milovidov).Recreate system.*_log tables in case of different engine/partition_by. #31824 (Azat Khuzhin).MaterializedMySQL: Fix issue with table named 'table'. #31781 (Håvard Kvålen).ClickHouse dictionary source: support predefined connections. Closes #31705. #31749 (Kseniia Sumarokova).Allow to use predefined connections configuration for Kafka and RabbitMQ engines (the same way as for other integration table engines). #31691 (Kseniia Sumarokova).Always re-render prompt while navigating history in clickhouse-client. This will improve usability of manipulating very long queries that don't fit on screen. #31675 (alexey-milovidov) (author: Amos Bird).Add key bindings for navigating through history (instead of lines/history). #31641 (Azat Khuzhin).Improve the max_execution_time checks. Fixed some cases when timeout checks do not happen and query could run too long. #31636 (Raúl Marín).Better exception message when users.xml cannot be loaded due to bad password hash. This closes #24126. #31557 (Vitaly Baranov).Use shard and replica name from Replicated database arguments when expanding macros in ReplicatedMergeTree arguments if these macros are not defined in config. Closes #31471. #31488 (tavplubix).Better analysis for min/max/count projection. Now, with enabled allow_experimental_projection_optimization, virtual min/max/count projection can be used together with columns from partition key. #31474 (Amos Bird).Add --pager support for clickhouse-local. #31457 (Azat Khuzhin).Fix waiting of the editor during interactive query edition (waitpid() returns -1 on SIGWINCH and EDITOR and clickhouse-local/clickhouse-client works concurrently). #31456 (Azat Khuzhin).Throw an exception if there is some garbage after field in JSONCompactStrings(EachRow) format. #31455 (Kruglov Pavel).Default value of http_send_timeout and http_receive_timeout settings changed from 1800 (30 minutes) to 180 (3 minutes). #31450 (tavplubix).MaterializedMySQL now handles CREATE TABLE ... LIKE ... DDL queries. #31410 (Stig Bakken).Return artificial create query when executing show create table on system's tables. #31391 (SuperDJY).Previously progress was shown only for numbers table function. Now for numbers_mt it is also shown. #31318 (Kseniia Sumarokova).Initial user's roles are used now to find row policies, see #31080. #31262 (Vitaly Baranov).If some obsolete setting is changed - show warning in system.warnings. #31252 (tavplubix).Improved backoff for background cleanup tasks in MergeTree. Settings merge_tree_clear_old_temporary_directories_interval_seconds and merge_tree_clear_old_parts_interval_seconds moved from users settings to merge tree settings. #31180 (tavplubix).Now every replica will send to client only incremental information about profile events counters. #31155 (Dmitry Novik). This makes --hardware_utilization option in clickhouse-client usable.Enable multiline editing in clickhouse-client by default. This addresses #31121 . #31123 (Amos Bird).Function name normalization for ALTER queries. This helps avoid metadata mismatch between creating table with indices/projections and adding indices/projections via alter commands. This is a follow-up PR of https://github.com/ClickHouse/ClickHouse/pull/20174. Mark as improvements as there are no bug reports and the senario is somehow rare. #31095 (Amos Bird).Support IF EXISTS modifier for RENAME DATABASE/TABLE/DICTIONARY query. If this directive is used, one will not get an error if the DATABASE/TABLE/DICTIONARY to be renamed doesn't exist. #31081 (victorgao).Cancel vertical merges when partition is dropped. This is a follow-up of https://github.com/ClickHouse/ClickHouse/pull/25684 and https://github.com/ClickHouse/ClickHouse/pull/30996. #31057 (Amos Bird).The local session inside a Clickhouse dictionary source won't send its events to the session log anymore. This fixes a possible deadlock (tsan alert) on shutdown. Also this PR fixes flaky test_dictionaries_dependency_xml/. #31013 (Vitaly Baranov).Less locking in ALTER command. #31010 (Amos Bird).Fix --verbose option in clickhouse-local interactive mode and allow logging into file. #30881 (Kseniia Sumarokova).Added \\l, \\d, \\c commands in clickhouse-client like in MySQL and PostgreSQL. #30876 (Pavel Medvedev).For clickhouse-local or clickhouse-client: if there is --interactive option with --query or --queries-file, then first execute them like in non-interactive and then start interactive mode. #30851 (Kseniia Sumarokova).Fix possible &quot;The local set of parts of X doesn't look like the set of parts in ZooKeeper&quot; error (if DROP fails during removing znodes from zookeeper). #30826 (Azat Khuzhin).Avro format works against Kafka. Setting output_format_avro_rows_in_file added. #30351 (Ilya Golshtein).Allow to specify one or any number of PostgreSQL schemas for one MaterializedPostgreSQL database. Closes #28901. Closes #29324. #28933 (Kseniia Sumarokova).Replaced default ports for clickhouse-keeper internal communication from 44444 to 9234. Fixes #30879. #31799 (alesapin).Implement function transform with Decimal arguments. #31839 (李帅).Fix abort in debug server and DB::Exception: std::out_of_range: basic_string error in release server in case of bad hdfs url by adding additional check of hdfs url structure. #31042 (Kruglov Pavel).Fix possible assert in hdfs table function/engine, add test. #31036 (Kruglov Pavel). Bug Fixes​ Fix group by / order by / limit by aliases with positional arguments enabled. Closes #31173. #31741 (Kseniia Sumarokova).Fix usage of Buffer table engine with type Map. Fixes #30546. #31742 (Anton Popov).Fix reading from MergeTree tables with enabled use_uncompressed_cache. #31826 (Anton Popov).Fixed the behavior when mutations that have nothing to do are stuck (with enabled setting empty_result_for_aggregation_by_empty_set). #32358 (Nikita Mikhaylov).Fix skipping columns while writing protobuf. This PR fixes #31160, see the comment #31160#issuecomment-980595318. #31988 (Vitaly Baranov).Fix bug when remove unneeded columns in subquery. If there is an aggregation function in query without group by, do not remove if it is unneeded. #32289 (dongyifeng).Quota limit was not reached, but the limit was exceeded. This PR fixes #31174. #31337 (sunny).Fix SHOW GRANTS when partial revokes are used. This PR fixes #31138. #31249 (Vitaly Baranov).Memory amount was incorrectly estimated when ClickHouse is run in containers with cgroup limits. #31157 (Pavel Medvedev).Fix ALTER ... MATERIALIZE COLUMN ... queries in case when data type of default expression is not equal to the data type of column. #32348 (Anton Popov).Fixed crash with SIGFPE in aggregate function avgWeighted with Decimal argument. Fixes #32053. #32303 (tavplubix).Server might fail to start with Cannot attach 1 tables due to cyclic dependencies error if Dictionary table looks at XML-dictionary with the same name, it's fixed. Fixes #31315. #32288 (tavplubix).Fix parsing error while NaN deserializing for Nullable(Float) for Quoted escaping rule. #32190 (Kruglov Pavel).XML dictionaries: identifiers, used in table create query, can be qualified to default_database during upgrade to newer version. Closes #31963. #32187 (Maksim Kita).Number of active replicas might be determined incorrectly when inserting with quorum if setting replicated_can_become_leader is disabled on some replicas. It's fixed. #32157 (tavplubix).Dictionaries: fix cases when {condition} does not work for custom database queries. #32117 (Maksim Kita).Fix CAST from Nullable with cast_keep_nullable (PARAMETER_OUT_OF_BOUND error before for i.e. toUInt32OrDefault(toNullable(toUInt32(1)))). #32080 (Azat Khuzhin).Fix CREATE TABLE of Join Storage in some obscure cases. Close #31680. #32066 (SuperDJY).Fixed Directory ... already exists and is not empty error when detaching part. #32063 (tavplubix).MaterializedMySQL (experimental feature): Fix misinterpretation of DECIMAL data from MySQL. #31990 (Håvard Kvålen).FileLog (experimental feature) engine unnesessary created meta data directory when create table failed. Fix #31962. #31967 (flynn).Some GET_PART entry might hang in replication queue if part is lost on all replicas and there are no other parts in the same partition. It's fixed in cases when partition key contains only columns of integer types or Date[Time]. Fixes #31485. #31887 (tavplubix).Fix functions empty and notEmpty with arguments of UUID type. Fixes #31819. #31883 (Anton Popov).Change configuration path from keeper_server.session_timeout_ms to keeper_server.coordination_settings.session_timeout_ms when constructing a KeeperTCPHandler. Same with operation_timeout. #31859 (JackyWoo).Fix invalid cast of Nullable type when nullable primary key is used. (Nullable primary key is a discouraged feature - please do not use). This fixes #31075. #31823 (Amos Bird).Fix crash in recursive UDF in SQL. Closes #30856. #31820 (Maksim Kita).Fix crash when function dictGet with type is used for dictionary attribute when type is Nullable. Fixes #30980. #31800 (Maksim Kita).Fix crash with empty result of ODBC query (with some ODBC drivers). Closes #31465. #31766 (Kseniia Sumarokova).Fix disabling query profiler (In case of query_profiler_real_time_period_ns&gt;0/query_profiler_cpu_time_period_ns&gt;0 query profiler can stayed enabled even after query finished). #31740 (Azat Khuzhin).Fixed rare segfault on concurrent ATTACH PARTITION queries. #31738 (tavplubix).Fix race in JSONEachRowWithProgress output format when data and lines with progress are mixed in output. #31736 (Kruglov Pavel).Fixed there are no such cluster here error on execution of ON CLUSTER query if specified cluster name is name of Replicated database. #31723 (tavplubix).Fix exception on some of the applications of decrypt function on Nullable columns. This closes #31662. This closes #31426. #31707 (alexey-milovidov).Fixed function ngrams when string contains UTF-8 characters. #31706 (yandd).Settings input_format_allow_errors_num and input_format_allow_errors_ratio did not work for parsing of domain types, such as IPv4, it's fixed. Fixes #31686. #31697 (tavplubix).Fixed null pointer exception in MATERIALIZE COLUMN. #31679 (Nikolai Kochetov).RENAME TABLE query worked incorrectly on attempt to rename an DDL dictionary in Ordinary database, it's fixed. #31638 (tavplubix).Implement sparkbar aggregate function as it was intended, see: #26175#issuecomment-960353867, comment. #31624 (小路).Fix invalid generated JSON when only column names contain invalid UTF-8 sequences. #31534 (Kevin Michel).Disable partial_merge_join_left_table_buffer_bytes before bug in this optimization is fixed. See #31009). Remove redundant option partial_merge_join_optimizations. #31528 (Vladimir C).Fix progress for short INSERT SELECT queries. #31510 (Azat Khuzhin).Fix wrong behavior with group by and positional arguments. Closes #31280#issuecomment-968696186. #31420 (Kseniia Sumarokova).Resolve nullptr in STS credentials provider for S3. #31409 (Vladimir Chebotarev).Remove notLike function from index analysis, because it was wrong. #31169 (sundyli).Fix bug in Keeper which can lead to inability to start when some coordination logs was lost and we have more fresh snapshot than our latest log. #31150 (alesapin).Rewrite right distributed table in local join. solves #25809. #31105 (abel-cheng).Fix Merge table with aliases and where (it did not work before at all). Closes #28802. #31044 (Kseniia Sumarokova).Fix JSON_VALUE/JSON_QUERY with quoted identifiers. This allows to have spaces in json path. Closes #30971. #31003 (Kseniia Sumarokova).Using formatRow function with not row-oriented formats led to segfault. Don't allow to use this function with such formats (because it doesn't make sense). #31001 (Kruglov Pavel).Fix bug which broke select queries if they happened after dropping materialized view. Found in #30691. #30997 (Kseniia Sumarokova).Skip max_partition_size_to_drop check in case of ATTACH PARTITION ... FROM and MOVE PARTITION ... #30995 (Amr Alaa).Fix some corner cases with INTERSECT and EXCEPT operators. Closes #30803. #30965 (Kseniia Sumarokova). Build/Testing/Packaging Improvement​ Fix incorrect filtering result on non-x86 builds. This closes #31417. This closes #31524. #31574 (alexey-milovidov).Make ClickHouse build fully reproducible (byte identical on different machines). This closes #22113. #31899 (alexey-milovidov). Remove filesystem path to the build directory from binaries to enable reproducible builds. This needed for #22113. #31838 (alexey-milovidov).Use our own CMakeLists for zlib-ng, cassandra, mariadb-connector-c and xz, re2, sentry, gsasl, arrow, protobuf. This is needed for #20151. Part of #9226. A small step towards removal of annoying trash from the build system. #30599 (alexey-milovidov).Hermetic builds: use fixed version of libc and make sure that no source or binary files from the host OS are using during build. This closes #27133. This closes #21435. This closes #30462. #30011 (alexey-milovidov).Adding function getFuzzerData() to easily fuzz particular functions. This closes #23227. #27526 (Alexey Boykov).More correct setting up capabilities inside Docker. #31802 (Constantine Peresypkin).Enable clang -fstrict-vtable-pointers, -fwhole-program-vtables compile options. #20151 (Maksim Kita).Avoid downloading toolchain tarballs for cross-compiling for FreeBSD. #31672 (alexey-milovidov).Initial support for risc-v. See development/build-cross-riscv for quirks and build command that was tested. #31309 (Vladimir Smirnov).Support compile in arm machine with parameter &quot;-DENABLE_TESTS=OFF&quot;. #31007 (zhanghuajie). "},{"title":"ClickHouse release v21.11, 2021-11-09​","type":1,"pageTitle":"2021","url":"docs/en/whats-new/changelog/2021#clickhouse-release-v2111-2021-11-09","content":"Backward Incompatible Change​ Change order of json_path and json arguments in SQL/JSON functions (to be consistent with the standard). Closes #30449. #30474 (Kseniia Sumarokova).Remove MergeTree table setting write_final_mark. It will be always true. #30455 (Kseniia Sumarokova). No actions required, all tables are compatible with the new version.Function bayesAB is removed. Please help to return this function back, refreshed. This closes #26233. #29934 (alexey-milovidov).This is relevant only if you already started using the experimental clickhouse-keeper support. Now ClickHouse Keeper snapshots compressed with ZSTD codec by default instead of custom ClickHouse LZ4 block compression. This behavior can be turned off with compress_snapshots_with_zstd_format coordination setting (must be equal on all quorum replicas). Backward incompatibility is quite rare and may happen only when new node will send snapshot (happens in case of recovery) to the old node which is unable to read snapshots in ZSTD format. #29417 (alesapin). New Feature​ New asynchronous INSERT mode allows to accumulate inserted data and store it in a single batch in background. On client it can be enabled by setting async_insert for INSERT queries with data inlined in query or in separate buffer (e.g. for INSERT queries via HTTP protocol). If wait_for_async_insert is true (by default) the client will wait until data will be flushed to table. On server-side it controlled by the settings async_insert_threads, async_insert_max_data_size and async_insert_busy_timeout_ms. Implements #18282. #27537 (Anton Popov). #20557 (Ivan). Notes on performance: with asynchronous inserts you can do up to around 10 000 individual INSERT queries per second, so it is still recommended to insert in batches if you want to achieve performance up to millions inserted rows per second.Add interactive mode for clickhouse-local. So, you can just run clickhouse-local to get a command line ClickHouse interface without connecting to a server and process data from files and external data sources. Also merge the code of clickhouse-client and clickhouse-local together. Closes #7203. Closes #25516. Closes #22401. #26231 (Kseniia Sumarokova).Added support for executable (scriptable) user defined functions. These are UDFs that can be written in any programming language. #28803 (Maksim Kita).Allow predefined connections to external data sources. This allows to avoid specifying credentials or addresses while using external data sources, they can be referenced by names instead. Closes #28367. #28577 (Kseniia Sumarokova).Added INFORMATION_SCHEMA database with SCHEMATA, TABLES, VIEWS and COLUMNS views to the corresponding tables in system database. Closes #9770. #28691 (tavplubix).Support EXISTS (subquery). Closes #6852. #29731 (Kseniia Sumarokova).Session logging for audit. Logging all successful and failed login and logout events to a new system.session_log table. #22415 (Vasily Nemkov) (Vitaly Baranov).Support multidimensional cosine distance and euclidean distance functions; L1, L2, Lp, Linf distances and norms. Scalar product on tuples and various arithmetic operators on tuples. This fully closes #4509 and even more. #27933 (Alexey Boykov).Add support for compression and decompression for INTO OUTFILE and FROM INFILE (with autodetect or with additional optional parameter). #27135 (Filatenkov Artur).Add CORS (Cross Origin Resource Sharing) support with HTTP OPTIONS request. It means, now Grafana will work with serverless requests without a kludges. Closes #18693. #29155 (Filatenkov Artur).Queries with JOIN ON now supports disjunctions (OR). #21320 (Ilya Golshtein).Added function tokens. That allow to split string into tokens using non-alpha numeric ASCII characters as separators. #29981 (Maksim Kita). Added function ngrams to extract ngrams from text. Closes #29699. #29738 (Maksim Kita).Add functions for Unicode normalization: normalizeUTF8NFC, normalizeUTF8NFD, normalizeUTF8NFKC, normalizeUTF8NFKD functions. #28633 (darkkeks).Streaming consumption of application log files in ClickHouse with FileLog table engine. It's like Kafka or RabbitMQ engine but for append-only and rotated logs in local filesystem. Closes #6953. #25969 (flynn) (Kseniia Sumarokova).Add CapnProto output format, refactor CapnProto input format. #29291 (Kruglov Pavel).Allow to write number in query as binary literal. Example SELECT 0b001;. #29304 (Maksim Kita).Added hashed_array dictionary type. It saves memory when using dictionaries with multiple attributes. Closes #30236. #30242 (Maksim Kita).Added JSONExtractKeys function. #30056 (Vitaly).Add a function getOSKernelVersion - it returns a string with OS kernel version. #29755 (Memo).Added MD4 and SHA384 functions. MD4 is an obsolete and insecure hash function, it can be used only in rare cases when MD4 is already being used in some legacy system and you need to get exactly the same result. #29602 (Nikita Tikhomirov).HSTS can be enabled for Clickhouse HTTP server by setting hsts_max_age in configuration file with a positive number. #29516 (凌涛).Huawei OBS Storage support. Closes #24294. #29511 (kevin wan).New function mapContainsKeyLike to get the map that key matches a simple regular expression. #29471 (凌涛). New function mapExtractKeyLike to get the map only kept elements matched specified pattern. #30793 (凌涛).Implemented ALTER TABLE x MODIFY COMMENT. #29264 (Vasily Nemkov).Adds H3 inspection functions that are missing from ClickHouse but are available via the H3 api: https://h3geo.org/docs/api/inspection. #29209 (Bharat Nallan).Allow non-replicated ALTER TABLE FETCH and ATTACH in Replicated databases. #29202 (Kevin Michel).Added a setting output_format_csv_null_representation: This is the same as output_format_tsv_null_representation but is for CSV output. #29123 (PHO).Added function zookeeperSessionUptime() which returns uptime of current ZooKeeper session in seconds. #28983 (tavplubix).Implements the h3ToGeoBoundary function. #28952 (Ivan Veselov).Add aggregate function exponentialMovingAverage that can be used as window function. This closes #27511. #28914 (alexey-milovidov).Allow to include subcolumns of table columns into DESCRIBE query result (can be enabled by setting describe_include_subcolumns). #28905 (Anton Popov).Executable, ExecutablePool added option send_chunk_header. If this option is true then chunk rows_count with line break will be sent to client before chunk. #28833 (Maksim Kita).tokenbf_v1 and ngram support Map with key of String of FixedSring type. It enhance data skipping in query with map key filter. sql CREATE TABLE map_tokenbf ( row_id UInt32, map Map(String, String), INDEX map_tokenbf map TYPE ngrambf_v1(4,256,2,0) GRANULARITY 1 ) Engine=MergeTree() Order by id With table above, the query select * from map_tokebf where map['K']='V' will skip the granule that doesn't contain key A . Of course, how many rows will skipped is depended on the granularity and index_granularity you set. #28511 (凌涛).Send profile events from server to client. New packet type ProfileEvents was introduced. Closes #26177. #28364 (Dmitry Novik).Bit shift operations for FixedString and String data types. This closes #27763. #28325 (小路).Support adding / deleting tables to replication from PostgreSQL dynamically in database engine MaterializedPostgreSQL. Support alter for database settings. Closes #27573. #28301 (Kseniia Sumarokova).Added function accurateCastOrDefault(x, T). Closes #21330. Authors @taiyang-li. #23028 (Maksim Kita).Add Function toUUIDOrDefault, toUInt8/16/32/64/256OrDefault, toInt8/16/32/64/128/256OrDefault, which enables user defining default value(not null) when string parsing is failed. #21330 (taiyang-li). Performance Improvement​ Background merges can be preempted by each other and they are scheduled with appropriate priorities. Now long running merges won't prevent short merges to proceed. This is needed for a better scheduling and controlling of merges execution. It reduces the chances to get &quot;too many parts&quot; error. #22381. #25165 (Nikita Mikhaylov). Added an ability to execute more merges and mutations than the number of threads in background pool. Merges and mutations will be executed step by step according to their sizes (lower is more prioritized). The ratio of the number of tasks to threads to execute is controlled by a setting background_merges_mutations_concurrency_ratio, 2 by default. #29140 (Nikita Mikhaylov).Allow to use asynchronous reads for remote filesystems. Lower the number of seeks while reading from remote filesystems. It improves performance tremendously and makes the experimental web and s3 disks to work faster than EBS under certain conditions. #29205 (Kseniia Sumarokova). In the meantime, the web disk type (static dataset hosted on a web server) is graduated from being experimental to be production ready.Queries with INTO OUTFILE in clickhouse-client will use multiple threads. Fix the issue with flickering progress-bar when using INTO OUTFILE. This closes #30873. This closes #30872. #30886 (alexey-milovidov).Reduce amount of redundant compressed data read from disk for some types SELECT queries (only for MergeTree engines family). #30111 (alesapin).Remove some redundant seek calls while reading compressed blocks in MergeTree table engines family. #29766 (alesapin).Make url table function to process multiple URLs in parallel. This closes #29670 and closes #29671. #29673 (alexey-milovidov).Improve performance of aggregation in order of primary key (with enabled setting optimize_aggregation_in_order). #30266 (Anton Popov).Now clickhouse is using DNS cache while communicating with external S3. #29999 (alesapin).Add support for pushdown of IS NULL/IS NOT NULL to external databases (i.e. MySQL). #29463 (Azat Khuzhin). Transform isNull/isNotNull to IS NULL/IS NOT NULL (for external dbs, i.e. MySQL). #29446 (Azat Khuzhin).SELECT queries from Dictionary tables will use multiple threads. #30500 (Maksim Kita).Improve performance for filtering (WHERE operation) of Decimal columns. #30431 (Jun Jin).Remove branchy code in filter operation with a better implementation with popcnt/ctz which have better performance. #29881 (Jun Jin).Improve filter bytemask generator (used for WHERE operator) function all in one with SSE/AVX2/AVX512 instructions. Note that by default ClickHouse is only using SSE, so it's only relevant for custom builds. #30014 (jasperzhu). #30670 (jasperzhu).Improve the performance of SUM aggregate function of Nullable floating point numbers. #28906 (Raúl Marín).Speed up part loading process with multiple disks are in use. The idea is similar to https://github.com/ClickHouse/ClickHouse/pull/16423 . Prod env shows improvement: 24 min -&gt; 16 min . #28363 (Amos Bird).Reduce default settings for S3 multipart upload part size to lower memory usage. #28679 (ianton-ru).Speed up bitmapAnd function. #28332 (dddounaiking).Removed sub-optimal mutation notifications in StorageMergeTree when merges are still going. #27552 (Vladimir Chebotarev).Attempt to improve performance of string comparison. #28767 (alexey-milovidov).Primary key index and partition filter can work in tuple. #29281 (凌涛).If query has multiple quantile aggregate functions with the same arguments but different level parameter, they will be fused together and executed in one pass if the setting optimize_syntax_fuse_functions is enabled. #26657 (hexiaoting).Now min-max aggregation over the first expression of primary key is optimized by projection. This is for #329. #29918 (Amos Bird). Experimental Feature​ Add ability to change nodes configuration (in .xml file) for ClickHouse Keeper. #30372 (alesapin).Add sparkbar aggregate function. This closes #26175. #27481 (小路). Note: there is one flaw in this function, the behaviour will be changed in future releases. Improvement​ Allow user to change log levels without restart. #29586 (Nikolay Degterinsky).Multiple improvements for SQL UDF. Queries for manipulation of SQL User Defined Functions now support ON CLUSTER clause. Example CREATE FUNCTION test_function ON CLUSTER 'cluster' AS x -&gt; x + 1;. Closes #30666. #30734 (Maksim Kita). Support CREATE OR REPLACE, CREATE IF NOT EXISTS syntaxes. #30454 (Maksim Kita). Added DROP IF EXISTS support. Example DROP FUNCTION IF EXISTS test_function. #30437 (Maksim Kita). Support lambdas. Example CREATE FUNCTION lambda_function AS x -&gt; arrayMap(element -&gt; element * 2, x);. #30435 (Maksim Kita). Support SQL user defined functions for clickhouse-local. #30179 (Maksim Kita).Enable per-query memory profiler (set to memory_profiler_step = 4MiB) globally. #29455 (Azat Khuzhin).Added columns data_compressed_bytes, data_uncompressed_bytes, marks_bytes into system.data_skipping_indices. Added columns secondary_indices_compressed_bytes, secondary_indices_uncompressed_bytes, secondary_indices_marks_bytes into system.parts. Closes #29697. #29896 (Maksim Kita).Add table alias to system.tables and database alias to system.databases #29677. #29882 (kevin wan).Correctly resolve interdependencies between tables on server startup. Closes #8004, closes #15170. #28373 (tavplubix).Avoid error &quot;Division by zero&quot; when denominator is Nullable in functions divide, intDiv and modulo. Closes #22621. #28352 (Kruglov Pavel).Allow to parse values of Date data type in text formats as YYYYMMDD in addition to YYYY-MM-DD. This closes #30870. #30871 (alexey-milovidov).Web UI: render bars in table cells. #29792 (alexey-milovidov).User can now create dictionaries with comments: CREATE DICTIONARY ... COMMENT 'vaue' ... #29899 (Vasily Nemkov). Users now can set comments to database in CREATE DATABASE statement ... #29429 (Vasily Nemkov).Introduce compiled_expression_cache_elements_size setting. If you will ever want to use this setting, you will already know what it does. #30667 (Maksim Kita).clickhouse-format now supports option --query. In previous versions you have to pass the query to stdin. #29325 (凌涛).Support ALTER TABLE for tables in Memory databases. Memory databases are used in clickhouse-local. #30866 (tavplubix).Arrays of all serializable types are now supported by arrayStringConcat. #30840 (Nickita Taranov).ClickHouse now will account docker/cgroups limitations to get system memory amount. See #25662. #30574 (Pavel Medvedev).Fetched table structure for PostgreSQL database is more reliable now. #30477 (Kseniia Sumarokova).Full support of positional arguments in GROUP BY and ORDER BY. #30433 (Kseniia Sumarokova).Allow extracting non-string element as string using JSONExtractString. This is for pull/25452#issuecomment-927123287. #30426 (Amos Bird).Added an ability to use FINAL clause in SELECT queries from GraphiteMergeTree. #30360 (Nikita Mikhaylov).Minor improvements in replica cloning and enqueuing fetch for broken parts, that should avoid extremely rare hanging of GET_PART entries in replication queue. #30346 (tavplubix).Allow symlinks to files in user_files directory for file table function. #30309 (Kseniia Sumarokova).Fixed comparison of Date32 with Date, DateTime, DateTime64 and String. #30219 (liang.huang).Allow to remove SAMPLE BY expression from MergeTree tables (ALTER TABLE &lt;table&gt; REMOVE SAMPLE BY). #30180 (Anton Popov).Now Keeper (as part of clickhouse-server) will start asynchronously if it can connect to some other node. #30170 (alesapin).Now clickhouse-client supports native multi-line editing. #30143 (Amos Bird).polygon dictionaries (reverse geocoding): added support for reading the dictionary content with SELECT query method if setting store_polygon_key_column = true. Closes #30090. #30142 (Maksim Kita).Add ClickHouse logo to Play UI. #29674 (alexey-milovidov).Better exception message while reading column from Arrow-supported formats like Arrow, ArrowStream, Parquet and ORC. This closes #29926. #29927 (alexey-milovidov).Fix data-race between flush and startup in Buffer tables. This can appear in tests. #29930 (Azat Khuzhin).Fix lock-order-inversion between DROP TABLE for DatabaseMemory and LiveView. Live View is an experimental feature. Memory database is used in clickhouse-local. #29929 (Azat Khuzhin).Fix lock-order-inversion between periodic dictionary reload and config reload. #29928 (Azat Khuzhin).Update zoneinfo files to 2021c. #29925 (alexey-milovidov).Add ability to configure retries and delays between them for clickhouse-copier. #29921 (Azat Khuzhin).Add shutdown_wait_unfinished_queries server setting to allowing waiting for running queries up to shutdown_wait_unfinished time. This is for #24451. #29914 (Amos Bird).Add ability to trace peak memory usage (with new trace_type in system.trace_log - MemoryPeak). #29858 (Azat Khuzhin).PostgreSQL foreign tables: Added partitioned table prefix 'p' for the query for fetching replica identity index. #29828 (Shoh Jahon).Apply max_untracked_memory/memory_profiler_step/memory_profiler_sample_probability during mutate/merge to profile memory usage during merges. #29681 (Azat Khuzhin).Query obfuscator: clickhouse-format --obfuscate now works with more types of queries. #29672 (alexey-milovidov).Fixed the issue: clickhouse-format --obfuscate cannot process queries with embedded dictionaries (functions regionTo...). #29667 (alexey-milovidov).Fix incorrect Nullable processing of JSON functions. This fixes #29615 . Mark as improvement because https://github.com/ClickHouse/ClickHouse/pull/28012 is not released. #29659 (Amos Bird).Increase listen_backlog by default (to match default in newer linux kernel). #29643 (Azat Khuzhin).Reload dictionaries, models, user defined executable functions if servers config dictionaries_config, models_config, user_defined_executable_functions_config changes. Closes #28142. #29529 (Maksim Kita).Get rid of pointless restriction on projection name. Now projection name can start with tmp_. #29520 (Amos Bird).Fixed There is no query or query context has expired error in mutations with nested subqueries. Do not allow subqueries in mutation if table is replicated and allow_nondeterministic_mutations setting is disabled. #29495 (tavplubix).Apply config changes to max_concurrent_queries during runtime (no need to restart). #29414 (Raúl Marín).Added setting use_skip_indexes. #29405 (Maksim Kita).Add support for FREEZEing in-memory parts (for backups). #29376 (Mo Xuan).Pass through initial query_id for clickhouse-benchmark (previously if you run remote query via clickhouse-benchmark, queries on shards will not be linked to the initial query via initial_query_id). #29364 (Azat Khuzhin).Skip indexes tokenbf_v1 and ngrambf_v1: added support for Array data type with key of String of FixedString type. #29280 (Maksim Kita). Skip indexes tokenbf_v1 and ngrambf_v1 added support for Map data type with key of String of FixedString type. Author @lingtaolf. #29220 (Maksim Kita).Function has: added support for Map data type. #29267 (Maksim Kita).Add compress_logs settings for clickhouse-keeper which allow to compress clickhouse-keeper logs (for replicated state machine) in ZSTD . Implements: #26977. #29223 (alesapin).Add a setting external_table_strict_query - it will force passing the whole WHERE expression in queries to foreign databases even if it is incompatible. #29206 (Azat Khuzhin).Disable projections when ARRAY JOIN is used. In previous versions projection analysis may break aliases in array join. #29139 (Amos Bird).Support more types in MsgPack input/output format. #29077 (Kruglov Pavel).Allow to input and output LowCardinality columns in ORC input/output format. #29062 (Kruglov Pavel).Select from system.distributed_ddl_queue might show incorrect values, it's fixed. #29061 (tavplubix).Correct behaviour with unknown methods for HTTP connection. Solves #29050. #29057 (Filatenkov Artur).clickhouse-keeper: Fix bug in clickhouse-keeper-converter which can lead to some data loss while restoring from ZooKeeper logs (not snapshot). #29030 (小路). Fix bug in clickhouse-keeper-converter which can lead to incorrect ZooKeeper log deserialization. #29071 (小路).Apply settings from CREATE ... AS SELECT queries (fixes: #28810). #28962 (Azat Khuzhin).Respect default database setting for ALTER TABLE ... ON CLUSTER ... REPLACE/MOVE PARTITION FROM/TO ... #28955 (anneji-dev).gRPC protocol: Allow change server-side compression from client. #28953 (Vitaly Baranov).Skip &quot;no data&quot; exception when reading thermal sensors for asynchronous metrics. This closes #28852. #28882 (alexey-milovidov).Fixed logical race condition that might cause Dictionary not found error for existing dictionary in rare cases. #28853 (tavplubix).Relax nested function for If-combinator check (but forbid nested identical combinators). #28828 (Azat Khuzhin).Fix possible uncaught exception during server termination. #28761 (Azat Khuzhin).Forbid cleaning of tmp directories that can be used by an active mutation/merge if mutation/merge is extraordinarily long. #28760 (Azat Khuzhin).Allow optimization optimize_arithmetic_operations_in_aggregate_functions = 1 when alias is used. #28746 (Amos Bird).Implement detach_not_byte_identical_parts setting for ReplicatedMergeTree, that will detach instead of remove not byte-identical parts (after mege/mutate). #28708 (Azat Khuzhin).Implement max_suspicious_broken_parts_bytes setting for MergeTree (to limit total size of all broken parts, default is 1GiB). #28707 (Azat Khuzhin).Enable expanding macros in RabbitMQ table settings. #28683 (Vitaly Baranov).Restore the possibility to read data of a table using the Log engine in multiple threads. #28125 (Vitaly Baranov).Fix misbehavior of NULL column handling in JSON functions. This fixes #27930. #28012 (Amos Bird).Allow to set the size of Mark/Uncompressed cache for skip indices separately from columns. #27961 (Amos Bird).Allow to mix JOIN with USING with other JOIN types. #23881 (darkkeks).Update aws-sdk submodule for throttling in Yandex Cloud S3. #30646 (ianton-ru).Fix releasing query ID and session ID at the end of query processing while handing gRPC call. #29954 (Vitaly Baranov).Fix shutdown of AccessControlManager to fix flaky test. #29951 (Vitaly Baranov).Fix failed assertion in reading from HDFS. Update libhdfs3 library to be able to run in tests in debug. Closes #29251. Closes #27814. #29276 (Kseniia Sumarokova). Build/Testing/Packaging Improvement​ Add support for FreeBSD builds for Aarch64 machines. #29952 (MikaelUrankar).Recursive submodules are no longer needed for ClickHouse. #30315 (alexey-milovidov).ClickHouse can be statically built with Musl. This is added as experiment, it does not support building odbc-bridge, library-bridge, integration with CatBoost and some libraries. #30248 (alexey-milovidov).Enable Protobuf, Arrow, ORC, Parquet for AArch64 and Darwin (macOS) builds. This closes #29248. This closes #28018. #30015 (alexey-milovidov).Add cross-build for PowerPC (powerpc64le). This closes #9589. Enable support for interaction with MySQL for AArch64 and PowerPC. This closes #26301. #30010 (alexey-milovidov).Leave only required files in cross-compile toolchains. Include them as submodules (earlier they were downloaded as tarballs). #29974 (alexey-milovidov).Implemented structure-aware fuzzing approach in ClickHouse for select statement parser. #30012 (Paul).Turning on experimental constexpr expressions evaluator for clang to speed up template code compilation. #29668 (myrrc).Add ability to compile using newer version fo glibc without using new symbols. #29594 (Azat Khuzhin).Reduce Debug build binary size by clang optimization option. #28736 (flynn).Now all images for CI will be placed in the separate dockerhub repo. #28656 (alesapin).Improve support for build with clang-13. #28046 (Sergei Semin).Add ability to print raw profile events to clickhouse-client (This can be useful for debugging and for testing). #30064 (Azat Khuzhin).Add time dependency for clickhouse-server unit (systemd and sysvinit init). #28891 (Azat Khuzhin).Reload stacktrace cache when symbol is reloaded. #28137 (Amos Bird). Bug Fix​ Functions for case-insensitive search in UTF-8 strings like positionCaseInsensitiveUTF8 and countSubstringsCaseInsensitiveUTF8 might find substrings that actually does not match in very rare cases, it's fixed. #30663 (tavplubix).Fix reading from empty file on encrypted disk. #30494 (Vitaly Baranov).Fix transformation of disjunctions chain to IN (controlled by settings optimize_min_equality_disjunction_chain_length) in distributed queries with settings legacy_column_name_of_tuple_literal = 0. #28658 (Anton Popov).Allow using a materialized column as the sharding key in a distributed table even if insert_allow_materialized_columns=0:. #28637 (Vitaly Baranov).Fix ORDER BY ... WITH FILL with set TO and FROM and no rows in result set. #30888 (Anton Popov).Fix set index not used in AND/OR expressions when there are more than two operands. This fixes #30416 . #30887 (Amos Bird).Fix crash when projection with hashing function is materialized. This fixes #30861 . The issue is similar to https://github.com/ClickHouse/ClickHouse/pull/28560 which is a lack of proper understanding of the invariant of header's emptyness. #30877 (Amos Bird).Fixed ambiguity when extracting auxiliary ZooKeeper name from ZooKeeper path in ReplicatedMergeTree. Previously server might fail to start with Unknown auxiliary ZooKeeper name if ZooKeeper path contains a colon. Fixes #29052. Also it was allowed to specify ZooKeeper path that does not start with slash, but now it's deprecated and creation of new tables with such path is not allowed. Slashes and colons in auxiliary ZooKeeper names are not allowed too. #30822 (tavplubix).Clean temporary directory when localBackup failed by some reason. #30797 (ianton-ru).Fixed a race condition between REPLACE/MOVE PARTITION and background merge in non-replicated MergeTree that might cause a part of moved/replaced data to remain in partition. Fixes #29327. #30717 (tavplubix).Fix PREWHERE with WHERE in case of always true PREWHERE. #30668 (Azat Khuzhin).Limit push down optimization could cause a error Cannot find column. Fixes #30438. #30562 (Nikolai Kochetov).Add missing parenthesis for isNotNull/isNull rewrites to IS [NOT] NULL (fixes queries that has something like isNotNull(1)+isNotNull(2)). #30520 (Azat Khuzhin).Fix deadlock on ALTER with scalar subquery to the same table, close #30461. #30492 (Vladimir C).Fixed segfault which might happen if session expired during execution of REPLACE PARTITION. #30432 (tavplubix).Queries with condition like IN (subquery) could return incorrect result in case if aggregate projection applied. Fixed creation of sets for projections. #30310 (Amos Bird).Fix column alias resolution of JOIN queries when projection is enabled. This fixes #30146. #30293 (Amos Bird).Fix some deficiency in replaceRegexpAll function. #30292 (Memo).Fix ComplexKeyHashedDictionary, ComplexKeySparseHashedDictionary parsing preallocate option from layout config. #30246 (Maksim Kita).Fix [I]LIKE function. Closes #28661. #30244 (Nikolay Degterinsky).Fix crash with shortcircuit and lowcardinality in multiIf. #30243 (Raúl Marín).FlatDictionary, HashedDictionary fix bytes_allocated calculation for nullable attributes. #30238 (Maksim Kita).Allow identifiers starting with numbers in multiple joins. #30230 (Vladimir C).Fix reading from MergeTree with max_read_buffer_size = 0 (when the user wants to shoot himself in the foot) (can lead to exceptions Can't adjust last granule, LOGICAL_ERROR, or even data loss). #30192 (Azat Khuzhin).Fix pread_fake_async/pread_threadpool with min_bytes_to_use_direct_io. #30191 (Azat Khuzhin).Fix INSERT SELECT incorrectly fills MATERIALIZED column based of Nullable column. #30189 (Azat Khuzhin).Support nullable arguments in function initializeAggregation. #30177 (Anton Popov).Fix error Port is already connected for queries with GLOBAL IN and WITH TOTALS. Only for 21.9 and 21.10. #30086 (Nikolai Kochetov).Fix race between MOVE PARTITION and merges/mutations for MergeTree. #30074 (Azat Khuzhin).Dropped Memory database might reappear after server restart, it's fixed (#29795). Also added force_remove_data_recursively_on_drop setting as a workaround for Directory not empty error when dropping Ordinary database (because it's not possible to remove data leftovers manually in cloud environment). #30054 (tavplubix).Fix crash of sample by tuple(), closes #30004. #30016 (flynn).try to close issue: #29965. #29976 (hexiaoting).Fix possible data-race between FileChecker and StorageLog/StorageStripeLog. #29959 (Azat Khuzhin).Fix data-race between LogSink::writeMarks() and LogSource in StorageLog. #29946 (Azat Khuzhin).Fix potential resource leak of the concurrent query limit of merge tree tables introduced in https://github.com/ClickHouse/ClickHouse/pull/19544. #29879 (Amos Bird).Fix system tables recreation check (fails to detect changes in enum values). #29857 (Azat Khuzhin).MaterializedMySQL: Fix an issue where if the connection to MySQL was lost, only parts of a transaction could be processed. #29837 (Håvard Kvålen).Avoid Timeout exceeded: elapsed 18446744073.709553 seconds error that might happen in extremely rare cases, presumably due to some bug in kernel. Fixes #29154. #29811 (tavplubix).Fix bad cast in ATTACH TABLE ... FROM 'path' query when non-string literal is used instead of path. It may lead to reading of uninitialized memory. #29790 (alexey-milovidov).Fix concurrent access to LowCardinality during GROUP BY (in combination with Buffer tables it may lead to troubles). #29782 (Azat Khuzhin).Fix incorrect GROUP BY (multiple rows with the same keys in result) in case of distributed query when shards had mixed versions &lt;= 21.3 and &gt;= 21.4, GROUP BY key had several columns all with fixed size, and two-level aggregation was activated (see group_by_two_level_threshold and group_by_two_level_threshold_bytes). Fixes #29580. #29735 (Nikolai Kochetov).Fixed incorrect behaviour of setting materialized_postgresql_tables_list at server restart. Found in #28529. #29686 (Kseniia Sumarokova).Condition in filter predicate could be lost after push-down optimisation. #29625 (Nikolai Kochetov).Fix JIT expression compilation with aliases and short-circuit expression evaluation. Closes #29403. #29574 (Maksim Kita).Fix rare segfault in ALTER MODIFY query when using incorrect table identifier in DEFAULT expression like x.y.z... Fixes #29184. #29573 (alesapin).Fix nullptr deference for GROUP BY WITH TOTALS HAVING (when the column from HAVING wasn't selected). #29553 (Azat Khuzhin).Avoid deadlocks when reading and writting on Join table engine tables at the same time. #29544 (Raúl Marín).Fix bug in check pathStartsWith becuase there was bug with the usage of std::mismatch: The behavior is undefined if the second range is shorter than the first range.. #29531 (Kseniia Sumarokova).In ODBC bridge add retries for error Invalid cursor state. It is a retriable error. Closes #29473. #29518 (Kseniia Sumarokova).Fixed incorrect table name parsing on loading of Lazy database. Fixes #29456. #29476 (tavplubix).Fix possible Block structure mismatch for subqueries with pushed-down HAVING predicate. Fixes #29010. #29475 (Nikolai Kochetov).Fix Logical error Cannot capture columns in functions greatest/least. Closes #29334. #29454 (Kruglov Pavel).RocksDB table engine: fix race condition during multiple DB opening (and get back some tests that triggers the problem on CI). #29393 (Azat Khuzhin).Fix replicated access storage not shutting down cleanly when misconfigured. #29388 (Kevin Michel).Remove window function nth_value as it is not memory-safe. This closes #29347. #29348 (alexey-milovidov).Fix vertical merges of projection parts. This fixes #29253 . This PR also fixes several projection merge/mutation issues introduced in https://github.com/ClickHouse/ClickHouse/pull/25165. #29337 (Amos Bird).Fix hanging DDL queries on Replicated database while adding a new replica. #29328 (Kevin Michel).Fix connection timeouts (send_timeout/receive_timeout). #29282 (Azat Khuzhin).Fix possible Table columns structure in ZooKeeper is different from local table structure exception while recreating or creating new replicas of ReplicatedMergeTree, when one of table columns have default expressions with case-insensitive functions. #29266 (Anton Popov).Send normal Database doesn't exist error (UNKNOWN_DATABASE) to the client (via TCP) instead of Attempt to read after eof (ATTEMPT_TO_READ_AFTER_EOF). #29229 (Azat Khuzhin).Fix segfault while inserting into column with type LowCardinality(Nullable) in Avro input format. #29132 (Kruglov Pavel).Do not allow to reuse previous credentials in case of inter-server secret (Before INSERT via Buffer/Kafka to Distributed table with interserver secret configured for that cluster, may re-use previously set user for that connection). #29060 (Azat Khuzhin).Handle any_join_distinct_right_table_keys when join with dictionary, close #29007. #29014 (Vladimir C).Fix &quot;Not found column ... in block&quot; error, when join on alias column, close #26980. #29008 (Vladimir C).Fix the number of threads used in GLOBAL IN subquery (it was executed in single threads since #19414 bugfix). #28997 (Nikolai Kochetov).Fix bad optimizations of ORDER BY if it contains WITH FILL. This closes #28908. This closes #26049. #28910 (alexey-milovidov).Fix higher-order array functions (SIGSEGV for arrayCompact/ILLEGAL_COLUMN for arrayDifference/arrayCumSumNonNegative) with consts. #28904 (Azat Khuzhin).Fix waiting for mutation with mutations_sync=2. #28889 (Azat Khuzhin).Fix queries to external databases (i.e. MySQL) with multiple columns in IN ( i.e. (k,v) IN ((1, 2)) ). #28888 (Azat Khuzhin).Fix bug with LowCardinality in short-curcuit function evaluation. Closes #28884. #28887 (Kruglov Pavel).Fix reading of subcolumns from compact parts. #28873 (Anton Popov).Fixed a race condition between DROP PART and REPLACE/MOVE PARTITION that might cause replicas to diverge in rare cases. #28864 (tavplubix).Fix expressions compilation with short circuit evaluation. #28821 (Azat Khuzhin).Fix extremely rare case when ReplicatedMergeTree replicas can diverge after hard reboot of all replicas. The error looks like Part ... intersects (previous|next) part .... #28817 (alesapin).Better check for connection usability and also catch any exception in RabbitMQ shutdown just in case. #28797 (Kseniia Sumarokova).Fix benign race condition in ReplicatedMergeTreeQueue. Shouldn't be visible for user, but can lead to subtle bugs. #28734 (alesapin).Fix possible crash for SELECT with partially created aggregate projection in case of exception. #28700 (Amos Bird).Fix the coredump in the creation of distributed tables, when the parameters passed in are wrong. #28686 (Zhiyong Wang).Add Settings.Names, Settings.Values aliases for system.processes table. #28685 (Vitaly).Support for S2 Geometry library: Fix the number of arguments required by s2RectAdd and s2RectContains functions. #28663 (Bharat Nallan).Fix invalid constant type conversion when Nullable or LowCardinality primary key is used. #28636 (Amos Bird).Fix &quot;Column is not under aggregate function and not in GROUP BY&quot; with PREWHERE (Fixes: #28461). #28502 (Azat Khuzhin). "},{"title":"ClickHouse release v21.10, 2021-10-16​","type":1,"pageTitle":"2021","url":"docs/en/whats-new/changelog/2021#clickhouse-release-v2110-2021-10-16","content":"Backward Incompatible Change​ Now the following MergeTree table-level settings: replicated_max_parallel_sends, replicated_max_parallel_sends_for_table, replicated_max_parallel_fetches, replicated_max_parallel_fetches_for_table do nothing. They never worked well and were replaced with max_replicated_fetches_network_bandwidth, max_replicated_sends_network_bandwidth and background_fetches_pool_size. #28404 (alesapin). New Feature​ Add feature for creating user-defined functions (UDF) as lambda expressions. Syntax CREATE FUNCTION {function_name} as ({parameters}) -&gt; {function core}. Example CREATE FUNCTION plus_one as (a) -&gt; a + 1. Authors @Realist007. #27796 (Maksim Kita) #23978 (Realist007).Added Executable storage engine and executable table function. It enables data processing with external scripts in streaming fashion. #28102 (Maksim Kita) (ruct).Added ExecutablePool storage engine. Similar to Executable but it's using a pool of long running processes. #28518 (Maksim Kita).Add ALTER TABLE ... MATERIALIZE COLUMN query. #27038 (Vladimir Chebotarev).Support for partitioned write into s3 table function. #23051 (Vladimir Chebotarev).Support lz4 compression format (in addition to gz, bz2, xz, zstd) for data import / export. #25310 (Bharat Nallan).Allow positional arguments under setting enable_positional_arguments. Closes #2592. #27530 (Kseniia Sumarokova).Accept user settings related to file formats in SETTINGS clause in CREATE query for s3 tables. This closes #27580. #28037 (Nikita Mikhaylov).Allow SSL connection for RabbitMQ engine. #28365 (Kseniia Sumarokova).Add getServerPort function to allow getting server port. When the port is not used by the server, throw an exception. #27900 (Amos Bird).Add conversion functions between &quot;snowflake id&quot; and DateTime, DateTime64. See #27058. #27704 (jasine).Add function SHA512. #27830 (zhanglistar).Add log_queries_probability setting that allows user to write to query_log only a sample of queries. Closes #16609. #27527 (Nikolay Degterinsky). Experimental Feature​ web type of disks to store readonly tables on web server in form of static files. See #23982. #25251 (Kseniia Sumarokova). This is mostly needed to faciliate testing of operation on shared storage and for easy importing of datasets. Not recommended to use before release 21.11.Added new commands BACKUP and RESTORE. #21945 (Vitaly Baranov). This is under development and not intended to be used in current version. Performance Improvement​ Speed up sumIf and countIf aggregation functions. #28272 (Raúl Marín).Create virtual projection for minmax indices. Now, when allow_experimental_projection_optimization is enabled, queries will use minmax index instead of reading the data when possible. #26286 (Amos Bird).Introducing two checks in sequenceMatch and sequenceCount that allow for early exit when some deterministic part of the sequence pattern is missing from the events list. This change unlocks many queries that would previously fail due to reaching operations cap, and generally speeds up the pipeline. #27729 (Jakub Kuklis).Enhance primary key analysis with always monotonic information of binary functions, notably non-zero constant division. #28302 (Amos Bird).Make hasAll filter condition leverage bloom filter data-skipping indexes. #27984 (Braulio Valdivielso Martínez).Speed up data parts loading by delaying table startup process. #28313 (Amos Bird).Fixed possible excessive number of conditions moved from WHERE to PREWHERE (optimization controlled by settings optimize_move_to_prewhere). #28139 (lthaooo).Enable optimize_distributed_group_by_sharding_key by default. #28105 (Azat Khuzhin). Improvement​ Check cluster name before creating Distributed table, do not allow to create a table with incorrect cluster name. Fixes #27832. #27927 (tavplubix).Add aggregate function quantileBFloat16Weighted similarly to other quantile...Weighted functions. This closes #27745. #27758 (Ivan Novitskiy).Allow to create dictionaries with empty attributes list. #27905 (Maksim Kita).Add interactive documentation in clickhouse-client about how to reset the password. This is useful in scenario when user has installed ClickHouse, set up the password and instantly forget it. See #27750. #27903 (alexey-milovidov).Support the case when the data is enclosed in array in JSONAsString input format. Closes #25517. #25633 (Kruglov Pavel).Add new column last_queue_update_exception to system.replicas table. #26843 (nvartolomei).Support reconnections on failover for MaterializedPostgreSQL tables. Closes #28529. #28614 (Kseniia Sumarokova).Generate a unique server UUID on first server start. #20089 (Bharat Nallan).Introduce connection_wait_timeout (default to 5 seconds, 0 - do not wait) setting for MySQL engine. #28474 (Azat Khuzhin).Do not allow creating MaterializedPostgreSQL with bad arguments. Closes #28423. #28430 (Kseniia Sumarokova).Use real tmp file instead of predefined &quot;rows_sources&quot; for vertical merges. This avoids generating garbage directories in tmp disks. #28299 (Amos Bird).Added libhdfs3_conf in server config instead of export env LIBHDFS3_CONF in clickhouse-server.service. This is for configuration of interaction with HDFS. #28268 (Zhichang Yu).Fix removing of parts in a Temporary state which can lead to an unexpected exception (Part %name% doesn't exist). Fixes #23661. #28221 #28221) (Azat Khuzhin).Fix zookeeper_log.address (before the first patch in this PR the address was always ::) and reduce number of calls getpeername(2) for this column (since each time entry for zookeeper_log is added getpeername() is called, cache this address in the zookeeper client to avoid this). #28212 (Azat Khuzhin).Support implicit conversions between index in operator [] and key of type Map (e.g. different Int types, String and FixedString). #28096 (Anton Popov).Support ON CONFLICT clause when inserting into PostgreSQL table engine or table function. Closes #27727. #28081 (Kseniia Sumarokova).Lower restrictions for Enum data type to allow attaching compatible data. Closes #26672. #28028 (Dmitry Novik).Add a setting empty_result_for_aggregation_by_constant_keys_on_empty_set to control the behavior of grouping by constant keys on empty set. This is to bring back the old baviour of #6842. #27932 (Amos Bird).Added replication_wait_for_inactive_replica_timeout setting. It allows to specify how long to wait for inactive replicas to execute ALTER/OPTIMZE/TRUNCATE query (default is 120 seconds). If replication_alter_partitions_sync is 2 and some replicas are not active for more than replication_wait_for_inactive_replica_timeout seconds, then UNFINISHED will be thrown. #27931 (tavplubix).Support lambda argument for APPLY column transformer which allows applying functions with more than one argument. This is for #27877. #27901 (Amos Bird).Enable tcp_keep_alive_timeout by default. #27882 (Azat Khuzhin).Improve remote query cancelation (in case of remote server abnormaly terminated). #27881 (Azat Khuzhin).Use Multipart copy upload for large S3 objects. #27858 (ianton-ru).Allow symlink traversal for library dictionaty path. #27815 (Kseniia Sumarokova).Now ALTER MODIFY COLUM T to Nullable(T) doesn't require mutation. #27787 (victorgao).Don't silently ignore errors and don't count delays in ReadBufferFromS3. #27484 (Vladimir Chebotarev).Improve ALTER ... MATERIALIZE TTL by recalculating metadata only without actual TTL action. #27019 (lthaooo).Allow reading the list of custom top level domains without a new line at EOF. #28213 (Azat Khuzhin). Bug Fix​ Fix cases, when reading compressed data from carbon-clickhouse fails with 'attempt to read after end of file'. Closes #26149. #28150 (FArthur-cmd).Fix checking access grants when executing GRANT WITH REPLACE statement with ON CLUSTER clause. This PR improves fix #27001. #27983 (Vitaly Baranov).Allow selecting with extremes = 1 from a column of the type LowCardinality(UUID). #27918 (Vitaly Baranov).Fix PostgreSQL-style cast (:: operator) with negative numbers. #27876 (Anton Popov).After #26864. Fix shutdown of NamedSessionStorage: session contexts stored in NamedSessionStorage are now destroyed before destroying the global context. #27875 (Vitaly Baranov).Bugfix for windowFunnel &quot;strict&quot; mode. This fixes #27469. #27563 (achimbab).Fix infinite loop while reading truncated bzip2 archive. #28543 (Azat Khuzhin).Fix UUID overlap in DROP TABLE for internal DDL from MaterializedMySQL. MaterializedMySQL is an experimental feature. #28533 (Azat Khuzhin).Fix There is no subcolumn error, while select from tables, which have Nested columns and scalar columns with dot in name and the same prefix as Nested (e.g. n.id UInt32, n.arr1 Array(UInt64), n.arr2 Array(UInt64)). #28531 (Anton Popov).Fix bug which can lead to error Existing table metadata in ZooKeeper differs in sorting key expression. after ALTER of ReplicatedVersionedCollapsingMergeTree. Fixes #28515. #28528 (alesapin).Fixed possible ZooKeeper watches leak (minor issue) on background processing of distributed DDL queue. Closes #26036. #28446 (tavplubix).Fix missing quoting of table names in MaterializedPostgreSQL engine. Closes #28316. #28433 (Kseniia Sumarokova).Fix the wrong behaviour of non joined rows from nullable column. Close #27691. #28349 (vdimir).Fix NOT-IN index optimization when not all key columns are used. This fixes #28120. #28315 (Amos Bird).Fix intersecting parts due to new part had been replaced with an empty part. #28310 (Azat Khuzhin).Fix inconsistent result in queries with ORDER BY and Merge tables with enabled setting optimize_read_in_order. #28266 (Anton Popov).Fix possible read of uninitialized memory for queries with Nullable(LowCardinality) type and the setting extremes set to 1. Fixes #28165. #28205 (Nikolai Kochetov).Multiple small fixes for projections. See detailed description in the PR. #28178 (Amos Bird).Fix extremely rare segfaults on shutdown due to incorrect order of context/config reloader shutdown. #28088 (nvartolomei).Fix handling null value with type of Nullable(String) in function JSONExtract. This fixes #27929 and #27930. This was introduced in https://github.com/ClickHouse/ClickHouse/pull/25452 . #27939 (Amos Bird).Multiple fixes for the new clickhouse-keeper tool. Fix a rare bug in clickhouse-keeper when the client can receive a watch response before request-response. #28197 (alesapin). Fix incorrect behavior in clickhouse-keeper when list watches (getChildren) triggered with set requests for children. #28190 (alesapin). Fix rare case when changes of clickhouse-keeper settings may lead to lost logs and server hung. #28360 (alesapin). Fix bug in clickhouse-keeper which can lead to endless logs when rotate_logs_interval decreased. #28152 (alesapin). Build/Testing/Packaging Improvement​ Enable Thread Fuzzer in Stress Test. Thread Fuzzer is ClickHouse feature that allows to test more permutations of thread scheduling and discover more potential issues. This closes #9813. This closes #9814. This closes #9515. This closes #9516. #27538 (alexey-milovidov).Add new log level test for testing environments. It is even more verbose than the default trace. #28559 (alesapin).Print out git status information at CMake configure stage. #28047 (Braulio Valdivielso Martínez).Temporarily switched ubuntu apt repository to mirror ru.archive.ubuntu.com as the default one (archive.ubuntu.com) is not responding from our CI. #28016 (Ilya Yatsishin). "},{"title":"ClickHouse release v21.9, 2021-09-09​","type":1,"pageTitle":"2021","url":"docs/en/whats-new/changelog/2021#clickhouse-release-v219-2021-09-09","content":"Backward Incompatible Change​ Do not output trailing zeros in text representation of Decimal types. Example: 1.23 will be printed instead of 1.230000 for decimal with scale 6. This closes #15794. It may introduce slight incompatibility if your applications somehow relied on the trailing zeros. Serialization in output formats can be controlled with the setting output_format_decimal_trailing_zeros. Implementation of toString and casting to String is changed unconditionally. #27680 (alexey-milovidov).Do not allow to apply parametric aggregate function with -Merge combinator to aggregate function state if state was produced by aggregate function with different parameters. For example, state of fooState(42)(x) cannot be finalized with fooMerge(s) or fooMerge(123)(s), parameters must be specified explicitly like fooMerge(42)(s) and must be equal. It does not affect some special aggregate functions like quantile and sequence* that use parameters for finalization only. #26847 (tavplubix).Under clickhouse-local, always treat local addresses with a port as remote. #26736 (Raúl Marín).Fix the issue that in case of some sophisticated query with column aliases identical to the names of expressions, bad cast may happen. This fixes #25447. This fixes #26914. This fix may introduce backward incompatibility: if there are different expressions with identical names, exception will be thrown. It may break some rare cases when enable_optimize_predicate_expression is set. #26639 (alexey-milovidov).Now, scalar subquery always returns Nullable result if it's type can be Nullable. It is needed because in case of empty subquery it's result should be Null. Previously, it was possible to get error about incompatible types (type deduction does not execute scalar subquery, and it could use not-nullable type). Scalar subquery with empty result which can't be converted to Nullable (like Array or Tuple) now throws error. Fixes #25411. #26423 (Nikolai Kochetov).Introduce syntax for here documents. Example SELECT $doc$ VALUE $doc$. #26671 (Maksim Kita). This change is backward incompatible if in query there are identifiers that contain $ #28768.Now indices can handle Nullable types, including isNull and isNotNull. #12433 and #12455 (Amos Bird) and #27250 (Azat Khuzhin). But this was done with on-disk format changes, and even though new server can read old data, old server cannot. Also, in case you have MINMAX data skipping indices, you may get Data after mutation/merge is not byte-identical error, since new index will have .idx2 extension while before it was .idx. That said, that you should not delay updating all existing replicas, in this case, otherwise, if old replica (&lt;21.9) will download data from new replica with 21.9+ it will not be able to apply index for downloaded part. New Feature​ Implementation of short circuit function evaluation, closes #12587. Add settings short_circuit_function_evaluation to configure short circuit function evaluation. #23367 (Kruglov Pavel).Add support for INTERSECT, EXCEPT, ANY, ALL operators. #24757 (Kirill Ershov). (Kseniia Sumarokova).Add support for encryption at the virtual file system level (data encryption at rest) using AES-CTR algorithm. #24206 (Latysheva Alexandra). (Vitaly Baranov) #26733 #26377 #26465.Added natural language processing (NLP) functions for tokenization, stemming, lemmatizing and search in synonyms extensions. #24997 (Nikolay Degterinsky).Added integration with S2 geometry library. #24980 (Andr0901). (Nikita Mikhaylov).Add SQLite table engine, table function, database engine. #24194 (Arslan Gumerov). (Kseniia Sumarokova).Added support for custom query for MySQL, PostgreSQL, ClickHouse, JDBC, Cassandra dictionary source. Closes #1270. #26995 (Maksim Kita).Add shared (replicated) storage of user, roles, row policies, quotas and settings profiles through ZooKeeper. #27426 (Kevin Michel).Add compression for INTO OUTFILE that automatically choose compression algorithm. Closes #3473. #27134 (Filatenkov Artur).Add INSERT ... FROM INFILE similarly to SELECT ... INTO OUTFILE. #27655 (Filatenkov Artur).Added complex_key_range_hashed dictionary. Closes #22029. #27629 (Maksim Kita).Support expressions in JOIN ON section. Close #21868. #24420 (Vladimir C).When client connects to server, it receives information about all warnings that are already were collected by server. (It can be disabled by using option --no-warnings). Add system.warnings table to collect warnings about server configuration. #26246 (Filatenkov Artur). #26282 (Filatenkov Artur).Allow using constant expressions from with and select in aggregate function parameters. Close #10945. #27531 (abel-cheng).Add tupleToNameValuePairs, a function that turns a named tuple into an array of pairs. #27505 (Braulio Valdivielso Martínez).Add support for bzip2 compression method for import/export. Closes #22428. #27377 (Nikolay Degterinsky).Added bitmapSubsetOffsetLimit(bitmap, offset, cardinality_limit) function. It creates a subset of bitmap limit the results to cardinality_limit with offset of offset. #27234 (DHBin).Add column default_database to system.users. #27054 (kevin wan).Supported cluster macros inside table functions 'cluster' and 'clusterAllReplicas'. #26913 (polyprogrammist).Add new functions currentRoles(), enabledRoles(), defaultRoles(). #26780 (Vitaly Baranov).New functions currentProfiles(), enabledProfiles(), defaultProfiles(). #26714 (Vitaly Baranov).Add functions that return (initial_)query_id of the current query. This closes #23682. #26410 (Alexey Boykov).Add REPLACE GRANT feature. #26384 (Caspian).EXPLAIN query now has EXPLAIN ESTIMATE ... mode that will show information about read rows, marks and parts from MergeTree tables. Closes #23941. #26131 (fastio).Added system.zookeeper_log table. All actions of ZooKeeper client are logged into this table. Implements #25449. #26129 (tavplubix).Zero-copy replication for ReplicatedMergeTree over HDFS storage. #25918 (Zhichang Yu).Allow to insert Nested type as array of structs in Arrow, ORC and Parquet input format. #25902 (Kruglov Pavel).Add a new datatype Date32 (store data as Int32), support date range same with DateTime64 support load parquet date32 to ClickHouse Date32 Add new function toDate32 like toDate. #25774 (LiuNeng).Allow setting default database for users. #25268. #25687 (kevin wan).Add an optional parameter to MongoDB engine to accept connection string options and support SSL connection. Closes #21189. Closes #21041. #22045 (Omar Bazaraa). Experimental Feature​ Added a compression codec AES_128_GCM_SIV which encrypts columns instead of compressing them. #19896 (PHO). Will be rewritten, do not use.Rename MaterializeMySQL to MaterializedMySQL. #26822 (tavplubix). Performance Improvement​ Improve the performance of fast queries when max_execution_time = 0 by reducing the number of clock_gettime system calls. #27325 (filimonov).Specialize date time related comparison to achieve better performance. This fixes #27083 . #27122 (Amos Bird).Share file descriptors in concurrent reads of the same files. There is no noticeable performance difference on Linux. But the number of opened files will be significantly (10..100 times) lower on typical servers and it makes operations easier. See #26214. #26768 (alexey-milovidov).Improve latency of short queries, that require reading from tables with large number of columns. #26371 (Anton Popov).Don't build sets for indices when analyzing a query. #26365 (Raúl Marín).Vectorize the SUM of Nullable integer types with native representation (David Manzanares, Raúl Marín). #26248 (Raúl Marín).Compile expressions involving columns with Enum types. #26237 (Maksim Kita).Compile aggregate functions groupBitOr, groupBitAnd, groupBitXor. #26161 (Maksim Kita).Improved memory usage with better block size prediction when reading empty DEFAULT columns. Closes #17317. #25917 (Vladimir Chebotarev).Reduce memory usage and number of read rows in queries with ORDER BY primary_key. #25721 (Anton Popov).Enable distributed_push_down_limit by default. #27104 (Azat Khuzhin).Make toTimeZone monotonicity when timeZone is a constant value to support partition puring when use sql like:. #26261 (huangzhaowei). Improvement​ Mark window functions as ready for general use. Remove the allow_experimental_window_functions setting. #27184 (Alexander Kuzmenkov).Improve compatibility with non-whole-minute timezone offsets. #27080 (Raúl Marín).If file descriptor in File table is regular file - allow to read multiple times from it. It allows clickhouse-local to read multiple times from stdin (with multiple SELECT queries or subqueries) if stdin is a regular file like clickhouse-local --query &quot;SELECT * FROM table UNION ALL SELECT * FROM table&quot; ... &lt; file. This closes #11124. Co-authored with (alexey-milovidov). #25960 (BoloniniD).Remove duplicate index analysis and avoid possible invalid limit checks during projection analysis. #27742 (Amos Bird).Enable query parameters to be passed in the body of HTTP requests. #27706 (Hermano Lustosa).Disallow arrayJoin on partition expressions. #27648 (Raúl Marín).Log client IP address if authentication fails. #27514 (Misko Lee).Use bytes instead of strings for binary data in the GRPC protocol. #27431 (Vitaly Baranov).Send response with error message if HTTP port is not set and user tries to send HTTP request to TCP port. #27385 (Braulio Valdivielso Martínez).Add _CAST function for internal usage, which will not preserve type nullability, but non-internal cast will preserve according to setting cast_keep_nullable. Closes #12636. #27382 (Kseniia Sumarokova).Add setting log_formatted_queries to log additional formatted query into system.query_log. It's useful for normalized query analysis because functions like normalizeQuery and normalizeQueryKeepNames don't parse/format queries in order to achieve better performance. #27380 (Amos Bird).Add two settings max_hyperscan_regexp_length and max_hyperscan_regexp_total_length to prevent huge regexp being used in hyperscan related functions, such as multiMatchAny. #27378 (Amos Bird).Memory consumed by bitmap aggregate functions now is taken into account for memory limits. This closes #26555. #27252 (alexey-milovidov).Add 10 seconds cache for S3 proxy resolver. #27216 (ianton-ru).Split global mutex into individual regexp construction. This helps avoid huge regexp construction blocking other related threads. #27211 (Amos Bird).Support schema for PostgreSQL database engine. Closes #27166. #27198 (Kseniia Sumarokova).Track memory usage in clickhouse-client. #27191 (Filatenkov Artur).Try recording query_kind in system.query_log even when query fails to start. #27182 (Amos Bird).Added columns replica_is_active that maps replica name to is replica active status to table system.replicas. Closes #27138. #27180 (Maksim Kita).Allow to pass query settings via server URI in Web UI. #27177 (kolsys).Add a new metric called MaxPushedDDLEntryID which is the maximum ddl entry id that current node push to zookeeper. #27174 (Fuwang Hu).Improved the existence condition judgment and empty string node judgment when clickhouse-keeper creates znode. #27125 (小路).Merge JOIN correctly handles empty set in the right. #27078 (Vladimir C).Now functions can be shard-level constants, which means if it's executed in the context of some distributed table, it generates a normal column, otherwise it produces a constant value. Notable functions are: hostName(), tcpPort(), version(), buildId(), uptime(), etc. #27020 (Amos Bird).Updated extractAllGroupsHorizontal - upper limit on the number of matches per row can be set via optional third argument. #26961 (Vasily Nemkov).Expose RocksDB statistics via system.rocksdb table. Read rocksdb options from ClickHouse config (rocksdb... keys). NOTE: ClickHouse does not rely on RocksDB, it is just one of the additional integration storage engines. #26821 (Azat Khuzhin).Less verbose internal RocksDB logs. NOTE: ClickHouse does not rely on RocksDB, it is just one of the additional integration storage engines. This closes #26252. #26789 (alexey-milovidov).Changing default roles affects new sessions only. #26759 (Vitaly Baranov).Watchdog is disabled in docker by default. Fix for not handling ctrl+c. #26757 (Mikhail f. Shiryaev).SET PROFILE now applies constraints too if they're set for a passed profile. #26730 (Vitaly Baranov).Improve handling of KILL QUERY requests. #26675 (Raúl Marín).mapPopulatesSeries function supports Map type. #26663 (Ildus Kurbangaliev).Fix excessive (x2) connect attempts with skip_unavailable_shards. #26658 (Azat Khuzhin).Avoid hanging clickhouse-benchmark if connection fails (i.e. on EMFILE). #26656 (Azat Khuzhin).Allow more threads to be used by the Kafka engine. #26642 (feihengye).Add round-robin support for clickhouse-benchmark (it does not differ from the regular multi host/port run except for statistics report). #26607 (Azat Khuzhin).Executable dictionaries (executable, executable_pool) enable creation with DDL query using clickhouse-local. Closes #22355. #26510 (Maksim Kita).Set client query kind for mysql and postgresql compatibility protocol handlers. #26498 (anneji-dev).Apply LIMIT on the shards for queries like SELECT * FROM dist ORDER BY key LIMIT 10 w/ distributed_push_down_limit=1. Avoid running Distinct/LIMIT BY steps for queries like SELECT DISTINCT shading_key FROM dist ORDER BY key. Now distributed_push_down_limit is respected by optimize_distributed_group_by_sharding_key optimization. #26466 (Azat Khuzhin).Updated protobuf to 3.17.3. Changelogs are available on https://github.com/protocolbuffers/protobuf/releases. #26424 (Ilya Yatsishin).Enable use_hedged_requests setting that allows to mitigate tail latencies on large clusters. #26380 (alexey-milovidov).Improve behaviour with non-existing host in user allowed host list. #26368 (ianton-ru).Add ability to set Distributed directory monitor settings via CREATE TABLE (i.e. CREATE TABLE dist (key Int) Engine=Distributed(cluster, db, table) SETTINGS monitor_batch_inserts=1 and similar). #26336 (Azat Khuzhin).Save server address in history URLs in web UI if it differs from the origin of web UI. This closes #26044. #26322 (alexey-milovidov).Add events to profile calls to sleep / sleepEachRow. #26320 (Raúl Marín).Allow to reuse connections of shards among different clusters. It also avoids creating new connections when using cluster table function. #26318 (Amos Bird).Control the execution period of clear old temporary directories by parameter with default value. #26212. #26313 (fastio).Add a setting function_range_max_elements_in_block to tune the safety threshold for data volume generated by function range. This closes #26303. #26305 (alexey-milovidov).Check hash function at table creation, not at sampling. Add settings for MergeTree, if someone create a table with incorrect sampling column but sampling never be used, disable this settings for starting the server without exception. #26256 (zhaoyu).Added output_format_avro_string_column_pattern setting to put specified String columns to Avro as string instead of default bytes. Implements #22414. #26245 (Ilya Golshtein).Add information about column sizes in system.columns table for Log and TinyLog tables. This closes #9001. #26241 (Nikolay Degterinsky).Don't throw exception when querying system.detached_parts table if there is custom disk configuration and detached directory does not exist on some disks. This closes #26078. #26236 (alexey-milovidov).Check for non-deterministic functions in keys, including constant expressions like now(), today(). This closes #25875. This closes #11333. #26235 (alexey-milovidov).convert timestamp and timestamptz data types to DateTime64 in PostgreSQL table engine. #26234 (jasine).Apply aggressive IN index analysis for projections so that better projection candidate can be selected. #26218 (Amos Bird).Remove GLOBAL keyword for IN when scalar function is passed. In previous versions, if user specified GLOBAL IN f(x) exception was thrown. #26217 (Amos Bird).Add error id (like BAD_ARGUMENTS) to exception messages. This closes #25862. #26172 (alexey-milovidov).Fix incorrect output with --progress option for clickhouse-local. Progress bar will be cleared once it gets to 100% - same as it is done for clickhouse-client. Closes #17484. #26128 (Kseniia Sumarokova).Add merge_selecting_sleep_ms setting. #26120 (lthaooo).Remove complicated usage of Linux AIO with one block readahead and replace it with plain simple synchronous IO with O_DIRECT. In previous versions, the setting min_bytes_to_use_direct_io may not work correctly if max_threads is greater than one. Reading with direct IO (that is disabled by default for queries and enabled by default for large merges) will work in less efficient way. This closes #25997. #26003 (alexey-milovidov).Flush Distributed table on REPLACE TABLE query. Resolves #24566 - Do not replace (or create) table on [CREATE OR] REPLACE TABLE ... AS SELECT query if insertion into new table fails. Resolves #23175. #25895 (tavplubix).Add views column to system.query_log containing the names of the (materialized or live) views executed by the query. Adds a new log table (system.query_views_log) that contains information about each view executed during a query. Modifies view execution: When an exception is thrown while executing a view, any view that has already startedwill continue running until it finishes. This used to be the behaviour under parallel_view_processing=true and now it's always the same behaviour. - Dependent views now report reading progress to the context. #25714 (Raúl Marín).Do connection draining asynchonously upon finishing executing distributed queries. A new server setting is added max_threads_for_connection_collector which specifies the number of workers to recycle connections in background. If the pool is full, connection will be drained synchronously but a bit different than before: It's drained after we send EOS to client, query will succeed immediately after receiving enough data, and any exception will be logged instead of throwing to the client. Added setting drain_timeout (3 seconds by default). Connection draining will disconnect upon timeout. #25674 (Amos Bird).Support for multiple includes in configuration. It is possible to include users configuration, remote servers configuration from multiple sources. Simply place &lt;include /&gt; element with from_zk, from_env or incl attribute and it will be replaced with the substitution. #24404 (nvartolomei).Fix multiple block insertion into distributed table with insert_distributed_one_random_shard = 1. This is a marginal feature. Mark as improvement. #23140 (Amos Bird).Support LowCardinality and FixedString keys/values for Map type. #21543 (hexiaoting).Enable reloading of local disk config. #19526 (taiyang-li). Bug Fix​ Fix a couple of bugs that may cause replicas to diverge. #27808 (tavplubix).Fix a rare bug in DROP PART which can lead to the error Unexpected merged part intersects drop range. #27807 (alesapin).Prevent crashes for some formats when NULL (tombstone) message was coming from Kafka. Closes #19255. #27794 (filimonov).Fix column filtering with union distinct in subquery. Closes #27578. #27689 (Kseniia Sumarokova).Fix bad type cast when functions like arrayHas are applied to arrays of LowCardinality of Nullable of different non-numeric types like DateTime and DateTime64. In previous versions bad cast occurs. In new version it will lead to exception. This closes #26330. #27682 (alexey-milovidov).Fix postgresql table function resulting in non-closing connections. Closes #26088. #27662 (Kseniia Sumarokova).Fixed another case of Unexpected merged part ... intersecting drop range ... error. #27656 (tavplubix).Fix an error with aliased column in Distributed table. #27652 (Vladimir C).After setting max_memory_usage* to non-zero value it was not possible to reset it back to 0 (unlimited). It's fixed. #27638 (tavplubix).Fixed underflow of the time value when constructing it from components. Closes #27193. #27605 (Vasily Nemkov).Fix crash during projection materialization when some parts contain missing columns. This fixes #27512. #27528 (Amos Bird).fix metric BackgroundMessageBrokerSchedulePoolTask, maybe mistyped. #27452 (Ben).Fix distributed queries with zero shards and aggregation. #27427 (Azat Khuzhin).Compatibility when /proc/meminfo does not contain KB suffix. #27361 (Mike Kot).Fix incorrect result for query with row-level security, PREWHERE and LowCardinality filter. Fixes #27179. #27329 (Nikolai Kochetov).Fixed incorrect validation of partition id for MergeTree tables that created with old syntax. #27328 (tavplubix).Fix MySQL protocol when using parallel formats (CSV / TSV). #27326 (Raúl Marín).Fix Cannot find column error for queries with sampling. Was introduced in #24574. Fixes #26522. #27301 (Nikolai Kochetov).Fix errors like Expected ColumnLowCardinality, gotUInt8 or Bad cast from type DB::ColumnVector&lt;char8_t&gt; to DB::ColumnLowCardinality for some queries with LowCardinality in PREWHERE. And more importantly, fix the lack of whitespace in the error message. Fixes #23515. #27298 (Nikolai Kochetov).Fix distributed_group_by_no_merge = 2 with distributed_push_down_limit = 1 or optimize_distributed_group_by_sharding_key = 1 with LIMIT BY and LIMIT OFFSET. #27249 (Azat Khuzhin). These are obscure combination of settings that no one is using.Fix mutation stuck on invalid partitions in non-replicated MergeTree. #27248 (Azat Khuzhin).In case of ambiguity, lambda functions prefer its arguments to other aliases or identifiers. #27235 (Raúl Marín).Fix column structure in merge join, close #27091. #27217 (Vladimir C).In rare cases system.detached_parts table might contain incorrect information for some parts, it's fixed. Fixes #27114. #27183 (tavplubix).Fix uninitialized memory in functions multiSearch* with empty array, close #27169. #27181 (Vladimir C).Fix synchronization in GRPCServer. This PR fixes #27024. #27064 (Vitaly Baranov).Fixed cache, complex_key_cache, ssd_cache, complex_key_ssd_cache configuration parsing. Options allow_read_expired_keys, max_update_queue_size, update_queue_push_timeout_milliseconds, query_wait_timeout_milliseconds were not parsed for dictionaries with non cache type. #27032 (Maksim Kita).Fix possible mutation stack due to race with DROP_RANGE. #27002 (Azat Khuzhin).Now partition ID in queries like ALTER TABLE ... PARTITION ID xxx validates for correctness. Fixes #25718. #26963 (alesapin).Fix &quot;Unknown column name&quot; error with multiple JOINs in some cases, close #26899. #26957 (Vladimir C).Fix reading of custom TLDs (stops processing with lower buffer or bigger file). #26948 (Azat Khuzhin).Fix error Missing columns: 'xxx' when DEFAULT column references other non materialized column without DEFAULT expression. Fixes #26591. #26900 (alesapin).Fix loading of dictionary keys in library-bridge for library dictionary source. #26834 (Kseniia Sumarokova).Aggregate function parameters might be lost when applying some combinators causing exceptions like Conversion from AggregateFunction(topKArray, Array(String)) to AggregateFunction(topKArray(10), Array(String)) is not supported. It's fixed. Fixes #26196 and #26433. #26814 (tavplubix).Add event_time_microseconds value for REMOVE_PART in system.part_log. In previous versions is was not set. #26720 (Azat Khuzhin).Do not remove data on ReplicatedMergeTree table shutdown to avoid creating data to metadata inconsistency. #26716 (nvartolomei).Sometimes SET ROLE could work incorrectly, this PR fixes that. #26707 (Vitaly Baranov).Some fixes for parallel formatting (https://github.com/ClickHouse/ClickHouse/issues/26694). #26703 (Raúl Marín).Fix potential nullptr dereference in window functions. This fixes #25276. #26668 (Alexander Kuzmenkov).Fix clickhouse-client history file conversion (when upgrading from the format of 3 years old version of clickhouse-client) if file is empty. #26589 (Azat Khuzhin).Fix incorrect function names of groupBitmapAnd/Or/Xor (can be displayed in some occasions). This fixes. #26557 (Amos Bird).Update chown cmd check in clickhouse-server docker entrypoint. It fixes the bug that cluster pod restart failed (or timeout) on kubernetes. #26545 (Ky Li).Fix crash in RabbitMQ shutdown in case RabbitMQ setup was not started. Closes #26504. #26529 (Kseniia Sumarokova).Fix issues with CREATE DICTIONARY query if dictionary name or database name was quoted. Closes #26491. #26508 (Maksim Kita).Fix broken column name resolution after rewriting column aliases. This fixes #26432. #26475 (Amos Bird).Fix some fuzzed msan crash. Fixes #22517. #26428 (Nikolai Kochetov).Fix infinite non joined block stream in partial_merge_join close #26325. #26374 (Vladimir C).Fix possible crash when login as dropped user. This PR fixes #26073. #26363 (Vitaly Baranov).Fix optimize_distributed_group_by_sharding_key for multiple columns (leads to incorrect result w/ optimize_skip_unused_shards=1/allow_nondeterministic_optimize_skip_unused_shards=1 and multiple columns in sharding key expression). #26353 (Azat Khuzhin).Fixed rare bug in lost replica recovery that may cause replicas to diverge. #26321 (tavplubix).Fix zstd decompression (for import/export in zstd framing format that is unrelated to tables data) in case there are escape sequences at the end of internal buffer. Closes #26013. #26314 (Kseniia Sumarokova).Fix logical error on join with totals, close #26017. #26250 (Vladimir C).Remove excessive newline in thread_name column in system.stack_trace table. This fixes #24124. #26210 (alexey-milovidov).Fix potential crash if more than one untuple expression is used. #26179 (alexey-milovidov).Don't throw exception in toString for Nullable Enum if Enum does not have a value for zero, close #25806. #26123 (Vladimir C).Fixed incorrect sequence_id in MySQL protocol packets that ClickHouse sends on exception during query execution. It might cause MySQL client to reset connection to ClickHouse server. Fixes #21184. #26051 (tavplubix).Fix for the case that cutToFirstSignificantSubdomainCustom()/cutToFirstSignificantSubdomainCustomWithWWW()/firstSignificantSubdomainCustom() returns incorrect type for consts, and hence optimize_skip_unused_shards does not work:. #26041 (Azat Khuzhin).Fix possible mismatched header when using normal projection with prewhere. This fixes #26020. #26038 (Amos Bird).Fix sharding_key from column w/o function for remote() (before select * from remote('127.1', system.one, dummy) leads to Unknown column: dummy, there are only columns . error). #25824 (Azat Khuzhin).Fixed Not found column ... and Missing column ... errors when selecting from MaterializeMySQL. Fixes #23708, #24830, #25794. #25822 (tavplubix).Fix optimize_skip_unused_shards_rewrite_in for non-UInt64 types (may select incorrect shards eventually or throw Cannot infer type of an empty tuple or Function tuple requires at least one argument). #25798 (Azat Khuzhin). Build/Testing/Packaging Improvement​ Now we ran stateful and stateless tests in random timezones. Fixes #12439. Reading String as DateTime and writing DateTime as String in Protobuf format now respect timezone. Reading UInt16 as DateTime in Arrow and Parquet formats now treat it as Date and then converts to DateTime with respect to DateTime's timezone, because Date is serialized in Arrow and Parquet as UInt16. GraphiteMergeTree now respect time zone for rounding of times. Fixes #5098. Author: @alexey-milovidov. #15408 (alesapin).clickhouse-test supports SQL tests with Jinja2 templates. #26579 (Vladimir C).Add support for build with clang-13. This closes #27705. #27714 (alexey-milovidov). #27777 (Sergei Semin)Add CMake options to build with or without specific CPU instruction set. This is for #17469 and #27509. #27508 (alexey-milovidov).Fix linking of auxiliar programs when using dynamic libraries. #26958 (Raúl Marín).Update RocksDB to 2021-07-16 master. #26411 (alexey-milovidov). "},{"title":"ClickHouse release v21.8, 2021-08-12​","type":1,"pageTitle":"2021","url":"docs/en/whats-new/changelog/2021#clickhouse-release-v218-2021-08-12","content":"Upgrade Notes​ New version is using Map data type for system logs tables (system.query_log, system.query_thread_log, system.processes, system.opentelemetry_span_log). These tables will be auto-created with new data types. Virtual columns are created to support old queries. Closes #18698. #23934, #25773 (hexiaoting, sundy-li, Maksim Kita). If you want to downgrade from version 21.8 to older versions, you will need to cleanup system tables with logs manually. Look at /var/lib/clickhouse/data/system/*_log. New Features​ Add support for a part of SQL/JSON standard. #24148 (l1tsolaiki, Kseniia Sumarokova).Collect common system metrics (in system.asynchronous_metrics and system.asynchronous_metric_log) on CPU usage, disk usage, memory usage, IO, network, files, load average, CPU frequencies, thermal sensors, EDAC counters, system uptime; also added metrics about the scheduling jitter and the time spent collecting the metrics. It works similar to atop in ClickHouse and allows access to monitoring data even if you have no additional tools installed. Close #9430. #24416 (alexey-milovidov, Yegor Levankov).Add MaterializedPostgreSQL table engine and database engine. This database engine allows replicating a whole database or any subset of database tables. #20470 (Kseniia Sumarokova).Add new functions leftPad(), rightPad(), leftPadUTF8(), rightPadUTF8(). #26075 (Vitaly Baranov).Add the FIRST keyword to the ADD INDEX command to be able to add the index at the beginning of the indices list. #25904 (xjewer).Introduce system.data_skipping_indices table containing information about existing data skipping indices. Close #7659. #25693 (Dmitry Novik).Add bin/unbin functions. #25609 (zhaoyu).Support Map and UInt128, Int128, UInt256, Int256 types in mapAdd and mapSubtract functions. #25596 (Ildus Kurbangaliev).Support DISTINCT ON (columns) expression, close #25404. #25589 (Zijie Lu).Add an ability to reset a custom setting to default and remove it from the table's metadata. It allows rolling back the change without knowing the system/config's default. Closes #14449. #17769 (xjewer).Render pipelines as graphs in Web UI if EXPLAIN PIPELINE graph = 1 query is submitted. #26067 (alexey-milovidov). Performance Improvements​ Compile aggregate functions. Use option compile_aggregate_expressions to enable it. #24789 (Maksim Kita).Improve latency of short queries that require reading from tables with many columns. #26371 (Anton Popov). Improvements​ Use Map data type for system logs tables (system.query_log, system.query_thread_log, system.processes, system.opentelemetry_span_log). These tables will be auto-created with new data types. Virtual columns are created to support old queries. Closes #18698. #23934, #25773 (hexiaoting, sundy-li, Maksim Kita).For a dictionary with a complex key containing only one attribute, allow not wrapping the key expression in tuple for functions dictGet, dictHas. #26130 (Maksim Kita).Implement function bin/hex from AggregateFunction states. #26094 (zhaoyu).Support arguments of UUID type for empty and notEmpty functions. UUID is empty if it is all zeros (nil UUID). Closes #3446. #25974 (zhaoyu).Add support for SET SQL_SELECT_LIMIT in MySQL protocol. Closes #17115. #25972 (Kseniia Sumarokova).More instrumentation for network interaction: add counters for recv/send bytes; add gauges for recvs/sends. Added missing documentation. Close #5897. #25962 (alexey-milovidov).Add setting optimize_move_to_prewhere_if_final. If query has FINAL, the optimization move_to_prewhere will be enabled only if both optimize_move_to_prewhere and optimize_move_to_prewhere_if_final are enabled. Closes #8684. #25940 (Kseniia Sumarokova).Allow complex quoted identifiers of JOINed tables. Close #17861. #25924 (alexey-milovidov).Add support for Unicode (e.g. Chinese, Cyrillic) components in Nested data types. Close #25594. #25923 (alexey-milovidov).Allow quantiles* functions to work with aggregate_functions_null_for_empty. Close #25892. #25919 (alexey-milovidov).Allow parameters for parametric aggregate functions to be arbitrary constant expressions (e.g., 1 + 2), not just literals. It also allows using the query parameters (in parameterized queries like {param:UInt8}) inside parametric aggregate functions. Closes #11607. #25910 (alexey-milovidov).Correctly throw the exception on the attempt to parse an invalid Date. Closes #6481. #25909 (alexey-milovidov).Support for multiple includes in configuration. It is possible to include users configuration, remote server configuration from multiple sources. Simply place &lt;include /&gt; element with from_zk, from_env or incl attribute, and it will be replaced with the substitution. #24404 (nvartolomei).Support for queries with a column named &quot;null&quot; (it must be specified in back-ticks or double quotes) and ON CLUSTER. Closes #24035. #25907 (alexey-milovidov).Support LowCardinality, Decimal, and UUID for JSONExtract. Closes #24606. #25900 (Kseniia Sumarokova).Convert history file from readline format to replxx format. #25888 (Azat Khuzhin).Fix an issue which can lead to intersecting parts after DROP PART or background deletion of an empty part. #25884 (alesapin).Better handling of lost parts for ReplicatedMergeTree tables. Fixes rare inconsistencies in ReplicationQueue. Fixes #10368. #25820 (alesapin).Allow starting clickhouse-client with unreadable working directory. #25817 (ianton-ru).Fix &quot;No available columns&quot; error for Merge storage. #25801 (Azat Khuzhin).MySQL Engine now supports the exchange of column comments between MySQL and ClickHouse. #25795 (Storozhuk Kostiantyn).Fix inconsistent behaviour of GROUP BY constant on empty set. Closes #6842. #25786 (Kseniia Sumarokova).Cancel already running merges in partition on DROP PARTITION and TRUNCATE for ReplicatedMergeTree. Resolves #17151. #25684 (tavplubix).Support ENUM` data type for MaterializeMySQL. #25676 (Storozhuk Kostiantyn).Support materialized and aliased columns in JOIN, close #13274. #25634 (Vladimir C).Fix possible logical race condition between ALTER TABLE ... DETACH and background merges. #25605 (Azat Khuzhin).Make NetworkReceiveElapsedMicroseconds metric to correctly include the time spent waiting for data from the client to INSERT. Close #9958. #25602 (alexey-milovidov).Support TRUNCATE TABLE for S3 and HDFS. Close #25530. #25550 (Kseniia Sumarokova).Support for dynamic reloading of config to change number of threads in pool for background jobs execution (merges, mutations, fetches). #25548 (Nikita Mikhaylov).Allow extracting of non-string element as string using JSONExtract. This is for #25414. #25452 (Amos Bird).Support regular expression in Database argument for StorageMerge. Close #776. #25064 (flynn).Web UI: if the value looks like a URL, automatically generate a link. #25965 (alexey-milovidov).Make sudo service clickhouse-server start to work on systems with systemd like Centos 8. Close #14298. Close #17799. #25921 (alexey-milovidov). Bug Fixes​ Fix incorrect SET ROLE in some cases. #26707 (Vitaly Baranov).Fix potential nullptr dereference in window functions. Fix #25276. #26668 (Alexander Kuzmenkov).Fix incorrect function names of groupBitmapAnd/Or/Xor. Fix #26557 (Amos Bird).Fix crash in RabbitMQ shutdown in case RabbitMQ setup was not started. Closes #26504. #26529 (Kseniia Sumarokova).Fix issues with CREATE DICTIONARY query if dictionary name or database name was quoted. Closes #26491. #26508 (Maksim Kita).Fix broken name resolution after rewriting column aliases. Fix #26432. #26475 (Amos Bird).Fix infinite non-joined block stream in partial_merge_join close #26325. #26374 (Vladimir C).Fix possible crash when login as dropped user. Fix #26073. #26363 (Vitaly Baranov).Fix optimize_distributed_group_by_sharding_key for multiple columns (leads to incorrect result w/ optimize_skip_unused_shards=1/allow_nondeterministic_optimize_skip_unused_shards=1 and multiple columns in sharding key expression). #26353 (Azat Khuzhin).CAST from Date to DateTime (or DateTime64) was not using the timezone of the DateTime type. It can also affect the comparison between Date and DateTime. Inference of the common type for Date and DateTime also was not using the corresponding timezone. It affected the results of function if and array construction. Closes #24128. #24129 (Maksim Kita).Fixed rare bug in lost replica recovery that may cause replicas to diverge. #26321 (tavplubix).Fix zstd decompression in case there are escape sequences at the end of internal buffer. Closes #26013. #26314 (Kseniia Sumarokova).Fix logical error on join with totals, close #26017. #26250 (Vladimir C).Remove excessive newline in thread_name column in system.stack_trace table. Fix #24124. #26210 (alexey-milovidov).Fix joinGet with LowCarinality columns, close #25993. #26118 (Vladimir C).Fix possible crash in pointInPolygon if the setting validate_polygons is turned off. #26113 (alexey-milovidov).Fix throwing exception when iterate over non-existing remote directory. #26087 (ianton-ru).Fix rare server crash because of abort in ZooKeeper client. Fixes #25813. #26079 (alesapin).Fix wrong thread count estimation for right subquery join in some cases. Close #24075. #26052 (Vladimir C).Fixed incorrect sequence_id in MySQL protocol packets that ClickHouse sends on exception during query execution. It might cause MySQL client to reset connection to ClickHouse server. Fixes #21184. #26051 (tavplubix).Fix possible mismatched header when using normal projection with PREWHERE. Fix #26020. #26038 (Amos Bird).Fix formatting of type Map with integer keys to JSON. #25982 (Anton Popov).Fix possible deadlock during query profiler stack unwinding. Fix #25968. #25970 (Maksim Kita).Fix crash on call dictGet() with bad arguments. #25913 (Vitaly Baranov).Fixed scram-sha-256 authentication for PostgreSQL engines. Closes #24516. #25906 (Kseniia Sumarokova).Fix extremely long backoff for background tasks when the background pool is full. Fixes #25836. #25893 (alesapin).Fix ARM exception handling with non default page size. Fixes #25512, #25044, #24901, #23183, #20221, #19703, #19028, #18391, #18121, #17994, #12483. #25854 (Maksim Kita).Fix sharding_key from column w/o function for remote() (before select * from remote('127.1', system.one, dummy) leads to Unknown column: dummy, there are only columns . error). #25824 (Azat Khuzhin).Fixed Not found column ... and Missing column ... errors when selecting from MaterializeMySQL. Fixes #23708, #24830, #25794. #25822 (tavplubix).Fix optimize_skip_unused_shards_rewrite_in for non-UInt64 types (may select incorrect shards eventually or throw Cannot infer type of an empty tuple or Function tuple requires at least one argument). #25798 (Azat Khuzhin).Fix rare bug with DROP PART query for ReplicatedMergeTree tables which can lead to error message Unexpected merged part intersecting drop range. #25783 (alesapin).Fix bug in TTL with GROUP BY expression which refuses to execute TTL after first execution in part. #25743 (alesapin).Allow StorageMerge to access tables with aliases. Closes #6051. #25694 (Kseniia Sumarokova).Fix slow dict join in some cases, close #24209. #25618 (Vladimir C).Fix ALTER MODIFY COLUMN of columns, which participates in TTL expressions. #25554 (Anton Popov).Fix assertion in PREWHERE with non-UInt8 type, close #19589. #25484 (Vladimir C).Fix some fuzzed msan crash. Fixes #22517. #26428 (Nikolai Kochetov).Update chown cmd check in clickhouse-server docker entrypoint. It fixes error 'cluster pod restart failed (or timeout)' on kubernetes. #26545 (Ky Li). "},{"title":"ClickHouse release v21.7, 2021-07-09​","type":1,"pageTitle":"2021","url":"docs/en/whats-new/changelog/2021#clickhouse-release-v217-2021-07-09","content":"Backward Incompatible Change​ Improved performance of queries with explicitly defined large sets. Added compatibility setting legacy_column_name_of_tuple_literal. It makes sense to set it to true, while doing rolling update of cluster from version lower than 21.7 to any higher version. Otherwise distributed queries with explicitly defined sets at IN clause may fail during update. #25371 (Anton Popov).Forward/backward incompatible change of maximum buffer size in clickhouse-keeper (an experimental alternative to ZooKeeper). Better to do it now (before production), than later. #25421 (alesapin). New Feature​ Support configuration in YAML format as alternative to XML. This closes #3607. #21858 (BoloniniD).Provides a way to restore replicated table when the data is (possibly) present, but the ZooKeeper metadata is lost. Resolves #13458. #13652 (Mike Kot).Support structs and maps in Arrow/Parquet/ORC and dictionaries in Arrow input/output formats. Present new setting output_format_arrow_low_cardinality_as_dictionary. #24341 (Kruglov Pavel).Added support for Array type in dictionaries. #25119 (Maksim Kita).Added function bitPositionsToArray. Closes #23792. Author [Kevin Wan] (@MaxWk). #25394 (Maksim Kita).Added function dateName to return names like 'Friday' or 'April'. Author [Daniil Kondratyev] (@dankondr). #25372 (Maksim Kita).Add toJSONString function to serialize columns to their JSON representations. #25164 (Amos Bird).Now query_log has two new columns: initial_query_start_time, initial_query_start_time_microsecond that record the starting time of a distributed query if any. #25022 (Amos Bird).Add aggregate function segmentLengthSum. #24250 (flynn).Add a new boolean setting prefer_global_in_and_join which defaults all IN/JOIN as GLOBAL IN/JOIN. #23434 (Amos Bird).Support ALTER DELETE queries for Join table engine. #23260 (foolchi).Add quantileBFloat16 aggregate function as well as the corresponding quantilesBFloat16 and medianBFloat16. It is very simple and fast quantile estimator with relative error not more than 0.390625%. This closes #16641. #23204 (Ivan Novitskiy).Implement sequenceNextNode() function useful for flow analysis. #19766 (achimbab). Experimental Feature​ Add support for virtual filesystem over HDFS. #11058 (overshov) (Kseniia Sumarokova).Now clickhouse-keeper (an experimental alternative to ZooKeeper) supports ZooKeeper-like digest ACLs. #24448 (alesapin). Performance Improvement​ Added optimization that transforms some functions to reading of subcolumns to reduce amount of read data. E.g., statement col IS NULL is transformed to reading of subcolumn col.null. Optimization can be enabled by setting optimize_functions_to_subcolumns which is currently off by default. #24406 (Anton Popov).Rewrite more columns to possible alias expressions. This may enable better optimization, such as projections. #24405 (Amos Bird).Index of type bloom_filter can be used for expressions with hasAny function with constant arrays. This closes: #24291. #24900 (Vasily Nemkov).Add exponential backoff to reschedule read attempt in case RabbitMQ queues are empty. (ClickHouse has support for importing data from RabbitMQ). Closes #24340. #24415 (Kseniia Sumarokova). Improvement​ Allow to limit bandwidth for replication. Add two Replicated*MergeTree settings: max_replicated_fetches_network_bandwidth and max_replicated_sends_network_bandwidth which allows to limit maximum speed of replicated fetches/sends for table. Add two server-wide settings (in default user profile): max_replicated_fetches_network_bandwidth_for_server and max_replicated_sends_network_bandwidth_for_server which limit maximum speed of replication for all tables. The settings are not followed perfectly accurately. Turned off by default. Fixes #1821. #24573 (alesapin).Resource constraints and isolation for ODBC and Library bridges. Use separate clickhouse-bridge group and user for bridge processes. Set oom_score_adj so the bridges will be first subjects for OOM killer. Set set maximum RSS to 1 GiB. Closes #23861. #25280 (Kseniia Sumarokova).Add standalone clickhouse-keeper symlink to the main clickhouse binary. Now it's possible to run coordination without the main clickhouse server. #24059 (alesapin).Use global settings for query to VIEW. Fixed the behavior when queries to VIEW use local settings, that leads to errors if setting on CREATE VIEW and SELECT were different. As for now, VIEW won't use these modified settings, but you can still pass additional settings in SETTINGS section of CREATE VIEW query. Close #20551. #24095 (Vladimir).On server start, parts with incorrect partition ID would not be ever removed, but always detached. #25070. #25166 (Nikolai Kochetov).Increase size of background schedule pool to 128 (background_schedule_pool_size setting). It allows avoiding replication queue hung on slow zookeeper connection. #25072 (alesapin).Add merge tree setting max_parts_to_merge_at_once which limits the number of parts that can be merged in the background at once. Doesn't affect OPTIMIZE FINAL query. Fixes #1820. #24496 (alesapin).Allow NOT IN operator to be used in partition pruning. #24894 (Amos Bird).Recognize IPv4 addresses like 127.0.1.1 as local. This is controversial and closes #23504. Michael Filimonov will test this feature. #24316 (alexey-milovidov).ClickHouse database created with MaterializeMySQL (it is an experimental feature) now contains all column comments from the MySQL database that materialized. #25199 (Storozhuk Kostiantyn).Add settings (connection_auto_close/connection_max_tries/connection_pool_size) for MySQL storage engine. #24146 (Azat Khuzhin).Improve startup time of Distributed engine. #25663 (Azat Khuzhin).Improvement for Distributed tables. Drop replicas from dirname for internal_replication=true (allows INSERT into Distributed with cluster from any number of replicas, before only 15 replicas was supported, everything more will fail with ENAMETOOLONG while creating directory for async blocks). #25513 (Azat Khuzhin).Added support Interval type for LowCardinality. It is needed for intermediate values of some expressions. Closes #21730. #25410 (Vladimir).Add == operator on time conditions for sequenceMatch and sequenceCount functions. For eg: sequenceMatch('(?1)(?t==1)(?2)')(time, data = 1, data = 2). #25299 (Christophe Kalenzaga).Add settings http_max_fields, http_max_field_name_size, http_max_field_value_size. #25296 (Ivan).Add support for function if with Decimal and Int types on its branches. This closes #20549. This closes #10142. #25283 (alexey-milovidov).Update prompt in clickhouse-client and display a message when reconnecting. This closes #10577. #25281 (alexey-milovidov).Correct memory tracking in aggregate function topK. This closes #25259. #25260 (alexey-milovidov).Fix topLevelDomain for IDN hosts (i.e. example.рф), before it returns empty string for such hosts. #25103 (Azat Khuzhin).Detect Linux kernel version at runtime (for worked nested epoll, that is required for async_socket_for_remote/use_hedged_requests, otherwise remote queries may stuck). #25067 (Azat Khuzhin).For distributed query, when optimize_skip_unused_shards=1, allow to skip shard with condition like (sharding key) IN (one-element-tuple). (Tuples with many elements were supported. Tuple with single element did not work because it is parsed as literal). #24930 (Amos Bird).Improved log messages of S3 errors, no more double whitespaces in case of empty keys and buckets. #24897 (Vladimir Chebotarev).Some queries require multi-pass semantic analysis. Try reusing built sets for IN in this case. #24874 (Amos Bird).Respect max_distributed_connections for insert_distributed_sync (otherwise for huge clusters and sync insert it may run out of max_thread_pool_size). #24754 (Azat Khuzhin).Avoid hiding errors like Limit for rows or bytes to read exceeded for scalar subqueries. #24545 (nvartolomei).Make String-to-Int parser stricter so that toInt64('+') will throw. #24475 (Amos Bird).If SSD_CACHE is created with DDL query, it can be created only inside user_files directory. #24466 (Maksim Kita).PostgreSQL support for specifying non default schema for insert queries. Closes #24149. #24413 (Kseniia Sumarokova).Fix IPv6 addresses resolving (i.e. fixes select * from remote('[::1]', system.one)). #24319 (Azat Khuzhin).Fix trailing whitespaces in FROM clause with subqueries in multiline mode, and also changes the output of the queries slightly in a more human friendly way. #24151 (Azat Khuzhin).Improvement for Distributed tables. Add ability to split distributed batch on failures (i.e. due to memory limits, corruptions), under distributed_directory_monitor_split_batch_on_failure (OFF by default). #23864 (Azat Khuzhin).Handle column name clashes for Join table engine. Closes #20309. #23769 (Vladimir).Display progress for File table engine in clickhouse-local and on INSERT query in clickhouse-client when data is passed to stdin. Closes #18209. #23656 (Kseniia Sumarokova).Bugfixes and improvements of clickhouse-copier. Allow to copy tables with different (but compatible schemas). Closes #9159. Added test to copy ReplacingMergeTree. Closes #22711. Support TTL on columns and Data Skipping Indices. It simply removes it to create internal Distributed table (underlying table will have TTL and skipping indices). Closes #19384. Allow to copy MATERIALIZED and ALIAS columns. There are some cases in which it could be helpful (e.g. if this column is in PRIMARY KEY). Now it could be allowed by setting allow_to_copy_alias_and_materialized_columns property to true in task configuration. Closes #9177. Closes [#11007] (https://github.com/ClickHouse/ClickHouse/issues/11007). Closes #9514. Added a property allow_to_drop_target_partitions in task configuration to drop partition in original table before moving helping tables. Closes #20957. Get rid of OPTIMIZE DEDUPLICATE query. This hack was needed, because ALTER TABLE MOVE PARTITION was retried many times and plain MergeTree tables don't have deduplication. Closes #17966. Write progress to ZooKeeper node on path task_path + /status in JSON format. Closes #20955. Support for ReplicatedTables without arguments. Closes #24834 .#23518 (Nikita Mikhaylov).Added sleep with backoff between read retries from S3. #23461 (Vladimir Chebotarev).Respect insert_allow_materialized_columns (allows materialized columns) for INSERT into Distributed table. #23349 (Azat Khuzhin).Add ability to push down LIMIT for distributed queries. #23027 (Azat Khuzhin).Fix zero-copy replication with several S3 volumes (Fixes #22679). #22864 (ianton-ru).Resolve the actual port number bound when a user requests any available port from the operating system to show it in the log message. #25569 (bnaecker).Fixed case, when sometimes conversion of postgres arrays resulted in String data type, not n-dimensional array, because attndims works incorrectly in some cases. Closes #24804. #25538 (Kseniia Sumarokova).Fix convertion of DateTime with timezone for MySQL, PostgreSQL, ODBC. Closes #5057. #25528 (Kseniia Sumarokova).Distinguish KILL MUTATION for different tables (fixes unexpected Cancelled mutating parts error). #25025 (Azat Khuzhin).Allow to declare S3 disk at root of bucket (S3 virtual filesystem is an experimental feature under development). #24898 (Vladimir Chebotarev).Enable reading of subcolumns (e.g. components of Tuples) for distributed tables. #24472 (Anton Popov).A feature for MySQL compatibility protocol: make user function to return correct output. Closes #25697. #25697 (sundyli). Bug Fix​ Improvement for backward compatibility. Use old modulo function version when used in partition key. Closes #23508. #24157 (Kseniia Sumarokova).Fix extremely rare bug on low-memory servers which can lead to the inability to perform merges without restart. Possibly fixes #24603. #24872 (alesapin).Fix extremely rare error Tagging already tagged part in replication queue during concurrent alter move/replace partition. Possibly fixes #22142. #24961 (alesapin).Fix potential crash when calculating aggregate function states by aggregation of aggregate function states of other aggregate functions (not a practical use case). See #24523. #25015 (alexey-milovidov).Fixed the behavior when query SYSTEM RESTART REPLICA or SYSTEM SYNC REPLICA does not finish. This was detected on server with extremely low amount of RAM. #24457 (Nikita Mikhaylov).Fix bug which can lead to ZooKeeper client hung inside clickhouse-server. #24721 (alesapin).If ZooKeeper connection was lost and replica was cloned after restoring the connection, its replication queue might contain outdated entries. Fixed failed assertion when replication queue contains intersecting virtual parts. It may rarely happen if some data part was lost. Print error in log instead of terminating. #24777 (tavplubix).Fix lost WHERE condition in expression-push-down optimization of query plan (setting query_plan_filter_push_down = 1 by default). Fixes #25368. #25370 (Nikolai Kochetov).Fix bug which can lead to intersecting parts after merges with TTL: Part all_40_40_0 is covered by all_40_40_1 but should be merged into all_40_41_1. This shouldn't happen often.. #25549 (alesapin).On ZooKeeper connection loss ReplicatedMergeTree table might wait for background operations to complete before trying to reconnect. It's fixed, now background operations are stopped forcefully. #25306 (tavplubix).Fix error Key expression contains comparison between inconvertible types for queries with ARRAY JOIN in case if array is used in primary key. Fixes #8247. #25546 (Anton Popov).Fix wrong totals for query WITH TOTALS and WITH FILL. Fixes #20872. #25539 (Anton Popov).Fix data race when querying system.clusters while reloading the cluster configuration at the same time. #25737 (Amos Bird).Fixed No such file or directory error on moving Distributed table between databases. Fixes #24971. #25667 (tavplubix).REPLACE PARTITION might be ignored in rare cases if the source partition was empty. It's fixed. Fixes #24869. #25665 (tavplubix).Fixed a bug in Replicated database engine that might rarely cause some replica to skip enqueued DDL query. #24805 (tavplubix).Fix null pointer dereference in EXPLAIN AST without query. #25631 (Nikolai Kochetov).Fix waiting of automatic dropping of empty parts. It could lead to full filling of background pool and stuck of replication. #23315 (Anton Popov).Fix restore of a table stored in S3 virtual filesystem (it is an experimental feature not ready for production). #25601 (ianton-ru).Fix nullptr dereference in Arrow format when using Decimal256. Add Decimal256 support for Arrow format. #25531 (Kruglov Pavel).Fix excessive underscore before the names of the preprocessed configuration files. #25431 (Vitaly Baranov).A fix for clickhouse-copier tool: Fix segfault when sharding_key is absent in task config for copier. #25419 (Nikita Mikhaylov).Fix REPLACE column transformer when used in DDL by correctly quoting the formated query. This fixes #23925. #25391 (Amos Bird).Fix the possibility of non-deterministic behaviour of the quantileDeterministic function and similar. This closes #20480. #25313 (alexey-milovidov).Support SimpleAggregateFunction(LowCardinality) for SummingMergeTree. Fixes #25134. #25300 (Nikolai Kochetov).Fix logical error with exception message &quot;Cannot sum Array/Tuple in min/maxMap&quot;. #25298 (Kruglov Pavel).Fix error Bad cast from type DB::ColumnLowCardinality to DB::ColumnVector&lt;char8_t&gt; for queries where LowCardinality argument was used for IN (this bug appeared in 21.6). Fixes #25187. #25290 (Nikolai Kochetov).Fix incorrect behaviour of joinGetOrNull with not-nullable columns. This fixes #24261. #25288 (Amos Bird).Fix incorrect behaviour and UBSan report in big integers. In previous versions CAST(1e19 AS UInt128) returned zero. #25279 (alexey-milovidov).Fixed an error which occurred while inserting a subset of columns using CSVWithNames format. Fixes #25129. #25169 (Nikita Mikhaylov).Do not use table's projection for SELECT with FINAL. It is not supported yet. #25163 (Amos Bird).Fix possible parts loss after updating up to 21.5 in case table used UUID in partition key. (It is not recommended to use UUID in partition key). Fixes #25070. #25127 (Nikolai Kochetov).Fix crash in query with cross join and joined_subquery_requires_alias = 0. Fixes #24011. #25082 (Nikolai Kochetov).Fix bug with constant maps in mapContains function that lead to error empty column was returned by function mapContains. Closes #25077. #25080 (Kruglov Pavel).Remove possibility to create tables with columns referencing themselves like a UInt32 ALIAS a + 1 or b UInt32 MATERIALIZED b. Fixes #24910, #24292. #25059 (alesapin).Fix wrong result when using aggregate projection with not empty GROUP BY key to execute query with GROUP BY by empty key. #25055 (Amos Bird).Fix serialization of splitted nested messages in Protobuf format. This PR fixes #24647. #25000 (Vitaly Baranov).Fix limit/offset settings for distributed queries (ignore on the remote nodes). #24940 (Azat Khuzhin).Fix possible heap-buffer-overflow in Arrow format. #24922 (Kruglov Pavel).Fixed possible error 'Cannot read from istream at offset 0' when reading a file from DiskS3 (S3 virtual filesystem is an experimental feature under development that should not be used in production). #24885 (Pavel Kovalenko).Fix &quot;Missing columns&quot; exception when joining Distributed Materialized View. #24870 (Azat Khuzhin).Allow NULL values in postgresql compatibility protocol. Closes #22622. #24857 (Kseniia Sumarokova).Fix bug when exception Mutation was killed can be thrown to the client on mutation wait when mutation not loaded into memory yet. #24809 (alesapin).Fixed bug in deserialization of random generator state with might cause some data types such as AggregateFunction(groupArraySample(N), T)) to behave in a non-deterministic way. #24538 (tavplubix).Disallow building uniqXXXXStates of other aggregation states. #24523 (Raúl Marín). Then allow it back by actually eliminating the root cause of the related issue. (alexey-milovidov).Fix usage of tuples in CREATE .. AS SELECT queries. #24464 (Anton Popov).Fix computation of total bytes in Buffer table. In current ClickHouse version total_writes.bytes counter decreases too much during the buffer flush. It leads to counter overflow and totalBytes return something around 17.44 EB some time after the flush. #24450 (DimasKovas).Fix incorrect information about the monotonicity of toWeek function. This fixes #24422 . This bug was introduced in https://github.com/ClickHouse/ClickHouse/pull/5212 , and was exposed later by smarter partition pruner. #24446 (Amos Bird).When user authentication is managed by LDAP. Fixed potential deadlock that can happen during LDAP role (re)mapping, when LDAP group is mapped to a nonexistent local role. #24431 (Denis Glazachev).In &quot;multipart/form-data&quot; message consider the CRLF preceding a boundary as part of it. Fixes #23905. #24399 (Ivan).Fix drop partition with intersect fake parts. In rare cases there might be parts with mutation version greater than current block number. #24321 (Amos Bird).Fixed a bug in moving Materialized View from Ordinary to Atomic database (RENAME TABLE query). Now inner table is moved to new database together with Materialized View. Fixes #23926. #24309 (tavplubix).Allow empty HTTP headers. Fixes #23901. #24285 (Ivan).Correct processing of mutations (ALTER UPDATE/DELETE) in Memory tables. Closes #24274. #24275 (flynn).Make column LowCardinality property in JOIN output the same as in the input, close #23351, close #20315. #24061 (Vladimir).A fix for Kafka tables. Fix the bug in failover behavior when Engine = Kafka was not able to start consumption if the same consumer had an empty assignment previously. Closes #21118. #21267 (filimonov). Build/Testing/Packaging Improvement​ Add darwin-aarch64 (Mac M1 / Apple Silicon) builds in CI #25560 (Ivan) and put the links to the docs and website (alexey-milovidov).Adds cross-platform embedding of binary resources into executables. It works on Illumos. #25146 (bnaecker).Add join related options to stress tests to improve fuzzing. #25200 (Vladimir).Enable build with s3 module in osx #25217. #25218 (kevin wan).Add integration test cases to cover JDBC bridge. #25047 (Zhichun Wu).Integration tests configuration has special treatment for dictionaries. Removed remaining dictionaries manual setup. #24728 (Ilya Yatsishin).Add libfuzzer tests for YAMLParser class. #24480 (BoloniniD).Ubuntu 20.04 is now used to run integration tests, docker-compose version used to run integration tests is updated to 1.28.2. Environment variables now take effect on docker-compose. Rework test_dictionaries_all_layouts_separate_sources to allow parallel run. #20393 (Ilya Yatsishin).Fix TOCTOU error in installation script. #25277 (alexey-milovidov). "},{"title":"ClickHouse release 21.6, 2021-06-05​","type":1,"pageTitle":"2021","url":"docs/en/whats-new/changelog/2021#clickhouse-release-216-2021-06-05","content":"Backward Incompatible Change​ uniqState / uniqHLL12State / uniqCombinedState / uniqCombined64State produce incompatible states with UUID type. #33607. Upgrade Notes​ zstd compression library is updated to v1.5.0. You may get messages about &quot;checksum does not match&quot; in replication. These messages are expected due to update of compression algorithm and you can ignore them. These messages are informational and do not indicate any kinds of undesired behaviour.The setting compile_expressions is enabled by default. Although it has been heavily tested on variety of scenarios, if you find some undesired behaviour on your servers, you can try turning this setting off.Values of UUID type cannot be compared with integer. For example, instead of writing uuid != 0 type uuid != '00000000-0000-0000-0000-000000000000'. New Feature​ Add Postgres-like cast operator (::). E.g.: [1, 2]::Array(UInt8), 0.1::Decimal(4, 4), number::UInt16. #23871 (Anton Popov).Make big integers production ready. Add support for UInt128 data type. Fix known issues with the Decimal256 data type. Support big integers in dictionaries. Support gcd/lcm functions for big integers. Support big integers in array search and conditional functions. Support LowCardinality(UUID). Support big integers in generateRandom table function and clickhouse-obfuscator. Fix error with returning UUID from scalar subqueries. This fixes #7834. This fixes #23936. This fixes #4176. This fixes #24018. Backward incompatible change: values of UUID type cannot be compared with integer. For example, instead of writing uuid != 0 type uuid != '00000000-0000-0000-0000-000000000000'. #23631 (alexey-milovidov).Support Array data type for inserting and selecting data in Arrow, Parquet and ORC formats. #21770 (taylor12805).Implement table comments. Closes #23225. #23548 (flynn).Support creating dictionaries with DDL queries in clickhouse-local. Closes #22354. Added support for DETACH DICTIONARY PERMANENTLY. Added support for EXCHANGE DICTIONARIES for Atomic database engine. Added support for moving dictionaries between databases using RENAME DICTIONARY. #23436 (Maksim Kita).Add aggregate function uniqTheta to support Theta Sketch in ClickHouse. #23894. #22609 (Ping Yu).Add function splitByRegexp. #24077 (abel-cheng).Add function arrayProduct which accept an array as the parameter, and return the product of all the elements in array. Closes #21613. #23782 (Maksim Kita).Add thread_name column in system.stack_trace. This closes #23256. #24124 (abel-cheng).If insert_null_as_default = 1, insert default values instead of NULL in INSERT ... SELECT and INSERT ... SELECT ... UNION ALL ... queries. Closes #22832. #23524 (Kseniia Sumarokova).Add support for progress indication in clickhouse-local with --progress option. #23196 (Egor Savin).Add support for HTTP compression (determined by Content-Encoding HTTP header) in http dictionary source. This fixes #8912. #23946 (FArthur-cmd).Added SYSTEM QUERY RELOAD MODEL, SYSTEM QUERY RELOAD MODELS. Closes #18722. #23182 (Maksim Kita).Add setting json (boolean, 0 by default) for EXPLAIN PLAN query. When enabled, query output will be a single JSON row. It is recommended to use TSVRaw format to avoid unnecessary escaping. #23082 (Nikolai Kochetov).Add setting indexes (boolean, disabled by default) to EXPLAIN PIPELINE query. When enabled, shows used indexes, number of filtered parts and granules for every index applied. Supported for MergeTree* tables. #22352 (Nikolai Kochetov).LDAP: implemented user DN detection functionality to use when mapping Active Directory groups to ClickHouse roles. #22228 (Denis Glazachev).New aggregate function deltaSumTimestamp for summing the difference between consecutive rows while maintaining ordering during merge by storing timestamps. #21888 (Russ Frank).Added less secure IMDS credentials provider for S3 which works under docker correctly. #21852 (Vladimir Chebotarev).Add back indexHint function. This is for #21238. This reverts #9542. This fixes #9540. #21304 (Amos Bird). Experimental Feature​ Add PROJECTION support for MergeTree* tables. #20202 (Amos Bird). Performance Improvement​ Enable compile_expressions setting by default. When this setting enabled, compositions of simple functions and operators will be compiled to native code with LLVM at runtime. #8482 (Maksim Kita, alexey-milovidov). Note: if you feel in trouble, turn this option off.Update re2 library. Performance of regular expressions matching is improved. Also this PR adds compatibility with gcc-11. #24196 (Raúl Marín).ORC input format reading by stripe instead of reading entire table into memory by once which is cost memory when file size is huge. #23102 (Chao Ma).Fusion of aggregate functions sum, count and avg in a query into single aggregate function. The optimization is controlled with the optimize_fuse_sum_count_avg setting. This is implemented with a new aggregate function sumCount. This function returns a tuple of two fields: sum and count. #21337 (hexiaoting).Update zstd to v1.5.0. The performance of compression is improved for single digits percentage. #24135 (Raúl Marín). Note: you may get messages about &quot;checksum does not match&quot; in replication. These messages are expected due to update of compression algorithm and you can ignore them.Improved performance of Buffer tables: do not acquire lock for total_bytes/total_rows for Buffer engine. #24066 (Azat Khuzhin).Preallocate support for hashed/sparse_hashed dictionaries is returned. #23979 (Azat Khuzhin).Enable async_socket_for_remote by default (lower amount of threads in querying Distributed tables with large fanout). #23683 (Nikolai Kochetov). Improvement​ Add _partition_value virtual column to MergeTree table family. It can be used to prune partition in a deterministic way. It's needed to implement partition matcher for mutations. #23673 (Amos Bird).Added region parameter for S3 storage and disk. #23846 (Vladimir Chebotarev).Allow configuring different log levels for different logging channels. Closes #19569. #23857 (filimonov).Keep default timezone on DateTime operations if it was not provided explicitly. For example, if you add one second to a value of DateTime type without timezone it will remain DateTime without timezone. In previous versions the value of default timezone was placed to the returned data type explicitly so it becomes DateTime('something'). This closes #4854. #23392 (alexey-milovidov).Allow user to specify empty string instead of database name for MySQL storage. Default database will be used for queries. In previous versions it was working for SELECT queries and not support for INSERT was also added. This closes #19281. This can be useful working with Sphinx or other MySQL-compatible foreign databases. #23319 (alexey-milovidov).Fixed quantile(s)TDigest. Added special handling of singleton centroids according to tdunning/t-digest 3.2+. Also a bug with over-compression of centroids in implementation of earlier version of the algorithm was fixed. #23314 (Vladimir Chebotarev).Function now64 now supports optional timezone argument. #24091 (Vasily Nemkov).Fix the case when a progress bar in interactive mode in clickhouse-client that appear in the middle of the data may rewrite some parts of visible data in terminal. This closes #19283. #23050 (alexey-milovidov).Fix crash when memory allocation fails in simdjson. https://github.com/simdjson/simdjson/pull/1567 . Mark as improvement because it's a very rare bug. #24147 (Amos Bird).Preserve dictionaries until storage shutdown (this will avoid possible external dictionary 'DICT' not found errors at server shutdown during final flush of the Buffer engine). #24068 (Azat Khuzhin).Flush Buffer tables before shutting down tables (within one database), to avoid discarding blocks due to underlying table had been already detached (and Destination table default.a_data_01870 doesn't exist. Block of data is discarded error in the log). #24067 (Azat Khuzhin).Now prefer_column_name_to_alias = 1 will also favor column names for group by, having and order by. This fixes #23882. #24022 (Amos Bird).Add support for ORDER BY WITH FILL with DateTime64. #24016 (kevin wan).Enable DateTime64 to be a version column in ReplacingMergeTree. #23992 (kevin wan).Log information about OS name, kernel version and CPU architecture on server startup. #23988 (Azat Khuzhin).Support specifying table schema for postgresql dictionary source. Closes #23958. #23980 (Kseniia Sumarokova).Add hints for names of Enum elements (suggest names in case of typos). Closes #17112. #23919 (flynn).Measure found rate (the percentage for which the value was found) for dictionaries (see found_rate in system.dictionaries). #23916 (Azat Khuzhin).Allow to add specific queue settings via table settng rabbitmq_queue_settings_list. (Closes #23737 and #23918). Allow user to control all RabbitMQ setup: if table setting rabbitmq_queue_consume is set to 1 - RabbitMQ table engine will only connect to specified queue and will not perform any RabbitMQ consumer-side setup like declaring exchange, queues, bindings. (Closes #21757). Add proper cleanup when RabbitMQ table is dropped - delete queues, which the table has declared and all bound exchanges - if they were created by the table. #23887 (Kseniia Sumarokova).Add broken_data_files/broken_data_compressed_bytes into system.distribution_queue. Add metric for number of files for asynchronous insertion into Distributed tables that has been marked as broken (BrokenDistributedFilesToInsert). #23885 (Azat Khuzhin).Querying system.tables does not go to ZooKeeper anymore. #23793 (Fuwang Hu).Respect lock_acquire_timeout_for_background_operations for OPTIMIZE queries. #23623 (Azat Khuzhin).Possibility to change S3 disk settings in runtime via new SYSTEM RESTART DISK SQL command. #23429 (Pavel Kovalenko).If user applied a misconfiguration by mistakenly setting max_distributed_connections to value zero, every query to a Distributed table will throw exception with a message containing &quot;logical error&quot;. But it's really an expected behaviour, not a logical error, so the exception message was slightly incorrect. It also triggered checks in our CI enviroment that ensures that no logical errors ever happen. Instead we will treat max_distributed_connections misconfigured to zero as the minimum possible value (one). #23348 (Azat Khuzhin).Disable min_bytes_to_use_mmap_io by default. #23322 (Azat Khuzhin).Support LowCardinality nullability with join_use_nulls, close #15101. #23237 (vdimir).Added possibility to restore MergeTree parts to detached directory for S3 disk. #23112 (Pavel Kovalenko).Retries on HTTP connection drops in S3. #22988 (Vladimir Chebotarev).Add settings external_storage_max_read_rows and external_storage_max_read_rows for MySQL table engine, dictionary source and MaterializeMySQL minor data fetches. #22697 (TCeason).MaterializeMySQL (experimental feature): Previously, MySQL 5.7.9 was not supported due to SQL incompatibility. Now leave MySQL parameter verification to the MaterializeMySQL. #23413 (TCeason).Enable reading of subcolumns for distributed tables. #24472 (Anton Popov).Fix usage of tuples in CREATE .. AS SELECT queries. #24464 (Anton Popov).Support for Parquet format in Kafka tables. #23412 (Chao Ma). Bug Fix​ Use old modulo function version when used in partition key and primary key. Closes #23508. #24157 (Kseniia Sumarokova). It was a source of backward incompatibility in previous releases.Fixed the behavior when query SYSTEM RESTART REPLICA or SYSTEM SYNC REPLICA is being processed infinitely. This was detected on server with extremely little amount of RAM. #24457 (Nikita Mikhaylov).Fix incorrect monotonicity of toWeek function. This fixes #24422 . This bug was introduced in #5212, and was exposed later by smarter partition pruner. #24446 (Amos Bird).Fix drop partition with intersect fake parts. In rare cases there might be parts with mutation version greater than current block number. #24321 (Amos Bird).Fixed a bug in moving Materialized View from Ordinary to Atomic database (RENAME TABLE query). Now inner table is moved to new database together with Materialized View. Fixes #23926. #24309 (tavplubix).Allow empty HTTP headers in client requests. Fixes #23901. #24285 (Ivan).Set max_threads = 1 to fix mutation fail of Memory tables. Closes #24274. #24275 (flynn).Fix typo in implementation of Memory tables, this bug was introduced at #15127. Closes #24192. #24193 (张中南).Fix abnormal server termination due to HDFS becoming not accessible during query execution. Closes #24117. #24191 (Kseniia Sumarokova).Fix crash on updating of Nested column with const condition. #24183 (hexiaoting).Fix race condition which could happen in RBAC under a heavy load. This PR fixes #24090, #24134,. #24176 (Vitaly Baranov).Fix a rare bug that could lead to a partially initialized table that can serve write requests (insert/alter/so on). Now such tables will be in readonly mode. #24122 (alesapin).Fix an issue: EXPLAIN PIPELINE with SELECT xxx FINAL showed a wrong pipeline. (hexiaoting).Fixed using const DateTime value vs DateTime64 column in WHERE. #24100 (Vasily Nemkov).Fix crash in merge JOIN, closes #24010. #24013 (vdimir).Some ALTER PARTITION queries might cause Part A intersects previous part B and Unexpected merged part C intersecting drop range D errors in replication queue. It's fixed. Fixes #23296. #23997 (tavplubix).Fix SIGSEGV for external GROUP BY and overflow row (i.e. queries like SELECT FROM GROUP BY WITH TOTALS SETTINGS max_bytes_before_external_group_by&gt;0, max_rows_to_group_by&gt;0, group_by_overflow_mode='any', totals_mode='before_having'). #23962 (Azat Khuzhin).Fix keys metrics accounting for CACHE dictionary with duplicates in the source (leads to DictCacheKeysRequestedMiss overflows). #23929 (Azat Khuzhin).Fix implementation of connection pool of PostgreSQL engine. Closes #23897. #23909 (Kseniia Sumarokova).Fix distributed_group_by_no_merge = 2 with GROUP BY and aggregate function wrapped into regular function (had been broken in #23546). Throw exception in case of someone trying to use distributed_group_by_no_merge = 2 with window functions. Disable optimize_distributed_group_by_sharding_key for queries with window functions. #23906 (Azat Khuzhin).A fix for s3 table function: better handling of HTTP errors. Response bodies of HTTP errors were being ignored earlier. #23844 (Vladimir Chebotarev).A fix for s3 table function: better handling of URI's. Fixed an incompatibility with URLs containing + symbol, data with such keys could not be read previously. #23822 (Vladimir Chebotarev).Fix error Can't initialize pipeline with empty pipe for queries with GLOBAL IN/JOIN and use_hedged_requests. Fixes #23431. #23805 (Nikolai Kochetov).Fix CLEAR COLUMN does not work when it is referenced by materialized view. Close #23764. #23781 (flynn).Fix heap use after free when reading from HDFS if Values format is used. #23761 (Kseniia Sumarokova).Avoid possible &quot;Cannot schedule a task&quot; error (in case some exception had been occurred) on INSERT into Distributed. #23744 (Azat Khuzhin).Fixed a bug in recovery of staled ReplicatedMergeTree replica. Some metadata updates could be ignored by staled replica if ALTER query was executed during downtime of the replica. #23742 (tavplubix).Fix a bug with Join and WITH TOTALS, close #17718. #23549 (vdimir).Fix possible Block structure mismatch error for queries with UNION which could possibly happen after filter-pushdown optimization. Fixes #23029. #23359 (Nikolai Kochetov).Add type conversion when the setting optimize_skip_unused_shards_rewrite_in is enabled. This fixes MSan report. #23219 (Azat Khuzhin).Add a missing check when updating nested subcolumns, close issue: #22353. #22503 (hexiaoting). Build/Testing/Packaging Improvement​ Support building on Illumos. #24144. Adds support for building on Solaris-derived operating systems. #23746 (bnaecker).Add more benchmarks for hash tables, including the Swiss Table from Google (that appeared to be slower than ClickHouse hash map in our specific usage scenario). #24111 (Maksim Kita).Update librdkafka 1.6.0-RC3 to 1.6.1. #23874 (filimonov).Always enable asynchronous-unwind-tables explicitly. It may fix query profiler on AArch64. #23602 (alexey-milovidov).Avoid possible build dependency on locale and filesystem order. This allows reproducible builds. #23600 (alexey-milovidov).Remove a source of nondeterminism from build. Now builds at different point of time will produce byte-identical binaries. Partially addressed #22113. #23559 (alexey-milovidov).Add simple tool for benchmarking (Zoo)Keeper. #23038 (alesapin). "},{"title":"ClickHouse release 21.5, 2021-05-20​","type":1,"pageTitle":"2021","url":"docs/en/whats-new/changelog/2021#clickhouse-release-215-2021-05-20","content":"Backward Incompatible Change​ Change comparison of integers and floating point numbers when integer is not exactly representable in the floating point data type. In new version comparison will return false as the rounding error will occur. Example: 9223372036854775808.0 != 9223372036854775808, because the number 9223372036854775808 is not representable as floating point number exactly (and 9223372036854775808.0 is rounded to 9223372036854776000.0). But in previous version the comparison will return as the numbers are equal, because if the floating point number 9223372036854776000.0 get converted back to UInt64, it will yield 9223372036854775808. For the reference, the Python programming language also treats these numbers as equal. But this behaviour was dependend on CPU model (different results on AMD64 and AArch64 for some out-of-range numbers), so we make the comparison more precise. It will treat int and float numbers equal only if int is represented in floating point type exactly. #22595 (alexey-milovidov).Remove support for argMin and argMax for single Tuple argument. The code was not memory-safe. The feature was added by mistake and it is confusing for people. These functions can be reintroduced under different names later. This fixes #22384 and reverts #17359. #23393 (alexey-milovidov). New Feature​ Added functions dictGetChildren(dictionary, key), dictGetDescendants(dictionary, key, level). Function dictGetChildren return all children as an array if indexes. It is a inverse transformation for dictGetHierarchy. Function dictGetDescendants return all descendants as if dictGetChildren was applied level times recursively. Zero level value is equivalent to infinity. Improved performance of dictGetHierarchy, dictIsIn functions. Closes #14656. #22096 (Maksim Kita).Added function dictGetOrNull. It works like dictGet, but return Null in case key was not found in dictionary. Closes #22375. #22413 (Maksim Kita).Added a table function s3Cluster, which allows to process files from s3 in parallel on every node of a specified cluster. #22012 (Nikita Mikhaylov).Added support for replicas and shards in MySQL/PostgreSQL table engine / table function. You can write SELECT * FROM mysql('host{1,2}-{1|2}', ...). Closes #20969. #22217 (Kseniia Sumarokova).Added ALTER TABLE ... FETCH PART ... query. It's similar to FETCH PARTITION, but fetches only one part. #22706 (turbo jason).Added a setting max_distributed_depth that limits the depth of recursive queries to Distributed tables. Closes #20229. #21942 (flynn). Performance Improvement​ Improved performance of intDiv by dynamic dispatch for AVX2. This closes #22314. #23000 (alexey-milovidov).Improved performance of reading from ArrowStream input format for sources other then local file (e.g. URL). #22673 (nvartolomei).Disabled compression by default when interacting with localhost (with clickhouse-client or server to server with distributed queries) via native protocol. It may improve performance of some import/export operations. This closes #22234. #22237 (alexey-milovidov).Exclude values that does not belong to the shard from right part of IN section for distributed queries (under optimize_skip_unused_shards_rewrite_in, enabled by default, since it still requires optimize_skip_unused_shards). #21511 (Azat Khuzhin).Improved performance of reading a subset of columns with File-like table engine and column-oriented format like Parquet, Arrow or ORC. This closes #issue:20129. #21302 (keenwolf).Allow to move more conditions to PREWHERE as it was before version 21.1 (adjustment of internal heuristics). Insufficient number of moved condtions could lead to worse performance. #23397 (Anton Popov).Improved performance of ODBC connections and fixed all the outstanding issues from the backlog. Using nanodbc library instead of Poco::ODBC. Closes #9678. Add support for DateTime64 and Decimal* for ODBC table engine. Closes #21961. Fixed issue with cyrillic text being truncated. Closes #16246. Added connection pools for odbc bridge. #21972 (Kseniia Sumarokova). Improvement​ Increase max_uri_size (the maximum size of URL in HTTP interface) to 1 MiB by default. This closes #21197. #22997 (alexey-milovidov).Set background_fetches_pool_size to 8 that is better for production usage with frequent small insertions or slow ZooKeeper cluster. #22945 (alexey-milovidov).FlatDictionary added initial_array_size, max_array_size options. #22521 (Maksim Kita).Add new setting non_replicated_deduplication_window for non-replicated MergeTree inserts deduplication. #22514 (alesapin).Update paths to the CatBoost model configs in config reloading. #22434 (Kruglov Pavel).Added Decimal256 type support in dictionaries. Decimal256 is experimental feature. Closes #20979. #22960 (Maksim Kita).Enabled async_socket_for_remote by default (using less amount of OS threads for distributed queries). #23683 (Nikolai Kochetov).Fixed quantile(s)TDigest. Added special handling of singleton centroids according to tdunning/t-digest 3.2+. Also a bug with over-compression of centroids in implementation of earlier version of the algorithm was fixed. #23314 (Vladimir Chebotarev).Make function name unhex case insensitive for compatibility with MySQL. #23229 (alexey-milovidov).Implement functions arrayHasAny, arrayHasAll, has, indexOf, countEqual for generic case when types of array elements are different. In previous versions the functions arrayHasAny, arrayHasAll returned false and has, indexOf, countEqual thrown exception. Also add support for Decimal and big integer types in functions has and similar. This closes #20272. #23044 (alexey-milovidov).Raised the threshold on max number of matches in result of the function extractAllGroupsHorizontal. #23036 (Vasily Nemkov).Do not perform optimize_skip_unused_shards for cluster with one node. #22999 (Azat Khuzhin).Added ability to run clickhouse-keeper (experimental drop-in replacement to ZooKeeper) with SSL. Config settings keeper_server.tcp_port_secure can be used for secure interaction between client and keeper-server. keeper_server.raft_configuration.secure can be used to enable internal secure communication between nodes. #22992 (alesapin).Added ability to flush buffer only in background for Buffer tables. #22986 (Azat Khuzhin).When selecting from MergeTree table with NULL in WHERE condition, in rare cases, exception was thrown. This closes #20019. #22978 (alexey-milovidov).Fix error handling in Poco HTTP Client for AWS. #22973 (kreuzerkrieg).Respect max_part_removal_threads for ReplicatedMergeTree. #22971 (Azat Khuzhin).Fix obscure corner case of MergeTree settings inactive_parts_to_throw_insert = 0 with inactive_parts_to_delay_insert &gt; 0. #22947 (Azat Khuzhin).dateDiff now works with DateTime64 arguments (even for values outside of DateTime range) #22931 (Vasily Nemkov).MaterializeMySQL (experimental feature): added an ability to replicate MySQL databases containing views without failing. This is accomplished by ignoring the views. #22760 (Christian).Allow RBAC row policy via postgresql protocol. Closes #22658. PostgreSQL protocol is enabled in configuration by default. #22755 (Kseniia Sumarokova).Add metric to track how much time is spend during waiting for Buffer layer lock. #22725 (Azat Khuzhin).Allow to use CTE in VIEW definition. This closes #22491. #22657 (Amos Bird).Clear the rest of the screen and show cursor in clickhouse-client if previous program has left garbage in terminal. This closes #16518. #22634 (alexey-milovidov).Make round function to behave consistently on non-x86_64 platforms. Rounding half to nearest even (Banker's rounding) is used. #22582 (alexey-milovidov).Correctly check structure of blocks of data that are sending by Distributed tables. #22325 (Azat Khuzhin).Allow publishing Kafka errors to a virtual column of Kafka engine, controlled by the kafka_handle_error_mode setting. #21850 (fastio).Add aliases simpleJSONExtract/simpleJSONHas to visitParam/visitParamExtract{UInt, Int, Bool, Float, Raw, String}. Fixes #21383. #21519 (fastio).Add clickhouse-library-bridge for library dictionary source. Closes #9502. #21509 (Kseniia Sumarokova).Forbid to drop a column if it's referenced by materialized view. Closes #21164. #21303 (flynn).Support dynamic interserver credentials (rotating credentials without downtime). #14113 (johnskopis).Add support for Kafka storage with Arrow and ArrowStream format messages. #23415 (Chao Ma).Fixed missing semicolon in exception message. The user may find this exception message unpleasant to read. #23208 (alexey-milovidov).Fixed missing whitespace in some exception messages about LowCardinality type. #23207 (alexey-milovidov).Some values were formatted with alignment in center in table cells in Markdown format. Not anymore. #23096 (alexey-milovidov).Remove non-essential details from suggestions in clickhouse-client. This closes #22158. #23040 (alexey-milovidov).Correct calculation of bytes_allocated field in system.dictionaries for sparse_hashed dictionaries. #22867 (Azat Khuzhin).Fixed approximate total rows accounting for reverse reading from MergeTree. #22726 (Azat Khuzhin).Fix the case when it was possible to configure dictionary with clickhouse source that was looking to itself that leads to infinite loop. Closes #14314. #22479 (Maksim Kita). Bug Fix​ Multiple fixes for hedged requests. Fixed an error Can't initialize pipeline with empty pipe for queries with GLOBAL IN/JOIN when the setting use_hedged_requests is enabled. Fixes #23431. #23805 (Nikolai Kochetov). Fixed a race condition in hedged connections which leads to crash. This fixes #22161. #22443 (Kruglov Pavel). Fix possible crash in case if unknown packet was received from remote query (with async_socket_for_remote enabled). Fixes #21167. #23309 (Nikolai Kochetov).Fixed the behavior when disabling input_format_with_names_use_header setting discards all the input with CSVWithNames format. This fixes #22406. #23202 (Nikita Mikhaylov).Fixed remote JDBC bridge timeout connection issue. Closes #9609. #23771 (Maksim Kita, alexey-milovidov).Fix the logic of initial load of complex_key_hashed if update_field is specified. Closes #23800. #23824 (Maksim Kita).Fixed crash when PREWHERE and row policy filter are both in effect with empty result. #23763 (Amos Bird).Avoid possible &quot;Cannot schedule a task&quot; error (in case some exception had been occurred) on INSERT into Distributed. #23744 (Azat Khuzhin).Added an exception in case of completely the same values in both samples in aggregate function mannWhitneyUTest. This fixes #23646. #23654 (Nikita Mikhaylov).Fixed server fault when inserting data through HTTP caused an exception. This fixes #23512. #23643 (Nikita Mikhaylov).Fixed misinterpretation of some LIKE expressions with escape sequences. #23610 (alexey-milovidov).Fixed restart / stop command hanging. Closes #20214. #23552 (filimonov).Fixed COLUMNS matcher in case of multiple JOINs in select query. Closes #22736. #23501 (Maksim Kita).Fixed a crash when modifying column's default value when a column itself is used as ReplacingMergeTree's parameter. #23483 (hexiaoting).Fixed corner cases in vertical merges with ReplacingMergeTree. In rare cases they could lead to fails of merges with exceptions like Incomplete granules are not allowed while blocks are granules size. #23459 (Anton Popov).Fixed bug that does not allow cast from empty array literal, to array with dimensions greater than 1, e.g. CAST([] AS Array(Array(String))). Closes #14476. #23456 (Maksim Kita).Fixed a bug when deltaSum aggregate function produced incorrect result after resetting the counter. #23437 (Russ Frank).Fixed Cannot unlink file error on unsuccessful creation of ReplicatedMergeTree table with multidisk configuration. This closes #21755. #23433 (tavplubix).Fixed incompatible constant expression generation during partition pruning based on virtual columns. This fixes https://github.com/ClickHouse/ClickHouse/pull/21401#discussion_r611888913. #23366 (Amos Bird).Fixed a crash when setting join_algorithm is set to 'auto' and Join is performed with a Dictionary. Close #23002. #23312 (Vladimir).Don't relax NOT conditions during partition pruning. This fixes #23305 and #21539. #23310 (Amos Bird).Fixed very rare race condition on background cleanup of old blocks. It might cause a block not to be deduplicated if it's too close to the end of deduplication window. #23301 (tavplubix).Fixed very rare (distributed) race condition between creation and removal of ReplicatedMergeTree tables. It might cause exceptions like node doesn't exist on attempt to create replicated table. Fixes #21419. #23294 (tavplubix).Fixed simple key dictionary from DDL creation if primary key is not first attribute. Fixes #23236. #23262 (Maksim Kita).Fixed reading from ODBC when there are many long column names in a table. Closes #8853. #23215 (Kseniia Sumarokova).MaterializeMySQL (experimental feature): fixed Not found column error when selecting from MaterializeMySQL with condition on key column. Fixes #22432. #23200 (tavplubix).Correct aliases handling if subquery was optimized to constant. Fixes #22924. Fixes #10401. #23191 (Maksim Kita).Server might fail to start if data_type_default_nullable setting is enabled in default profile, it's fixed. Fixes #22573. #23185 (tavplubix).Fixed a crash on shutdown which happened because of wrong accounting of current connections. #23154 (Vitaly Baranov).Fixed Table .inner_id... doesn't exist error when selecting from Materialized View after detaching it from Atomic database and attaching back. #23047 (tavplubix).Fix error Cannot find column in ActionsDAG result which may happen if subquery uses untuple. Fixes #22290. #22991 (Nikolai Kochetov).Fix usage of constant columns of type Map with nullable values. #22939 (Anton Popov).fixed formatDateTime() on DateTime64 and &quot;%C&quot; format specifier fixed toDateTime64() for large values and non-zero scale. #22937 (Vasily Nemkov).Fixed a crash when using mannWhitneyUTest and rankCorr with window functions. This fixes #22728. #22876 (Nikita Mikhaylov).LIVE VIEW (experimental feature): fixed possible hanging in concurrent DROP/CREATE of TEMPORARY LIVE VIEW in TemporaryLiveViewCleaner, see. #22858 (Vitaly Baranov).Fixed pushdown of HAVING in case, when filter column is used in aggregation. #22763 (Anton Popov).Fixed possible hangs in Zookeeper requests in case of OOM exception. Fixes #22438. #22684 (Nikolai Kochetov).Fixed wait for mutations on several replicas for ReplicatedMergeTree table engines. Previously, mutation/alter query may finish before mutation actually executed on other replicas. #22669 (alesapin).Fixed exception for Log with nested types without columns in the SELECT clause. #22654 (Azat Khuzhin).Fix unlimited wait for auxiliary AWS requests. #22594 (Vladimir Chebotarev).Fixed a crash when client closes connection very early #22579. #22591 (nvartolomei).Map data type (experimental feature): fixed an incorrect formatting of function map in distributed queries. #22588 (foolchi).Fixed deserialization of empty string without newline at end of TSV format. This closes #20244. Possible workaround without version update: set input_format_null_as_default to zero. It was zero in old versions. #22527 (alexey-milovidov).Fixed wrong cast of a column of LowCardinality type in Merge Join algorithm. Close #22386, close #22388. #22510 (Vladimir).Buffer overflow (on read) was possible in tokenbf_v1 full text index. The excessive bytes are not used but the read operation may lead to crash in rare cases. This closes #19233. #22421 (alexey-milovidov).Do not limit HTTP chunk size. Fixes #21907. #22322 (Ivan).Fixed a bug, which leads to underaggregation of data in case of enabled optimize_aggregation_in_order and many parts in table. Slightly improve performance of aggregation with enabled optimize_aggregation_in_order. #21889 (Anton Popov).Check if table function view is used as a column. This complements #20350. #21465 (Amos Bird).Fix &quot;unknown column&quot; error for tables with Merge engine in queris with JOIN and aggregation. Closes #18368, close #22226. #21370 (Vladimir).Fixed name clashes in pushdown optimization. It caused incorrect WHERE filtration after FULL JOIN. Close #20497. #20622 (Vladimir).Fixed very rare bug when quorum insert with quorum_parallel=1 is not really &quot;quorum&quot; because of deduplication. #18215 (filimonov - reported, alesapin - fixed). Build/Testing/Packaging Improvement​ Run stateless tests in parallel in CI. #22300 (alesapin).Simplify debian packages. This fixes #21698. #22976 (alexey-milovidov).Added support for ClickHouse build on Apple M1. #21639 (changvvb).Fixed ClickHouse Keeper build for MacOS. #22860 (alesapin).Fixed some tests on AArch64 platform. #22596 (alexey-milovidov).Added function alignment for possibly better performance. #21431 (Danila Kutenin).Adjust some tests to output identical results on amd64 and aarch64 (qemu). The result was depending on implementation specific CPU behaviour. #22590 (alexey-milovidov).Allow query profiling only on x86_64. See #15174 and #15638. This closes #15638. #22580 (alexey-milovidov).Allow building with unbundled xz (lzma) using USE_INTERNAL_XZ_LIBRARY=OFF CMake option. #22571 (Kfir Itzhak).Enable bundled openldap on ppc64le #22487 (Kfir Itzhak).Disable incompatible libraries (platform specific typically) on ppc64le #22475 (Kfir Itzhak).Add Jepsen test in CI for clickhouse Keeper. #22373 (alesapin).Build jemalloc with support for heap profiling. #22834 (nvartolomei).Avoid UB in *Log engines for rwlock unlock due to unlock from another thread. #22583 (Azat Khuzhin).Fixed UB by unlocking the rwlock of the TinyLog from the same thread. #22560 (Azat Khuzhin). "},{"title":"ClickHouse release 21.4​","type":1,"pageTitle":"2021","url":"docs/en/whats-new/changelog/2021#clickhouse-release-214","content":""},{"title":"ClickHouse release 21.4.1 2021-04-12​","type":1,"pageTitle":"2021","url":"docs/en/whats-new/changelog/2021#clickhouse-release-2141-2021-04-12","content":"Backward Incompatible Change​ The toStartOfIntervalFunction will align hour intervals to the midnight (in previous versions they were aligned to the start of unix epoch). For example, toStartOfInterval(x, INTERVAL 11 HOUR) will split every day into three intervals: 00:00:00..10:59:59, 11:00:00..21:59:59 and 22:00:00..23:59:59. This behaviour is more suited for practical needs. This closes #9510. #22060 (alexey-milovidov).Age and Precision in graphite rollup configs should increase from retention to retention. Now it's checked and the wrong config raises an exception. #21496 (Mikhail f. Shiryaev).Fix cutToFirstSignificantSubdomainCustom()/firstSignificantSubdomainCustom() returning wrong result for 3+ level domains present in custom top-level domain list. For input domains matching these custom top-level domains, the third-level domain was considered to be the first significant one. This is now fixed. This change may introduce incompatibility if the function is used in e.g. the sharding key. #21946 (Azat Khuzhin).Column keys in table system.dictionaries was replaced to columns key.names and key.types. Columns key.names, key.types, attribute.names, attribute.types from system.dictionaries table does not require dictionary to be loaded. #21884 (Maksim Kita).Now replicas that are processing the ALTER TABLE ATTACH PART[ITION] command search in their detached/ folders before fetching the data from other replicas. As an implementation detail, a new command ATTACH_PART is introduced in the replicated log. Parts are searched and compared by their checksums. #18978 (Mike Kot). Note: ATTACH PART[ITION] queries may not work during cluster upgrade.It's not possible to rollback to older ClickHouse version after executing ALTER ... ATTACH query in new version as the old servers would fail to pass the ATTACH_PART entry in the replicated log. In this version, empty &lt;remote_url_allow_hosts&gt;&lt;/remote_url_allow_hosts&gt; will block all access to remote hosts while in previous versions it did nothing. If you want to keep old behaviour and you have empty remote_url_allow_hosts element in configuration file, remove it. #20058 (Vladimir Chebotarev). New Feature​ Extended range of DateTime64 to support dates from year 1925 to 2283. Improved support of DateTime around zero date (1970-01-01). #9404 (alexey-milovidov, Vasily Nemkov). Not every time and date functions are working for extended range of dates.Added support of Kerberos authentication for preconfigured users and HTTP requests (GSS-SPNEGO). #14995 (Denis Glazachev).Add prefer_column_name_to_alias setting to use original column names instead of aliases. it is needed to be more compatible with common databases' aliasing rules. This is for #9715 and #9887. #22044 (Amos Bird).Added functions dictGetChildren(dictionary, key), dictGetDescendants(dictionary, key, level). Function dictGetChildren return all children as an array if indexes. It is a inverse transformation for dictGetHierarchy. Function dictGetDescendants return all descendants as if dictGetChildren was applied level times recursively. Zero level value is equivalent to infinity. Closes #14656. #22096 (Maksim Kita).Added executable_pool dictionary source. Close #14528. #21321 (Maksim Kita).Added table function dictionary. It works the same way as Dictionary engine. Closes #21560. #21910 (Maksim Kita).Support Nullable type for PolygonDictionary attribute. #21890 (Maksim Kita).Functions dictGet, dictHas use current database name if it is not specified for dictionaries created with DDL. Closes #21632. #21859 (Maksim Kita).Added function dictGetOrNull. It works like dictGet, but return Null in case key was not found in dictionary. Closes #22375. #22413 (Maksim Kita).Added async update in ComplexKeyCache, SSDCache, SSDComplexKeyCache dictionaries. Added support for Nullable type in Cache, ComplexKeyCache, SSDCache, SSDComplexKeyCache dictionaries. Added support for multiple attributes fetch with dictGet, dictGetOrDefault functions. Fixes #21517. #20595 (Maksim Kita).Support dictHas function for RangeHashedDictionary. Fixes #6680. #19816 (Maksim Kita).Add function timezoneOf that returns the timezone name of DateTime or DateTime64 data types. This does not close #9959. Fix inconsistencies in function names: add aliases timezone and timeZone as well as toTimezone and toTimeZone and timezoneOf and timeZoneOf. #22001 (alexey-milovidov).Add new optional clause GRANTEES for CREATE/ALTER USER commands. It specifies users or roles which are allowed to receive grants from this user on condition this user has also all required access granted with grant option. By default GRANTEES ANY is used which means a user with grant option can grant to anyone. Syntax: CREATE USER ... GRANTEES {user | role | ANY | NONE} [,...] [EXCEPT {user | role} [,...]]. #21641 (Vitaly Baranov).Add new column slowdowns_count to system.clusters. When using hedged requests, it shows how many times we switched to another replica because this replica was responding slowly. Also show actual value of errors_count in system.clusters. #21480 (Kruglov Pavel).Add _partition_id virtual column for MergeTree* engines. Allow to prune partitions by _partition_id. Add partitionID() function to calculate partition id string. #21401 (Amos Bird).Add function isIPAddressInRange to test if an IPv4 or IPv6 address is contained in a given CIDR network prefix. #21329 (PHO).Added new SQL command ALTER TABLE 'table_name' UNFREEZE [PARTITION 'part_expr'] WITH NAME 'backup_name'. This command is needed to properly remove 'freezed' partitions from all disks. #21142 (Pavel Kovalenko).Supports implicit key type conversion for JOIN. #19885 (Vladimir). Experimental Feature​ Support RANGE OFFSET frame (for window functions) for floating point types. Implement lagInFrame/leadInFrame window functions, which are analogous to lag/lead, but respect the window frame. They are identical when the frame is between unbounded preceding and unbounded following. This closes #5485. #21895 (Alexander Kuzmenkov).Zero-copy replication for ReplicatedMergeTree over S3 storage. #16240 (ianton-ru).Added possibility to migrate existing S3 disk to the schema with backup-restore capabilities. #22070 (Pavel Kovalenko). Performance Improvement​ Supported parallel formatting in clickhouse-local and everywhere else. #21630 (Nikita Mikhaylov).Support parallel parsing for CSVWithNames and TSVWithNames formats. This closes #21085. #21149 (Nikita Mikhaylov).Enable read with mmap IO for file ranges from 64 MiB (the settings min_bytes_to_use_mmap_io). It may lead to moderate performance improvement. #22326 (alexey-milovidov).Add cache for files read with min_bytes_to_use_mmap_io setting. It makes significant (2x and more) performance improvement when the value of the setting is small by avoiding frequent mmap/munmap calls and the consequent page faults. Note that mmap IO has major drawbacks that makes it less reliable in production (e.g. hung or SIGBUS on faulty disks; less controllable memory usage). Nevertheless it is good in benchmarks. #22206 (alexey-milovidov).Avoid unnecessary data copy when using codec NONE. Please note that codec NONE is mostly useless - it's recommended to always use compression (LZ4 is by default). Despite the common belief, disabling compression may not improve performance (the opposite effect is possible). The NONE codec is useful in some cases: - when data is uncompressable; - for synthetic benchmarks. #22145 (alexey-milovidov).Faster GROUP BY with small max_rows_to_group_by and group_by_overflow_mode='any'. #21856 (Nikolai Kochetov).Optimize performance of queries like SELECT ... FINAL ... WHERE. Now in queries with FINAL it's allowed to move to PREWHERE columns, which are in sorting key. #21830 (foolchi).Improved performance by replacing memcpy to another implementation. This closes #18583. #21520 (alexey-milovidov).Improve performance of aggregation in order of sorting key (with enabled setting optimize_aggregation_in_order). #19401 (Anton Popov). Improvement​ Add connection pool for PostgreSQL table/database engine and dictionary source. Should fix #21444. #21839 (Kseniia Sumarokova).Support non-default table schema for postgres storage/table-function. Closes #21701. #21711 (Kseniia Sumarokova).Support replicas priority for postgres dictionary source. #21710 (Kseniia Sumarokova).Introduce a new merge tree setting min_bytes_to_rebalance_partition_over_jbod which allows assigning new parts to different disks of a JBOD volume in a balanced way. #16481 (Amos Bird).Added Grant, Revoke and System values of query_kind column for corresponding queries in system.query_log. #21102 (Vasily Nemkov).Allow customizing timeouts for HTTP connections used for replication independently from other HTTP timeouts. #20088 (nvartolomei).Better exception message in client in case of exception while server is writing blocks. In previous versions client may get misleading message like Data compressed with different methods. #22427 (alexey-milovidov).Fix error Directory tmp_fetch_XXX already exists which could happen after failed fetch part. Delete temporary fetch directory if it already exists. Fixes #14197. #22411 (nvartolomei).Fix MSan report for function range with UInt256 argument (support for large integers is experimental). This closes #22157. #22387 (alexey-milovidov).Add current_database column to system.processes table. It contains the current database of the query. #22365 (Alexander Kuzmenkov).Add case-insensitive history search/navigation and subword movement features to clickhouse-client. #22105 (Amos Bird).If tuple of NULLs, e.g. (NULL, NULL) is on the left hand side of IN operator with tuples of non-NULLs on the right hand side, e.g. SELECT (NULL, NULL) IN ((0, 0), (3, 1)) return 0 instead of throwing an exception about incompatible types. The expression may also appear due to optimization of something like SELECT (NULL, NULL) = (8, 0) OR (NULL, NULL) = (3, 2) OR (NULL, NULL) = (0, 0) OR (NULL, NULL) = (3, 1). This closes #22017. #22063 (alexey-milovidov).Update used version of simdjson to 0.9.1. This fixes #21984. #22057 (Vitaly Baranov).Added case insensitive aliases for CONNECTION_ID() and VERSION() functions. This fixes #22028. #22042 (Eugene Klimov).Add option strict_increase to windowFunnel function to calculate each event once (resolve #21835). #22025 (Vladimir).If partition key of a MergeTree table does not include Date or DateTime columns but includes exactly one DateTime64 column, expose its values in the min_time and max_time columns in system.parts and system.parts_columns tables. Add min_time and max_time columns to system.parts_columns table (these was inconsistency to the system.parts table). This closes #18244. #22011 (alexey-milovidov).Supported replication_alter_partitions_sync=1 setting in clickhouse-copier for moving partitions from helping table to destination. Decreased default timeouts. Fixes #21911. #21912 (turbo jason).Show path to data directory of EmbeddedRocksDB tables in system tables. #21903 (tavplubix).Add profile event HedgedRequestsChangeReplica, change read data timeout from sec to ms. #21886 (Kruglov Pavel).DiskS3 (experimental feature under development). Fixed bug with the impossibility to move directory if the destination is not empty and cache disk is used. #21837 (Pavel Kovalenko).Better formatting for Array and Map data types in Web UI. #21798 (alexey-milovidov).Update clusters only if their configurations were updated. #21685 (Kruglov Pavel).Propagate query and session settings for distributed DDL queries. Set distributed_ddl_entry_format_version to 2 to enable this. Added distributed_ddl_output_mode setting. Supported modes: none, throw (default), null_status_on_timeout and never_throw. Miscellaneous fixes and improvements for Replicated database engine. #21535 (tavplubix).If PODArray was instantiated with element size that is neither a fraction or a multiple of 16, buffer overflow was possible. No bugs in current releases exist. #21533 (alexey-milovidov).Add last_error_time/last_error_message/last_error_stacktrace/remote columns for system.errors. #21529 (Azat Khuzhin).Add aliases simpleJSONExtract/simpleJSONHas to visitParam/visitParamExtract{UInt, Int, Bool, Float, Raw, String}. Fixes #21383. #21519 (fastio).Add setting optimize_skip_unused_shards_limit to limit the number of sharding key values for optimize_skip_unused_shards. #21512 (Azat Khuzhin).Improve clickhouse-format to not throw exception when there are extra spaces or comment after the last query, and throw exception early with readable message when format ASTInsertQuery with data . #21311 (flynn).Improve support of integer keys in data type Map. #21157 (Anton Popov).MaterializeMySQL: attempt to reconnect to MySQL if the connection is lost. #20961 (Håvard Kvålen).Support more cases to rewrite CROSS JOIN to INNER JOIN. #20392 (Vladimir).Do not create empty parts on INSERT when optimize_on_insert setting enabled. Fixes #20304. #20387 (Kruglov Pavel).MaterializeMySQL: add minmax skipping index for _version column. #20382 (Stig Bakken).Add option --backslash for clickhouse-format, which can add a backslash at the end of each line of the formatted query. #21494 (flynn).Now clickhouse will not throw LOGICAL_ERROR exception when we try to mutate the already covered part. Fixes #22013. #22291 (alesapin). Bug Fix​ Remove socket from epoll before cancelling packet receiver in HedgedConnections to prevent possible race. Fixes #22161. #22443 (Kruglov Pavel).Add (missing) memory accounting in parallel parsing routines. In previous versions OOM was possible when the resultset contains very large blocks of data. This closes #22008. #22425 (alexey-milovidov).Fix exception which may happen when SELECT has constant WHERE condition and source table has columns which names are digits. #22270 (LiuNeng).Fix query cancellation with use_hedged_requests=0 and async_socket_for_remote=1. #22183 (Azat Khuzhin).Fix uncaught exception in InterserverIOHTTPHandler. #22146 (Azat Khuzhin).Fix docker entrypoint in case http_port is not in the config. #22132 (Ewout).Fix error Invalid number of rows in Chunk in JOIN with TOTALS and arrayJoin. Closes #19303. #22129 (Vladimir).Fix the background thread pool name which used to poll message from Kafka. The Kafka engine with the broken thread pool will not consume the message from message queue. #22122 (fastio).Fix waiting for OPTIMIZE and ALTER queries for ReplicatedMergeTree table engines. Now the query will not hang when the table was detached or restarted. #22118 (alesapin).Disable async_socket_for_remote/use_hedged_requests for buggy Linux kernels. #22109 (Azat Khuzhin).Docker entrypoint: avoid chown of . in case when LOG_PATH is empty. Closes #22100. #22102 (filimonov).The function decrypt was lacking a check for the minimal size of data encrypted in AEAD mode. This closes #21897. #22064 (alexey-milovidov).In rare case, merge for CollapsingMergeTree may create granule with index_granularity + 1 rows. Because of this, internal check, added in #18928 (affects 21.2 and 21.3), may fail with error Incomplete granules are not allowed while blocks are granules size. This error did not allow parts to merge. #21976 (Nikolai Kochetov).Reverted #15454 that may cause significant increase in memory usage while loading external dictionaries of hashed type. This closes #21935. #21948 (Maksim Kita).Prevent hedged connections overlaps (Unknown packet 9 from server error). #21941 (Azat Khuzhin).Fix reading the HTTP POST request with &quot;multipart/form-data&quot; content type in some cases. #21936 (Ivan).Fix wrong ORDER BY results when a query contains window functions, and optimization for reading in primary key order is applied. Fixes #21828. #21915 (Alexander Kuzmenkov).Fix deadlock in first catboost model execution. Closes #13832. #21844 (Kruglov Pavel).Fix incorrect query result (and possible crash) which could happen when WHERE or HAVING condition is pushed before GROUP BY. Fixes #21773. #21841 (Nikolai Kochetov).Better error handling and logging in WriteBufferFromS3. #21836 (Pavel Kovalenko).Fix possible crashes in aggregate functions with combinator Distinct, while using two-level aggregation. This is a follow-up fix of #18365 . Can only reproduced in production env. #21818 (Amos Bird).Fix scalar subquery index analysis. This fixes #21717 , which was introduced in #18896. #21766 (Amos Bird).Fix bug for ReplicatedMerge table engines when ALTER MODIFY COLUMN query doesn't change the type of Decimal column if its size (32 bit or 64 bit) doesn't change. #21728 (alesapin).Fix possible infinite waiting when concurrent OPTIMIZE and DROP are run for ReplicatedMergeTree. #21716 (Azat Khuzhin).Fix function arrayElement with type Map for constant integer arguments. #21699 (Anton Popov).Fix SIGSEGV on not existing attributes from ip_trie with access_to_key_from_attributes. #21692 (Azat Khuzhin).Server now start accepting connections only after DDLWorker and dictionaries initialization. #21676 (Azat Khuzhin).Add type conversion for keys of tables of type Join (previously led to SIGSEGV). #21646 (Azat Khuzhin).Fix distributed requests cancellation (for example simple select from multiple shards with limit, i.e. select * from remote('127.{2,3}', system.numbers) limit 100) with async_socket_for_remote=1. #21643 (Azat Khuzhin).Fix fsync_part_directory for horizontal merge. #21642 (Azat Khuzhin).Remove unknown columns from joined table in WHERE for queries to external database engines (MySQL, PostgreSQL). close #14614, close #19288 (dup), close #19645 (dup). #21640 (Vladimir).std::terminate was called if there is an error writing data into s3. #21624 (Vladimir).Fix possible error Cannot find column when optimize_skip_unused_shards is enabled and zero shards are used. #21579 (Azat Khuzhin).In case if query has constant WHERE condition, and setting optimize_skip_unused_shards enabled, all shards may be skipped and query could return incorrect empty result. #21550 (Amos Bird).Fix table function clusterAllReplicas returns wrong _shard_num. close #21481. #21498 (flynn).Fix that S3 table holds old credentials after config update. #21457 (Grigory Pervakov).Fixed race on SSL object inside SecureSocket in Poco. #21456 (Nikita Mikhaylov).Fix Avro format parsing for Kafka. Fixes #21437. #21438 (Ilya Golshtein).Fix receive and send timeouts and non-blocking read in secure socket. #21429 (Kruglov Pavel).force_drop_table flag didn't work for MATERIALIZED VIEW, it's fixed. Fixes #18943. #20626 (tavplubix).Fix name clashes in PredicateRewriteVisitor. It caused incorrect WHERE filtration after full join. Close #20497. #20622 (Vladimir). Build/Testing/Packaging Improvement​ Add Jepsen tests for ClickHouse Keeper. #21677 (alesapin).Run stateless tests in parallel in CI. Depends on #22181. #22300 (alesapin).Enable status check for SQLancer CI run. #22015 (Ilya Yatsishin).Multiple preparations for PowerPC builds: Enable the bundled openldap on ppc64le. #22487 (Kfir Itzhak). Enable compiling on ppc64le with Clang. #22476 (Kfir Itzhak). Fix compiling boost on ppc64le. #22474 (Kfir Itzhak). Fix CMake error about internal CMake variable CMAKE_ASM_COMPILE_OBJECT not set on ppc64le. #22469 (Kfir Itzhak). Fix Fedora/RHEL/CentOS not finding libclang_rt.builtins on ppc64le. #22458 (Kfir Itzhak). Enable building with jemalloc on ppc64le. #22447 (Kfir Itzhak). Fix ClickHouse's config embedding and cctz's timezone embedding on ppc64le. #22445 (Kfir Itzhak). Fixed compiling on ppc64le and use the correct instruction pointer register on ppc64le. #22430 (Kfir Itzhak).Re-enable the S3 (AWS) library on aarch64. #22484 (Kfir Itzhak).Add tzdata to Docker containers because reading ORC formats requires it. This closes #14156. #22000 (alexey-milovidov).Introduce 2 arguments for clickhouse-server image Dockerfile: deb_location &amp; single_binary_location. #21977 (filimonov).Allow to use clang-tidy with release builds by enabling assertions if it is used. #21914 (alexey-milovidov).Add llvm-12 binaries name to search in cmake scripts. Implicit constants conversions to mute clang warnings. Updated submodules to build with CMake 3.19. Mute recursion in macro expansion in readpassphrase library. Deprecated -fuse-ld changed to --ld-path for clang. #21597 (Ilya Yatsishin).Updating docker/test/testflows/runner/dockerd-entrypoint.sh to use Yandex dockerhub-proxy, because Docker Hub has enabled very restrictive rate limits #21551 (vzakaznikov).Fix macOS shared lib build. #20184 (nvartolomei).Add ctime option to zookeeper-dump-tree. It allows to dump node creation time. #21842 (Ilya). "},{"title":"ClickHouse release 21.3 (LTS)​","type":1,"pageTitle":"2021","url":"docs/en/whats-new/changelog/2021#clickhouse-release-213-lts","content":""},{"title":"ClickHouse release v21.3, 2021-03-12​","type":1,"pageTitle":"2021","url":"docs/en/whats-new/changelog/2021#clickhouse-release-v213-2021-03-12","content":"Backward Incompatible Change​ Now it's not allowed to create MergeTree tables in old syntax with table TTL because it's just ignored. Attach of old tables is still possible. #20282 (alesapin).Now all case-insensitive function names will be rewritten to their canonical representations. This is needed for projection query routing (the upcoming feature). #20174 (Amos Bird).Fix creation of TTL in cases, when its expression is a function and it is the same as ORDER BY key. Now it's allowed to set custom aggregation to primary key columns in TTL with GROUP BY. Backward incompatible: For primary key columns, which are not in GROUP BY and aren't set explicitly now is applied function any instead of max, when TTL is expired. Also if you use TTL with WHERE or GROUP BY you can see exceptions at merges, while making rolling update. #15450 (Anton Popov). New Feature​ Add file engine settings: engine_file_empty_if_not_exists and engine_file_truncate_on_insert. #20620 (M0r64n).Add aggregate function deltaSum for summing the differences between consecutive rows. #20057 (Russ Frank).New event_time_microseconds column in system.part_log table. #20027 (Bharat Nallan).Added timezoneOffset(datetime) function which will give the offset from UTC in seconds. This close #issue:19850. #19962 (keenwolf).Add setting insert_shard_id to support insert data into specific shard from distributed table. #19961 (flynn).Function reinterpretAs updated to support big integers. Fixes #19691. #19858 (Maksim Kita).Added Server Side Encryption Customer Keys (the x-amz-server-side-encryption-customer-(key/md5) header) support in S3 client. See the link. Closes #19428. #19748 (Vladimir Chebotarev).Added implicit_key option for executable dictionary source. It allows to avoid printing key for every record if records comes in the same order as the input keys. Implements #14527. #19677 (Maksim Kita).Add quota type query_selects and query_inserts. #19603 (JackyWoo).Add function extractTextFromHTML #19600 (zlx19950903), (alexey-milovidov).Tables with MergeTree* engine now have two new table-level settings for query concurrency control. Setting max_concurrent_queries limits the number of concurrently executed queries which are related to this table. Setting min_marks_to_honor_max_concurrent_queries tells to apply previous setting only if query reads at least this number of marks. #19544 (Amos Bird).Added file function to read file from user_files directory as a String. This is different from the file table function. This implements #issue:18851. #19204 (keenwolf). Experimental feature​ Add experimental Replicated database engine. It replicates DDL queries across multiple hosts. #16193 (tavplubix).Introduce experimental support for window functions, enabled with allow_experimental_window_functions = 1. This is a preliminary, alpha-quality implementation that is not suitable for production use and will change in backward-incompatible ways in future releases. Please see the documentation for the list of supported features. #20337 (Alexander Kuzmenkov).Add the ability to backup/restore metadata files for DiskS3. #18377 (Pavel Kovalenko). Performance Improvement​ Hedged requests for remote queries. When setting use_hedged_requests enabled (off by default), allow to establish many connections with different replicas for query. New connection is enabled in case existent connection(s) with replica(s) were not established within hedged_connection_timeout or no data was received within receive_data_timeout. Query uses the first connection which send non empty progress packet (or data packet, if allow_changing_replica_until_first_data_packet); other connections are cancelled. Queries with max_parallel_replicas &gt; 1 are supported. #19291 (Kruglov Pavel). This allows to significantly reduce tail latencies on very large clusters.Added support for PREWHERE (and enable the corresponding optimization) when tables have row-level security expressions specified. #19576 (Denis Glazachev).The setting distributed_aggregation_memory_efficient is enabled by default. It will lower memory usage and improve performance of distributed queries. #20599 (alexey-milovidov).Improve performance of GROUP BY multiple fixed size keys. #20472 (alexey-milovidov).Improve performance of aggregate functions by more strict aliasing. #19946 (alexey-milovidov).Speed up reading from Memory tables in extreme cases (when reading speed is in order of 50 GB/sec) by simplification of pipeline and (consequently) less lock contention in pipeline scheduling. #20468 (alexey-milovidov).Partially reimplement HTTP server to make it making less copies of incoming and outgoing data. It gives up to 1.5 performance improvement on inserting long records over HTTP. #19516 (Ivan).Add compress setting for Memory tables. If it's enabled the table will use less RAM. On some machines and datasets it can also work faster on SELECT, but it is not always the case. This closes #20093. Note: there are reasons why Memory tables can work slower than MergeTree: (1) lack of compression (2) static size of blocks (3) lack of indices and prewhere... #20168 (alexey-milovidov).Slightly better code in aggregation. #20978 (alexey-milovidov).Add back intDiv/modulo specializations for better performance. This fixes #21293 . The regression was introduced in https://github.com/ClickHouse/ClickHouse/pull/18145 . #21307 (Amos Bird).Do not squash blocks too much on INSERT SELECT if inserting into Memory table. In previous versions inefficient data representation was created in Memory table after INSERT SELECT. This closes #13052. #20169 (alexey-milovidov).Fix at least one case when DataType parser may have exponential complexity (found by fuzzer). This closes #20096. #20132 (alexey-milovidov).Parallelize SELECT with FINAL for single part with level &gt; 0 when do_not_merge_across_partitions_select_final setting is 1. #19375 (Kruglov Pavel).Fill only requested columns when querying system.parts and system.parts_columns. Closes #19570. #21035 (Anmol Arora).Perform algebraic optimizations of arithmetic expressions inside avg aggregate function. close #20092. #20183 (flynn). Improvement​ Case-insensitive compression methods for table functions. Also fixed LZMA compression method which was checked in upper case. #21416 (Vladimir Chebotarev).Add two settings to delay or throw error during insertion when there are too many inactive parts. This is useful when server fails to clean up parts quickly enough. #20178 (Amos Bird).Provide better compatibility for mysql clients. 1. mysql jdbc 2. mycli. #21367 (Amos Bird).Forbid to drop a column if it's referenced by materialized view. Closes #21164. #21303 (flynn).MySQL dictionary source will now retry unexpected connection failures (Lost connection to MySQL server during query) which sometimes happen on SSL/TLS connections. #21237 (Alexander Kazakov).Usability improvement: more consistent DateTime64 parsing: recognize the case when unix timestamp with subsecond resolution is specified as scaled integer (like 1111111111222 instead of 1111111111.222). This closes #13194. #21053 (alexey-milovidov).Do only merging of sorted blocks on initiator with distributed_group_by_no_merge. #20882 (Azat Khuzhin).When loading config for mysql source ClickHouse will now randomize the list of replicas with the same priority to ensure the round-robin logics of picking mysql endpoint. This closes #20629. #20632 (Alexander Kazakov).Function 'reinterpretAs(x, Type)' renamed into 'reinterpret(x, Type)'. #20611 (Maksim Kita).Support vhost for RabbitMQ engine #20576. #20596 (Kseniia Sumarokova).Improved serialization for data types combined of Arrays and Tuples. Improved matching enum data types to protobuf enum type. Fixed serialization of the Map data type. Omitted values are now set by default. #20506 (Vitaly Baranov).Fixed race between execution of distributed DDL tasks and cleanup of DDL queue. Now DDL task cannot be removed from ZooKeeper if there are active workers. Fixes #20016. #20448 (tavplubix).Make FQDN and other DNS related functions work correctly in alpine images. #20336 (filimonov).Do not allow early constant folding of explicitly forbidden functions. #20303 (Azat Khuzhin).Implicit conversion from integer to Decimal type might succeeded if integer value doe not fit into Decimal type. Now it throws ARGUMENT_OUT_OF_BOUND. #20232 (tavplubix).Lockless SYSTEM FLUSH DISTRIBUTED. #20215 (Azat Khuzhin).Normalize count(constant), sum(1) to count(). This is needed for projection query routing. #20175 (Amos Bird).Support all native integer types in bitmap functions. #20171 (Amos Bird).Updated CacheDictionary, ComplexCacheDictionary, SSDCacheDictionary, SSDComplexKeyDictionary to use LRUHashMap as underlying index. #20164 (Maksim Kita).The setting access_management is now configurable on startup by providing CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT, defaults to disabled (0) which was the prior value. #20139 (Marquitos).Fix toDateTime64(toDate()/toDateTime()) for DateTime64 - Implement DateTime64 clamping to match DateTime behaviour. #20131 (Azat Khuzhin).Quota improvements: SHOW TABLES is now considered as one query in the quota calculations, not two queries. SYSTEM queries now consume quota. Fix calculation of interval's end in quota consumption. #20106 (Vitaly Baranov).Supports path IN (set) expressions for system.zookeeper table. #20105 (小路).Show full details of MaterializeMySQL tables in system.tables. #20051 (Stig Bakken).Fix data race in executable dictionary that was possible only on misuse (when the script returns data ignoring its input). #20045 (alexey-milovidov).The value of MYSQL_OPT_RECONNECT option can now be controlled by &quot;opt_reconnect&quot; parameter in the config section of mysql replica. #19998 (Alexander Kazakov).If user calls JSONExtract function with Float32 type requested, allow inaccurate conversion to the result type. For example the number 0.1 in JSON is double precision and is not representable in Float32, but the user still wants to get it. Previous versions return 0 for non-Nullable type and NULL for Nullable type to indicate that conversion is imprecise. The logic was 100% correct but it was surprising to users and leading to questions. This closes #13962. #19960 (alexey-milovidov).Add conversion of block structure for INSERT into Distributed tables if it does not match. #19947 (Azat Khuzhin).Improvement for the system.distributed_ddl_queue table. Initialize MaxDDLEntryID to the last value after restarting. Before this PR, MaxDDLEntryID will remain zero until a new DDLTask is processed. #19924 (Amos Bird).Show MaterializeMySQL tables in system.parts. #19770 (Stig Bakken).Add separate config directive for Buffer profile. #19721 (Azat Khuzhin).Move conditions that are not related to JOIN to WHERE clause. #18720. #19685 (hexiaoting).Add ability to throttle INSERT into Distributed based on amount of pending bytes for async send (bytes_to_delay_insert/max_delay_to_insert and bytes_to_throw_insert settings for Distributed engine has been added). #19673 (Azat Khuzhin).Fix some rare cases when write errors can be ignored in destructors. #19451 (Azat Khuzhin).Print inline frames in stack traces for fatal errors. #19317 (Ivan). Bug Fix​ Fix redundant reconnects to ZooKeeper and the possibility of two active sessions for a single clickhouse server. Both problems introduced in #14678. #21264 (alesapin).Fix error Bad cast from type ... to DB::ColumnLowCardinality while inserting into table with LowCardinality column from Values format. Fixes #21140 #21357 (Nikolai Kochetov).Fix a deadlock in ALTER DELETE mutations for non replicated MergeTree table engines when the predicate contains the table itself. Fixes #20558. #21477 (alesapin).Fix SIGSEGV for distributed queries on failures. #21434 (Azat Khuzhin).Now ALTER MODIFY COLUMN queries will correctly affect changes in partition key, skip indices, TTLs, and so on. Fixes #13675. #21334 (alesapin).Fix bug with join_use_nulls and joining TOTALS from subqueries. This closes #19362 and #21137. #21248 (vdimir).Fix crash in EXPLAIN for query with UNION. Fixes #20876, #21170. #21246 (flynn).Now mutations allowed only for table engines that support them (MergeTree family, Memory, MaterializedView). Other engines will report a more clear error. Fixes #21168. #21183 (alesapin).Fixes #21112. Fixed bug that could cause duplicates with insert query (if one of the callbacks came a little too late). #21138 (Kseniia Sumarokova).Fix input_format_null_as_default take effective when types are nullable. This fixes #21116 . #21121 (Amos Bird).fix bug related to cast Tuple to Map. Closes #21029. #21120 (hexiaoting).Fix the metadata leak when the Replicated*MergeTree with custom (non default) ZooKeeper cluster is dropped. #21119 (fastio).Fix type mismatch issue when using LowCardinality keys in joinGet. This fixes #21114. #21117 (Amos Bird).fix default_replica_path and default_replica_name values are useless on Replicated(*)MergeTree engine when the engine needs specify other parameters. #21060 (mxzlxy).Out of bound memory access was possible when formatting specifically crafted out of range value of type DateTime64. This closes #20494. This closes #20543. #21023 (alexey-milovidov).Block parallel insertions into storage join. #21009 (vdimir).Fixed behaviour, when ALTER MODIFY COLUMN created mutation, that will knowingly fail. #21007 (Anton Popov).Closes #9969. Fixed Brotli http compression error, which reproduced for large data sizes, slightly complicated structure and with json output format. Update Brotli to the latest version to include the &quot;fix rare access to uninitialized data in ring-buffer&quot;. #20991 (Kseniia Sumarokova).Fix 'Empty task was returned from async task queue' on query cancellation. #20881 (Azat Khuzhin).USE database; query did not work when using MySQL 5.7 client to connect to ClickHouse server, it's fixed. Fixes #18926. #20878 (tavplubix).Fix usage of -Distinct combinator with -State combinator in aggregate functions. #20866 (Anton Popov).Fix subquery with union distinct and limit clause. close #20597. #20610 (flynn).Fixed inconsistent behavior of dictionary in case of queries where we look for absent keys in dictionary. #20578 (Nikita Mikhaylov).Fix the number of threads for scalar subqueries and subqueries for index (after #19007 single thread was always used). Fixes #20457, #20512. #20550 (Nikolai Kochetov).Fix crash which could happen if unknown packet was received from remove query (was introduced in #17868). #20547 (Azat Khuzhin).Add proper checks while parsing directory names for async INSERT (fixes SIGSEGV). #20498 (Azat Khuzhin).Fix function transform does not work properly for floating point keys. Closes #20460. #20479 (flynn).Fix infinite loop when propagating WITH aliases to subqueries. This fixes #20388. #20476 (Amos Bird).Fix abnormal server termination when http client goes away. #20464 (Azat Khuzhin).Fix LOGICAL_ERROR for join_use_nulls=1 when JOIN contains const from SELECT. #20461 (Azat Khuzhin).Check if table function view is used in expression list and throw an error. This fixes #20342. #20350 (Amos Bird).Avoid invalid dereference in RANGE_HASHED() dictionary. #20345 (Azat Khuzhin).Fix null dereference with join_use_nulls=1. #20344 (Azat Khuzhin).Fix incorrect result of binary operations between two constant decimals of different scale. Fixes #20283. #20339 (Maksim Kita).Fix too often retries of failed background tasks for ReplicatedMergeTree table engines family. This could lead to too verbose logging and increased CPU load. Fixes #20203. #20335 (alesapin).Restrict to DROP or RENAME version column of *CollapsingMergeTree and ReplacingMergeTree table engines. #20300 (alesapin).Fixed the behavior when in case of broken JSON we tried to read the whole file into memory which leads to exception from the allocator. Fixes #19719. #20286 (Nikita Mikhaylov).Fix exception during vertical merge for MergeTree table engines family which don't allow to perform vertical merges. Fixes #20259. #20279 (alesapin).Fix rare server crash on config reload during the shutdown. Fixes #19689. #20224 (alesapin).Fix CTE when using in INSERT SELECT. This fixes #20187, fixes #20195. #20211 (Amos Bird).Fixes #19314. #20156 (Ivan).fix toMinute function to handle special timezone correctly. #20149 (keenwolf).Fix server crash after query with if function with Tuple type of then/else branches result. Tuple type must contain Array or another complex type. Fixes #18356. #20133 (alesapin).The MongoDB table engine now establishes connection only when it's going to read data. ATTACH TABLE won't try to connect anymore. #20110 (Vitaly Baranov).Bugfix in StorageJoin. #20079 (vdimir).Fix the case when calculating modulo of division of negative number by small divisor, the resulting data type was not large enough to accomodate the negative result. This closes #20052. #20067 (alexey-milovidov).MaterializeMySQL: Fix replication for statements that update several tables. #20066 (Håvard Kvålen).Prevent &quot;Connection refused&quot; in docker during initialization script execution. #20012 (filimonov).EmbeddedRocksDB is an experimental storage. Fix the issue with lack of proper type checking. Simplified code. This closes #19967. #19972 (alexey-milovidov).Fix a segfault in function fromModifiedJulianDay when the argument type is Nullable(T) for any integral types other than Int32. #19959 (PHO).BloomFilter index crash fix. Fixes #19757. #19884 (Maksim Kita).Deadlock was possible if system.text_log is enabled. This fixes #19874. #19875 (alexey-milovidov).Fix starting the server with tables having default expressions containing dictGet(). Allow getting return type of dictGet() without loading dictionary. #19805 (Vitaly Baranov).Fix clickhouse-client abort exception while executing only select. #19790 (taiyang-li).Fix a bug that moving pieces to destination table may failed in case of launching multiple clickhouse-copiers. #19743 (madianjun).Background thread which executes ON CLUSTER queries might hang waiting for dropped replicated table to do something. It's fixed. #19684 (yiguolei). Build/Testing/Packaging Improvement​ Allow to build ClickHouse with AVX-2 enabled globally. It gives slight performance benefits on modern CPUs. Not recommended for production and will not be supported as official build for now. #20180 (alexey-milovidov).Fix some of the issues found by Coverity. See #19964. #20010 (alexey-milovidov).Allow to start up with modified binary under gdb. In previous version if you set up breakpoint in gdb before start, server will refuse to start up due to failed integrity check. #21258 (alexey-milovidov).Add a test for different compression methods in Kafka. #21111 (filimonov).Fixed port clash from test_storage_kerberized_hdfs test. #19974 (Ilya Yatsishin).Print stdout and stderr to log when failed to start docker in integration tests. Before this PR there was a very short error message in this case which didn't help to investigate the problems. #20631 (Vitaly Baranov). "},{"title":"ClickHouse release 21.2​","type":1,"pageTitle":"2021","url":"docs/en/whats-new/changelog/2021#clickhouse-release-212","content":""},{"title":"ClickHouse release v21.2.2.8-stable, 2021-02-07​","type":1,"pageTitle":"2021","url":"docs/en/whats-new/changelog/2021#clickhouse-release-v21228-stable-2021-02-07","content":"Backward Incompatible Change​ Bitwise functions (bitAnd, bitOr, etc) are forbidden for floating point arguments. Now you have to do explicit cast to integer. #19853 (Azat Khuzhin).Forbid lcm/gcd for floats. #19532 (Azat Khuzhin).Fix memory tracking for OPTIMIZE TABLE/merges; account query memory limits and sampling for OPTIMIZE TABLE/merges. #18772 (Azat Khuzhin).Disallow floating point column as partition key, see #18421. #18464 (hexiaoting).Excessive parenthesis in type definitions no longer supported, example: Array((UInt8)). New Feature​ Added PostgreSQL table engine (both select/insert, with support for multidimensional arrays), also as table function. Added PostgreSQL dictionary source. Added PostgreSQL database engine. #18554 (Kseniia Sumarokova).Data type Nested now supports arbitrary levels of nesting. Introduced subcolumns of complex types, such as size0 in Array, null in Nullable, names of Tuple elements, which can be read without reading of whole column. #17310 (Anton Popov).Added Nullable support for FlatDictionary, HashedDictionary, ComplexKeyHashedDictionary, DirectDictionary, ComplexKeyDirectDictionary, RangeHashedDictionary. #18236 (Maksim Kita).Adds a new table called system.distributed_ddl_queue that displays the queries in the DDL worker queue. #17656 (Bharat Nallan).Added support of mapping LDAP group names, and attribute values in general, to local roles for users from ldap user directories. #17211 (Denis Glazachev).Support insert into table function cluster, and for both table functions remote and cluster, support distributing data across nodes by specify sharding key. Close #16752. #18264 (flynn).Add function decodeXMLComponent to decode characters for XML. Example: SELECT decodeXMLComponent('Hello,&amp;quot;world&amp;quot;!') #17659. #18542 (nauta).Added functions parseDateTimeBestEffortUSOrZero, parseDateTimeBestEffortUSOrNull. #19712 (Maksim Kita).Add sign math function. #19527 (flynn).Add information about used features (functions, table engines, etc) into system.query_log. #18495. #19371 (Kseniia Sumarokova).Function formatDateTime support the %Q modification to format date to quarter. #19224 (Jianmei Zhang).Support MetaKey+Enter hotkey binding in play UI. #19012 (sundyli).Add three functions for map data type: 1. mapContains(map, key) to check weather map.keys include the second parameter key. 2. mapKeys(map) return all the keys in Array format 3. mapValues(map) return all the values in Array format. #18788 (hexiaoting).Add log_comment setting related to #18494. #18549 (Zijie Lu).Add support of tuple argument to argMin and argMax functions. #17359 (Ildus Kurbangaliev).Support EXISTS VIEW syntax. #18552 (Du Chuan).Add SELECT ALL syntax. closes #18706. #18723 (flynn). Performance Improvement​ Faster parts removal by lowering the number of stat syscalls. This returns the optimization that existed while ago. More safe interface of IDisk. This closes #19065. #19086 (alexey-milovidov).Aliases declared in WITH statement are properly used in index analysis. Queries like WITH column AS alias SELECT ... WHERE alias = ... may use index now. #18896 (Amos Bird).Add optimize_alias_column_prediction (on by default), that will: - Respect aliased columns in WHERE during partition pruning and skipping data using secondary indexes; - Respect aliased columns in WHERE for trivial count queries for optimize_trivial_count; - Respect aliased columns in GROUP BY/ORDER BY for optimize_aggregation_in_order/optimize_read_in_order. #16995 (sundyli).Speed up aggregate function sum. Improvement only visible on synthetic benchmarks and not very practical. #19216 (alexey-milovidov).Update libc++ and use another ABI to provide better performance. #18914 (Danila Kutenin).Rewrite sumIf() and sum(if()) function to countIf() function when logically equivalent. #17041 (flynn).Use a connection pool for S3 connections, controlled by the s3_max_connections settings. #13405 (Vladimir Chebotarev).Add support for zstd long option for better compression of string columns to save space. #17184 (ygrek).Slightly improve server latency by removing access to configuration on every connection. #19863 (alexey-milovidov).Reduce lock contention for multiple layers of the Buffer engine. #19379 (Azat Khuzhin).Support splitting Filter step of query plan into Expression + Filter pair. Together with Expression + Expression merging optimization (#17458) it may delay execution for some expressions after Filter step. #19253 (Nikolai Kochetov). Improvement​ SELECT count() FROM table now can be executed if only one any column can be selected from the table. This PR fixes #10639. #18233 (Vitaly Baranov).Set charset to utf8mb4 when interacting with remote MySQL servers. Fixes #19795. #19800 (alexey-milovidov).S3 table function now supports auto compression mode (autodetect). This closes #18754. #19793 (Vladimir Chebotarev).Correctly output infinite arguments for formatReadableTimeDelta function. In previous versions, there was implicit conversion to implementation specific integer value. #19791 (alexey-milovidov).Table function S3 will use global region if the region can't be determined exactly. This closes #10998. #19750 (Vladimir Chebotarev).In distributed queries if the setting async_socket_for_remote is enabled, it was possible to get stack overflow at least in debug build configuration if very deeply nested data type is used in table (e.g. Array(Array(Array(...more...)))). This fixes #19108. This change introduces minor backward incompatibility: excessive parenthesis in type definitions no longer supported, example: Array((UInt8)). #19736 (alexey-milovidov).Add separate pool for message brokers (RabbitMQ and Kafka). #19722 (Azat Khuzhin).Fix rare max_number_of_merges_with_ttl_in_pool limit overrun (more merges with TTL can be assigned) for non-replicated MergeTree. #19708 (alesapin).Dictionary: better error message during attribute parsing. #19678 (Maksim Kita).Add an option to disable validation of checksums on reading. Should never be used in production. Please do not expect any benefits in disabling it. It may only be used for experiments and benchmarks. The setting only applicable for tables of MergeTree family. Checksums are always validated for other table engines and when receiving data over network. In my observations there is no performance difference or it is less than 0.5%. #19588 (alexey-milovidov).Support constant result in function multiIf. #19533 (Maksim Kita).Enable function length/empty/notEmpty for datatype Map, which returns keys number in Map. #19530 (taiyang-li).Add --reconnect option to clickhouse-benchmark. When this option is specified, it will reconnect before every request. This is needed for testing. #19872 (alexey-milovidov).Support using the new location of .debug file. This fixes #19348. #19520 (Amos Bird).toIPv6 function parses IPv4 addresses. #19518 (Bharat Nallan).Add http_referer field to system.query_log, system.processes, etc. This closes #19389. #19390 (alexey-milovidov).Improve MySQL compatibility by making more functions case insensitive and adding aliases. #19387 (Daniil Kondratyev).Add metrics for MergeTree parts (Wide/Compact/InMemory) types. #19381 (Azat Khuzhin).Allow docker to be executed with arbitrary uid. #19374 (filimonov).Fix wrong alignment of values of IPv4 data type in Pretty formats. They were aligned to the right, not to the left. This closes #19184. #19339 (alexey-milovidov).Allow change max_server_memory_usage without restart. This closes #18154. #19186 (alexey-milovidov).The exception when function bar is called with certain NaN argument may be slightly misleading in previous versions. This fixes #19088. #19107 (alexey-milovidov).Explicitly set uid / gid of clickhouse user &amp; group to the fixed values (101) in clickhouse-server images. #19096 (filimonov).Fixed PeekableReadBuffer: Memory limit exceed error when inserting data with huge strings. Fixes #18690. #18979 (tavplubix).Docker image: several improvements for clickhouse-server entrypoint. #18954 (filimonov).Add normalizeQueryKeepNames and normalizedQueryHashKeepNames to normalize queries without masking long names with ?. This helps better analyze complex query logs. #18910 (Amos Bird).Check per-block checksum of the distributed batch on the sender before sending (without reading the file twice, the checksums will be verified while reading), this will avoid stuck of the INSERT on the receiver (on truncated .bin file on the sender). Avoid reading .bin files twice for batched INSERT (it was required to calculate rows/bytes to take squashing into account, now this information included into the header, backward compatible is preserved). #18853 (Azat Khuzhin).Fix issues with RIGHT and FULL JOIN of tables with aggregate function states. In previous versions exception about cloneResized method was thrown. #18818 (templarzq).Added prefix-based S3 endpoint settings. #18812 (Vladimir Chebotarev).Add [UInt8, UInt16, UInt32, UInt64] arguments types support for bitmapTransform, bitmapSubsetInRange, bitmapSubsetLimit, bitmapContains functions. This closes #18713. #18791 (sundyli).Allow CTE (Common Table Expressions) to be further aliased. Propagate CSE (Common Subexpressions Elimination) to subqueries in the same level when enable_global_with_statement = 1. This fixes #17378 . This fixes https://github.com/ClickHouse/ClickHouse/pull/16575#issuecomment-753416235 . #18684 (Amos Bird).Update librdkafka to v1.6.0-RC2. Fixes #18668. #18671 (filimonov).In case of unexpected exceptions automatically restart background thread which is responsible for execution of distributed DDL queries. Fixes #17991. #18285 (徐炘).Updated AWS C++ SDK in order to utilize global regions in S3. #17870 (Vladimir Chebotarev).Added support for WITH ... [AND] [PERIODIC] REFRESH [interval_in_sec] clause when creating LIVE VIEW tables. #14822 (vzakaznikov).Restrict MODIFY TTL queries for MergeTree tables created in old syntax. Previously the query succeeded, but actually it had no effect. #19064 (Anton Popov). Bug Fix​ Fix index analysis of binary functions with constant argument which leads to wrong query results. This fixes #18364. #18373 (Amos Bird).Fix starting the server with tables having default expressions containing dictGet(). Allow getting return type of dictGet() without loading dictionary. #19805 (Vitaly Baranov).Fix server crash after query with if function with Tuple type of then/else branches result. Tuple type must contain Array or another complex type. Fixes #18356. #20133 (alesapin).MaterializeMySQL (experimental feature): Fix replication for statements that update several tables. #20066 (Håvard Kvålen).Prevent &quot;Connection refused&quot; in docker during initialization script execution. #20012 (filimonov).EmbeddedRocksDB is an experimental storage. Fix the issue with lack of proper type checking. Simplified code. This closes #19967. #19972 (alexey-milovidov).Fix a segfault in function fromModifiedJulianDay when the argument type is Nullable(T) for any integral types other than Int32. #19959 (PHO).The function greatCircleAngle returned inaccurate results in previous versions. This closes #19769. #19789 (alexey-milovidov).Fix rare bug when some replicated operations (like mutation) cannot process some parts after data corruption. Fixes #19593. #19702 (alesapin).Background thread which executes ON CLUSTER queries might hang waiting for dropped replicated table to do something. It's fixed. #19684 (yiguolei).Fix wrong deserialization of columns description. It makes INSERT into a table with a column named \\ impossible. #19479 (alexey-milovidov).Mark distributed batch as broken in case of empty data block in one of files. #19449 (Azat Khuzhin).Fixed very rare bug that might cause mutation to hang after DROP/DETACH/REPLACE/MOVE PARTITION. It was partially fixed by #15537 for the most cases. #19443 (tavplubix).Fix possible error Extremes transform was already added to pipeline. Fixes #14100. #19430 (Nikolai Kochetov).Fix default value in join types with non-zero default (e.g. some Enums). Closes #18197. #19360 (vdimir).Do not mark file for distributed send as broken on EOF. #19290 (Azat Khuzhin).Fix leaking of pipe fd for async_socket_for_remote. #19153 (Azat Khuzhin).Fix infinite reading from file in ORC format (was introduced in #10580). Fixes #19095. #19134 (Nikolai Kochetov).Fix issue in merge tree data writer which can lead to marks with bigger size than fixed granularity size. Fixes #18913. #19123 (alesapin).Fix startup bug when clickhouse was not able to read compression codec from LowCardinality(Nullable(...)) and throws exception Attempt to read after EOF. Fixes #18340. #19101 (alesapin).Simplify the implementation of tupleHammingDistance. Support for tuples of any equal length. Fixes #19029. #19084 (Nikolai Kochetov).Make sure groupUniqArray returns correct type for argument of Enum type. This closes #17875. #19019 (alexey-milovidov).Fix possible error Expected single dictionary argument for function if use function ignore with LowCardinality argument. Fixes #14275. #19016 (Nikolai Kochetov).Fix inserting of LowCardinality column to table with TinyLog engine. Fixes #18629. #19010 (Nikolai Kochetov).Fix minor issue in JOIN: Join tries to materialize const columns, but our code waits for them in other places. #18982 (Nikita Mikhaylov).Disable optimize_move_functions_out_of_any because optimization is not always correct. This closes #18051. This closes #18973. #18981 (alexey-milovidov).Fix possible exception QueryPipeline stream: different number of columns caused by merging of query plan's Expression steps. Fixes #18190. #18980 (Nikolai Kochetov).Fixed very rare deadlock at shutdown. #18977 (tavplubix).Fixed rare crashes when server run out of memory. #18976 (tavplubix).Fix incorrect behavior when ALTER TABLE ... DROP PART 'part_name' query removes all deduplication blocks for the whole partition. Fixes #18874. #18969 (alesapin).Fixed issue #18894 Add a check to avoid exception when long column alias('table.column' style, usually auto-generated by BI tools like Looker) equals to long table name. #18968 (Daniel Qin).Fix error Task was not found in task queue (possible only for remote queries, with async_socket_for_remote = 1). #18964 (Nikolai Kochetov).Fix bug when mutation with some escaped text (like ALTER ... UPDATE e = CAST('foo', 'Enum8(\\'foo\\' = 1') serialized incorrectly. Fixes #18878. #18944 (alesapin).ATTACH PARTITION will reset mutations. #18804. #18935 (fastio).Fix issue with bitmapOrCardinality that may lead to nullptr dereference. This closes #18911. #18912 (sundyli).Fixed Attempt to read after eof error when trying to CAST NULL from Nullable(String) to Nullable(Decimal(P, S)). Now function CAST returns NULL when it cannot parse decimal from nullable string. Fixes #7690. #18718 (Winter Zhang).Fix data type convert issue for MySQL engine. #18124 (bo zeng).Fix clickhouse-client abort exception while executing only select. #19790 (taiyang-li). Build/Testing/Packaging Improvement​ Run SQLancer (logical SQL fuzzer) in CI. #19006 (Ilya Yatsishin).Query Fuzzer will fuzz newly added tests more extensively. This closes #18916. #19185 (alexey-milovidov).Integrate with Big List of Naughty Strings for better fuzzing. #19480 (alexey-milovidov).Add integration tests run with MSan. #18974 (alesapin).Fixed MemorySanitizer errors in cyrus-sasl and musl. #19821 (Ilya Yatsishin).Insuffiient arguments check in positionCaseInsensitiveUTF8 function triggered address sanitizer. #19720 (alexey-milovidov).Remove --project-directory for docker-compose in integration test. Fix logs formatting from docker container. #19706 (Ilya Yatsishin).Made generation of macros.xml easier for integration tests. No more excessive logging from dicttoxml. dicttoxml project is not active for 5+ years. #19697 (Ilya Yatsishin).Allow to explicitly enable or disable watchdog via environment variable CLICKHOUSE_WATCHDOG_ENABLE. By default it is enabled if server is not attached to terminal. #19522 (alexey-milovidov).Allow building ClickHouse with Kafka support on arm64. #19369 (filimonov).Allow building librdkafka without ssl. #19337 (filimonov).Restore Kafka input in FreeBSD builds. #18924 (Alexandre Snarskii).Fix potential nullptr dereference in table function VALUES. #19357 (alexey-milovidov).Avoid UBSan reports in arrayElement function, substring and arraySum. Fixes #19305. Fixes #19287. This closes #19336. #19347 (alexey-milovidov). "},{"title":"ClickHouse release 21.1​","type":1,"pageTitle":"2021","url":"docs/en/whats-new/changelog/2021#clickhouse-release-211","content":""},{"title":"ClickHouse release v21.1.3.32-stable, 2021-02-03​","type":1,"pageTitle":"2021","url":"docs/en/whats-new/changelog/2021#clickhouse-release-v211332-stable-2021-02-03","content":"Bug Fix​ BloomFilter index crash fix. Fixes #19757. #19884 (Maksim Kita).Fix crash when pushing down predicates to union distinct subquery. This fixes #19855. #19861 (Amos Bird).Fix filtering by UInt8 greater than 127. #19799 (Anton Popov).In previous versions, unusual arguments for function arrayEnumerateUniq may cause crash or infinite loop. This closes #19787. #19788 (alexey-milovidov).Fixed stack overflow when using accurate comparison of arithmetic type with string type. #19773 (tavplubix).Fix crash when nested column name was used in WHERE or PREWHERE. Fixes #19755. #19763 (Nikolai Kochetov).Fix a segmentation fault in bitmapAndnot function. Fixes #19668. #19713 (Maksim Kita).Some functions with big integers may cause segfault. Big integers is experimental feature. This closes #19667. #19672 (alexey-milovidov).Fix wrong result of function neighbor for LowCardinality argument. Fixes #10333. #19617 (Nikolai Kochetov).Fix use-after-free of the CompressedWriteBuffer in Connection after disconnect. #19599 (Azat Khuzhin).DROP/DETACH TABLE table ON CLUSTER cluster SYNC query might hang, it's fixed. Fixes #19568. #19572 (tavplubix).Query CREATE DICTIONARY id expression fix. #19571 (Maksim Kita).Fix SIGSEGV with merge_tree_min_rows_for_concurrent_read/merge_tree_min_bytes_for_concurrent_read=0/UINT64_MAX. #19528 (Azat Khuzhin).Buffer overflow (on memory read) was possible if addMonth function was called with specifically crafted arguments. This fixes #19441. This fixes #19413. #19472 (alexey-milovidov).Uninitialized memory read was possible in encrypt/decrypt functions if empty string was passed as IV. This closes #19391. #19397 (alexey-milovidov).Fix possible buffer overflow in Uber H3 library. See https://github.com/uber/h3/issues/392. This closes #19219. #19383 (alexey-milovidov).Fix system.parts _state column (LOGICAL_ERROR when querying this column, due to incorrect order). #19346 (Azat Khuzhin).Fixed possible wrong result or segfault on aggregation when Materialized View and its target table have different structure. Fixes #18063. #19322 (tavplubix).Fix error Cannot convert column now64() because it is constant but values of constants are different in source and result. Continuation of #7156. #19316 (Nikolai Kochetov).Fix bug when concurrent ALTER and DROP queries may hang while processing ReplicatedMergeTree table. #19237 (alesapin).Fixed There is no checkpoint error when inserting data through http interface using Template or CustomSeparated format. Fixes #19021. #19072 (tavplubix).Disable constant folding for subqueries on the analysis stage, when the result cannot be calculated. #18446 (Azat Khuzhin).Mutation might hang waiting for some non-existent part after MOVE or REPLACE PARTITION or, in rare cases, after DETACH or DROP PARTITION. It's fixed. #15537 (tavplubix). "},{"title":"ClickHouse release v21.1.2.15-stable 2021-01-18​","type":1,"pageTitle":"2021","url":"docs/en/whats-new/changelog/2021#clickhouse-release-v211215-stable-2021-01-18","content":"Backward Incompatible Change​ The setting input_format_null_as_default is enabled by default. #17525 (alexey-milovidov).Check settings constraints for profile settings from config. Server will fail to start if users.xml contain settings that do not meet corresponding constraints. #18486 (tavplubix).Restrict ALTER MODIFY SETTING from changing storage settings that affects data parts (write_final_mark and enable_mixed_granularity_parts). #18306 (Amos Bird).Set insert_quorum_parallel to 1 by default. It is significantly more convenient to use than &quot;sequential&quot; quorum inserts. But if you rely to sequential consistency, you should set the setting back to zero. #17567 (alexey-milovidov).Remove sumburConsistentHash function. This closes #18120. #18656 (alexey-milovidov).Removed aggregate functions timeSeriesGroupSum, timeSeriesGroupRateSum because a friend of mine said they never worked. This fixes #16869. If you have luck using these functions, write a email to clickhouse-feedback@yandex-team.com. #17423 (alexey-milovidov).Prohibit toUnixTimestamp(Date()) (before it just returns UInt16 representation of Date). #17376 (Azat Khuzhin).Allow using extended integer types (Int128, Int256, UInt256) in avg and avgWeighted functions. Also allow using different types (integer, decimal, floating point) for value and for weight in avgWeighted function. This is a backward-incompatible change: now the avg and avgWeighted functions always return Float64 (as documented). Before this change the return type for Decimal arguments was also Decimal. #15419 (Mike).Expression toUUID(N) no longer works. Replace with toUUID('00000000-0000-0000-0000-000000000000'). This change is motivated by non-obvious results of toUUID(N) where N is non zero.SSL Certificates with incorrect &quot;key usage&quot; are rejected. In previous versions they are used to work. See #19262.incl references to substitutions file (/etc/metrika.xml) were removed from the default config (&lt;remote_servers&gt;, &lt;zookeeper&gt;, &lt;macros&gt;, &lt;compression&gt;, &lt;networks&gt;). If you were using substitutions file and were relying on those implicit references, you should put them back manually and explicitly by adding corresponding sections with incl=&quot;...&quot; attributes before the update. See #18740 (alexey-milovidov). New Feature​ Implement gRPC protocol in ClickHouse. #15111 (Vitaly Baranov).Allow to use multiple zookeeper clusters. #17070 (fastio).Implemented REPLACE TABLE and CREATE OR REPLACE TABLE queries. #18521 (tavplubix).Implement UNION DISTINCT and treat the plain UNION clause as UNION DISTINCT by default. Add a setting union_default_mode that allows to treat it as UNION ALL or require explicit mode specification. #16338 (flynn).Added function accurateCastOrNull. This closes #10290. Add type conversions in x IN (subquery) expressions. This closes #10266. #16724 (Maksim Kita).IP Dictionary supports IPv4 / IPv6 types directly. #17571 (vdimir).IP Dictionary supports key fetching. Resolves #18241. #18480 (vdimir).Add *.zst compression/decompression support for data import and export. It enables using *.zst in file() function and Content-encoding: zstd in HTTP client. This closes #16791 . #17144 (Abi Palagashvili).Added mannWitneyUTest, studentTTest and welchTTest aggregate functions. Refactored rankCorr a bit. #16883 (Nikita Mikhaylov).Add functions countMatches/countMatchesCaseInsensitive. #17459 (Azat Khuzhin).Implement countSubstrings()/countSubstringsCaseInsensitive()/countSubstringsCaseInsensitiveUTF8() (Count the number of substring occurrences). #17347 (Azat Khuzhin).Add information about used databases, tables and columns in system.query_log. Add query_kind and normalized_query_hash fields. #17726 (Amos Bird).Add a setting optimize_on_insert. When enabled, do the same transformation for INSERTed block of data as if merge was done on this block (e.g. Replacing, Collapsing, Aggregating...). This setting is enabled by default. This can influence Materialized View and MaterializeMySQL behaviour (see detailed description). This closes #10683. #16954 (Kruglov Pavel).Kerberos Authenticaiton for HDFS. #16621 (Ilya Golshtein).Support SHOW SETTINGS statement to show parameters in system.settings. SHOW CHANGED SETTINGS and LIKE/ILIKE clause are also supported. #18056 (Jianmei Zhang).Function position now supports POSITION(needle IN haystack) synax for SQL compatibility. This closes #18701. ... #18779 (Jianmei Zhang).Now we have a new storage setting max_partitions_to_read for tables in the MergeTree family. It limits the max number of partitions that can be accessed in one query. A user setting force_max_partition_limit is also added to enforce this constraint. #18712 (Amos Bird).Add query_id column to system.part_log for inserted parts. Closes #10097. #18644 (flynn).Allow create table as select with columns specification. Example CREATE TABLE t1 (x String) ENGINE = Memory AS SELECT 1;. #18060 (Maksim Kita).Added arrayMin, arrayMax, arrayAvg aggregation functions. #18032 (Maksim Kita).Implemented ATTACH TABLE name FROM 'path/to/data/' (col1 Type1, ... query. It creates new table with provided structure and attaches table data from provided directory in user_files. #17903 (tavplubix).Add mutation support for StorageMemory. This closes #9117. #15127 (flynn).Support syntax EXISTS DATABASE name. #18458 (Du Chuan).Support builtin function isIPv4String &amp;&amp; isIPv6String like MySQL. #18349 (Du Chuan).Add a new setting insert_distributed_one_random_shard = 1 to allow insertion into multi-sharded distributed table without any distributed key. #18294 (Amos Bird).Add settings min_compress_block_size and max_compress_block_size to MergeTreeSettings, which have higher priority than the global settings and take effect when they are set. close 13890. #17867 (flynn).Add support for 64bit roaring bitmaps. #17858 (Andy Yang).Extended OPTIMIZE ... DEDUPLICATE syntax to allow explicit (or implicit with asterisk/column transformers) list of columns to check for duplicates on. ... #17846 (Vasily Nemkov).Added functions toModifiedJulianDay, fromModifiedJulianDay, toModifiedJulianDayOrNull, and fromModifiedJulianDayOrNull. These functions convert between Proleptic Gregorian calendar date and Modified Julian Day number. #17750 (PHO).Add ability to use custom TLD list: added functions firstSignificantSubdomainCustom, cutToFirstSignificantSubdomainCustom. #17748 (Azat Khuzhin).Add support for PROXYv1 protocol to wrap native TCP interface. Allow quotas to be keyed by proxy-forwarded IP address (applied for PROXYv1 address and for X-Forwarded-For from HTTP interface). This is useful when you provide access to ClickHouse only via trusted proxy (e.g. CloudFlare) but want to account user resources by their original IP addresses. This fixes #17268. #17707 (alexey-milovidov).Now clickhouse-client supports opening EDITOR to edit commands. Alt-Shift-E. #17665 (Amos Bird).Add function encodeXMLComponent to escape characters to place string into XML text node or attribute. #17659 (nauta).Introduce DETACH TABLE/VIEW ... PERMANENTLY syntax, so that after restarting the table does not reappear back automatically on restart (only by explicit request). The table can still be attached back using the short syntax ATTACH TABLE. Implements #5555. Fixes #13850. #17642 (filimonov).Add asynchronous metrics on total amount of rows, bytes and parts in MergeTree tables. This fix #11714. #17639 (flynn).Add settings limit and offset for out-of-SQL pagination: #16176 They are useful for building APIs. These two settings will affect SELECT query as if it is added like select * from (your_original_select_query) t limit xxx offset xxx;. #17633 (hexiaoting).Provide a new aggregator combinator : -SimpleState to build SimpleAggregateFunction types via query. It's useful for defining MaterializedView of AggregatingMergeTree engine, and will benefit projections too. #16853 (Amos Bird).Added queries-file parameter for clickhouse-client and clickhouse-local. #15930 (Maksim Kita).Added query parameter for clickhouse-benchmark. #17832 (Maksim Kita).EXPLAIN AST now support queries other then SELECT. #18136 (taiyang-li). Experimental Feature​ Added functions for calculation of minHash and simHash of text n-grams and shingles. They are intended for semi-duplicate search. Also functions bitHammingDistance and tupleHammingDistance are added. #7649 (flynn).Add new data type Map. See #1841. First version for Map only supports String type of key and value. #15806 (hexiaoting).Implement alternative SQL parser based on ANTLR4 runtime and generated from EBNF grammar. #11298 (Ivan). Performance Improvement​ New IP Dictionary implementation with lower memory consumption, improved performance for some cases, and fixed bugs. #16804 (vdimir).Parallel formatting for data export. #11617 (Nikita Mikhaylov).LDAP integration: Added verification_cooldown parameter in LDAP server connection configuration to allow caching of successful &quot;bind&quot; attempts for configurable period of time. #15988 (Denis Glazachev).Add --no-system-table option for clickhouse-local to run without system tables. This avoids initialization of DateLUT that may take noticeable amount of time (tens of milliseconds) at startup. #18899 (alexey-milovidov).Replace PODArray with PODArrayWithStackMemory in AggregateFunctionWindowFunnelData to improve windowFunnel function performance. #18817 (flynn).Don't send empty blocks to shards on synchronous INSERT into Distributed table. This closes #14571. #18775 (alexey-milovidov).Optimized read for StorageMemory. #18052 (Maksim Kita).Using Dragonbox algorithm for float to string conversion instead of ryu. This improves performance of float to string conversion significantly. #17831 (Maksim Kita).Speedup IPv6CIDRToRange implementation. #17569 (vdimir).Add remerge_sort_lowered_memory_bytes_ratio setting (If memory usage after remerge does not reduced by this ratio, remerge will be disabled). #17539 (Azat Khuzhin).Improve performance of AggregatingMergeTree with SimpleAggregateFunction(String) in PK. #17109 (Azat Khuzhin).Now the -If combinator is devirtualized, and count is properly vectorized. It is for this PR. #17043 (Amos Bird).Fix performance of reading from Merge tables over huge number of MergeTree tables. Fixes #7748. #16988 (Anton Popov).Improved performance of function repeat. #16937 (satanson).Slightly improved performance of float parsing. #16809 (Maksim Kita).Add possibility to skip merged partitions for OPTIMIZE TABLE ... FINAL. #15939 (Kruglov Pavel).Integrate with fast_float from Daniel Lemire to parse floating point numbers. #16787 (Maksim Kita). It is not enabled, because performance its performance is still lower than rough float parser in ClickHouse.Fix max_distributed_connections (affects prefer_localhost_replica = 1 and max_threads != max_distributed_connections). #17848 (Azat Khuzhin).Adaptive choice of single/multi part upload when sending data to S3. Single part upload is controlled by a new setting max_single_part_upload_size. #17934 (Pavel Kovalenko).Support for async tasks in PipelineExecutor. Initial support of async sockets for remote queries. #17868 (Nikolai Kochetov).Allow to use optimize_move_to_prewhere optimization with compact parts, when sizes of columns are unknown. #17330 (Anton Popov). Improvement​ Avoid deadlock when executing INSERT SELECT into itself from a table with TinyLog or Log table engines. This closes #6802. This closes #18691. This closes #16812. This closes #14570. #15260 (alexey-milovidov).Support SHOW CREATE VIEW name syntax like MySQL. #18095 (Du Chuan).All queries of type Decimal * Float or vice versa are allowed, including aggregate ones (e.g. SELECT sum(decimal_field * 1.1) or SELECT dec_col * float_col), the result type is Float32 or Float64. #18145 (Mike).Improved minimal Web UI: add history; add sharing support; avoid race condition of different requests; add request in-flight and ready indicators; add favicon; detect Ctrl+Enter if textarea is not in focus. #17293 #17770 (alexey-milovidov).clickhouse-server didn't send close request to ZooKeeper server. #16837 (alesapin).Avoid server abnormal termination in case of too low memory limits (max_memory_usage = 1 / max_untracked_memory = 1). #17453 (Azat Khuzhin).Fix non-deterministic result of windowFunnel function in case of same timestamp for different events. #18884 (Fuwang Hu).Docker: Explicitly set uid / gid of clickhouse user &amp; group to the fixed values (101) in clickhouse-server Docker images. #19096 (filimonov).Asynchronous INSERTs to Distributed tables: Two new settings (by analogy with MergeTree family) has been added: - fsync_after_insert - Do fsync for every inserted. Will decreases performance of inserts. - fsync_directories - Do fsync for temporary directory (that is used for async INSERT only) after all operations (writes, renames, etc.). #18864 (Azat Khuzhin).SYSTEM KILL command started to work in Docker. This closes #18847. #18848 (alexey-milovidov).Expand macros in the zk path when executing FETCH PARTITION. #18839 (fastio).Apply ALTER TABLE &lt;replicated_table&gt; ON CLUSTER MODIFY SETTING ... to all replicas. Because we don't replicate such alter commands. #18789 (Amos Bird).Allow column transformer EXCEPT to accept a string as regular expression matcher. This resolves #18685 . #18699 (Amos Bird).Fix SimpleAggregateFunction in SummingMergeTree. Now it works like AggregateFunction. In previous versions values were summed together regardless to the aggregate function. This fixes #18564 . #8052. #18637 (Amos Bird). Another fix of using SimpleAggregateFunction in SummingMergeTree. This fixes #18676 . #18677 (Amos Bird).Fixed assertion error inside allocator in case when last argument of function bar is NaN. Now simple ClickHouse's exception is being thrown. This fixes #17876. #18520 (Nikita Mikhaylov).Fix usability issue: no newline after exception message in some tools. #18444 (alexey-milovidov).Add ability to modify primary and partition key column type from LowCardinality(Type) to Type and vice versa. Also add an ability to modify primary key column type from EnumX to IntX type. Fixes #5604. #18362 (alesapin).Implement untuple field access. #18133. #18309 (hexiaoting).Allow to parse Array fields from CSV if it is represented as a string containing array that was serialized as nested CSV. Example: &quot;[&quot;&quot;Hello&quot;&quot;, &quot;&quot;world&quot;&quot;, &quot;&quot;42&quot;&quot;&quot;&quot; TV&quot;&quot;]&quot; will parse as ['Hello', 'world', '42&quot; TV']. Allow to parse array in CSV in a string without enclosing braces. Example: &quot;'Hello', 'world', '42&quot;&quot; TV'&quot; will parse as ['Hello', 'world', '42&quot; TV']. #18271 (alexey-milovidov).Make better adaptive granularity calculation for merge tree wide parts. #18223 (alesapin).Now clickhouse install could work on Mac. The problem was that there is no procfs on this platform. #18201 (Nikita Mikhaylov).Better hints for SHOW ... query syntax. #18183 (Du Chuan).Array aggregation arrayMin, arrayMax, arraySum, arrayAvg support for Int128, Int256, UInt256. #18147 (Maksim Kita).Add disk to Set and Join storage settings. #18112 (Grigory Pervakov).Access control: Now table function merge() requires current user to have SELECT privilege on each table it receives data from. This PR fixes #16964. #18104 #17983 (Vitaly Baranov).Temporary tables are visible in the system tables system.tables and system.columns now only in those session where they have been created. The internal database _temporary_and_external_tables is now hidden in those system tables; temporary tables are shown as tables with empty database with the is_temporary flag set instead. #18014 (Vitaly Baranov).Fix clickhouse-client rendering issue when the size of terminal window changes. #18009 (Amos Bird).Decrease log verbosity of the events when the client drops the connection from Warning to Information. #18005 (filimonov).Forcibly removing empty or bad metadata files from filesystem for DiskS3. S3 is an experimental feature. #17935 (Pavel Kovalenko).Access control: allow_introspection_functions=0 prohibits usage of introspection functions but doesn't prohibit giving grants for them anymore (the grantee will need to set allow_introspection_functions=1 for himself to be able to use that grant). Similarly allow_ddl=0 prohibits usage of DDL commands but doesn't prohibit giving grants for them anymore. #17908 (Vitaly Baranov).Usability improvement: hints for column names. #17112. #17857 (fastio).Add diagnostic information when two merge tables try to read each other's data. #17854 (徐炘).Let the possibility to override timeout value for running script using the ClickHouse docker image. #17818 (Guillaume Tassery).Check system log tables' engine definition grammar to prevent some configuration errors. Notes that this grammar check is not semantical, that means such mistakes as non-existent columns / expression functions would be not found out util the table is created. #17739 (Du Chuan).Removed exception throwing at RabbitMQ table initialization if there was no connection (it will be reconnecting in the background). #17709 (Kseniia Sumarokova).Do not ignore server memory limits during Buffer flush. #17646 (Azat Khuzhin).Switch to patched version of RocksDB (from ClickHouse-Extras) to fix use-after-free error. #17643 (Nikita Mikhaylov).Added an offset to exception message for parallel parsing. This fixes #17457. #17641 (Nikita Mikhaylov).Don't throw &quot;Too many parts&quot; error in the middle of INSERT query. #17566 (alexey-milovidov).Allow query parameters in UPDATE statement of ALTER query. Fixes #10976. #17563 (alexey-milovidov).Query obfuscator: avoid usage of some SQL keywords for identifier names. #17526 (alexey-milovidov).Export current max ddl entry executed by DDLWorker via server metric. It's useful to check if DDLWorker hangs somewhere. #17464 (Amos Bird).Export asynchronous metrics of all servers current threads. It's useful to track down issues like this. #17463 (Amos Bird).Include dynamic columns like MATERIALIZED / ALIAS for wildcard query when settings asterisk_include_materialized_columns and asterisk_include_alias_columns are turned on. #17462 (Ken Chen).Allow specifying TTL to remove old entries from system log tables, using the &lt;ttl&gt; attribute in config.xml. #17438 (Du Chuan).Now queries coming to the server via MySQL and PostgreSQL protocols have distinctive interface types (which can be seen in the interface column of the tablesystem.query_log): 4 for MySQL, and 5 for PostgreSQL, instead of formerly used 1 which is now used for the native protocol only. #17437 (Vitaly Baranov).Fix parsing of SETTINGS clause of the INSERT ... SELECT ... SETTINGS query. #17414 (Azat Khuzhin).Correctly account memory in RadixSort. #17412 (Nikita Mikhaylov).Add eof check in receiveHello in server to prevent getting Attempt to read after eof exception. #17365 (Kruglov Pavel).Avoid possible stack overflow in bigint conversion. Big integers are experimental. #17269 (flynn).Now set indices will work with GLOBAL IN. This fixes #17232 , #5576 . #17253 (Amos Bird).Add limit for http redirects in request to S3 storage (s3_max_redirects). #17220 (ianton-ru).When -OrNull combinator combined -If, -Merge, -MergeState, -State combinators, we should put -OrNull in front. #16935 (flynn).Support HTTP proxy and HTTPS S3 endpoint configuration. #16861 (Pavel Kovalenko).Added proper authentication using environment, ~/.aws and AssumeRole for S3 client. #16856 (Vladimir Chebotarev).Add more OpenTelemetry spans. Add an example of how to export the span data to Zipkin. #16535 (Alexander Kuzmenkov).Cache dictionaries: Completely eliminate callbacks and locks for acquiring them. Keys are not divided into &quot;not found&quot; and &quot;expired&quot;, but stored in the same map during query. #14958 (Nikita Mikhaylov).Fix never worked fsync_part_directory/fsync_after_insert/in_memory_parts_insert_sync (experimental feature). #18845 (Azat Khuzhin).Allow using Atomic engine for nested database of MaterializeMySQL engine. #14849 (tavplubix). Bug Fix​ Fix the issue when server can stop accepting connections in very rare cases. #17542 (Amos Bird, alexey-milovidov).Fix index analysis of binary functions with constant argument which leads to wrong query results. This fixes #18364. #18373 (Amos Bird).Fix possible wrong index analysis when the types of the index comparison are different. This fixes #17122. #17145 (Amos Bird).Disable write with AIO during merges because it can lead to extremely rare data corruption of primary key columns during merge. #18481 (alesapin).Restrict merges from wide to compact parts. In case of vertical merge it led to broken result part. #18381 (Anton Popov).Fix possible incomplete query result while reading from MergeTree* in case of read backoff (message &lt;Debug&gt; MergeTreeReadPool: Will lower number of threads in logs). Was introduced in #16423. Fixes #18137. #18216 (Nikolai Kochetov).Fix use after free bug in rocksdb library. #18862 (sundyli).Fix infinite reading from file in ORC format (was introduced in #10580). Fixes #19095. #19134 (Nikolai Kochetov).Fix bug in merge tree data writer which can lead to marks with bigger size than fixed granularity size. Fixes #18913. #19123 (alesapin).Fix startup bug when clickhouse was not able to read compression codec from LowCardinality(Nullable(...)) and throws exception Attempt to read after EOF. Fixes #18340. #19101 (alesapin).Restrict MODIFY TTL queries for MergeTree tables created in old syntax. Previously the query succeeded, but actually it had no effect. #19064 (Anton Popov).Make sure groupUniqArray returns correct type for argument of Enum type. This closes #17875. #19019 (alexey-milovidov).Fix possible error Expected single dictionary argument for function if use function ignore with LowCardinality argument. Fixes #14275. #19016 (Nikolai Kochetov).Fix inserting of LowCardinality column to table with TinyLog engine. Fixes #18629. #19010 (Nikolai Kochetov).Join tries to materialize const columns, but our code wants them in other places. #18982 (Nikita Mikhaylov).Disable optimize_move_functions_out_of_any because optimization is not always correct. This closes #18051. This closes #18973. #18981 (alexey-milovidov).Fix possible exception QueryPipeline stream: different number of columns caused by merging of query plan's Expression steps. Fixes #18190. #18980 (Nikolai Kochetov).Fixed very rare deadlock at shutdown. #18977 (tavplubix).Fix incorrect behavior when ALTER TABLE ... DROP PART 'part_name' query removes all deduplication blocks for the whole partition. Fixes #18874. #18969 (alesapin).Attach partition should reset the mutation. #18804. #18935 (fastio).Fix issue with bitmapOrCardinality that may lead to nullptr dereference. This closes #18911. #18912 (sundyli).Fix possible hang at shutdown in clickhouse-local. This fixes #18891. #18893 (alexey-milovidov).Queries for external databases (MySQL, ODBC, JDBC) were incorrectly rewritten if there was an expression in form of x IN table. This fixes #9756. #18876 (alexey-milovidov).Fix *If combinator with unary function and Nullable types. #18806 (Azat Khuzhin).Fix the issue that asynchronous distributed INSERTs can be rejected by the server if the setting network_compression_method is globally set to non-default value. This fixes #18741. #18776 (alexey-milovidov).Fixed Attempt to read after eof error when trying to CAST NULL from Nullable(String) to Nullable(Decimal(P, S)). Now function CAST returns NULL when it cannot parse decimal from nullable string. Fixes #7690. #18718 (Winter Zhang).Fix minor issue with logging. #18717 (sundyli).Fix removing of empty parts in ReplicatedMergeTree tables, created with old syntax. Fixes #18582. #18614 (Anton Popov).Fix previous bug when date overflow with different values. Strict Date value limit to &quot;2106-02-07&quot;, cast date &gt; &quot;2106-02-07&quot; to value 0. #18565 (hexiaoting).Add FixedString data type support for replication from MySQL. Replication from MySQL is an experimental feature. This patch fixes #18450 Also fixes #6556. #18553 (awesomeleo).Fix possible Pipeline stuck error while using ORDER BY after subquery with RIGHT or FULL join. #18550 (Nikolai Kochetov).Fix bug which may lead to ALTER queries hung after corresponding mutation kill. Found by thread fuzzer. #18518 (alesapin).Proper support for 12AM in parseDateTimeBestEffort function. This fixes #18402. #18449 (vladimir-golovchenko).Fixed value is too short error when executing toType(...) functions (toDate, toUInt32, etc) with argument of type Nullable(String). Now such functions return NULL on parsing errors instead of throwing exception. Fixes #7673. #18445 (tavplubix).Fix the unexpected behaviour of SHOW TABLES. #18431 (fastio).Fix -SimpleState combinator generates incompatible arugment type and return type. #18404 (Amos Bird).Fix possible race condition in concurrent usage of Set or Join tables and selects from system.tables. #18385 (alexey-milovidov).Fix filling table system.settings_profile_elements. This PR fixes #18231. #18379 (Vitaly Baranov).Fix possible crashes in aggregate functions with combinator Distinct, while using two-level aggregation. Fixes #17682. #18365 (Anton Popov).Fixed issue when clickhouse-odbc-bridge process is unreachable by server on machines with dual IPv4/IPv6 stack; Fixed issue when ODBC dictionary updates are performed using malformed queries and/or cause crashes of the odbc-bridge process; Possibly closes #14489. #18278 (Denis Glazachev).Access control: SELECT count() FROM table now can be executed if the user has access to at least single column from a table. This PR fixes #10639. #18233 (Vitaly Baranov).Access control: SELECT JOIN now requires the SELECT privilege on each of the joined tables. This PR fixes #17654. #18232 (Vitaly Baranov).Fix key comparison between Enum and Int types. This fixes #17989. #18214 (Amos Bird).Replication from MySQL (experimental feature). Fixes #18186 Fixes #16372 Fix unique key convert issue in MaterializeMySQL database engine. #18211 (Winter Zhang).Fix inconsistency for queries with both WITH FILL and WITH TIES #17466. #18188 (hexiaoting).Fix inserting a row with default value in case of parsing error in the last column. Fixes #17712. #18182 (Jianmei Zhang).Fix Unknown setting profile error on attempt to set settings profile. #18167 (tavplubix).Fix error when query MODIFY COLUMN ... REMOVE TTL doesn't actually remove column TTL. #18130 (alesapin).Fixed std::out_of_range: basic_string in S3 URL parsing. #18059 (Vladimir Chebotarev).Fix comparison of DateTime64 and Date. Fixes #13804 and #11222. ... #18050 (Vasily Nemkov).Replication from MySQL (experimental feature): Fixes #15187 Fixes #17912 support convert MySQL prefix index for MaterializeMySQL. #17944 (Winter Zhang).When server log rotation was configured using logger.size parameter with numeric value larger than 2^32, the logs were not rotated properly. This is fixed. #17905 (Alexander Kuzmenkov).Trivial query optimization was producing wrong result if query contains ARRAY JOIN (so query is actually non trivial). #17887 (sundyli).Fix possible segfault in topK aggregate function. This closes #17404. #17845 (Maksim Kita).WAL (experimental feature): Do not restore parts from WAL if in_memory_parts_enable_wal is disabled. #17802 (detailyang).Exception message about max table size to drop was displayed incorrectly. #17764 (alexey-milovidov).Fixed possible segfault when there is not enough space when inserting into Distributed table. #17737 (tavplubix).Fixed problem when ClickHouse fails to resume connection to MySQL servers. #17681 (Alexander Kazakov).Windows: Fixed Function not implemented error when executing RENAME query in Atomic database with ClickHouse running on Windows Subsystem for Linux. Fixes #17661. #17664 (tavplubix).In might be determined incorrectly if cluster is circular- (cross-) replicated or not when executing ON CLUSTER query due to race condition when pool_size &gt; 1. It's fixed. #17640 (tavplubix).Fix empty system.stack_trace table when server is running in daemon mode. #17630 (Amos Bird).Exception fmt::v7::format_error can be logged in background for MergeTree tables. This fixes #17613. #17615 (alexey-milovidov).When clickhouse-client is used in interactive mode with multiline queries, single line comment was erronously extended till the end of query. This fixes #13654. #17565 (alexey-milovidov).Fix alter query hang when the corresponding mutation was killed on the different replica. Fixes #16953. #17499 (alesapin).Fix issue with memory accounting when mark cache size was underestimated by clickhouse. It may happen when there are a lot of tiny files with marks. #17496 (alesapin).Fix ORDER BY with enabled setting optimize_redundant_functions_in_order_by. #17471 (Anton Popov).Fix duplicates after DISTINCT which were possible because of incorrect optimization. Fixes #17294. #17296 (li chengxiang). #17439 (Nikolai Kochetov).Fixed high CPU usage in background tasks of *MergeTree tables. #17416 (tavplubix).Fix possible crash while reading from JOIN table with LowCardinality types. Fixes #17228. #17397 (Nikolai Kochetov).Replication from MySQL (experimental feature): Fixes #16835 try fix miss match header with MySQL SHOW statement. #17366 (Winter Zhang).Fix nondeterministic functions with predicate optimizer. This fixes #17244. #17273 (Winter Zhang).Fix possible Unexpected packet Data received from client error for Distributed queries with LIMIT. #17254 (Azat Khuzhin).Fix set index invalidation when there are const columns in the subquery. This fixes #17246. #17249 (Amos Bird).clickhouse-copier: Fix for non-partitioned tables #15235. #17248 (Qi Chen).Fixed possible not-working mutations for parts stored on S3 disk (experimental feature). #17227 (Pavel Kovalenko).Bug fix for funciton fuzzBits, related issue: #16980. #17051 (hexiaoting).Fix optimize_distributed_group_by_sharding_key for query with OFFSET only. #16996 (Azat Khuzhin).Fix queries from Merge tables over Distributed tables with JOINs. #16993 (Azat Khuzhin).Fix order by optimization with monotonic functions. Fixes #16107. #16956 (Anton Popov).Fix incorrect comparison of types DateTime64 with different scales. Fixes #16655 ... #16952 (Vasily Nemkov).Fix optimization of group by with enabled setting optimize_aggregators_of_group_by_keys and joins. Fixes #12604. #16951 (Anton Popov).Minor fix in SHOW ACCESS query. #16866 (tavplubix).Fix the behaviour with enabled optimize_trivial_count_query setting with partition predicate. #16767 (Azat Khuzhin).Return number of affected rows for INSERT queries via MySQL wire protocol. Previously ClickHouse used to always return 0, it's fixed. Fixes #16605. #16715 (Winter Zhang).Fix inconsistent behavior caused by select_sequential_consistency for optimized trivial count query and system tables. #16309 (Hao Chen).Throw error when REPLACE column transformer operates on non existing column. #16183 (hexiaoting).Throw exception in case of not equi-join ON expression in RIGH|FULL JOIN. #15162 (Artem Zuikov). Build/Testing/Packaging Improvement​ Add simple integrity check for ClickHouse binary. It allows to detect corruption due to faulty hardware (bit rot on storage media or bit flips in RAM). #18811 (alexey-milovidov).Change OpenSSL to BoringSSL. It allows to avoid issues with sanitizers. This fixes #12490. This fixes #17502. This fixes #12952. #18129 (alexey-milovidov).Simplify Sys/V init script. It was not working on Ubuntu 12.04 or older. #17428 (alexey-milovidov).Multiple improvements in ./clickhouse install script. #17421 (alexey-milovidov).Now ClickHouse can pretend to be a fake ZooKeeper. Currently, storage implementation is just stored in-memory hash-table, and server partially support ZooKeeper protocol. #16877 (alesapin).Fix dead list watches removal for TestKeeperStorage (a mock for ZooKeeper). #18065 (alesapin).Add SYSTEM SUSPEND command for fault injection. It can be used to faciliate failover tests. This closes #15979. #18850 (alexey-milovidov).Generate build id when ClickHouse is linked with lld. It's appeared that lld does not generate it by default on my machine. Build id is used for crash reports and introspection. #18808 (alexey-milovidov).Fix shellcheck errors in style check. #18566 (Ilya Yatsishin).Update timezones info to 2020e. #18531 (alesapin).Fix codespell warnings. Split style checks into separate parts. Update style checks docker image. #18463 (Ilya Yatsishin).Automated check for leftovers of conflict markers in docs. #18332 (alexey-milovidov).Enable Thread Fuzzer for stateless tests flaky check. #18299 (alesapin).Do not use non thread-safe function strerror. #18204 (alexey-milovidov).Update anchore/scan-action@main workflow action (was moved from master to main). #18192 (Stig Bakken).Now clickhouse-test does DROP/CREATE databases with a timeout. #18098 (alesapin).Enable experimental support for Pytest framework for stateless tests. #17902 (Ivan).Now we use the fresh docker daemon version in integration tests. #17671 (alesapin).Send info about official build, memory, cpu and free disk space to Sentry if it is enabled. Sentry is opt-in feature to help ClickHouse developers. This closes #17279. #17543 (alexey-milovidov).There was an uninitialized variable in the code of clickhouse-copier. #17363 (Nikita Mikhaylov).Fix one MSan report from #17309. #17344 (Nikita Mikhaylov).Fix for the issue with IPv6 in Arrow Flight library. See the comments for details. #16664 (Zhanna).Add a library that replaces some libc functions to traps that will terminate the process. #16366 (alexey-milovidov).Provide diagnostics in server logs in case of stack overflow, send error message to clickhouse-client. This closes #14840. #16346 (alexey-milovidov).Now we can run almost all stateless functional tests in parallel. #15236 (alesapin).Fix corruption in librdkafka snappy decompression (was a problem only for gcc10 builds, but official builds uses clang already, so at least recent official releases are not affected). #18053 (Azat Khuzhin).If server was terminated by OOM killer, print message in log. #13516 (alexey-milovidov).PODArray: Avoid call to memcpy with (nullptr, 0) arguments (Fix UBSan report). This fixes #18525. #18526 (alexey-milovidov).Minor improvement for path concatenation of zookeeper paths inside DDLWorker. #17767 (Bharat Nallan).Allow to reload symbols from debug file. This PR also fixes a build-id issue. #17637 (Amos Bird). "},{"title":"Changelog for 2020​","type":1,"pageTitle":"2021","url":"docs/en/whats-new/changelog/2021#changelog-for-2020","content":""},{"title":"Connect Airbyte to ClickHouse","type":0,"sectionRef":"#","url":"docs/integrations/airbyte-and-clickhouse","content":"","keywords":"clickhouse airbyte connect integrate etl data integration"},{"title":"1. Download and run Airbyte​","type":1,"pageTitle":"Connect Airbyte to ClickHouse","url":"docs/integrations/airbyte-and-clickhouse#1-download-and-run-airbyte","content":"Airbyte runs on Docker and uses docker-compose. Make sure to download and install the latest versions of Docker. Deploy Airbyte by cloning the official Github repository and running docker-compose up in your favorite terminal: git clone https://github.com/airbytehq/airbyte.git cd airbyte docker-compose up Once you see the Airbyte banner in your terminal, you can connect to localhost:8000 note Alternatively, you can signup and use Airbyte Cloud "},{"title":"2. Add ClickHouse as a destination​","type":1,"pageTitle":"Connect Airbyte to ClickHouse","url":"docs/integrations/airbyte-and-clickhouse#2-add-clickhouse-as-a-destination","content":"In this section, we will display how to add a ClickHouse instance as a destination. Start your ClickHouse server (Airbyte is compatible with ClickHouse version 21.8.10.19 or above): clickhouse-server start Within Airbyte, select the &quot;Destinations&quot; page and add a new destination: Pick a name for your destination and select ClickHouse from the &quot;Destination type&quot; drop-down list: Fill out the &quot;Set up the destination&quot; form by providing your ClickHouse hostname and ports, database name, username and password and select if it's a TLS connection (equivalent to the --secure flag in the clickhouse-client). Congratulations! you have now added ClickHouse as a destination in Airbyte. note In order to use ClickHouse as a destination, the user you'll use need to have the permissions to create databases, tables and insert rows. We recommend creating a dedicated user for Airbyte (eg. my_airbyte_user) with the following permissions: GRANT CREATE ON * TO my_airbyte_user;  "},{"title":"3. Add a dataset as a source​","type":1,"pageTitle":"Connect Airbyte to ClickHouse","url":"docs/integrations/airbyte-and-clickhouse#3-add-a-dataset-as-a-source","content":"The example dataset we will use is the New York City Taxi Data (on Github). For this tutorial, we will use a subset of this dataset which corresponds to the month of July 2021. Within Airbyte, select the &quot;Sources&quot; page and add a new source of type file. Fill out the &quot;Set up the source&quot; form by naming the source and providing the URL of the NYC Taxi July 2021 file (see below). Make sure to pick csv as file format, HTTPS Public Web as Storage Provider and nyc_taxi_072021 as Dataset Name. https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-07.csv Congratulations! You have now added a source file in Airbyte. "},{"title":"4. Create a connection and load the dataset into ClickHouse​","type":1,"pageTitle":"Connect Airbyte to ClickHouse","url":"docs/integrations/airbyte-and-clickhouse#4-create-a-connection-and-load-the-dataset-into-clickhouse","content":"Within Airbyte, select the &quot;Connections&quot; page and add a new connection Select &quot;Use existing source&quot; and select the New York City Taxi Data, the select &quot;Use existing destination&quot; and select you ClickHouse instance. Fill out the &quot;Set up the connection&quot; form by choosing a Replication Frequency (we will use manual for this tutorial) and select nyc_taxi_072021 as the stream you want to sync. Make sure you pick Normalized Tabular Data as a Normalization. Now that the connection is created, click on &quot;Sync now&quot; to trigger the data loading (since we picked Manual as a Replication Frequency) Your data will start loading, you can expand the view to see Airbyte logs and progress. Once the operation finishes, you'll see a Completed successfully message in the logs: Connect to your ClickHouse instance using your preferred SQL Client and check the resulting table: SELECT * FROM nyc_taxi_072021 LIMIT 10 The response should look like: Query id: 1dbe609f-9136-49cf-a642-51a2305e1027 ┌─extra─┬─mta_tax─┬─VendorID─┬─RatecodeID─┬─tip_amount─┬─fare_amount─┬─DOLocationID─┬─PULocationID─┬─payment_type─┬─tolls_amount─┬─total_amount─┬─trip_distance─┬─passenger_count─┬─store_and_fwd_flag─┬─congestion_surcharge─┬─tpep_pickup_datetime─┬─improvement_surcharge─┬─tpep_dropoff_datetime─┬─_airbyte_ab_id───────────────────────┬─────_airbyte_emitted_at─┬─_airbyte_normalized_at─┬─_airbyte_nyc_taxi_072021_hashid──┐ │ 3.5 │ 0.5 │ 1 │ 1 │ 0 │ 11.5 │ 237 │ 162 │ 2 │ 0 │ 15.8 │ 2.3 │ 1 │ ᴺᵁᴸᴸ │ 2.5 │ 2021-07-07 17:49:32 │ 0.3 │ 2021-07-07 18:04:30 │ 00000005-a90c-41b7-8883-1ab75c0ad9da │ 2022-03-16 13:02:50.000 │ 2022-03-16 13:09:48 │ DE8F3E68A49EC6CB00919501E6726335 │ │ 0 │ 0.5 │ 2 │ 1 │ 10 │ 23 │ 256 │ 233 │ 1 │ 0 │ 36.3 │ 5.4 │ 1 │ ᴺᵁᴸᴸ │ 2.5 │ 2021-07-15 07:23:36 │ 0.3 │ 2021-07-15 07:50:28 │ 00001877-58ba-4614-90d4-4e5eba3cd593 │ 2022-03-16 13:04:46.000 │ 2022-03-16 13:09:48 │ 7915C6A4D33BCE7CF58D66CF1F2E1A61 │ │ 0.5 │ 0.5 │ 2 │ 1 │ 5 │ 30.5 │ 138 │ 137 │ 1 │ 6.55 │ 45.85 │ 10.93 │ 1 │ ᴺᵁᴸᴸ │ 2.5 │ 2021-07-18 05:00:28 │ 0.3 │ 2021-07-18 05:18:54 │ 00001885-d93e-49d7-a92c-c09fd49e8b39 │ 2022-03-16 13:05:37.000 │ 2022-03-16 13:09:48 │ A7346163EA6D6F0CBBA562CE1C5F9401 │ │ 2.5 │ 0.5 │ 1 │ 1 │ 0 │ 5 │ 100 │ 186 │ 2 │ 0 │ 8.3 │ 1 │ 1 │ ᴺᵁᴸᴸ │ 2.5 │ 2021-07-07 09:47:59 │ 0.3 │ 2021-07-07 09:52:13 │ 000029d1-2e26-4d83-9efe-51cb182282d9 │ 2022-03-16 13:02:42.000 │ 2022-03-16 13:09:48 │ C6389A8B2B6E24A74612F7FB53DAA9A0 │ │ 1 │ 0.5 │ 2 │ 1 │ 4 │ 19.5 │ 13 │ 161 │ 1 │ 0 │ 27.8 │ 5.06 │ 3 │ ᴺᵁᴸᴸ │ 2.5 │ 2021-07-12 17:54:49 │ 0.3 │ 2021-07-12 18:17:43 │ 00003433-6886-4267-b8a9-da1b366537c4 │ 2022-03-16 13:04:06.000 │ 2022-03-16 13:09:48 │ 8E7C4E55F366901E4B6DFB02C3CAE838 │ │ 0 │ 0.5 │ 2 │ 1 │ 0 │ 7 │ 233 │ 140 │ 2 │ 0 │ 10.3 │ 1.3 │ 1 │ ᴺᵁᴸᴸ │ 2.5 │ 2021-07-15 13:06:34 │ 0.3 │ 2021-07-15 13:13:24 │ 000049ae-b0c8-4e07-a3e6-ea19916fb6c3 │ 2022-03-16 13:04:51.000 │ 2022-03-16 13:09:48 │ 704F99F611D1A71713A4870406E28B54 │ │ 3.5 │ 0.5 │ 1 │ 1 │ 9.8 │ 35 │ 138 │ 230 │ 1 │ 0 │ 49.1 │ 9.9 │ 0 │ ᴺᵁᴸᴸ │ 2.5 │ 2021-07-09 16:09:24 │ 0.3 │ 2021-07-09 16:45:15 │ 00004cc2-868e-4465-a24b-7efcb5da8cd4 │ 2022-03-16 13:03:20.000 │ 2022-03-16 13:09:48 │ 8AB6444AD089BA300B303447C4B70500 │ │ 2.5 │ 0.5 │ 1 │ 1 │ 3 │ 10 │ 232 │ 224 │ 1 │ 0 │ 16.3 │ 2.6 │ 0 │ ᴺᵁᴸᴸ │ 2.5 │ 2021-07-06 15:21:57 │ 0.3 │ 2021-07-06 15:30:09 │ 00005277-bc5f-4d1e-b116-d3777fef87f7 │ 2022-03-16 13:02:33.000 │ 2022-03-16 13:09:48 │ AC5A4F12E7EC61116F146DE90375A74B │ │ 0.5 │ 0.5 │ 2 │ 1 │ 2.34 │ 6.5 │ 42 │ 41 │ 1 │ 0 │ 10.14 │ 1.02 │ 1 │ ᴺᵁᴸᴸ │ 0 │ 2021-07-16 20:27:38 │ 0.3 │ 2021-07-16 20:33:46 │ 0000571b-6698-43f4-878d-d0d3f91e40d1 │ 2022-03-16 13:05:16.000 │ 2022-03-16 13:09:48 │ A447703038C0257801F7DA3CBBCA47CB │ │ 0 │ 0.5 │ 2 │ 1 │ 0 │ 24 │ 232 │ 48 │ 2 │ 0 │ 27.3 │ 6.74 │ 1 │ ᴺᵁᴸᴸ │ 2.5 │ 2021-07-10 15:00:11 │ 0.3 │ 2021-07-10 15:27:38 │ 000060b7-76b5-4d73-ae7f-0c475f69078b │ 2022-03-16 13:03:35.000 │ 2022-03-16 13:09:48 │ 6A593070389760D2339DDBD76E913447 │ └───────┴─────────┴──────────┴────────────┴────────────┴─────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴───────────────┴─────────────────┴────────────────────┴──────────────────────┴──────────────────────┴───────────────────────┴───────────────────────┴──────────────────────────────────────┴─────────────────────────┴────────────────────────┴──────────────────────────────────┘ SELECT count(*) FROM nyc_taxi_072021 The response is: Query id: a9172d39-50f7-421e-8330-296de0baa67e ┌─count()─┐ │ 2821515 │ └─────────┘  Notice that Airbyte automatically inferred the data types and added 4 columns to the destination table. These columns are used by Airbyte to manage the replication logic and log the operations. More details are available in the Airbyte official documentation. `_airbyte_ab_id` String, `_airbyte_emitted_at` DateTime64(3, 'GMT'), `_airbyte_normalized_at` DateTime, `_airbyte_nyc_taxi_072021_hashid` String Now that the dataset is loaded on your ClickHouse instance, you can create an new table and use more suitable ClickHouse data types (more details). Congratulations - you have successfully loaded the NYC taxi data into ClickHouse using Airbyte! "},{"title":"Connecting Applications to ClickHouse with JDBC","type":0,"sectionRef":"#","url":"docs/integrations/jdbc/jdbc-with-clickhouse-2","content":"Connecting Applications to ClickHouse with JDBC Overview: The ClickHouse JDBC driver enables a Java application to interact with ClickHouse: In this lesson we will create a minimal Java application that uses the ClickHouse JDBC driver for querying a ClickHouse database. Let's get started! Prerequisites You have access to a machine that has: a Unix shell and internet access wget installeda current version of Java (e.g. OpenJDK Version &gt;= 17) installeda current version of ClickHouse installed and running Let's start by connecting to a Unix shell on your machine where Java is installed and create a project directory for our minimal Java application (feel free to name the folder anything you like and put it anywhere you like): mkdir ~/hello-clickhouse-java-app Now we download the current version of the ClickHouse JDBC driver into a subfolder of the project directory: cd ~/hello-clickhouse-java-app mkdir lib wget -P lib https://repo1.maven.org/maven2/com/clickhouse/clickhouse-jdbc/0.3.2-patch7/clickhouse-jdbc-0.3.2-patch7-shaded.jar Next we create a file for the Java main class of our minimal Java application in a subdirectory structure: cd ~/hello-clickhouse-java-app mkdir -p src/main/java/helloclickhouse touch src/main/java/helloclickhouse/HelloClickHouse.java You can now copy and paste the following Java code into the file ~/hello-clickhouse-java-app/src/main/java/helloclickhouse/HelloClickHouse.java: import com.clickhouse.jdbc.*; import java.sql.*; import java.util.*; public class HelloClickHouse { public static void main(String[] args) throws Exception { String url = &quot;jdbc:ch://&lt;host&gt;:&lt;port&gt;&quot;; Properties properties = new Properties(); // properties.setProperty(&quot;ssl&quot;, &quot;true&quot;); // properties.setProperty(&quot;sslmode&quot;, &quot;NONE&quot;); // NONE to trust all servers; STRICT for trusted only ClickHouseDataSource dataSource = new ClickHouseDataSource(url, properties); try (Connection connection = dataSource.getConnection(&lt;username&gt;, &lt;password&gt;); Statement statement = connection.createStatement(); ResultSet resultSet = statement.executeQuery(&quot;select * from system.tables limit 10&quot;)) { ResultSetMetaData resultSetMetaData = resultSet.getMetaData(); int columns = resultSetMetaData.getColumnCount(); while (resultSet.next()) { for (int c = 1; c &lt;= columns; c++) { System.out.print(resultSetMetaData.getColumnName(c) + &quot;:&quot; + resultSet.getString(c) + (c &lt; columns ? &quot;, &quot; : &quot;\\n&quot;)); } } } } } note in the Java class file above in the first code line inside the main method you need to replace &lt;host&gt;, and &lt;port&gt; with values matching your running ClickHouse instance, e.g. &quot;jdbc:ch://localhost:8123&quot;you also need to replace &lt;username&gt; and &lt;password&gt; with your ClickHouse instance credentials, if you don't use a password, you can replace &lt;password&gt; with null That was all! Now we are ready to start our minimal Java application from the Unix shell: cd ~/hello-clickhouse-java-app java -classpath lib/clickhouse-jdbc-0.3.2-patch7-shaded.jar src/main/java/helloclickhouse/HelloClickHouse.java ","keywords":"clickhouse jdbc connect integrate"},{"title":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","type":0,"sectionRef":"#","url":"docs/guides/improving-query-performance/sparse-primary-indexes","content":"","keywords":""},{"title":"Data Set​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"docs/guides/improving-query-performance/sparse-primary-indexes#data-set","content":"Throughout this article we will use a sample anonymized web traffic data set. We will use a subset of 8.87 million rows (events) from the sample data set. The uncompressed data size is 8.87 million events and about 700 MB. This compresses to 200 mb when stored in ClickHouse.In our subset, each row contains three columns that indicate an internet user (UserID column) who clicked on a URL (URL column) at a specific time (EventTime column).  With these three columns we can already formulate some typical web analytics queries such as: &quot;What are the top 10 most clicked urls for a specific user?” &quot;What are the top 10 users that most frequently clicked a specific URL?&quot; “What are the most popular times (e.g. days of the week) at which a user clicks on a specific URL?” "},{"title":"Test Machine​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"docs/guides/improving-query-performance/sparse-primary-indexes#test-machine","content":"All runtime numbers given in this document are based on running ClickHouse 22.2.1 locally on a MacBook Pro with the Apple M1 Pro chip and 16GB of RAM. "},{"title":"A full table scan​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"docs/guides/improving-query-performance/sparse-primary-indexes#a-full-table-scan","content":"In order to see how a query is executed over our data set without a primary key, we create a table (with a MergeTree table engine) by executing the following SQL DDL statement: CREATE TABLE hits_NoPrimaryKey ( `UserID` UInt32, `URL` String, `EventTime` DateTime ) ENGINE = MergeTree PRIMARY KEY tuple();  Next insert a subset of the hits data set into the table with the following SQL insert statement. This uses the URL table function in combination with schema inference in order to load a subset of the full dataset hosted remotely at clickhouse.com: INSERT INTO hits_NoPrimaryKey SELECT intHash32(c11::UInt64) AS UserID, c15 AS URL, c5 AS EventTime FROM url('https://datasets.clickhouse.com/hits/tsv/hits_v1.tsv.xz') WHERE URL != '';  The response is: Ok. 0 rows in set. Elapsed: 145.993 sec. Processed 8.87 million rows, 18.40 GB (60.78 thousand rows/s., 126.06 MB/s.)  ClickHouse client’s result output shows us that the statement above inserted 8.87 million rows into the table. Lastly, in order to simplify the discussions later on in this article and to make the diagrams and results reproducible, we optimize the table using the FINAL keyword: OPTIMIZE TABLE hits_NoPrimaryKey FINAL;  note In general it is not required nor recommended to immediately optimize a table after loading data into it. Why this is necessary for this example will become apparent. Now we execute our first web analytics query. The following is calculating the top 10 most clicked urls for the internet user with the UserID 749927693: SELECT URL, count(URL) as Count FROM hits_NoPrimaryKey WHERE UserID = 749927693 GROUP BY URL ORDER BY Count DESC LIMIT 10;  The response is: ┌─URL────────────────────────────┬─Count─┐ │ http://auto.ru/chatay-barana.. │ 170 │ │ http://auto.ru/chatay-id=371...│ 52 │ │ http://public_search │ 45 │ │ http://kovrik-medvedevushku-...│ 36 │ │ http://forumal │ 33 │ │ http://korablitz.ru/L_1OFFER...│ 14 │ │ http://auto.ru/chatay-id=371...│ 14 │ │ http://auto.ru/chatay-john-D...│ 13 │ │ http://auto.ru/chatay-john-D...│ 10 │ │ http://wot/html?page/23600_m...│ 9 │ └────────────────────────────────┴───────┘ 10 rows in set. Elapsed: 0.022 sec. Processed 8.87 million rows, 70.45 MB (398.53 million rows/s., 3.17 GB/s.)  ClickHouse client’s result output indicates that ClickHouse executed a full table scan! Each single row of the 8.87 million rows of our table was streamed into ClickHouse. That doesn’t scale. To make this (way) more efficient and (much) faster, we need to use a table with a appropriate primary key. This will allow ClickHouse to automatically (based on the primary key’s column(s)) create a sparse primary index which can then be used to significantly speed up the execution of our example query. "},{"title":"A table with a primary key​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"docs/guides/improving-query-performance/sparse-primary-indexes#a-table-with-a-primary-key","content":"Create a table that has a compound primary key with key columns UserID and URL: CREATE TABLE hits_UserID_URL ( `UserID` UInt32, `URL` String, `EventTime` DateTime ) ENGINE = MergeTree PRIMARY KEY (UserID, URL) ORDER BY (UserID, URL, EventTime) SETTINGS index_granularity = 8192, index_granularity_bytes = 0;  DDL Statement Details In order to simplify the discussions later on in this article, as well as make the diagrams and results reproducible, the DDL statement specifies a compound sorting key for the table via an ORDER BY clause explicitly controls how many index entries the primary index will have through the settings: index_granularity: explicitly set to its default value of 8192. This means that for each group of 8192 rows, the primary index will have one index entry, e.g. if the table contains 16384 rows then the index will have two index entries. index_granularity_bytes: set to 0 in order to disable adaptive index granularity. Adaptive index granularity means that ClickHouse automatically creates one index entry for a group of n rows if either n is less than 8192 but the size of the combined row data for that n rows is larger than or equal 10 MB (the default value for index_granularity_bytes) orif the combined row data size for n rows is less than 10 MB but n is 8192. The primary key in the DDL statement above causes the creation of primary index based on the two specified key columns.  Next insert the data: INSERT INTO hits_UserID_URL SELECT intHash32(c11::UInt64) AS UserID, c15 AS URL, c5 AS EventTime FROM url('https://datasets.clickhouse.com/hits/tsv/hits_v1.tsv.xz') WHERE URL != '';  The response looks like: 0 rows in set. Elapsed: 149.432 sec. Processed 8.87 million rows, 18.40 GB (59.38 thousand rows/s., 123.16 MB/s.)   And optimize the table: OPTIMIZE TABLE hits_UserID_URL FINAL;   We can use the following query to obtain metadata about our table: SELECT part_type, path, formatReadableQuantity(rows) AS rows, formatReadableSize(data_uncompressed_bytes) AS data_uncompressed_bytes, formatReadableSize(data_compressed_bytes) AS data_compressed_bytes, formatReadableSize(primary_key_bytes_in_memory) AS primary_key_bytes_in_memory, marks, formatReadableSize(bytes_on_disk) AS bytes_on_disk FROM system.parts WHERE (table = 'hits_UserID_URL') AND (active = 1) FORMAT Vertical;  The response is: part_type: Wide path: ./store/d9f/d9f36a1a-d2e6-46d4-8fb5-ffe9ad0d5aed/all_1_9_2/ rows: 8.87 million data_uncompressed_bytes: 733.28 MiB data_compressed_bytes: 206.94 MiB primary_key_bytes_in_memory: 96.93 KiB marks: 1083 bytes_on_disk: 207.07 MiB 1 rows in set. Elapsed: 0.003 sec.  The output of the ClickHouse client shows: The table’s data is stored in wide format in a specific directory on disk meaning that there will be one data file (and one mark file) per table column inside that directory.The table has 8.87 million rows.The uncompressed data size of all rows together is 733.28 MB.The compressed on disk data size of all rows together is 206.94 MB.The table has a primary index with 1083 entries (called ‘marks’) and the size of the index is 96.93 KB.In total the table’s data and mark files and primary index file together take 207.07 MB on disk. "},{"title":"An index design for massive data scales​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"docs/guides/improving-query-performance/sparse-primary-indexes#an-index-design-for-massive-data-scales","content":"In traditional relational database management systems the primary index would contain one entry per table row. For our data set this would result in the primary index - often a B(+)-Tree data structure - containing 8.87 million entries. Such an index allows the fast location of specific rows, resulting in high efficiency for lookup queries and point updates. Searching an entry in a B(+)-Tree data structure has average time complexity of O(log2 n). For a table of 8.87 million rows this means 23 steps are required to locate any index entry. This capability comes at a cost: additional disk and memory overheads and higher insertion costs when adding new rows to to the table and entries to the index (and also sometimes rebalancing of the B-Tree). Considering the challenges associated with B-Tee indexes, table engines in ClickHouse utilise a different approach. The ClickHouse MergeTree Engine Family has been designed and optimized to handle massive data volumes. These tables are designed to receive millions of row inserts per second and store very large (100s of Petabytes) volumes of data. Data is quickly written to a table part by part, with rules applied for merging the parts in the background. In ClickHouse each part has its own primary index. When parts are merged then also the merged part’s primary indexes are merged. At the very large scale that ClickHouse is designed for, it is paramount to be very disk and memory efficient. Therefore, instead of indexing every row, the primary index for a part has one index entry (known as a ‘mark’) per group of rows (called ‘granule’). This sparse indexing is possible because ClickHouse is storing the rows for a part on disk ordered by the primary key column(s). Instead of directly locating single rows (like a B-Tree based index), the sparse primary index allows it to quickly (via a binary search over index entries) identify groups of rows that could possibly match the query. The located groups of potentially matching rows (granules) are then in parallel streamed into the ClickHouse engine in order to find the matches. This index design allows for the primary index to be small (it can and must completely fit into the main memory), whilst still significantly speeding up query execution times: especially for range queries that are typical in data analytics use cases. The following illustrates in detail how ClickHouse is building and using its sparse primary index. Later on in the article we will discuss some best practices for choosing, removing, and ordering the table columns that are used to build the index (primary key columns). "},{"title":"Data is stored on disk ordered by primary key column(s)​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"docs/guides/improving-query-performance/sparse-primary-indexes#data-is-stored-on-disk-ordered-by-primary-key-columns","content":"Our table that we created above has a compound primary key (UserID, URL) and a compound sorting key (UserID, URL, EventTime).  note The primary key needs to be a prefix of the sorting key if both are specified. The inserted rows are stored on disk in lexicographical order (ascending) by the primary key columns. note ClickHouse allows inserting multiple rows with identical primary key column values. In this case (see row 1 and row 2 in the diagram below), the final order is determined by the specified sorting key and therefore the value of the EventTime column. ClickHouse is a column-oriented database management system. As show in the diagram below for the on disk representation, there is a single data file (*.bin) per table column where all the values for that column are stored in a compressed format, andthe 8.87 million rows are stored on disk in lexicographic ascending order by the primary key columns (and sort key columns) i.e. in this case first by UserID, then by URL, and lastly by EventTime:  UserID.bin, URL.bin, and EventTime.bin are the data files on disk where the values of the UserID , URL , and EventTime columns are stored.   note As the primary key defines the lexicographical order of the rows on disk, a table can only have one primary key. We are numbering rows starting with 0 in order to be aligned with the ClickHouse internal row numbering scheme that is also used for logging messages. "},{"title":"Data is organized into granules for parallel data processing​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"docs/guides/improving-query-performance/sparse-primary-indexes#data-is-organized-into-granules-for-parallel-data-processing","content":"For data processing purposes, a table's column values are logically divided into granules. A granule is the smallest indivisible data set that is streamed into ClickHouse for data processing. This means that instead of reading individual rows, ClickHouse is always reading (in a streaming fashion and in parallel) a whole group (granule) of rows. note Column values are not physically stored inside granules: granules are just a logical organization of the column values for query processing. The following diagram shows how the (column values of) 8.87 million rows of our table are organized into 1083 granules, as a result of the table's DDL statement containing the setting index_granularity (set to its default value of 8192).  The first (based on physical order on disk) 8192 rows (their column values) logically belong to granule 0, then the next 8192 rows (their column values) belong to granule 1 and so on. note The last granule (granule 1082) &quot;contains&quot; less than 8192 rows. We marked some column values from our primary key columns (UserID, URL) in orange. These orange marked column values are the minimum value of each primary key column in each granule. The exception here is the last granule (granule 1082 in the diagram above) where we mark the maximum values. As we will see below, these orange marked column values will be the entries in the table's primary index. We are numbering granules starting with 0 in order to be aligned with the ClickHouse internal numbering scheme that is also used for logging messages. "},{"title":"The primary index has one entry per granule​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"docs/guides/improving-query-performance/sparse-primary-indexes#the-primary-index-has-one-entry-per-granule","content":"The primary index is created based on the granules shown in the diagram above. This index is an uncompressed flat array file (primary.idx), containing so-called numerical index marks starting at 0. The diagram below shows that the index stores the minimum primary key column values (the values marked in orange in the diagram above) for each granule. For example the first index entry (‘mark 0’ in the diagram below) is storing the minimum values for the primary key columns of granule 0 from the diagram above, the second index entry (‘mark 1’ in the diagram below) is storing the minimum values for the primary key columns of granule 1 from the diagram above, and so on.   In total the index has 1083 entries for our table with 8.87 million rows and 1083 granules:  note The last index entry (‘mark 1082’ in the diagram below) is storing the maximum values for the primary key columns of granule 1082 from the diagram above. Index entries (index marks) are not based on specific rows from our table but on granules. E.g. for index entry ‘mark 0’ in the diagram above there is no row in our table where UserID is 240.923 and URL is &quot;goal://metry=10000467796a411...&quot;, instead, there is a granule 0 for the table where within that granule the minimum UserID vale is 240.923 and the minimum URL value is &quot;goal://metry=10000467796a411...&quot; and these two values are from separate rows. The primary key entries are called index marks because each index entry is marking the start of a specific data range. Specifically for the example table: UserID index marks: The stored UserID values in the primary index are sorted in ascending order. ‘mark 1’ in the diagram above thus indicates that the UserID values of all table rows in granule 1, and in all following granules, are guaranteed to be greater than or equal to 4.073.710. As we will see later, this global order enables ClickHouse to use a binary search algorithm over the index marks for the first key column when a query is filtering on the first column of the primary key. URL index marks: The quite similar cardinality of the primary key columns UserID and URL means that the index marks for all key columns after the first column in general only indicate a data range per granule. For example, ‘mark 0’ for the URL column in the diagram above is indicating that the URL values of all table rows in granule 0 are guaranteed to be larger or equal to goal://metry=10000467796a411.... However, this same guarantee cannot also be given for the URL values of all table rows in granule 1 because ‘mark 1‘ for the UserID column has a different UserID value than ‘mark 0‘. We will discuss the consequences of this on query execution performance in more detail later. "},{"title":"The primary index is used for selecting granules​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"docs/guides/improving-query-performance/sparse-primary-indexes#the-primary-index-is-used-for-selecting-granules","content":"We can now execute our queries with support from the primary index.  The following calculates the top 10 most clicked urls for the UserID 749927693. SELECT URL, count(URL) AS Count FROM hits_UserID_URL WHERE UserID = 749927693 GROUP BY URL ORDER BY Count DESC LIMIT 10;  The response is:  ┌─URL────────────────────────────┬─Count─┐ │ http://auto.ru/chatay-barana.. │ 170 │ │ http://auto.ru/chatay-id=371...│ 52 │ │ http://public_search │ 45 │ │ http://kovrik-medvedevushku-...│ 36 │ │ http://forumal │ 33 │ │ http://korablitz.ru/L_1OFFER...│ 14 │ │ http://auto.ru/chatay-id=371...│ 14 │ │ http://auto.ru/chatay-john-D...│ 13 │ │ http://auto.ru/chatay-john-D...│ 10 │ │ http://wot/html?page/23600_m...│ 9 │ └────────────────────────────────┴───────┘ 10 rows in set. Elapsed: 0.005 sec. Processed 8.19 thousand rows, 740.18 KB (1.53 million rows/s., 138.59 MB/s.)  The output for the ClickHouse client is now showing that instead of doing a full table scan, only 8.19 thousand rows were streamed into ClickHouse. If trace logging is enabled then the ClickHouse server log file shows that ClickHouse was running a binary search over the 1083 UserID index marks, in order to identify granules that possibly can contain rows with a UserID column value of 749927693. This requires 19 steps with an average time complexity of O(log2 n): ...Executor): Key condition: (column 0 in [749927693, 749927693]) ...Executor): Running binary search on index range for part all_1_9_2 (1083 marks) ...Executor): Found (LEFT) boundary mark: 176 ...Executor): Found (RIGHT) boundary mark: 177 ...Executor): Found continuous range in 19 steps ...Executor): Selected 1/1 parts by partition key, 1 parts by primary key, 1/1083 marks by primary key, 1 marks to read from 1 ranges ...Reading ...approx. 8192 rows starting from 1441792  We can see in the trace log above, that one mark out of the 1083 existing marks satisfied the query. Trace Log Details Mark 176 was identified (the 'found left boundary mark' is inclusive, the 'found right boundary mark' is exclusive), and therefore all 8192 rows from granule 176 (which starts at row 1.441.792 - we will see that later on in this article) are then streamed into ClickHouse in order to find the actual rows with a UserID column value of 749927693.  We can also reproduce this by using the EXPLAIN clause in our example query: EXPLAIN indexes = 1 SELECT URL, count(URL) AS Count FROM hits_UserID_URL WHERE UserID = 749927693 GROUP BY URL ORDER BY Count DESC LIMIT 10;  The response looks like: ┌─explain───────────────────────────────────────────────────────────────────────────────┐ │ Expression (Projection) │ │ Limit (preliminary LIMIT (without OFFSET)) │ │ Sorting (Sorting for ORDER BY) │ │ Expression (Before ORDER BY) │ │ Aggregating │ │ Expression (Before GROUP BY) │ │ Filter (WHERE) │ │ SettingQuotaAndLimits (Set limits and quota after reading from storage) │ │ ReadFromMergeTree │ │ Indexes: │ │ PrimaryKey │ │ Keys: │ │ UserID │ │ Condition: (UserID in [749927693, 749927693]) │ │ Parts: 1/1 │ │ Granules: 1/1083 │ └───────────────────────────────────────────────────────────────────────────────────────┘ 16 rows in set. Elapsed: 0.003 sec.  The client output is showing that one out of the 1083 granules was selected as possibly containing rows with a UserID column value of 749927693. Conclusion When a query is filtering on a column that is part of a compound key and is the first key column, then ClickHouse is running the binary search algorithm over the key column's index marks.  As discussed above, ClickHouse is using its sparse primary index for quickly (via binary search) selecting granules that could possibly contain rows that match a query. This is the first stage (granule selection) of ClickHouse query execution. In the second stage (data reading), ClickHouse is locating the selected granules in order to stream all their rows into the ClickHouse engine in order to find the rows that are actually matching the query. We discuss that second stage in more detail in the following section. "},{"title":"Mark files are used for locating granules​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"docs/guides/improving-query-performance/sparse-primary-indexes#mark-files-are-used-for-locating-granules","content":"The following diagram illustrates a part of the primary index file for our table.  As discussed above, via a binary search over the index’s 1083 UserID marks, mark 176 were identified. Its corresponding granule 176 can therefore possibly contain rows with a UserID column value of 749.927.693. Granule Selection Details The diagram above shows that mark 176 is the first index entry where both the minimum UserID value of the associated granule 176 is smaller than 749.927.693, and the minimum UserID value of granule 177 for the next mark (mark 177) is greater than this value. Therefore only the corresponding granule 176 for mark 176 can possibly contain rows with a UserID column value of 749.927.693. In order to confirm (or not) that some row(s) in granule 176 contain a UserID column value of 749.927.693, all 8192 rows belonging to this granule need to be streamed into ClickHouse. To achieve this, ClickHouse needs to know the physical location of granule 176. In ClickHouse the physical locations of all granules for our table are stored in mark files. Similar to data files, there is one mark file per table column. The following diagram shows the three mark files UserID.mrk, URL.mrk, and EventTime.mrk that store the physical locations of the granules for the table’s UserID, URL, and EventTime columns.  We have discussed how the primary index is a flat uncompressed array file (primary.idx), containing index marks that are numbered starting at 0. Similarily, a mark file is also a flat uncompressed array file (*.mrk) containing marks that are numbered starting at 0. Once ClickHouse has identified and selected the index mark for a granule that can possibly contain matching rows for a query, a positional array lookup can be performed in the mark files in order to obtain the physical locations of the granule. Each mark file entry for a specific column is storing two locations in the form of offsets: The first offset ('block_offset' in the diagram above) is locating the block in the compressed column data file that contains the compressed version of the selected granule. This compressed block potentially contains a few compressed granules. The located compressed file block is uncompressed into the main memory on read. The second offset ('granule_offset' in the diagram above) from the mark-file provides the location of the granule within the uncompressed block data. All the 8192 rows belonging to the located uncompressed granule are then streamed into ClickHouse for further processing. Why Mark Files Why does the primary index not directly contain the physical locations of the granules that are corresponding to index marks? Because at that very large scale that ClickHouse is designed for, it is important to be very disk and memory efficient. The primary index file needs to fit into the main memory. For our example query ClickHouse used the primary index and selected a single granule that can possibly contain rows matching our query. Only for that one granule does ClickHouse then need the physical locations in order to stream the corresponding rows for further processing. Furthermore, this offset information is only needed for the UserID and URL columns. Offset information is not needed for columns that are not used in the query e.g. the EventTime. For our sample query, Clickhouse needs only the two physical location offsets for granule 176 in the UserID data file (UserID.bin) and the two physical location offsets for granule 176 in the URL data file (URL.data). The indirection provided by mark files avoids storing, directly within the primary index, entries for the physical locations of all 1083 granules for all three columns: thus avoiding having unnecessary (potentially unused) data in main memory. The following diagram and the text below illustrates how for our example query ClickHouse locates granule 176 in the UserID.bin data file.  We discussed earlier in this article that ClickHouse selected the primary index mark 176 and therefore granule 176 as possibly containing matching rows for our query. ClickHouse now uses the selected mark number (176) from the index for a positional array lookup in the UserID.mrk mark file in order to get the two offsets for locating granule 176. As shown, the first offset is locating the compressed file block within the UserID.bin data file that in turn contains the compressed version of granule 176. Once the located file block is uncompressed into the main memory, the second offset from the mark file can be used to locate granule 176 within the uncompressed data. ClickHouse needs to locate (and stream all values from) granule 176 from both the UserID.bin data file and the URL.bin data file in order to execute our example query (top 10 most clicked urls for the internet user with the UserID 749.927.693). The diagram above shows how ClickHouse is locating the granule for the UserID.bin data file. In parallel, ClickHouse is doing the same for granule 176 for the URL.bin data file. The two respective granules are aligned and streamed into the ClickHouse engine for further processing i.e. aggregating and counting the URL values per group for all rows where the UserID is 749.927.693, before finally outputting the 10 largest URL groups in descending count order. "},{"title":"Performance issues when filtering on key columns after the first​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"docs/guides/improving-query-performance/sparse-primary-indexes#performance-issues-when-filtering-on-key-columns-after-the-first","content":"When a query is filtering on a column that is part of a compound key and is the first key column, then ClickHouse is running the binary search algorithm over the key column's index marks. But what happens when a query is filtering on a column that is part of a compound key, but is not the first key column? note We discuss a scenario when a query is explicitly not filtering on the first key colum, but on any key column after the first. When a query is filtering on both the first key column and on any key column(s) after the first then ClickHouse is running binary search over the first key column's index marks.    We use a query that calculates the top 10 users that have most frequently clicked on the URL &quot;http://public_search&quot;: SELECT UserID, count(UserID) AS Count FROM hits_UserID_URL WHERE URL = 'http://public_search' GROUP BY UserID ORDER BY Count DESC LIMIT 10;  The response is:  ┌─────UserID─┬─Count─┐ │ 2459550954 │ 3741 │ │ 1084649151 │ 2484 │ │ 723361875 │ 729 │ │ 3087145896 │ 695 │ │ 2754931092 │ 672 │ │ 1509037307 │ 582 │ │ 3085460200 │ 573 │ │ 2454360090 │ 556 │ │ 3884990840 │ 539 │ │ 765730816 │ 536 │ └────────────┴───────┘ 10 rows in set. Elapsed: 0.086 sec. Processed 8.81 million rows, 799.69 MB (102.11 million rows/s., 9.27 GB/s.)  The client output indicates that ClickHouse almost executed a full table scan despite the URL column being part of the compound primary key! ClickHouse reads 8.81 million rows from the 8.87 million rows of the table. If trace logging is enabled then the ClickHouse server log file shows that ClickHouse used a generic exclusion search over the 1083 URL index marks in order to identify those granules that possibly can contain rows with a URL column value of &quot;http://public_search&quot;: ...Executor): Key condition: (column 1 in ['http://public_search', 'http://public_search']) ...Executor): Used generic exclusion search over index for part all_1_9_2 with 1537 steps ...Executor): Selected 1/1 parts by partition key, 1 parts by primary key, 1076/1083 marks by primary key, 1076 marks to read from 5 ranges ...Executor): Reading approx. 8814592 rows with 10 streams  We can see in the sample trace log above, that 1076 (via the marks) out of 1083 granules were selected as possibly containing rows with a matching URL value. This results in 8.81 million rows being streamed into the ClickHouse engine (in parallel by using 10 streams), in order to identify the rows that are actually contain the URL value &quot;http://public_search&quot;. However, as we will see later only 39 granules out of that selected 1076 granules actually contain matching rows. Whilst the primary index based on the compound primary key (UserID, URL) was very useful for speeding up queries filtering for rows with a specific UserID value, the index is not providing significant help with speeding up the query that filters for rows with a specific URL value. The reason for this is that the URL column is not the first key column and therefore ClickHouse is using a generic exclusion search algorithm (instead of binary search) over the URL column's index marks, and the effectiveness of that algorithm is dependant on the cardinality difference between the URL column and it's predecessor key column UserID. In order to illustrate that, we give some details about how the generic exclusion search works. Generic exclusion search algorithm details The following is illustrating how the ClickHouse generic exclusion search algorithm works when granules are selected via any column after the first, when the predecessor key column has a low(er) or high(er) cardinality. As an example for both cases we will assume: a query that is searching for rows with URL value = &quot;W3&quot;.an abstract version of our hits table with simplified values for UserID and URL.the same compound primary key (UserID, URL) for the index. This means rows are first ordered by UserID values. Rows with the same UserID value are then ordered by URL.a granule size of two i.e. each granule contains two rows. We have marked the minimum key column values for each granule in orange in the diagrams below.. Predecessor key column has low(er) cardinality Suppose UserID had low cardinality. In this case it would be likely that the same UserID value is spread over multiple table rows and granules and therefore index marks. For index marks with the same UserID, the URL values for the index marks are sorted in ascending order (because the table rows are ordered first by UserID and then by URL). This allows efficient filtering as described below: There are three different scenarios for the granule selection process for our abstract sample data in the diagram above: Index mark 0 for which the (minimum) URL value is smaller than W3 and for which the URL value of the directly succeeding index mark is also smaller than W3 can be excluded because mark 0, 1, and 2 have the same UserID value. Note that this exclusion-precondition ensures that granule 0 and the next granule 1 are completely composed of U1 UserID values so that ClickHouse can assume that also the maximum URL value in granule 0 is smaller than W3 and exclude the granule. Index mark 1 for which the URL value is smaller (or equal) than W3 and for which the URL value of the directly succeeding index mark is greater (or equal) than W3 is selected because it means that granule 1 can possibly contain rows with URL W3). Index marks 2 and 3 for which the URL value is greater than W3 can be excluded, since index marks of a primary index store the minimum key column values for each granule and therefore granule 2 and 3 can't possibly contain URL value W3. Predecessor key column has high(er) cardinality When the UserID has high cardinality then it is unlikely that the same UserID value is spread over multiple table rows and granules. This means the URL values for the index marks are not monotonically increasing: As we can see in the diagram above, all shown marks whose URL values are smaller than W3 are getting selected for streaming its associated granule's rows into the ClickHouse engine. This is because whilst all index marks in the diagram fall into scenario 1 described above, they do not satisfy the mentioned exclusion-precondition that the two directly succeeding index marks both have the same UserID value as the current mark and thus can’t be excluded. For example, consider index mark 0 for which the URL value is smaller than W3 and for which the URL value of the directly succeeding index mark is also smaller than W3. This can not be excluded because the two directly succeeding index marks 1 and 2 do not have the same UserID value as the current mark 0. Note the requirement for the two succeeding index marks to have the same UserID value. This ensures that the granules for the current and the next mark are completely composed of U1 UserID values. If only the next mark had the same UserID, the URL value of the next mark could potentially stem from a table row with a different UserID - which is indeed the case when you look at the diagram above i.e. W2 stems from a row with U2 not U1. This ultimately prevents ClickHouse from making assumptions about the maximum URL value in granule 0. Instead it has to assume that granule 0 potentially contains rows with URL value W3 and is forced to select mark 0. The same scenario is true for mark 1, 2, and 3. Conclusion The generic exclusion search algorithm that ClickHouse is using instead of the binary search algorithm when a query is filtering on a column that is part of a compound key, but is not the first key column is most effective when the predecessor key column has low(er) cardinality. In our sample data set both key columns (UserID, URL) have similar high cardinality, and, as explained, the generic exclusion search algorithm is not very effective when the predecessor key column of the URL column has a high(er) or similar cardinality. note about data skipping index Because of the similarly high cardinality of UserID and URL, our query filtering on URL also wouldn't benefit much from creating a secondary data skipping index on the URL column of our table with compound primary key (UserID, URL). For example this two statements create and populate a minmax data skipping index on the URL column of our table: ALTER TABLE hits_UserID_URL ADD INDEX url_skipping_index URL TYPE minmax GRANULARITY 4; ALTER TABLE hits_UserID_URL MATERIALIZE INDEX url_skipping_index; ClickHouse now created an additional index that is storing - per group of 4 consecutive granules (note the GRANULARITY 4 clause in the ALTER TABLE statement above) - the minimum and maximum URL value: The first index entry (‘mark 0’ in the diagram above) is storing the minimum and maximum URL values for the rows belonging to the first 4 granules of our table. The second index entry (‘mark 1’) is storing the minimum and maximum URL values for the rows belonging to the next 4 granules of our table, and so on. (ClickHouse also created a special mark file for to the data skipping index for locating the groups of granules associated with the index marks.) Because of the similarly high cardinality of UserID and URL, this secondary data skipping index can't help with excluding granules from being selected when our query filtering on URL is executed. The specific URL value that the query is looking for (i.e. 'http://public_search') very likely is between the minimum and maximum value stored by the index for each group of granules resulting in ClickHouse being forced to select the group of granules (because they might contain row(s) matching the query). As a consequence, if we want to significantly speed up our sample query that filters for rows with a specific URL then we need to use a primary index optimized to that query. If in addition we want to keep the good performance of our sample query that filters for rows with a specific UserID then we need to use multiple primary indexes. The following is showing ways for achieving that. "},{"title":"Performance tuning with multiple primary indexes​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"docs/guides/improving-query-performance/sparse-primary-indexes#performance-tuning-with-multiple-primary-indexes","content":"If we want to significantly speed up both of our sample queries - the one that filters for rows with a specific UserID and the one that filters for rows with a specific URL - then we need to use multiple primary indexes by using one if these three options: Creating a second table with a different primary key.Creating a materialized view on our existing table.Adding a projection to our existing table. All three options will effectively duplicate our sample data into a additional table in order to reorganize the table primary index and row sort order. However, the three options differ in how transparent that additional table is to the user with respect to the routing of queries and insert statements. When creating a second table with a different primary key then queries must be explicitly send to the table version best suited for the query, and new data must be inserted explicitly into both tables in order to keep the tables in sync:  With a materialized view the additional table is hidden and data is automatically kept in sync between both tables:  And the projection is the most transparent option because next to automatically keeping the hidden additional table in sync with data changes, ClickHouse will automatically chose the most effective table version for queries:  In the following we discuss this three options for creating and using multiple primary indexes in more detail and with real examples. "},{"title":"Multiple primary indexes via secondary tables​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"docs/guides/improving-query-performance/sparse-primary-indexes#multiple-primary-indexes-via-secondary-tables","content":" We are creating a new additional table where we switch the order of the key columns (compared to our original table) in the primary key: CREATE TABLE hits_URL_UserID ( `UserID` UInt32, `URL` String, `EventTime` DateTime ) ENGINE = MergeTree PRIMARY KEY (URL, UserID) ORDER BY (URL, UserID, EventTime) SETTINGS index_granularity = 8192, index_granularity_bytes = 0;  Insert all 8.87 million rows from our original table into the additional table: INSERT INTO hits_URL_UserID SELECT * from hits_UserID_URL;  The response looks like: Ok. 0 rows in set. Elapsed: 2.898 sec. Processed 8.87 million rows, 838.84 MB (3.06 million rows/s., 289.46 MB/s.)  And finally optimize the table: OPTIMIZE TABLE hits_URL_UserID FINAL;  Because we switched the order of the columns in the primary key, the inserted rows are now stored on disk in a different lexicographical order (compared to our original table) and therefore also the 1083 granules of that table are containing different values than before:  This is the resulting primary key:  That can now be used to significantly speed up the execution of our example query filtering on the URL column in order to calculate the top 10 users that most frequently clicked on the URL &quot;http://public_search&quot;: SELECT UserID, count(UserID) AS Count FROM hits_URL_UserID WHERE URL = 'http://public_search' GROUP BY UserID ORDER BY Count DESC LIMIT 10;  The response is:  ┌─────UserID─┬─Count─┐ │ 2459550954 │ 3741 │ │ 1084649151 │ 2484 │ │ 723361875 │ 729 │ │ 3087145896 │ 695 │ │ 2754931092 │ 672 │ │ 1509037307 │ 582 │ │ 3085460200 │ 573 │ │ 2454360090 │ 556 │ │ 3884990840 │ 539 │ │ 765730816 │ 536 │ └────────────┴───────┘ 10 rows in set. Elapsed: 0.017 sec. Processed 319.49 thousand rows, 11.38 MB (18.41 million rows/s., 655.75 MB/s.)  Now, instead of almost doing a full table scan, ClickHouse executed that query much more effective. With the primary index from the original table where UserID was the first, and URL the second key column, ClickHouse used a generic exclusion search over the index marks for executing that query and that was not very effective because of the similarly high cardinality of UserID and URL. With URL as the first column in the primary index, ClickHouse is now running binary search over the index marks. The corresponding trace log in the ClickHouse server log file confirms that: ...Executor): Key condition: (column 0 in ['http://public_search', 'http://public_search']) ...Executor): Running binary search on index range for part all_1_9_2 (1083 marks) ...Executor): Found (LEFT) boundary mark: 644 ...Executor): Found (RIGHT) boundary mark: 683 ...Executor): Found continuous range in 19 steps ...Executor): Selected 1/1 parts by partition key, 1 parts by primary key, 39/1083 marks by primary key, 39 marks to read from 1 ranges ...Executor): Reading approx. 319488 rows with 2 streams  ClickHouse selected only 39 index marks, instead of 1076 when generic exclusion search was used. Note that the additional table is optimized for speeding up the execution of our example query filtering on URLs. Similar to the bad performance of that query with our original table, our example query filtering on UserIDs will not run very effectively with the new additional table, because UserID is now the second key column in the primary index of that table and therefore ClickHouse will use generic exclusion search for granule selection, which is not very effective for similarly high cardinality of UserID and URL. Open the details box for specifics. Query filtering on UserIDs now has bad performance SELECT URL, count(URL) AS Count FROM hits_URL_UserID WHERE UserID = 749927693 GROUP BY URL ORDER BY Count DESC LIMIT 10; The response is: ┌─URL────────────────────────────┬─Count─┐ │ http://auto.ru/chatay-barana.. │ 170 │ │ http://auto.ru/chatay-id=371...│ 52 │ │ http://public_search │ 45 │ │ http://kovrik-medvedevushku-...│ 36 │ │ http://forumal │ 33 │ │ http://korablitz.ru/L_1OFFER...│ 14 │ │ http://auto.ru/chatay-id=371...│ 14 │ │ http://auto.ru/chatay-john-D...│ 13 │ │ http://auto.ru/chatay-john-D...│ 10 │ │ http://wot/html?page/23600_m...│ 9 │ └────────────────────────────────┴───────┘ 10 rows in set. Elapsed: 0.024 sec. Processed 8.02 million rows, 73.04 MB (340.26 million rows/s., 3.10 GB/s.) Server Log: ...Executor): Key condition: (column 1 in [749927693, 749927693]) ...Executor): Used generic exclusion search over index for part all_1_9_2 with 1453 steps ...Executor): Selected 1/1 parts by partition key, 1 parts by primary key, 980/1083 marks by primary key, 980 marks to read from 23 ranges ...Executor): Reading approx. 8028160 rows with 10 streams  We now have two tables. Optimized for speeding up queries filtering on UserIDs, and speeding up queries filtering on URLs, respectively:  "},{"title":"Multiple primary indexes via materialized views​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"docs/guides/improving-query-performance/sparse-primary-indexes#multiple-primary-indexes-via-materialized-views","content":"Create a materialized view on our existing table. CREATE MATERIALIZED VIEW mv_hits_URL_UserID ENGINE = MergeTree() PRIMARY KEY (URL, UserID) ORDER BY (URL, UserID, EventTime) POPULATE AS SELECT * FROM hits_UserID_URL;  The response looks like: Ok. 0 rows in set. Elapsed: 2.935 sec. Processed 8.87 million rows, 838.84 MB (3.02 million rows/s., 285.84 MB/s.)  note we switch the order of the key columns (compared to our original table ) in the view's primary keythe materialzed view is backed by a hidden table whose row order and primary index is based on the given primary key definitionwe use the POPULATE keyword in order to immediately populate the hidden table with all 8.87 million rows from the source table hits_UserID_URL if new rows are inserted into the source table hits_UserID_URL, then that rows are automatically also inserted into the hidden table Effectively the implicitly created hidden table has the same row order and primary index as the secondary table that we created explicitly: ClickHouse is storing the column data files (.bin), the mark files (.mrk2) and the primary index (primary.idx) of the hidden table in a special folder withing the ClickHouse server's data directory: The hidden table (and it's primary index) backing the materialized view can now be used to significantly speed up the execution of our example query filtering on the URL column: SELECT UserID, count(UserID) AS Count FROM mv_hits_URL_UserID WHERE URL = 'http://public_search' GROUP BY UserID ORDER BY Count DESC LIMIT 10;  The response is: ┌─────UserID─┬─Count─┐ │ 2459550954 │ 3741 │ │ 1084649151 │ 2484 │ │ 723361875 │ 729 │ │ 3087145896 │ 695 │ │ 2754931092 │ 672 │ │ 1509037307 │ 582 │ │ 3085460200 │ 573 │ │ 2454360090 │ 556 │ │ 3884990840 │ 539 │ │ 765730816 │ 536 │ └────────────┴───────┘ 10 rows in set. Elapsed: 0.026 sec. Processed 335.87 thousand rows, 13.54 MB (12.91 million rows/s., 520.38 MB/s.)  Because effectively the hidden table (and it's primary index) backing the materialized view is identical to the secondary table that we created explicitly, the query is executed in the same effective way as with the explicitly created table. The corresponding trace log in the ClickHouse server log file confirms that ClickHouse is running binary search over the index marks: ...Executor): Key condition: (column 0 in ['http://public_search', 'http://public_search']) ...Executor): Running binary search on index range ... ... ...Executor): Selected 4/4 parts by partition key, 4 parts by primary key, 41/1083 marks by primary key, 41 marks to read from 4 ranges ...Executor): Reading approx. 335872 rows with 4 streams  "},{"title":"Multiple primary indexes via projections​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"docs/guides/improving-query-performance/sparse-primary-indexes#multiple-primary-indexes-via-projections","content":"Projections are an experimental feature at the moment, therefore we need to tell ClickHouse that we know what we are doing first: SET allow_experimental_projection_optimization = 1;  Create a projection on our existing table: ALTER TABLE hits_UserID_URL ADD PROJECTION prj_url_userid ( SELECT * ORDER BY (URL, UserID) );  And materialize the projection: ALTER TABLE hits_UserID_URL MATERIALIZE PROJECTION prj_url_userid;  note the projection is creating a hidden table whose row order and primary index is based on the given ORDER BY clause of the projectionwe use the MATERIALIZE keyword in order to immediately populate the hidden table with all 8.87 million rows from the source table hits_UserID_URLif new rows are inserted into the source table hits_UserID_URL, then that rows are automatically also inserted into the hidden tablea query is always (syntactically) targeting the source table hits_UserID_URL, but if the row order and primary index of the hidden table allows a more effective query execution, then that hidden table will be used insteadEffectively the implicitly created hidden table has the same row order and primary index as the secondary table that we created explicitly: ClickHouse is storing the column data files (.bin), the mark files (.mrk2) and the primary index (primary.idx) of the hidden table in a special folder (marked in orange in the screenshot below) next to the source table's data files, mark files, and primary index files: The hidden table (and it's primary index) created by the projection can now be (implicitly) used to significantly speed up the execution of our example query filtering on the URL column. Note that the query is syntactically targeting the source table of the projection. SELECT UserID, count(UserID) AS Count FROM hits_UserID_URL WHERE URL = 'http://public_search' GROUP BY UserID ORDER BY Count DESC LIMIT 10;  The response is: ┌─────UserID─┬─Count─┐ │ 2459550954 │ 3741 │ │ 1084649151 │ 2484 │ │ 723361875 │ 729 │ │ 3087145896 │ 695 │ │ 2754931092 │ 672 │ │ 1509037307 │ 582 │ │ 3085460200 │ 573 │ │ 2454360090 │ 556 │ │ 3884990840 │ 539 │ │ 765730816 │ 536 │ └────────────┴───────┘ 10 rows in set. Elapsed: 0.029 sec. Processed 319.49 thousand rows, 1 1.38 MB (11.05 million rows/s., 393.58 MB/s.)  Because effectively the hidden table (and it's primary index) created by the projection is identical to the secondary table that we created explicitly, the query is executed in the same effective way as with the explicitly created table. The corresponding trace log in the ClickHouse server log file confirms that ClickHouse is running binary search over the index marks: ...Executor): Key condition: (column 0 in ['http://public_search', 'http://public_search']) ...Executor): Running binary search on index range for part prj_url_userid (1083 marks) ...Executor): ... ...Executor): Choose complete Normal projection prj_url_userid ...Executor): projection required columns: URL, UserID ...Executor): Selected 1/1 parts by partition key, 1 parts by primary key, 39/1083 marks by primary key, 39 marks to read from 1 ranges ...Executor): Reading approx. 319488 rows with 2 streams  "},{"title":"Removing inefficient key columns​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"docs/guides/improving-query-performance/sparse-primary-indexes#removing-inefficient-key-columns","content":"The primary index of our table with compound primary key (UserID, URL) was very useful for speeding up a query filtering on UserID. But that index is not providing significant help with speeding up a query filtering on URL, despite the URL column being part of the compound primary key. And vice versa: The primary index of our table with compound primary key (URL, UserID) was speeding up a query filtering on URL, but didn't provide much support for a query filtering on UserID. Because of the similarly high cardinality of the primary key columns UserID and URL, a query that filters on the second key column doesn’t benefit much from the second key column being in the index. Therefore it makes sense to remove the second key column from the primary index (resulting in less memory consumption of the index) and to use multiple primary indexes instead. However if the key columns in a compound primary key have big differences in cardinality, then it is beneficial for queries to order the primary key columns by cardinality in ascending order. The higher the cardinality difference between the key columns is, the more the order of those columns in the key matters. We will demonstrate that in a future article. Stay tuned. "},{"title":"Connecting ClickHouse to external data sources with JDBC","type":0,"sectionRef":"#","url":"docs/integrations/jdbc/jdbc-with-clickhouse","content":"","keywords":"clickhouse jdbc connect integrate"},{"title":"Install the ClickHouse JDBC Bridge locally​","type":1,"pageTitle":"Connecting ClickHouse to external data sources with JDBC","url":"docs/integrations/jdbc/jdbc-with-clickhouse#install-the-clickhouse-jdbc-bridge-locally","content":"The easiest way to use the ClickHouse JDBC Bridge is to install and run it on the same host where also ClickHouse is running: Let's start by connecting to the Unix shell on the machine where ClickHouse is running and create a local folder where we will later install the ClickHouse JDBC Bridge into (feel free to name the folder anything you like and put it anywhere you like): mkdir ~/clickhouse-jdbc-bridge  Now we download the current version of the ClickHouse JDBC Bridge into that folder: cd ~/clickhouse-jdbc-bridge wget https://github.com/ClickHouse/clickhouse-jdbc-bridge/releases/download/v2.0.7/clickhouse-jdbc-bridge-2.0.7-shaded.jar  In order to be able to connect to MySQL we are creating a named data source: cd ~/clickhouse-jdbc-bridge mkdir -p config/datasources touch config/datasources/mysql8.json  You can now copy and paste the following configuration into the file ~/clickhouse-jdbc-bridge/config/datasources/mysql8.json: { &quot;mysql8&quot;: { &quot;driverUrls&quot;: [ &quot;https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.28/mysql-connector-java-8.0.28.jar&quot; ], &quot;jdbcUrl&quot;: &quot;jdbc:mysql://&lt;host&gt;:&lt;port&gt;&quot;, &quot;username&quot;: &quot;&lt;username&gt;&quot;, &quot;password&quot;: &quot;&lt;password&gt;&quot; } }  note in the config file above you are free to use any name you like for the datasource, we used mysql8in the value for the jdbcUrl you need to replace &lt;host&gt;, and &lt;port&gt; with appropriate values according to your running MySQL instance, e.g. &quot;jdbc:mysql://localhost:3306&quot;you need to replace &lt;username&gt; and &lt;password&gt; with your MySQL credentials, if you don't use a password, you can delete the &quot;password&quot;: &quot;&lt;password&gt;&quot; line in the config file abovein the value for driverUrls we just specified a URL from which the current version of the MySQL JDBC driver can be downloaded. That's all we have to do, and the ClickHouse JDBC Bridge will automatically download that JDBC driver (into a OS specific directory).  Now we are ready to start the ClickHouse JDBC Bridge: cd ~/clickhouse-jdbc-bridge java -jar clickhouse-jdbc-bridge-2.0.7-shaded.jar  note We started the ClickHouse JDBC Bridge in foreground mode. In order to stop the Bridge you can bring the Unix shell window from above in foreground and press CTRL+C. "},{"title":"Use the JDBC connection from within ClickHouse​","type":1,"pageTitle":"Connecting ClickHouse to external data sources with JDBC","url":"docs/integrations/jdbc/jdbc-with-clickhouse#use-the-jdbc-connection-from-within-clickhouse","content":"ClickHouse can now access MySQL data by either using the jdbc Table Function or the JDBC Table Engine. The easiest way to execute the following examples is to copy and paste them into the native ClickHouse command-line client or into the ClickHouse play HTTP Interface. jdbc Table Function: SELECT * FROM jdbc('mysql8', 'mydatabase', 'mytable'); note As the first paramter for the jdbc table funtion we are using the name of the named data source that we configured above. JDBC Table Engine: CREATE TABLE mytable ( &lt;column&gt; &lt;column_type&gt;, ... ) ENGINE = JDBC('mysql8', 'mydatabase', 'mytable'); SELECT * FROM mytable; note As the first paramter for the jdbc engine clause we are using the name of the named data source that we configured above The schema of the ClickHouse JDBC engine table and schema of the connected MySQL table must be aligned, e.g. the column names and order must be the same, and the column data types must be compatible "},{"title":"Install the ClickHouse JDBC Bridge externally​","type":1,"pageTitle":"Connecting ClickHouse to external data sources with JDBC","url":"docs/integrations/jdbc/jdbc-with-clickhouse#install-the-clickhouse-jdbc-bridge-externally","content":"For a distributed ClickHouse cluster (a cluster with more than one ClickHouse host) it makes sense to install and run the ClickHouse JDBC Bridge externally on its own host:  This has the advantage that each ClickHouse host can access the JDBC Bridge. Otherwise the JDBC Bridge would need to be installed locally for each ClickHouse instance that is supposed to access external data sources via the Bridge. In order to install the ClickHouse JDBC Bridge externally, we do the following steps: We install, configure and run the ClickHouse JDBC Bridge on a dedicated host by following the steps described in section 1 of this guide. On each ClickHouse Host we add the following configuration block to the ClickHouse server configuration (depending on your chosen configuration format, use either the XML or YAML version): XMLYAML &lt;jdbc_bridge&gt; &lt;host&gt;JDBC-Bridge-Host&lt;/host&gt; &lt;port&gt;9019&lt;/port&gt; &lt;/jdbc_bridge&gt;  note you need to replace JDBC-Bridge-Host with the hostname or ip address of the dedicated ClickHouse JDBC Bridge hostwe specified the default ClickHouse JDBC Bridge port 9019, if you are using a different port for the JDBC Bridge then you must adapt the configuration above accordingly  "},{"title":"Managing Clusters","type":0,"sectionRef":"#","url":"docs/guides/sre/scaling-clusters","content":"","keywords":""},{"title":"Rebalancing Data​","type":1,"pageTitle":"Managing Clusters","url":"docs/guides/sre/scaling-clusters#rebalancing-data","content":"ClickHouse does not support automatic shard rebalancing. However, there are ways to rebalance shards in order of preference: Adjust the shard for the distributed table, allowing writes to be biased to the new shard. This potentially will cause load imbalances and hot spots on the cluster but can be viable in most scenarios where write throughput is not extremely high. It does not require the user to change their write target i.e. It can remain as the distributed table. This does not assist with rebalancing existing data.As an alternative to (1), modify the existing cluster and write exclusively to the new shard until the cluster is balanced - manually weighting writes. This has the same limitations as (1).If you need to rebalance existing data and you have partitioned your data, consider detaching partitions and manually relocating them to another node before reattaching to the new shard. This is more manual than subsequent techniques but may be faster and less resource-intensive. This is a manual operation and thus needs to consider the rebalancing of the data.Create a new cluster with the new topology and copy the data using ClickHouse Copier. Alternatively, create a new database within the existing cluster and migrate the data using ClickHouse Copier. This can be potentially computationally expensive and may impact your production environment. Building a new cluster on separate hardware, and applying this technique, is an option to mitigate this at the expense of cost.Export the data from the source cluster to the new cluster via an INSERT FROM SELECT. This will not be performant on very large datasets and will potentially incur significant IO on the source cluster and use considerable network resources. This represents a last resort. There is an internal effort to reconsider how rebalancing could be implemented. There is some relevant discussion here. "},{"title":"Defining SQL Users and Roles","type":0,"sectionRef":"#","url":"docs/guides/sre/users-and-roles","content":"","keywords":""},{"title":"1. Enabling SQL user mode and defining users​","type":1,"pageTitle":"Defining SQL Users and Roles","url":"docs/guides/sre/users-and-roles#1-enabling-sql-user-mode-and-defining-users","content":"Enable SQL user mode in the users.xml file under the &lt;default&gt; user: &lt;access_management&gt;1&lt;/access_management&gt; note The default user is the only user that gets created with a fresh install and is also the account used for internode communications, by default. In production, it is recommended to disable this user once the inter-node commnication has been configured with a SQL admin user and inter-node communications have been set with &lt;secret&gt;, cluster credentials and/or internode http and transport protocol credentials since the default account is used for internode communication. Restart the nodes to apply the changes. Start the ClickHouse client: clickhouse-client --user default --password &lt;password&gt; Create a SQL administrator account: CREATE USER clickhouse_admin IDENTIFIED WITH plaintext_password BY 'password'; note In this example, a plain text password is used. However, there are several options available for other user directories such as LDAP and Active Directory. Please refer to user guides and documentation for configuring other options. Grant the new user full administrative rights GRANT ALL ON *.* TO clickhouse_admin WITH GRANT OPTION; Create regular user to restrict columns CREATE USER column_user IDENTIFIED WITH plaintext_password BY 'password'; Create a regular user to restrict by row values CREATE USER row_user IDENTIFIED WITH plaintext_password BY 'password';  "},{"title":"2. Creating a sample database, table and rows​","type":1,"pageTitle":"Defining SQL Users and Roles","url":"docs/guides/sre/users-and-roles#2-creating-a-sample-database-table-and-rows","content":"Create a test database CREATE DATABASE db1; Create a table CREATE TABLE db1.table1 ( id UInt64, column1 String, column2 String ) ENGINE MergeTree ORDER BY id; Populate the table with sample rows INSERT INTO db1.table1 (id, column1, column2) VALUES (1, 'A', 'abc'), (2, 'A', 'def'), (3, 'B', 'abc'), (4, 'B', 'def'); Verify the table: SELECT * FROM db1.table1 Query id: 475015cc-6f51-4b20-bda2-3c9c41404e49 ┌─id─┬─column1─┬─column2─┐ │ 1 │ A │ abc │ │ 2 │ A │ def │ │ 3 │ B │ abc │ │ 4 │ B │ def │ └────┴─────────┴─────────┘  "},{"title":"3. Creating roles​","type":1,"pageTitle":"Defining SQL Users and Roles","url":"docs/guides/sre/users-and-roles#3-creating-roles","content":"With this set of examples, roles for different privileges such as columns and rows will be created, privileges will be granted to the roles and users will be assigned to each role. Roles are used to define groups of users for certain privileges instead of managing each user seperately. Create a role to restrict users of this role to only see column1 in database db1 and table1: CREATE ROLE column1_users; Set privileges to allow view on column1 GRANT SELECT(id, column1) ON db1.table1 TO column1_users; Add the column_user user to the column1_users role GRANT column1_users TO column_user; Create a role to restrict users of this role to only see selected rows, in this case only rows containing A in column1 CREATE ROLE A_rows_users; Add the row_user to the A_rows_users role GRANT A_rows_users TO row_user; Create a policy to allow view on only where column1 has the values of A CREATE ROW POLICY A_row_filter ON db1.table1 FOR SELECT USING column1 = 'A' TO A_rows_users; Set privileges to the database and table GRANT SELECT(id, column1, column2) ON db1.table1 TO A_rows_users; grant explicit permissions for other roles to still have access to all rows CREATE ROW POLICY allow_other_users_filter ON db1.table1 FOR SELECT USING 1 TO clickhouse_admin, column1_users; note When attaching a policy to a table, the system will apply that policy and only those users and roles defined will be able to do operations on the table, all others will be denied any operations. In order to not have the restrictive row policy applied to other users, another policy must be defined to allow other users and roles to have regular or other types of access. "},{"title":"4. Testing role privileges with column restricted user​","type":1,"pageTitle":"Defining SQL Users and Roles","url":"docs/guides/sre/users-and-roles#4-testing-role-privileges-with-column-restricted-user","content":"Log into the clickhouse client using the clickhouse_admin user clickhouse-client --user clickhouse_admin --password password Verify access to database, table and all rows with the admin user. SELECT * FROM db1.table1 Query id: f5e906ea-10c6-45b0-b649-36334902d31d ┌─id─┬─column1─┬─column2─┐ │ 1 │ A │ abc │ │ 2 │ A │ def │ │ 3 │ B │ abc │ │ 4 │ B │ def │ └────┴─────────┴─────────┘ Log into the ClickHouse client using the column_user user clickhouse-client --user column_user --password password Test SELECT using all columns SELECT * FROM db1.table1 Query id: 5576f4eb-7450-435c-a2d6-d6b49b7c4a23  0 rows in set. Elapsed: 0.006 sec. Received exception from server (version 22.3.2): Code: 497. DB::Exception: Received from localhost:9000. DB::Exception: column_user: Not enough privileges. To execute this query it's necessary to have grant SELECT(id, column1, column2) ON db1.table1. (ACCESS_DENIED) ``` :::note Access is denied since all columns were specified and the user only has access to `id` and `column1` :::  Verify SELECT query with only columns specified and allowed: SELECT id, column1 FROM db1.table1 Query id: cef9a083-d5ce-42ff-9678-f08dc60d4bb9 ┌─id─┬─column1─┐ │ 1 │ A │ │ 2 │ A │ │ 3 │ B │ │ 4 │ B │ └────┴─────────┘  "},{"title":"5. Testing role privileges with row restricted user​","type":1,"pageTitle":"Defining SQL Users and Roles","url":"docs/guides/sre/users-and-roles#5-testing-role-privileges-with-row-restricted-user","content":"Log into the ClickHouse client using row_user clickhouse-client --user row_user --password password View rows available SELECT * FROM db1.table1 Query id: a79a113c-1eca-4c3f-be6e-d034f9a220fb ┌─id─┬─column1─┬─column2─┐ │ 1 │ A │ abc │ │ 2 │ A │ def │ └────┴─────────┴─────────┘  "},{"title":"4. Modifying Users and Roles​","type":1,"pageTitle":"Defining SQL Users and Roles","url":"docs/guides/sre/users-and-roles#4-modifying-users-and-roles","content":"Users can be assigned multiple roles for a combination of privileges needed. When using multiple roles, the system will combine the roles to determine privileges, the net effect will be that the role permissions will be cumulative. For example, if one role1 allows for only select on column1 and role2 allows for select on column1 and column2 then the user will have access to both columns. Using the admin account, create new user to restrict by both row and column with default roles CREATE USER row_and_column_user IDENTIFIED WITH plaintext_password BY 'password' DEFAULT ROLE A_rows_users; Remove prior privileges for A_rows_users role REVOKE SELECT(id, column1, column2) ON db1.table1 FROM A_rows_users; Allow A_row_users role to only select from column1 GRANT SELECT(id, column1) ON db1.table1 TO A_rows_users; Log into the ClickHouse client using row_and_column_user clickhouse-client --user row_and_column_user --password password; Test with all columns: SELECT * FROM db1.table1 Query id: 8cdf0ff5-e711-4cbe-bd28-3c02e52e8bc4  0 rows in set. Elapsed: 0.005 sec. Received exception from server (version 22.3.2): Code: 497. DB::Exception: Received from localhost:9000. DB::Exception: row_and_column_user: Not enough privileges. To execute this query it's necessary to have grant SELECT(id, column1, column2) ON db1.table1. (ACCESS_DENIED) ```  Test with limited allowed columns: SELECT id, column1 FROM db1.table1 Query id: 5e30b490-507a-49e9-9778-8159799a6ed0 ┌─id─┬─column1─┐ │ 1 │ A │ │ 2 │ A │ └────┴─────────┘ Examples on how to delete privileges, policies, unassign users from roles, delete users and roles: Remove privilege from a role REVOKE SELECT(column1, id) ON db1.table1 FROM A_rows_users; Delete a policy DROP ROW POLICY A_row_filter ON db1.table1; Unassign a user from a role REVOKE A_rows_users FROM row_user; Delete a role DROP ROLE A_rows_users; Delete a user DROP USER row_user;  "},{"title":"5. Troubleshooting​","type":1,"pageTitle":"Defining SQL Users and Roles","url":"docs/guides/sre/users-and-roles#5-troubleshooting","content":"There are occasions when privileges intersect or combine to produce unexpected results, the following commands can be used to narrow the issue using an admin account Listing the grants and roles for a user SHOW GRANTS FOR row_and_column_user Query id: 6a73a3fe-2659-4aca-95c5-d012c138097b ┌─GRANTS FOR row_and_column_user───────────────────────────┐ │ GRANT A_rows_users, column1_users TO row_and_column_user │ └──────────────────────────────────────────────────────────┘ List roles in ClickHouse SHOW ROLES Query id: 1e21440a-18d9-4e75-8f0e-66ec9b36470a ┌─name────────────┐ │ A_rows_users │ │ column1_users │ └─────────────────┘ Display the policies SHOW ROW POLICIES Query id: f2c636e9-f955-4d79-8e80-af40ea227ebc ┌─name───────────────────────────────────┐ │ A_row_filter ON db1.table1 │ │ allow_other_users_filter ON db1.table1 │ └────────────────────────────────────────┘ View how a policy was defined and current privileges SHOW CREATE ROW POLICY A_row_filter ON db1.table1 Query id: 0d3b5846-95c7-4e62-9cdd-91d82b14b80b ┌─CREATE ROW POLICY A_row_filter ON db1.table1────────────────────────────────────────────────┐ │ CREATE ROW POLICY A_row_filter ON db1.table1 FOR SELECT USING column1 = 'A' TO A_rows_users │ └─────────────────────────────────────────────────────────────────────────────────────────────┘  "},{"title":"Summary​","type":1,"pageTitle":"Defining SQL Users and Roles","url":"docs/guides/sre/users-and-roles#summary","content":"This article demostrated the basics of creating SQL users and roles and provided steps to set and modify privileges for users and roles. For more detailed information on each please refer to our user guides and reference documenation. "},{"title":"Choosing an option","type":0,"sectionRef":"#","url":"docs/integrations/kafka/kafka-choosing-an-approach","content":"Choosing an option When integrating Kafka with ClickHouse, you will need to make early architectural decisions about the high-level approach used. We outline the most common approaches below: Kafka table engine - The Kafka table engine provides a Native ClickHouse integration. This table engine pulls data from the source system. This requires ClickHouse to have direct access to Kafka. (This approach is not currently supported in ClickHouse Cloud.)Kafka Connect - Kafka Connect is a free, open-source component of Apache Kafka® that works as a centralized data hub for simple data integration between Kafka and other data systems. Connectors provide a simple means of scalably and reliably streaming data to and from Kafka. Source Connectors inserts data to Kafka topics from other systems, whilst Sink Connectors delivers data from Kafka topics into other data stores such as ClickHouse. Custom code - Custom code using respective client libraries for Kafka and ClickHouse may be appropriate cases where custom processing of events is required. This is beyond the scope of this documentation.Vector - Coming soon Choosing an approach will come down to a few decision points: Connectivity - The Kafka table engine needs to be able to pull from Kafka if ClickHouse is the destination. This requires bi-directional connectivity. If there is a network separation, e.g. ClickHouse is in the Cloud and Kafka is self-managed, you may be hesitant to remove this for compliance and security reasons. (This approach is not currently supported in ClickHouse Cloud.) The Kafka table engine utilizes resources within ClickHouse itself, utilizing threads for the consumers. Placing this resource pressure on ClickHouse may not be possible due to resource constraints, or your architects may prefer a separation of concerns. In this case, tools such as Kafka Connect, which run as a separate process and can be deployed on different hardware may be preferable. This allows the process responsible for pulling Kafka data to be scaled independently of ClickHouse.External enrichment - Whilst messages can be manipulated before insertion into ClickHouse, through the use of functions in the select statement of the materialized view, users may prefer to move complex enrichment external to ClickHouse.","keywords":""},{"title":"HTTP Sink Connector","type":0,"sectionRef":"#","url":"docs/integrations/kafka/kafka-connect-http","content":"","keywords":""},{"title":"Self-Managed​","type":1,"pageTitle":"HTTP Sink Connector","url":"docs/integrations/kafka/kafka-connect-http#self-managed","content":""},{"title":"Steps​","type":1,"pageTitle":"HTTP Sink Connector","url":"docs/integrations/kafka/kafka-connect-http#steps","content":"1. Install Kafka Connect and Connector​ Download the Confluent package and install it locally. Follow the installation instructions for installing the connector as documented here. If you use the confluent-hub installation method, your local configuration files will be updated. 2. Prepare Configuration​ Follow these instructions for setting up Connect relevant to your installation type, noting the differences between a standalone and distributed cluster. If using Confluent Cloud, the distributed setup is relevant. The most important parameter is the http.api.url. The HTTP interface for ClickHouse requires you to encode the INSERT statement as a parameter in the URL. This must include the format (JSONEachRow in this case) and target database. The format must be consistent with the Kafka data, which will be converted to a string in the HTTP payload. These parameters must be URL escaped. An example of this format for the Github dataset (assuming you are running ClickHouse locally) is shown below: &lt;protocol&gt;://&lt;clickhouse_host&gt;:&lt;clickhouse_port&gt;?query=INSERT%20INTO%20&lt;database&gt;.&lt;table&gt;%20FORMAT%20JSONEachRow http://localhost:8123?query=INSERT%20INTO%20default.github%20FORMAT%20JSONEachRow  This URL is error-prone. Ensure escaping is precise to avoid issues. The following additional parameters are relevant to using the HTTP Sink with ClickHouse. A complete parameter list can be found here: request.method - Set to POST**retry.on.status.codes - Set to 400-500 to retry on any error codes. Refine based expected errors in data.request.body.format - In most cases this will be JSON.auth.type - Set to BASIC if you security with ClickHouse. Other ClickHouse compatible authentication mechanisms are not currently supported.ssl.enabled - set to true if using SSL.headers - set to &quot;Content-Type: application/json&quot;connection.user - username for ClickHouse.connection.password - password for ClickHouse.batch.max.size - The number of rows to send in a single batch. Ensure this set is to an appropriately large number. Per ClickHouse recommendations a value of 1000 is should be considered a minimum.tasks.max - The HTTP Sink connector supports running one or more tasks. This can be used to increase performance. Along with batch size this represents your primary means of improving performance.key.converter - set according to the types of your keys.value.converter - set based on the type of data on your topic. This data does not need a schema. The format here must be consistent with the FORMAT specified in the parameter http.api.url. The simplest here is to use JSON and the org.apache.kafka.connect.json.JsonConverter converter. Treating the value as a string, via the converter org.apache.kafka.connect.storage.StringConverter, is also possible - although this will require the user to extract a value in the insert statement using functions. Avro format is also supported in ClickHouse if using the io.confluent.connect.avro.AvroConverter converter.  A full list of settings, including how to configure a proxy, retries, and advanced SSL, can be found here. Example configuration files for the Github sample data can be found here, assuming Connect is run in standalone mode and Kafka is hosted in Confluent Cloud. 3. Create the ClickHouse table​ Ensure the table has been created. An example for a minimal github dataset using a standard MergeTree is shown below. CREATE TABLE github ( file_time DateTime, event_type Enum('CommitCommentEvent' = 1, 'CreateEvent' = 2, 'DeleteEvent' = 3, 'ForkEvent' = 4,'GollumEvent' = 5, 'IssueCommentEvent' = 6, 'IssuesEvent' = 7, 'MemberEvent' = 8, 'PublicEvent' = 9, 'PullRequestEvent' = 10, 'PullRequestReviewCommentEvent' = 11, 'PushEvent' = 12, 'ReleaseEvent' = 13, 'SponsorshipEvent' = 14, 'WatchEvent' = 15, 'GistEvent' = 16, 'FollowEvent' = 17, 'DownloadEvent' = 18, 'PullRequestReviewEvent' = 19, 'ForkApplyEvent' = 20, 'Event' = 21, 'TeamAddEvent' = 22), actor_login LowCardinality(String), repo_name LowCardinality(String), created_at DateTime, updated_at DateTime, action Enum('none' = 0, 'created' = 1, 'added' = 2, 'edited' = 3, 'deleted' = 4, 'opened' = 5, 'closed' = 6, 'reopened' = 7, 'assigned' = 8, 'unassigned' = 9, 'labeled' = 10, 'unlabeled' = 11, 'review_requested' = 12, 'review_request_removed' = 13, 'synchronize' = 14, 'started' = 15, 'published' = 16, 'update' = 17, 'create' = 18, 'fork' = 19, 'merged' = 20), comment_id UInt64, path String, ref LowCardinality(String), ref_type Enum('none' = 0, 'branch' = 1, 'tag' = 2, 'repository' = 3, 'unknown' = 4), creator_user_login LowCardinality(String), number UInt32, title String, labels Array(LowCardinality(String)), state Enum('none' = 0, 'open' = 1, 'closed' = 2), assignee LowCardinality(String), assignees Array(LowCardinality(String)), closed_at DateTime, merged_at DateTime, merge_commit_sha String, requested_reviewers Array(LowCardinality(String)), merged_by LowCardinality(String), review_comments UInt32, member_login LowCardinality(String) ) ENGINE = MergeTree ORDER BY (event_type, repo_name, created_at)  4. Add data to Kafka​ Insert messages to Kafka. Below we use kcat to insert 10k messages. head -n 10000 github_all_columns.ndjson | kafkacat -b &lt;host&gt;:&lt;port&gt; -X security.protocol=sasl_ssl -X sasl.mechanisms=PLAIN -X sasl.username=&lt;username&gt; -X sasl.password=&lt;password&gt; -t github  A simple read on the target table “Github” should confirm the insertion of data. SELECT count() FROM default.github; | count\\(\\) | | :--- | | 10000 |  "},{"title":"Confluent Cloud​","type":1,"pageTitle":"HTTP Sink Connector","url":"docs/integrations/kafka/kafka-connect-http#confluent-cloud","content":"A fully managed version of HTTP Sink is available for those using Confluent Cloud for their Kafka hosting. This requires your ClickHouse environment to be accessible from Confluent Cloud. We assume you have taken the appropriate measures to secure this. The instructions for creating an HTTP Sink in Confluent Cloud can be found here. The following settings are relevant if connecting to ClickHouse. If not specified, form defaults are applicable: Input messages - Depends on your source data but in most cases JSON or Avro. We assume JSON in the following settings.Kafka Cluster credentials - Confluent cloud allows you to generate these for the appropriate topic from which you wish to pull messages.HTTP server details - The connection details for ClickHouse. Specifically: HTTP Url - This should be of the same format as the self-managed configuration parameter http.api.url i.e. &lt;protocol&gt;://&lt;clickhouse_host&gt;:&lt;clickhouse_port&gt;?query=INSERT%20INTO%20&lt;database&gt;.&lt;table&gt;%20FORMAT%20JSONEachRowHTTP Request Method - Set to POSTHTTP Headers - “Content Type: application/json” HTTP server batches Request Body Format - jsonBatch batch size - Per ClickHouse recommendations, set this to at least 1000. HTTP server authentication Endpoint Authentication type - BASICAuth username - ClickHouse usernameAuth password - ClickHouse password HTTP server retries - Settings here can be adjusted according to requirements. Timeouts specifically may need adjusting depending on latency. Retry on HTTP codes - 400-500 but adapt as required e.g. this may change if you have an HTTP proxy in front of ClickHouse.Maximum Reties - the default (10) is appropriate but feel to adjust for more robust retries.  "},{"title":"2020","type":0,"sectionRef":"#","url":"docs/en/whats-new/changelog/2020","content":"","keywords":""},{"title":"ClickHouse release 20.12​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-2012","content":""},{"title":"ClickHouse release v20.12.5.14-stable, 2020-12-28​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v2012514-stable-2020-12-28","content":"Bug Fix​ Disable write with AIO during merges because it can lead to extremely rare data corruption of primary key columns during merge. #18481 (alesapin).Fixed value is too short error when executing toType(...) functions (toDate, toUInt32, etc) with argument of type Nullable(String). Now such functions return NULL on parsing errors instead of throwing exception. Fixes #7673. #18445 (tavplubix).Restrict merges from wide to compact parts. In case of vertical merge it led to broken result part. #18381 (Anton Popov).Fix filling table system.settings_profile_elements. This PR fixes #18231. #18379 (Vitaly Baranov).Fix possible crashes in aggregate functions with combinator Distinct, while using two-level aggregation. Fixes #17682. #18365 (Anton Popov).Fix error when query MODIFY COLUMN ... REMOVE TTL does not actually remove column TTL. #18130 (alesapin). Build/Testing/Packaging Improvement​ Update timezones info to 2020e. #18531 (alesapin). "},{"title":"ClickHouse release v20.12.4.5-stable, 2020-12-24​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v201245-stable-2020-12-24","content":"Bug Fix​ Fixed issue when clickhouse-odbc-bridge process is unreachable by server on machines with dual IPv4/IPv6 stack; - Fixed issue when ODBC dictionary updates are performed using malformed queries and/or cause crashes; Possibly closes #14489. #18278 (Denis Glazachev).Fixed key comparison between Enum and Int types. This fixes #17989. #18214 (Amos Bird).Fixed unique key convert crash in MaterializeMySQL database engine. This fixes #18186 and fixes #16372 #18211 (Winter Zhang).Fixed std::out_of_range: basic_string in S3 URL parsing. #18059 (Vladimir Chebotarev).Fixed the issue when some tables not synchronized to ClickHouse from MySQL caused by the fact that convertion MySQL prefix index wasn't supported for MaterializeMySQL. This fixes #15187 and fixes #17912 #17944 (Winter Zhang).Fixed the issue when query optimization was producing wrong result if query contains ARRAY JOIN. #17887 (sundyli).Fixed possible segfault in topK aggregate function. This closes #17404. #17845 (Maksim Kita).Fixed empty system.stack_trace table when server is running in daemon mode. #17630 (Amos Bird). "},{"title":"ClickHouse release v20.12.3.3-stable, 2020-12-13​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v201233-stable-2020-12-13","content":"Backward Incompatible Change​ Enable use_compact_format_in_distributed_parts_names by default (see the documentation for the reference). #16728 (Azat Khuzhin).Accept user settings related to file formats (e.g. format_csv_delimiter) in the SETTINGS clause when creating a table that uses File engine, and use these settings in all INSERTs and SELECTs. The file format settings changed in the current user session, or in the SETTINGS clause of a DML query itself, no longer affect the query. #16591 (Alexander Kuzmenkov). New Feature​ add *.xz compression/decompression support.It enables using *.xz in file() function. This closes #8828. #16578 (Abi Palagashvili).Introduce the query ALTER TABLE ... DROP|DETACH PART 'part_name'. #15511 (nvartolomei).Added new ALTER UPDATE/DELETE IN PARTITION syntax. #13403 (Vladimir Chebotarev).Allow formatting named tuples as JSON objects when using JSON input/output formats, controlled by the output_format_json_named_tuples_as_objects setting, disabled by default. #17175 (Alexander Kuzmenkov).Add a possibility to input enum value as it's id in TSV and CSV formats by default. #16834 (Kruglov Pavel).Add COLLATE support for Nullable, LowCardinality, Array and Tuple, where nested type is String. Also refactor the code associated with collations in ColumnString.cpp. #16273 (Kruglov Pavel).New tcpPort function returns TCP port listened by this server. #17134 (Ivan).Add new math functions: acosh, asinh, atan2, atanh, cosh, hypot, log1p, sinh. #16636 (Konstantin Malanchev).Possibility to distribute the merges between different replicas. Introduces the execute_merges_on_single_replica_time_threshold mergetree setting. #16424 (filimonov).Add setting aggregate_functions_null_for_empty for SQL standard compatibility. This option will rewrite all aggregate functions in a query, adding -OrNull suffix to them. Implements 10273. #16123 (flynn).Updated DateTime, DateTime64 parsing to accept string Date literal format. #16040 (Maksim Kita).Make it possible to change the path to history file in clickhouse-client using the --history_file parameter. #15960 (Maksim Kita). Bug Fix​ Fix the issue when server can stop accepting connections in very rare cases. #17542 (Amos Bird).Fixed Function not implemented error when executing RENAME query in Atomic database with ClickHouse running on Windows Subsystem for Linux. Fixes #17661. #17664 (tavplubix).Do not restore parts from WAL if in_memory_parts_enable_wal is disabled. #17802 (detailyang).fix incorrect initialization of max_compress_block_size of MergeTreeWriterSettings with min_compress_block_size. #17833 (flynn).Exception message about max table size to drop was displayed incorrectly. #17764 (alexey-milovidov).Fixed possible segfault when there is not enough space when inserting into Distributed table. #17737 (tavplubix).Fixed problem when ClickHouse fails to resume connection to MySQL servers. #17681 (Alexander Kazakov).In might be determined incorrectly if cluster is circular- (cross-) replicated or not when executing ON CLUSTER query due to race condition when pool_size &gt; 1. It's fixed. #17640 (tavplubix).Exception fmt::v7::format_error can be logged in background for MergeTree tables. This fixes #17613. #17615 (alexey-milovidov).When clickhouse-client is used in interactive mode with multiline queries, single line comment was erronously extended till the end of query. This fixes #13654. #17565 (alexey-milovidov).Fix alter query hang when the corresponding mutation was killed on the different replica. Fixes #16953. #17499 (alesapin).Fix issue when mark cache size was underestimated by clickhouse. It may happen when there are a lot of tiny files with marks. #17496 (alesapin).Fix ORDER BY with enabled setting optimize_redundant_functions_in_order_by. #17471 (Anton Popov).Fix duplicates after DISTINCT which were possible because of incorrect optimization. Fixes #17294. #17296 (li chengxiang). #17439 (Nikolai Kochetov).Fix crash while reading from JOIN table with LowCardinality types. Fixes #17228. #17397 (Nikolai Kochetov).fix toInt256(inf) stack overflow. Int256 is an experimental feature. Closed #17235. #17257 (flynn).Fix possible Unexpected packet Data received from client error logged for Distributed queries with LIMIT. #17254 (Azat Khuzhin).Fix set index invalidation when there are const columns in the subquery. This fixes #17246. #17249 (Amos Bird).Fix possible wrong index analysis when the types of the index comparison are different. This fixes #17122. #17145 (Amos Bird).Fix ColumnConst comparison which leads to crash. This fixed #17088 . #17135 (Amos Bird).Multiple fixed for MaterializeMySQL (experimental feature). Fixes #16923 Fixes #15883 Fix MaterializeMySQL SYNC failure when the modify MySQL binlog_checksum. #17091 (Winter Zhang).Fix bug when ON CLUSTER queries may hang forever for non-leader ReplicatedMergeTreeTables. #17089 (alesapin).Fixed crash on CREATE TABLE ... AS some_table query when some_table was created AS table_function() Fixes #16944. #17072 (tavplubix).Bug unfinished implementation for funciton fuzzBits, related issue: #16980. #17051 (hexiaoting).Fix LLVM's libunwind in the case when CFA register is RAX. This is the bug in LLVM's libunwind. We already have workarounds for this bug. #17046 (alexey-milovidov).Avoid unnecessary network errors for remote queries which may be cancelled while execution, like queries with LIMIT. #17006 (Azat Khuzhin).Fix optimize_distributed_group_by_sharding_key setting (that is disabled by default) for query with OFFSET only. #16996 (Azat Khuzhin).Fix for Merge tables over Distributed tables with JOIN. #16993 (Azat Khuzhin).Fixed wrong result in big integers (128, 256 bit) when casting from double. Big integers support is experimental. #16986 (Mike).Fix possible server crash after ALTER TABLE ... MODIFY COLUMN ... NewType when SELECT have WHERE expression on altering column and alter does not finished yet. #16968 (Amos Bird).Blame info was not calculated correctly in clickhouse-git-import. #16959 (alexey-milovidov).Fix order by optimization with monotonous functions. Fixes #16107. #16956 (Anton Popov).Fix optimization of group by with enabled setting optimize_aggregators_of_group_by_keys and joins. Fixes #12604. #16951 (Anton Popov).Fix possible error Illegal type of argument for queries with ORDER BY. Fixes #16580. #16928 (Nikolai Kochetov).Fix strange code in InterpreterShowAccessQuery. #16866 (tavplubix).Prevent clickhouse server crashes when using the function timeSeriesGroupSum. The function is removed from newer ClickHouse releases. #16865 (filimonov).Fix rare silent crashes when query profiler is on and ClickHouse is installed on OS with glibc version that has (supposedly) broken asynchronous unwind tables for some functions. This fixes #15301. This fixes #13098. #16846 (alexey-milovidov).Fix crash when using any without any arguments. This is for #16803 . cc @azat. #16826 (Amos Bird).If no memory can be allocated while writing table metadata on disk, broken metadata file can be written. #16772 (alexey-milovidov).Fix trivial query optimization with partition predicate. #16767 (Azat Khuzhin).Fix IN operator over several columns and tuples with enabled transform_null_in setting. Fixes #15310. #16722 (Anton Popov).Return number of affected rows for INSERT queries via MySQL protocol. Previously ClickHouse used to always return 0, it's fixed. Fixes #16605. #16715 (Winter Zhang).Fix remote query failure when using 'if' suffix aggregate function. Fixes #16574 Fixes #16231 #16610 (Winter Zhang).Fix inconsistent behavior caused by select_sequential_consistency for optimized trivial count query and system.tables. #16309 (Hao Chen). Improvement​ Remove empty parts after they were pruned by TTL, mutation, or collapsing merge algorithm. #16895 (Anton Popov).Enable compact format of directories for asynchronous sends in Distributed tables: use_compact_format_in_distributed_parts_names is set to 1 by default. #16788 (Azat Khuzhin).Abort multipart upload if no data was written to S3. #16840 (Pavel Kovalenko).Reresolve the IP of the format_avro_schema_registry_url in case of errors. #16985 (filimonov).Mask password in data_path in the system.distribution_queue. #16727 (Azat Khuzhin).Throw error when use column transformer replaces non existing column. #16183 (hexiaoting).Turn off parallel parsing when there is no enough memory for all threads to work simultaneously. Also there could be exceptions like &quot;Memory limit exceeded&quot; when somebody will try to insert extremely huge rows (&gt; min_chunk_bytes_for_parallel_parsing), because each piece to parse has to be independent set of strings (one or more). #16721 (Nikita Mikhaylov).Install script should always create subdirs in config folders. This is only relevant for Docker build with custom config. #16936 (filimonov).Correct grammar in error message in JSONEachRow, JSONCompactEachRow, and RegexpRow input formats. #17205 (nico piderman).Set default host and port parameters for SOURCE(CLICKHOUSE(...)) to current instance and set default user value to 'default'. #16997 (vdimir).Throw an informative error message when doing ATTACH/DETACH TABLE &lt;DICTIONARY&gt;. Before this PR, detach table &lt;dict&gt; works but leads to an ill-formed in-memory metadata. #16885 (Amos Bird).Add cutToFirstSignificantSubdomainWithWWW(). #16845 (Azat Khuzhin).Server refused to startup with exception message if wrong config is given (metric_log.collect_interval_milliseconds is missing). #16815 (Ivan).Better exception message when configuration for distributed DDL is absent. This fixes #5075. #16769 (Nikita Mikhaylov).Usability improvement: better suggestions in syntax error message when CODEC expression is misplaced in CREATE TABLE query. This fixes #12493. #16768 (alexey-milovidov).Remove empty directories for async INSERT at start of Distributed engine. #16729 (Azat Khuzhin).Workaround for use S3 with nginx server as proxy. Nginx currenty does not accept urls with empty path like http://domain.com?delete, but vanilla aws-sdk-cpp produces this kind of urls. This commit uses patched aws-sdk-cpp version, which makes urls with &quot;/&quot; as path in this cases, like http://domain.com/?delete. #16709 (ianton-ru).Allow reinterpretAs* functions to work for integers and floats of the same size. Implements 16640. #16657 (flynn).Now, &lt;auxiliary_zookeepers&gt; configuration can be changed in config.xml and reloaded without server startup. #16627 (Amos Bird).Support SNI in https connections to remote resources. This will allow to connect to Cloudflare servers that require SNI. This fixes #10055. #16252 (alexey-milovidov).Make it possible to connect to clickhouse-server secure endpoint which requires SNI. This is possible when clickhouse-server is hosted behind TLS proxy. #16938 (filimonov).Fix possible stack overflow if a loop of materialized views is created. This closes #15732. #16048 (alexey-milovidov).Simplify the implementation of background tasks processing for the MergeTree table engines family. There should be no visible changes for user. #15983 (alesapin).Improvement for MaterializeMySQL (experimental feature). Throw exception about right sync privileges when MySQL sync user has error privileges. #15977 (TCeason).Made indexOf() use BloomFilter. #14977 (achimbab). Performance Improvement​ Use Floyd-Rivest algorithm, it is the best for the ClickHouse use case of partial sorting. Bechmarks are in https://github.com/danlark1/miniselect and here. #16825 (Danila Kutenin).Now ReplicatedMergeTree tree engines family uses a separate thread pool for replicated fetches. Size of the pool limited by setting background_fetches_pool_size which can be tuned with a server restart. The default value of the setting is 3 and it means that the maximum amount of parallel fetches is equal to 3 (and it allows to utilize 10G network). Fixes #520. #16390 (alesapin).Fixed uncontrolled growth of the state of quantileTDigest. #16680 (hrissan).Add VIEW subquery description to EXPLAIN. Limit push down optimisation for VIEW. Add local replicas of Distributed to query plan. #14936 (Nikolai Kochetov).Fix optimize_read_in_order/optimize_aggregation_in_order with max_threads &gt; 0 and expression in ORDER BY. #16637 (Azat Khuzhin).Fix performance of reading from Merge tables over huge number of MergeTree tables. Fixes #7748. #16988 (Anton Popov).Now we can safely prune partitions with exact match. Useful case: Suppose table is partitioned by intHash64(x) % 100 and the query has condition on intHash64(x) % 100 verbatim, not on x. #16253 (Amos Bird). Experimental Feature​ Add EmbeddedRocksDB table engine (can be used for dictionaries). #15073 (sundyli). Build/Testing/Packaging Improvement​ Improvements in test coverage building images. #17233 (alesapin).Update embedded timezone data to version 2020d (also update cctz to the latest master). #17204 (filimonov).Fix UBSan report in Poco. This closes #12719. #16765 (alexey-milovidov).Do not instrument 3rd-party libraries with UBSan. #16764 (alexey-milovidov).Fix UBSan report in cache dictionaries. This closes #12641. #16763 (alexey-milovidov).Fix UBSan report when trying to convert infinite floating point number to integer. This closes #14190. #16677 (alexey-milovidov). "},{"title":"ClickHouse release 20.11​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-2011","content":""},{"title":"ClickHouse release v20.11.7.16-stable, 2021-03-02​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v2011716-stable-2021-03-02","content":"Improvement​ Explicitly set uid / gid of clickhouse user &amp; group to the fixed values (101) in clickhouse-server images. #19096 (filimonov). Bug Fix​ BloomFilter index crash fix. Fixes #19757. #19884 (Maksim Kita).Deadlock was possible if system.text_log is enabled. This fixes #19874. #19875 (alexey-milovidov).In previous versions, unusual arguments for function arrayEnumerateUniq may cause crash or infinite loop. This closes #19787. #19788 (alexey-milovidov).Fixed stack overflow when using accurate comparison of arithmetic type with string type. #19773 (tavplubix).Fix a segmentation fault in bitmapAndnot function. Fixes #19668. #19713 (Maksim Kita).Some functions with big integers may cause segfault. Big integers is experimental feature. This closes #19667. #19672 (alexey-milovidov).Fix wrong result of function neighbor for LowCardinality argument. Fixes #10333. #19617 (Nikolai Kochetov).Fix use-after-free of the CompressedWriteBuffer in Connection after disconnect. #19599 (Azat Khuzhin).DROP/DETACH TABLE table ON CLUSTER cluster SYNC query might hang, it's fixed. Fixes #19568. #19572 (tavplubix).Query CREATE DICTIONARY id expression fix. #19571 (Maksim Kita).Fix SIGSEGV with merge_tree_min_rows_for_concurrent_read/merge_tree_min_bytes_for_concurrent_read=0/UINT64_MAX. #19528 (Azat Khuzhin).Buffer overflow (on memory read) was possible if addMonth function was called with specifically crafted arguments. This fixes #19441. This fixes #19413. #19472 (alexey-milovidov).Mark distributed batch as broken in case of empty data block in one of files. #19449 (Azat Khuzhin).Fix possible buffer overflow in Uber H3 library. See https://github.com/uber/h3/issues/392. This closes #19219. #19383 (alexey-milovidov).Fix system.parts _state column (LOGICAL_ERROR when querying this column, due to incorrect order). #19346 (Azat Khuzhin).Fix error Cannot convert column now64() because it is constant but values of constants are different in source and result. Continuation of #7156. #19316 (Nikolai Kochetov).Fix bug when concurrent ALTER and DROP queries may hang while processing ReplicatedMergeTree table. #19237 (alesapin).Fix infinite reading from file in ORC format (was introduced in #10580). Fixes #19095. #19134 (Nikolai Kochetov).Fix startup bug when clickhouse was not able to read compression codec from LowCardinality(Nullable(...)) and throws exception Attempt to read after EOF. Fixes #18340. #19101 (alesapin).Fixed There is no checkpoint error when inserting data through http interface using Template or CustomSeparated format. Fixes #19021. #19072 (tavplubix).Restrict MODIFY TTL queries for MergeTree tables created in old syntax. Previously the query succeeded, but actually it had no effect. #19064 (Anton Popov).Make sure groupUniqArray returns correct type for argument of Enum type. This closes #17875. #19019 (alexey-milovidov).Fix possible error Expected single dictionary argument for function if use function ignore with LowCardinality argument. Fixes #14275. #19016 (Nikolai Kochetov).Fix inserting of LowCardinality column to table with TinyLog engine. Fixes #18629. #19010 (Nikolai Kochetov).Disable optimize_move_functions_out_of_any because optimization is not always correct. This closes #18051. This closes #18973. #18981 (alexey-milovidov).Fixed very rare deadlock at shutdown. #18977 (tavplubix).Fix bug when mutation with some escaped text (like ALTER ... UPDATE e = CAST('foo', 'Enum8(\\'foo\\' = 1') serialized incorrectly. Fixes #18878. #18944 (alesapin).Attach partition should reset the mutation. #18804. #18935 (fastio).Fix possible hang at shutdown in clickhouse-local. This fixes #18891. #18893 (alexey-milovidov).Fix *If combinator with unary function and Nullable types. #18806 (Azat Khuzhin).Asynchronous distributed INSERTs can be rejected by the server if the setting network_compression_method is globally set to non-default value. This fixes #18741. #18776 (alexey-milovidov).Fixed Attempt to read after eof error when trying to CAST NULL from Nullable(String) to Nullable(Decimal(P, S)). Now function CAST returns NULL when it cannot parse decimal from nullable string. Fixes #7690. #18718 (Winter Zhang).Fix Logger with unmatched arg size. #18717 (sundyli).Add FixedString Data type support. I'll get this exception &quot;Code: 50, e.displayText() = DB::Exception: Unsupported type FixedString(1)&quot; when replicating data from MySQL to ClickHouse. This patch fixes bug #18450 Also fixes #6556. #18553 (awesomeleo).Fix possible Pipeline stuck error while using ORDER BY after subquery with RIGHT or FULL join. #18550 (Nikolai Kochetov).Fix bug which may lead to ALTER queries hung after corresponding mutation kill. Found by thread fuzzer. #18518 (alesapin).Disable write with AIO during merges because it can lead to extremely rare data corruption of primary key columns during merge. #18481 (alesapin).Disable constant folding for subqueries on the analysis stage, when the result cannot be calculated. #18446 (Azat Khuzhin).Fixed value is too short error when executing toType(...) functions (toDate, toUInt32, etc) with argument of type Nullable(String). Now such functions return NULL on parsing errors instead of throwing exception. Fixes #7673. #18445 (tavplubix).Restrict merges from wide to compact parts. In case of vertical merge it led to broken result part. #18381 (Anton Popov).Fix filling table system.settings_profile_elements. This PR fixes #18231. #18379 (Vitaly Baranov).Fix index analysis of binary functions with constant argument which leads to wrong query results. This fixes #18364. #18373 (Amos Bird).Fix possible crashes in aggregate functions with combinator Distinct, while using two-level aggregation. Fixes #17682. #18365 (Anton Popov).SELECT count() FROM table now can be executed if only one any column can be selected from the table. This PR fixes #10639. #18233 (Vitaly Baranov).SELECT JOIN now requires the SELECT privilege on each of the joined tables. This PR fixes #17654. #18232 (Vitaly Baranov).Fix possible incomplete query result while reading from MergeTree* in case of read backoff (message &lt;Debug&gt; MergeTreeReadPool: Will lower number of threads in logs). Was introduced in #16423. Fixes #18137. #18216 (Nikolai Kochetov).Fix error when query MODIFY COLUMN ... REMOVE TTL does not actually remove column TTL. #18130 (alesapin).Fix indeterministic functions with predicate optimizer. This fixes #17244. #17273 (Winter Zhang).Mutation might hang waiting for some non-existent part after MOVE or REPLACE PARTITION or, in rare cases, after DETACH or DROP PARTITION. It's fixed. #15537 (tavplubix). Build/Testing/Packaging Improvement​ Update timezones info to 2020e. #18531 (alesapin). "},{"title":"ClickHouse release v20.11.6.6-stable, 2020-12-24​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v201166-stable-2020-12-24","content":"Bug Fix​ Fixed issue when clickhouse-odbc-bridge process is unreachable by server on machines with dual IPv4/IPv6 stack and fixed issue when ODBC dictionary updates are performed using malformed queries and/or cause crashes. This possibly closes #14489. #18278 (Denis Glazachev).Fixed key comparison between Enum and Int types. This fixes #17989. #18214 (Amos Bird).Fixed unique key convert crash in MaterializeMySQL database engine. This fixes #18186 and fixes #16372 #18211 (Winter Zhang).Fixed std::out_of_range: basic_string in S3 URL parsing. #18059 (Vladimir Chebotarev).Fixed the issue when some tables not synchronized to ClickHouse from MySQL caused by the fact that convertion MySQL prefix index wasn't supported for MaterializeMySQL. This fixes #15187 and fixes #17912 #17944 (Winter Zhang).Fixed the issue when query optimization was producing wrong result if query contains ARRAY JOIN. #17887 (sundyli).Fix possible segfault in topK aggregate function. This closes #17404. #17845 (Maksim Kita).Do not restore parts from WAL if in_memory_parts_enable_wal is disabled. #17802 (detailyang).Fixed problem when ClickHouse fails to resume connection to MySQL servers. #17681 (Alexander Kazakov).Fixed inconsistent behaviour of optimize_trivial_count_query with partition predicate. #17644 (Azat Khuzhin).Fixed empty system.stack_trace table when server is running in daemon mode. #17630 (Amos Bird).Fixed the behaviour when xxception fmt::v7::format_error can be logged in background for MergeTree tables. This fixes #17613. #17615 (alexey-milovidov).Fixed the behaviour when clickhouse-client is used in interactive mode with multiline queries and single line comment was erronously extended till the end of query. This fixes #13654. #17565 (alexey-milovidov).Fixed the issue when server can stop accepting connections in very rare cases. #17542 (alexey-milovidov).Fixed alter query hang when the corresponding mutation was killed on the different replica. This fixes #16953. #17499 (alesapin).Fixed bug when mark cache size was underestimated by clickhouse. It may happen when there are a lot of tiny files with marks. #17496 (alesapin).Fixed ORDER BY with enabled setting optimize_redundant_functions_in_order_by. #17471 (Anton Popov).Fixed duplicates after DISTINCT which were possible because of incorrect optimization. This fixes #17294. #17296 (li chengxiang). #17439 (Nikolai Kochetov).Fixed crash while reading from JOIN table with LowCardinality types. This fixes #17228. #17397 (Nikolai Kochetov).Fixed set index invalidation when there are const columns in the subquery. This fixes #17246 . #17249 (Amos Bird).Fixed possible wrong index analysis when the types of the index comparison are different. This fixes #17122. #17145 (Amos Bird).Fixed ColumnConst comparison which leads to crash. This fixes #17088 . #17135 (Amos Bird).Fixed bug when ON CLUSTER queries may hang forever for non-leader ReplicatedMergeTreeTables. #17089 (alesapin).Fixed fuzzer-found bug in funciton fuzzBits. This fixes #16980. #17051 (hexiaoting).Avoid unnecessary network errors for remote queries which may be cancelled while execution, like queries with LIMIT. #17006 (Azat Khuzhin).Fixed wrong result in big integers (128, 256 bit) when casting from double. #16986 (Mike).Reresolve the IP of the format_avro_schema_registry_url in case of errors. #16985 (filimonov).Fixed possible server crash after ALTER TABLE ... MODIFY COLUMN ... NewType when SELECT have WHERE expression on altering column and alter does not finished yet. #16968 (Amos Bird).Blame info was not calculated correctly in clickhouse-git-import. #16959 (alexey-milovidov).Fixed order by optimization with monotonous functions. Fixes #16107. #16956 (Anton Popov).Fixed optimization of group by with enabled setting optimize_aggregators_of_group_by_keys and joins. This fixes #12604. #16951 (Anton Popov).Install script should always create subdirs in config folders. This is only relevant for Docker build with custom config. #16936 (filimonov).Fixed possible error Illegal type of argument for queries with ORDER BY. This fixes #16580. #16928 (Nikolai Kochetov).Abort multipart upload if no data was written to WriteBufferFromS3. #16840 (Pavel Kovalenko).Fixed crash when using any without any arguments. This fixes #16803. #16826 (Amos Bird).Fixed the behaviour when ClickHouse used to always return 0 insted of a number of affected rows for INSERT queries via MySQL protocol. This fixes #16605. #16715 (Winter Zhang).Fixed uncontrolled growth of TDigest. #16680 (hrissan).Fixed remote query failure when using suffix if in Aggregate function. This fixes #16574 fixes #16231 #16610 (Winter Zhang).Fixed inconsistent behavior caused by select_sequential_consistency for optimized trivial count query and system.tables. #16309 (Hao Chen).Throw error when use ColumnTransformer replace non exist column. #16183 (hexiaoting). "},{"title":"ClickHouse release v20.11.3.3-stable, 2020-11-13​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v201133-stable-2020-11-13","content":"Bug Fix​ Fix rare silent crashes when query profiler is on and ClickHouse is installed on OS with glibc version that has (supposedly) broken asynchronous unwind tables for some functions. This fixes #15301. This fixes #13098. #16846 (alexey-milovidov). "},{"title":"ClickHouse release v20.11.2.1, 2020-11-11​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v201121-2020-11-11","content":"Backward Incompatible Change​ If some profile was specified in distributed_ddl config section, then this profile could overwrite settings of default profile on server startup. It's fixed, now settings of distributed DDL queries should not affect global server settings. #16635 (tavplubix).Restrict to use of non-comparable data types (like AggregateFunction) in keys (Sorting key, Primary key, Partition key, and so on). #16601 (alesapin).Remove ANALYZE and AST queries, and make the setting enable_debug_queries obsolete since now it is the part of full featured EXPLAIN query. #16536 (Ivan).Aggregate functions boundingRatio, rankCorr, retention, timeSeriesGroupSum, timeSeriesGroupRateSum, windowFunnel were erroneously made case-insensitive. Now their names are made case sensitive as designed. Only functions that are specified in SQL standard or made for compatibility with other DBMS or functions similar to those should be case-insensitive. #16407 (alexey-milovidov).Make rankCorr function return nan on insufficient data #16124. #16135 (hexiaoting).When upgrading from versions older than 20.5, if rolling update is performed and cluster contains both versions 20.5 or greater and less than 20.5, if ClickHouse nodes with old versions are restarted and old version has been started up in presence of newer versions, it may lead to Part ... intersects previous part errors. To prevent this error, first install newer clickhouse-server packages on all cluster nodes and then do restarts (so, when clickhouse-server is restarted, it will start up with the new version). New Feature​ Added support of LDAP as a user directory for locally non-existent users. #12736 (Denis Glazachev).Add system.replicated_fetches table which shows currently running background fetches. #16428 (alesapin).Added setting date_time_output_format. #15845 (Maksim Kita).Added minimal web UI to ClickHouse. #16158 (alexey-milovidov).Allows to read/write Single protobuf message at once (w/o length-delimiters). #15199 (filimonov).Added initial OpenTelemetry support. ClickHouse now accepts OpenTelemetry traceparent headers over Native and HTTP protocols, and passes them downstream in some cases. The trace spans for executed queries are saved into the system.opentelemetry_span_log table. #14195 (Alexander Kuzmenkov).Allow specify primary key in column list of CREATE TABLE query. This is needed for compatibility with other SQL dialects. #15823 (Maksim Kita).Implement OFFSET offset_row_count {ROW | ROWS} FETCH {FIRST | NEXT} fetch_row_count {ROW | ROWS} {ONLY | WITH TIES} in SELECT query with ORDER BY. This is the SQL-standard way to specify LIMIT. #15855 (hexiaoting).errorCodeToName function - return variable name of the error (useful for analyzing query_log and similar). system.errors table - shows how many times errors has been happened (respects system_events_show_zero_values). #16438 (Azat Khuzhin).Added function untuple which is a special function which can introduce new columns to the SELECT list by expanding a named tuple. #16242 (Nikolai Kochetov, Amos Bird).Now we can provide identifiers via query parameters. And these parameters can be used as table objects or columns. #16594 (Amos Bird).Added big integers (UInt256, Int128, Int256) and UUID data types support for MergeTree BloomFilter index. Big integers is an experimental feature. #16642 (Maksim Kita).Add farmFingerprint64 function (non-cryptographic string hashing). #16570 (Jacob Hayes).Add log_queries_min_query_duration_ms, only queries slower than the value of this setting will go to query_log/query_thread_log (i.e. something like slow_query_log in mysql). #16529 (Azat Khuzhin).Ability to create a docker image on the top of Alpine. Uses precompiled binary and glibc components from ubuntu 20.04. #16479 (filimonov).Added toUUIDOrNull, toUUIDOrZero cast functions. #16337 (Maksim Kita).Add max_concurrent_queries_for_all_users setting, see #6636 for use cases. #16154 (nvartolomei).Add a new option print_query_id to clickhouse-client. It helps generate arbitrary strings with the current query id generated by the client. Also print query id in clickhouse-client by default. #15809 (Amos Bird).Add tid and logTrace functions. This closes #9434. #15803 (flynn).Add function formatReadableTimeDelta that format time delta to human readable string ... #15497 (Filipe Caixeta).Added disable_merges option for volumes in multi-disk configuration. #13956 (Vladimir Chebotarev). Experimental Feature​ New functions encrypt, aes_encrypt_mysql, decrypt, aes_decrypt_mysql. These functions are working slowly, so we consider it as an experimental feature. #11844 (Vasily Nemkov). Bug Fix​ Mask password in data_path in the system.distribution_queue. #16727 (Azat Khuzhin).Fix IN operator over several columns and tuples with enabled transform_null_in setting. Fixes #15310. #16722 (Anton Popov).The setting max_parallel_replicas worked incorrectly if the queried table has no sampling. This fixes #5733. #16675 (alexey-milovidov).Fix optimize_read_in_order/optimize_aggregation_in_order with max_threads &gt; 0 and expression in ORDER BY. #16637 (Azat Khuzhin).Calculation of DEFAULT expressions was involving possible name collisions (that was very unlikely to encounter). This fixes #9359. #16612 (alexey-milovidov).Fix query_thread_log.query_duration_ms unit. #16563 (Azat Khuzhin).Fix a bug when using MySQL Master -&gt; MySQL Slave -&gt; ClickHouse MaterializeMySQL Engine. MaterializeMySQL is an experimental feature. #16504 (TCeason).Specifically crafted argument of round function with Decimal was leading to integer division by zero. This fixes #13338. #16451 (alexey-milovidov).Fix DROP TABLE for Distributed (racy with INSERT). #16409 (Azat Khuzhin).Fix processing of very large entries in replication queue. Very large entries may appear in ALTER queries if table structure is extremely large (near 1 MB). This fixes #16307. #16332 (alexey-milovidov).Fixed the inconsistent behaviour when a part of return data could be dropped because the set for its filtration wasn't created. #16308 (Nikita Mikhaylov).Fix dictGet in sharding_key (and similar places, i.e. when the function context is stored permanently). #16205 (Azat Khuzhin).Fix the exception thrown in clickhouse-local when trying to execute OPTIMIZE command. Fixes #16076. #16192 (filimonov).Fixes #15780 regression, e.g. indexOf([1, 2, 3], toLowCardinality(1)) now is prohibited but it should not be. #16038 (Mike).Fix bug with MySQL database. When MySQL server used as database engine is down some queries raise Exception, because they try to get tables from disabled server, while it's unnecessary. For example, query SELECT ... FROM system.parts should work only with MergeTree tables and don't touch MySQL database at all. #16032 (Kruglov Pavel).Now exception will be thrown when ALTER MODIFY COLUMN ... DEFAULT ... has incompatible default with column type. Fixes #15854. #15858 (alesapin).Fixed IPv4CIDRToRange/IPv6CIDRToRange functions to accept const IP-column values. #15856 (vladimir-golovchenko). Improvement​ Treat INTERVAL '1 hour' as equivalent to INTERVAL 1 HOUR, to be compatible with Postgres and similar. This fixes #15637. #15978 (flynn).Enable parsing enum values by their numeric ids for CSV, TSV and JSON input formats. #15685 (vivarum).Better read task scheduling for JBOD architecture and MergeTree storage. New setting read_backoff_min_concurrency which serves as the lower limit to the number of reading threads. #16423 (Amos Bird).Add missing support for LowCardinality in Avro format. #16521 (Mike).Workaround for use S3 with nginx server as proxy. Nginx currenty does not accept urls with empty path like http://domain.com?delete, but vanilla aws-sdk-cpp produces this kind of urls. This commit uses patched aws-sdk-cpp version, which makes urls with &quot;/&quot; as path in this cases, like http://domain.com/?delete. #16814 (ianton-ru).Better diagnostics on parse errors in input data. Provide row number on Cannot read all data errors. #16644 (alexey-milovidov).Make the behaviour of minMap and maxMap more desireable. It will not skip zero values in the result. Fixes #16087. #16631 (Ildus Kurbangaliev).Better update of ZooKeeper configuration in runtime. #16630 (sundyli).Apply SETTINGS clause as early as possible. It allows to modify more settings in the query. This closes #3178. #16619 (alexey-milovidov).Now event_time_microseconds field stores in Decimal64, not UInt64. #16617 (Nikita Mikhaylov).Now paratmeterized functions can be used in APPLY column transformer. #16589 (Amos Bird).Improve scheduling of background task which removes data of dropped tables in Atomic databases. Atomic databases do not create broken symlink to table data directory if table actually has no data directory. #16584 (tavplubix).Subqueries in WITH section (CTE) can reference previous subqueries in WITH section by their name. #16575 (Amos Bird).Add current_database into system.query_thread_log. #16558 (Azat Khuzhin).Allow to fetch parts that are already committed or outdated in the current instance into the detached directory. It's useful when migrating tables from another cluster and having N to 1 shards mapping. It's also consistent with the current fetchPartition implementation. #16538 (Amos Bird).Multiple improvements for RabbitMQ: Fixed bug for #16263. Also minimized event loop lifetime. Added more efficient queues setup. #16426 (Kseniia Sumarokova).Fix debug assertion in quantileDeterministic function. In previous version it may also transfer up to two times more data over the network. Although no bug existed. This fixes #15683. #16410 (alexey-milovidov).Add TablesToDropQueueSize metric. It's equal to number of dropped tables, that are waiting for background data removal. #16364 (tavplubix).Better diagnostics when client has dropped connection. In previous versions, Attempt to read after EOF and Broken pipe exceptions were logged in server. In new version, it's information message Client has dropped the connection, cancel the query.. #16329 (alexey-milovidov).Add total_rows/total_bytes (from system.tables) support for Set/Join table engines. #16306 (Azat Khuzhin).Now it's possible to specify PRIMARY KEY without ORDER BY for MergeTree table engines family. Closes #15591. #16284 (alesapin).If there is no tmp folder in the system (chroot, misconfigutation etc) clickhouse-local will create temporary subfolder in the current directory. #16280 (filimonov).Add support for nested data types (like named tuple) as sub-types. Fixes #15587. #16262 (Ivan).Support for database_atomic_wait_for_drop_and_detach_synchronously/NO DELAY/SYNC for DROP DATABASE. #16127 (Azat Khuzhin).Add allow_nondeterministic_optimize_skip_unused_shards (to allow non deterministic like rand() or dictGet() in sharding key). #16105 (Azat Khuzhin).Fix memory_profiler_step/max_untracked_memory for queries via HTTP (test included). Fix the issue that adjusting this value globally in xml config does not help either, since those settings are not applied anyway, only default (4MB) value is used. Fix query_id for the most root ThreadStatus of the http query (by initializing QueryScope after reading query_id). #16101 (Azat Khuzhin).Now it's allowed to execute ALTER ... ON CLUSTER queries regardless of the &lt;internal_replication&gt; setting in cluster config. #16075 (alesapin).Fix rare issue when clickhouse-client may abort on exit due to loading of suggestions. This fixes #16035. #16047 (alexey-milovidov).Add support of cache layout for Redis dictionaries with complex key. #15985 (Anton Popov).Fix query hang (endless loop) in case of misconfiguration (connections_with_failover_max_tries set to 0). #15876 (Azat Khuzhin).Change level of some log messages from information to debug, so information messages will not appear for every query. This closes #5293. #15816 (alexey-milovidov).Remove MemoryTrackingInBackground* metrics to avoid potentially misleading results. This fixes #15684. #15813 (alexey-milovidov).Add reconnects to zookeeper-dump-tree tool. #15711 (alexey-milovidov).Allow explicitly specify columns list in CREATE TABLE table AS table_function(...) query. Fixes #9249 Fixes #14214. #14295 (tavplubix). Performance Improvement​ Do not merge parts across partitions in SELECT FINAL. #15938 (Kruglov Pavel).Improve performance of -OrNull and -OrDefault aggregate functions. #16661 (alexey-milovidov).Improve performance of quantileMerge. In previous versions it was obnoxiously slow. This closes #1463. #16643 (alexey-milovidov).Improve performance of logical functions a little. #16347 (alexey-milovidov).Improved performance of merges assignment in MergeTree table engines. Shouldn't be visible for the user. #16191 (alesapin).Speedup hashed/sparse_hashed dictionary loading by preallocating the hash table. #15454 (Azat Khuzhin).Now trivial count optimization becomes slightly non-trivial. Predicates that contain exact partition expr can be optimized too. This also fixes #11092 which returns wrong count when max_parallel_replicas &gt; 1. #15074 (Amos Bird). Build/Testing/Packaging Improvement​ Add flaky check for stateless tests. It will detect potentially flaky functional tests in advance, before they are merged. #16238 (alesapin).Use proper version for croaring instead of amalgamation. #16285 (sundyli).Improve generation of build files for ya.make build system (Arcadia). #16700 (alexey-milovidov).Add MySQL BinLog file check tool for MaterializeMySQL database engine. MaterializeMySQL is an experimental feature. #16223 (Winter Zhang).Check for executable bit on non-executable files. People often accidentially commit executable files from Windows. #15843 (alexey-milovidov).Check for #pragma once in headers. #15818 (alexey-milovidov).Fix illegal code style &amp;vector[idx] in libhdfs3. This fixes libcxx debug build. See also https://github.com/ClickHouse-Extras/libhdfs3/pull/8 . #15815 (Amos Bird).Fix build of one miscellaneous example tool on Mac OS. Note that we don't build examples on Mac OS in our CI (we build only ClickHouse binary), so there is zero chance it will not break again. This fixes #15804. #15808 (alexey-milovidov).Simplify Sys/V init script. #14135 (alexey-milovidov).Added boost::program_options to db_generator in order to increase its usability. This closes #15940. #15973 (Nikita Mikhaylov). "},{"title":"ClickHouse release 20.10​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-2010","content":""},{"title":"ClickHouse release v20.10.7.4-stable, 2020-12-24​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v201074-stable-2020-12-24","content":"Bug Fix​ Fixed issue when clickhouse-odbc-bridge process is unreachable by server on machines with dual IPv4/IPv6 stack and fixed issue when ODBC dictionary updates are performed using malformed queries and/or cause crashes. This possibly closes #14489. #18278 (Denis Glazachev).Fix key comparison between Enum and Int types. This fixes #17989. #18214 (Amos Bird).Fixed unique key convert crash in MaterializeMySQL database engine. This fixes #18186 and fixes #16372 #18211 (Winter Zhang).Fixed std::out_of_range: basic_string in S3 URL parsing. #18059 (Vladimir Chebotarev).Fixed the issue when some tables not synchronized to ClickHouse from MySQL caused by the fact that convertion MySQL prefix index wasn't supported for MaterializeMySQL. This fixes #15187 and fixes #17912 #17944 (Winter Zhang).Fix possible segfault in topK aggregate function. This closes #17404. #17845 (Maksim Kita).Do not restore parts from WAL if in_memory_parts_enable_wal is disabled. #17802 (detailyang).Fixed problem when ClickHouse fails to resume connection to MySQL servers. #17681 (Alexander Kazakov).Fixed empty system.stack_trace table when server is running in daemon mode. #17630 (Amos Bird).Fixed the behaviour when clickhouse-client is used in interactive mode with multiline queries and single line comment was erronously extended till the end of query. This fixes #13654. #17565 (alexey-milovidov).Fixed the issue when server can stop accepting connections in very rare cases. #17542 (alexey-milovidov).Fixed ALTER query hang when the corresponding mutation was killed on the different replica. This fixes #16953. #17499 (alesapin).Fixed bug when mark cache size was underestimated by clickhouse. It may happen when there are a lot of tiny files with marks. #17496 (alesapin).Fixed ORDER BY with enabled setting optimize_redundant_functions_in_order_by. #17471 (Anton Popov).Fixed duplicates after DISTINCT which were possible because of incorrect optimization. Fixes #17294. #17296 (li chengxiang). #17439 (Nikolai Kochetov).Fixed crash while reading from JOIN table with LowCardinality types. This fixes #17228. #17397 (Nikolai Kochetov).Fixed set index invalidation when there are const columns in the subquery. This fixes #17246 . #17249 (Amos Bird).Fixed ColumnConst comparison which leads to crash. This fixed #17088 . #17135 (Amos Bird).Fixed bug when ON CLUSTER queries may hang forever for non-leader ReplicatedMergeTreeTables. #17089 (alesapin).Fixed fuzzer-found bug in function fuzzBits. This fixes #16980. #17051 (hexiaoting).Avoid unnecessary network errors for remote queries which may be cancelled while execution, like queries with LIMIT. #17006 (Azat Khuzhin).Fixed wrong result in big integers (128, 256 bit) when casting from double. #16986 (Mike).Reresolve the IP of the format_avro_schema_registry_url in case of errors. #16985 (filimonov).Fixed possible server crash after ALTER TABLE ... MODIFY COLUMN ... NewType when SELECT have WHERE expression on altering column and alter does not finished yet. #16968 (Amos Bird).Blame info was not calculated correctly in clickhouse-git-import. #16959 (alexey-milovidov).Fixed order by optimization with monotonous functions. This fixes #16107. #16956 (Anton Popov).Fixrf optimization of group by with enabled setting optimize_aggregators_of_group_by_keys and joins. This fixes #12604. #16951 (Anton Popov).Install script should always create subdirs in config folders. This is only relevant for Docker build with custom config. #16936 (filimonov).Fixrf possible error Illegal type of argument for queries with ORDER BY. This fixes #16580. #16928 (Nikolai Kochetov).Abort multipart upload if no data was written to WriteBufferFromS3. #16840 (Pavel Kovalenko).Fixed crash when using any without any arguments. This fixes #16803. #16826 (Amos Bird).Fixed the behaviour when ClickHouse used to always return 0 insted of a number of affected rows for INSERT queries via MySQL protocol. This fixes #16605. #16715 (Winter Zhang).Fixed uncontrolled growth of TDigest. #16680 (hrissan).Fixed remote query failure when using suffix if in Aggregate function. This fixes #16574 fixes #16231 #16610 (Winter Zhang). "},{"title":"ClickHouse release v20.10.4.1-stable, 2020-11-13​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v201041-stable-2020-11-13","content":"Bug Fix​ Fix rare silent crashes when query profiler is on and ClickHouse is installed on OS with glibc version that has (supposedly) broken asynchronous unwind tables for some functions. This fixes #15301. This fixes #13098. #16846 (alexey-milovidov).Fix IN operator over several columns and tuples with enabled transform_null_in setting. Fixes #15310. #16722 (Anton Popov).This will fix optimize_read_in_order/optimize_aggregation_in_order with max_threads&gt;0 and expression in ORDER BY. #16637 (Azat Khuzhin).Now when parsing AVRO from input the LowCardinality is removed from type. Fixes #16188. #16521 (Mike).Fix rapid growth of metadata when using MySQL Master -&gt; MySQL Slave -&gt; ClickHouse MaterializeMySQL Engine, and slave_parallel_worker enabled on MySQL Slave, by properly shrinking GTID sets. This fixes #15951. #16504 (TCeason).Fix DROP TABLE for Distributed (racy with INSERT). #16409 (Azat Khuzhin).Fix processing of very large entries in replication queue. Very large entries may appear in ALTER queries if table structure is extremely large (near 1 MB). This fixes #16307. #16332 (alexey-milovidov).Fix bug with MySQL database. When MySQL server used as database engine is down some queries raise Exception, because they try to get tables from disabled server, while it's unnecessary. For example, query SELECT ... FROM system.parts should work only with MergeTree tables and don't touch MySQL database at all. #16032 (Kruglov Pavel). Improvement​ Workaround for use S3 with nginx server as proxy. Nginx currenty does not accept urls with empty path like http://domain.com?delete, but vanilla aws-sdk-cpp produces this kind of urls. This commit uses patched aws-sdk-cpp version, which makes urls with &quot;/&quot; as path in this cases, like http://domain.com/?delete. #16813 (ianton-ru). "},{"title":"ClickHouse release v20.10.3.30, 2020-10-28​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v2010330-2020-10-28","content":"Backward Incompatible Change​ Make multiple_joins_rewriter_version obsolete. Remove first version of joins rewriter. #15472 (Artem Zuikov).Change default value of format_regexp_escaping_rule setting (it's related to Regexp format) to Raw (it means - read whole subpattern as a value) to make the behaviour more like to what users expect. #15426 (alexey-milovidov).Add support for nested multiline comments /* comment /* comment */ */ in SQL. This conforms to the SQL standard. #14655 (alexey-milovidov).Added MergeTree settings (max_replicated_merges_with_ttl_in_queue and max_number_of_merges_with_ttl_in_pool) to control the number of merges with TTL in the background pool and replicated queue. This change breaks compatibility with older versions only if you use delete TTL. Otherwise, replication will stay compatible. You can avoid incompatibility issues if you update all shard replicas at once or execute SYSTEM STOP TTL MERGES until you finish the update of all replicas. If you'll get an incompatible entry in the replication queue, first of all, execute SYSTEM STOP TTL MERGES and after ALTER TABLE ... DETACH PARTITION ... the partition where incompatible TTL merge was assigned. Attach it back on a single replica. #14490 (alesapin).When upgrading from versions older than 20.5, if rolling update is performed and cluster contains both versions 20.5 or greater and less than 20.5, if ClickHouse nodes with old versions are restarted and old version has been started up in presence of newer versions, it may lead to Part ... intersects previous part errors. To prevent this error, first install newer clickhouse-server packages on all cluster nodes and then do restarts (so, when clickhouse-server is restarted, it will start up with the new version). New Feature​ Background data recompression. Add the ability to specify TTL ... RECOMPRESS codec_name for MergeTree table engines family. #14494 (alesapin).Add parallel quorum inserts. This closes #15601. #15601 (Latysheva Alexandra).Settings for additional enforcement of data durability. Useful for non-replicated setups. #11948 (Anton Popov).When duplicate block is written to replica where it does not exist locally (has not been fetched from replicas), don't ignore it and write locally to achieve the same effect as if it was successfully replicated. #11684 (alexey-milovidov).Now we support WITH &lt;identifier&gt; AS (subquery) ... to introduce named subqueries in the query context. This closes #2416. This closes #4967. #14771 (Amos Bird).Introduce enable_global_with_statement setting which propagates the first select's WITH statements to other select queries at the same level, and makes aliases in WITH statements visible to subqueries. #15451 (Amos Bird).Secure inter-cluster query execution (with initial_user as current query user). #13156 (Azat Khuzhin). #15551 (Azat Khuzhin).Add the ability to remove column properties and table TTLs. Introduced queries ALTER TABLE MODIFY COLUMN col_name REMOVE what_to_remove and ALTER TABLE REMOVE TTL. Both operations are lightweight and executed at the metadata level. #14742 (alesapin).Added format RawBLOB. It is intended for input or output a single value without any escaping and delimiters. This closes #15349. #15364 (alexey-milovidov).Add the reinterpretAsUUID function that allows to convert a big-endian byte string to UUID. #15480 (Alexander Kuzmenkov).Implement force_data_skipping_indices setting. #15642 (Azat Khuzhin).Add a setting output_format_pretty_row_numbers to numerate the result in Pretty formats. This closes #15350. #15443 (flynn).Added query obfuscation tool. It allows to share more queries for better testing. This closes #15268. #15321 (alexey-milovidov).Add table function null('structure'). #14797 (vxider).Added formatReadableQuantity function. It is useful for reading big numbers by human. #14725 (Artem Hnilov).Add format LineAsString that accepts a sequence of lines separated by newlines, every line is parsed as a whole as a single String field. #14703 (Nikita Mikhaylov), #13846 (hexiaoting).Add JSONStrings format which output data in arrays of strings. #14333 (hcz).Add support for &quot;Raw&quot; column format for Regexp format. It allows to simply extract subpatterns as a whole without any escaping rules. #15363 (alexey-milovidov).Allow configurable NULL representation for TSV output format. It is controlled by the setting output_format_tsv_null_representation which is \\N by default. This closes #9375. Note that the setting only controls output format and \\N is the only supported NULL representation for TSV input format. #14586 (Kruglov Pavel).Support Decimal data type for MaterializeMySQL. MaterializeMySQL is an experimental feature. #14535 (Winter Zhang).Add new feature: SHOW DATABASES LIKE 'xxx'. #14521 (hexiaoting).Added a script to import (arbitrary) git repository to ClickHouse as a sample dataset. #14471 (alexey-milovidov).Now insert statements can have asterisk (or variants) with column transformers in the column list. #14453 (Amos Bird).New query complexity limit settings max_rows_to_read_leaf, max_bytes_to_read_leaf for distributed queries to limit max rows/bytes read on the leaf nodes. Limit is applied for local reads only, excluding the final merge stage on the root node. #14221 (Roman Khavronenko).Allow user to specify settings for ReplicatedMergeTree* storage in &lt;replicated_merge_tree&gt; section of config file. It works similarly to &lt;merge_tree&gt; section. For ReplicatedMergeTree* storages settings from &lt;merge_tree&gt; and &lt;replicated_merge_tree&gt; are applied together, but settings from &lt;replicated_merge_tree&gt; has higher priority. Added system.replicated_merge_tree_settings table. #13573 (Amos Bird).Add mapPopulateSeries function. #13166 (Ildus Kurbangaliev).Supporting MySQL types: decimal (as ClickHouse Decimal) and datetime with sub-second precision (as DateTime64). #11512 (Vasily Nemkov).Introduce event_time_microseconds field to system.text_log, system.trace_log, system.query_log and system.query_thread_log tables. #14760 (Bharat Nallan).Add event_time_microseconds to system.asynchronous_metric_log &amp; system.metric_log tables. #14514 (Bharat Nallan).Add query_start_time_microseconds field to system.query_log &amp; system.query_thread_log tables. #14252 (Bharat Nallan). Bug Fix​ Fix the case when memory can be overallocated regardless to the limit. This closes #14560. #16206 (alexey-milovidov).Fix executable dictionary source hang. In previous versions, when using some formats (e.g. JSONEachRow) data was not feed to a child process before it outputs at least something. This closes #1697. This closes #2455. #14525 (alexey-milovidov).Fix double free in case of exception in function dictGet. It could have happened if dictionary was loaded with error. #16429 (Nikolai Kochetov).Fix group by with totals/rollup/cube modifers and min/max functions over group by keys. Fixes #16393. #16397 (Anton Popov).Fix async Distributed INSERT with prefer_localhost_replica=0 and internal_replication. #16358 (Azat Khuzhin).Fix a very wrong code in TwoLevelStringHashTable implementation, which might lead to memory leak. #16264 (Amos Bird).Fix segfault in some cases of wrong aggregation in lambdas. #16082 (Anton Popov).Fix ALTER MODIFY ... ORDER BY query hang for ReplicatedVersionedCollapsingMergeTree. This fixes #15980. #16011 (alesapin).MaterializeMySQL (experimental feature): Fix collate name &amp; charset name parser and support length = 0 for string type. #16008 (Winter Zhang).Allow to use direct layout for dictionaries with complex keys. #16007 (Anton Popov).Prevent replica hang for 5-10 mins when replication error happens after a period of inactivity. #15987 (filimonov).Fix rare segfaults when inserting into or selecting from MaterializedView and concurrently dropping target table (for Atomic database engine). #15984 (tavplubix).Fix ambiguity in parsing of settings profiles: CREATE USER ... SETTINGS profile readonly is now considered as using a profile named readonly, not a setting named profile with the readonly constraint. This fixes #15628. #15982 (Vitaly Baranov).MaterializeMySQL (experimental feature): Fix crash on create database failure. #15954 (Winter Zhang).Fixed DROP TABLE IF EXISTS failure with Table ... does not exist error when table is concurrently renamed (for Atomic database engine). Fixed rare deadlock when concurrently executing some DDL queries with multiple tables (like DROP DATABASE and RENAME TABLE) - Fixed DROP/DETACH DATABASE failure with Table ... does not exist when concurrently executing DROP/DETACH TABLE. #15934 (tavplubix).Fix incorrect empty result for query from Distributed table if query has WHERE, PREWHERE and GLOBAL IN. Fixes #15792. #15933 (Nikolai Kochetov).Fixes #12513: difference expressions with same alias when query is reanalyzed. #15886 (Winter Zhang).Fix possible very rare deadlocks in RBAC implementation. #15875 (Vitaly Baranov).Fix exception Block structure mismatch in SELECT ... ORDER BY DESC queries which were executed after ALTER MODIFY COLUMN query. Fixes #15800. #15852 (alesapin).MaterializeMySQL (experimental feature): Fix select count() inaccuracy. #15767 (tavplubix).Fix some cases of queries, in which only virtual columns are selected. Previously Not found column _nothing in block exception may be thrown. Fixes #12298. #15756 (Anton Popov).Fix drop of materialized view with inner table in Atomic database (hangs all subsequent DROP TABLE due to hang of the worker thread, due to recursive DROP TABLE for inner table of MV). #15743 (Azat Khuzhin).Possibility to move part to another disk/volume if the first attempt was failed. #15723 (Pavel Kovalenko).Fix error Cannot find column which may happen at insertion into MATERIALIZED VIEW in case if query for MV containes ARRAY JOIN. #15717 (Nikolai Kochetov).Fixed too low default value of max_replicated_logs_to_keep setting, which might cause replicas to become lost too often. Improve lost replica recovery process by choosing the most up-to-date replica to clone. Also do not remove old parts from lost replica, detach them instead. #15701 (tavplubix).Fix rare race condition in dictionaries and tables from MySQL. #15686 (alesapin).Fix (benign) race condition in AMQP-CPP. #15667 (alesapin).Fix error Cannot add simple transform to empty Pipe which happened while reading from Buffer table which has different structure than destination table. It was possible if destination table returned empty result for query. Fixes #15529. #15662 (Nikolai Kochetov).Proper error handling during insert into MergeTree with S3. MergeTree over S3 is an experimental feature. #15657 (Pavel Kovalenko).Fixed bug with S3 table function: region from URL was not applied to S3 client configuration. #15646 (Vladimir Chebotarev).Fix the order of destruction for resources in ReadFromStorage step of query plan. It might cause crashes in rare cases. Possibly connected with #15610. #15645 (Nikolai Kochetov).Subtract ReadonlyReplica metric when detach readonly tables. #15592 (sundyli).Fixed Element ... is not a constant expression error when using JSON* function result in VALUES, LIMIT or right side of IN operator. #15589 (tavplubix).Query will finish faster in case of exception. Cancel execution on remote replicas if exception happens. #15578 (Azat Khuzhin).Prevent the possibility of error message Could not calculate available disk space (statvfs), errno: 4, strerror: Interrupted system call. This fixes #15541. #15557 (alexey-milovidov).Fix Database &lt;db&gt; does not exist. in queries with IN and Distributed table when there's no database on initiator. #15538 (Artem Zuikov).Mutation might hang waiting for some non-existent part after MOVE or REPLACE PARTITION or, in rare cases, after DETACH or DROP PARTITION. It's fixed. #15537 (tavplubix).Fix bug when ILIKE operator stops being case insensitive if LIKE with the same pattern was executed. #15536 (alesapin).Fix Missing columns errors when selecting columns which absent in data, but depend on other columns which also absent in data. Fixes #15530. #15532 (alesapin).Throw an error when a single parameter is passed to ReplicatedMergeTree instead of ignoring it. #15516 (nvartolomei).Fix bug with event subscription in DDLWorker which rarely may lead to query hangs in ON CLUSTER. Introduced in #13450. #15477 (alesapin).Report proper error when the second argument of boundingRatio aggregate function has a wrong type. #15407 (detailyang).Fixes #15365: attach a database with MySQL engine throws exception (no query context). #15384 (Winter Zhang).Fix the case of multiple occurrences of column transformers in a select query. #15378 (Amos Bird).Fixed compression in S3 storage. #15376 (Vladimir Chebotarev).Fix bug where queries like SELECT toStartOfDay(today()) fail complaining about empty time_zone argument. #15319 (Bharat Nallan).Fix race condition during MergeTree table rename and background cleanup. #15304 (alesapin).Fix rare race condition on server startup when system logs are enabled. #15300 (alesapin).Fix hang of queries with a lot of subqueries to same table of MySQL engine. Previously, if there were more than 16 subqueries to same MySQL table in query, it hang forever. #15299 (Anton Popov).Fix MSan report in QueryLog. Uninitialized memory can be used for the field memory_usage. #15258 (alexey-milovidov).Fix 'Unknown identifier' in GROUP BY when query has JOIN over Merge table. #15242 (Artem Zuikov).Fix instance crash when using joinGet with LowCardinality types. This fixes #15214. #15220 (Amos Bird).Fix bug in table engine Buffer which does not allow to insert data of new structure into Buffer after ALTER query. Fixes #15117. #15192 (alesapin).Adjust Decimal field size in MySQL column definition packet. #15152 (maqroll).Fixes Data compressed with different methods in join_algorithm='auto'. Keep LowCardinality as type for left table join key in join_algorithm='partial_merge'. #15088 (Artem Zuikov).Update jemalloc to fix percpu_arena with affinity mask. #15035 (Azat Khuzhin). #14957 (Azat Khuzhin).We already use padded comparison between String and FixedString (https://github.com/ClickHouse/ClickHouse/blob/master/src/Functions/FunctionsComparison.h#L333). This PR applies the same logic to field comparison which corrects the usage of FixedString as primary keys. This fixes #14908. #15033 (Amos Bird).If function bar was called with specifically crafted arguments, buffer overflow was possible. This closes #13926. #15028 (alexey-milovidov).Fixed Cannot rename ... errno: 22, strerror: Invalid argument error on DDL query execution in Atomic database when running clickhouse-server in Docker on Mac OS. #15024 (tavplubix).Fix crash in RIGHT or FULL JOIN with join_algorith='auto' when memory limit exceeded and we should change HashJoin with MergeJoin. #15002 (Artem Zuikov).Now settings number_of_free_entries_in_pool_to_execute_mutation and number_of_free_entries_in_pool_to_lower_max_size_of_merge can be equal to background_pool_size. #14975 (alesapin).Fix to make predicate push down work when subquery contains finalizeAggregation function. Fixes #14847. #14937 (filimonov).Publish CPU frequencies per logical core in system.asynchronous_metrics. This fixes #14923. #14924 (Alexander Kuzmenkov).MaterializeMySQL (experimental feature): Fixed .metadata.tmp File exists error. #14898 (Winter Zhang).Fix the issue when some invocations of extractAllGroups function may trigger &quot;Memory limit exceeded&quot; error. This fixes #13383. #14889 (alexey-milovidov).Fix SIGSEGV for an attempt to INSERT into StorageFile with file descriptor. #14887 (Azat Khuzhin).Fixed segfault in cache dictionary #14837. #14879 (Nikita Mikhaylov).MaterializeMySQL (experimental feature): Fixed bug in parsing MySQL binlog events, which causes Attempt to read after eof and Packet payload is not fully read in MaterializeMySQL database engine. #14852 (Winter Zhang).Fix rare error in SELECT queries when the queried column has DEFAULT expression which depends on the other column which also has DEFAULT and not present in select query and not exists on disk. Partially fixes #14531. #14845 (alesapin).Fix a problem where the server may get stuck on startup while talking to ZooKeeper, if the configuration files have to be fetched from ZK (using the from_zk include option). This fixes #14814. #14843 (Alexander Kuzmenkov).Fix wrong monotonicity detection for shrunk Int -&gt; Int cast of signed types. It might lead to incorrect query result. This bug is unveiled in #14513. #14783 (Amos Bird).Replace column transformer should replace identifiers with cloned ASTs. This fixes #14695 . #14734 (Amos Bird).Fixed missed default database name in metadata of materialized view when executing ALTER ... MODIFY QUERY. #14664 (tavplubix).Fix bug when ALTER UPDATE mutation with Nullable column in assignment expression and constant value (like UPDATE x = 42) leads to incorrect value in column or segfault. Fixes #13634, #14045. #14646 (alesapin).Fix wrong Decimal multiplication result caused wrong decimal scale of result column. #14603 (Artem Zuikov).Fix function has with LowCardinality of Nullable. #14591 (Mike).Cleanup data directory after Zookeeper exceptions during CreateQuery for StorageReplicatedMergeTree Engine. #14563 (Bharat Nallan).Fix rare segfaults in functions with combinator -Resample, which could appear in result of overflow with very large parameters. #14562 (Anton Popov).Fix a bug when converting Nullable(String) to Enum. Introduced by #12745. This fixes #14435. #14530 (Amos Bird).Fixed the incorrect sorting order of Nullable column. This fixes #14344. #14495 (Nikita Mikhaylov).Fix currentDatabase() function cannot be used in ON CLUSTER ddl query. #14211 (Winter Zhang).MaterializeMySQL (experimental feature): Fixed Packet payload is not fully read error in MaterializeMySQL database engine. #14696 (BohuTANG). Improvement​ Enable Atomic database engine by default for newly created databases. #15003 (tavplubix).Add the ability to specify specialized codecs like Delta, T64, etc. for columns with subtypes. Implements #12551, fixes #11397, fixes #4609. #15089 (alesapin).Dynamic reload of zookeeper config. #14678 (sundyli).Now it's allowed to execute ALTER ... ON CLUSTER queries regardless of the &lt;internal_replication&gt; setting in cluster config. #16075 (alesapin).Now joinGet supports multi-key lookup. Continuation of #12418. #13015 (Amos Bird).Wait for DROP/DETACH TABLE to actually finish if NO DELAY or SYNC is specified for Atomic database. #15448 (tavplubix).Now it's possible to change the type of version column for VersionedCollapsingMergeTree with ALTER query. #15442 (alesapin).Unfold {database}, {table} and {uuid} macros in zookeeper_path on replicated table creation. Do not allow RENAME TABLE if it may break zookeeper_path after server restart. Fixes #6917. #15348 (tavplubix).The function now allows an argument with timezone. This closes 15264. #15285 (flynn).Do not allow connections to ClickHouse server until all scripts in /docker-entrypoint-initdb.d/ are executed. #15244 (Aleksei Kozharin).Added optimize setting to EXPLAIN PLAN query. If enabled, query plan level optimisations are applied. Enabled by default. #15201 (Nikolai Kochetov).Proper exception message for wrong number of arguments of CAST. This closes #13992. #15029 (alexey-milovidov).Add option to disable TTL move on data part insert. #15000 (Pavel Kovalenko).Ignore key constraints when doing mutations. Without this pull request, it's not possible to do mutations when force_index_by_date = 1 or force_primary_key = 1. #14973 (Amos Bird).Allow to drop Replicated table if previous drop attempt was failed due to ZooKeeper session expiration. This fixes #11891. #14926 (alexey-milovidov).Fixed excessive settings constraint violation when running SELECT with SETTINGS from a distributed table. #14876 (Amos Bird).Provide a load_balancing_first_offset query setting to explicitly state what the first replica is. It's used together with FIRST_OR_RANDOM load balancing strategy, which allows to control replicas workload. #14867 (Amos Bird).Show subqueries for SET and JOIN in EXPLAIN result. #14856 (Nikolai Kochetov).Allow using multi-volume storage configuration in storage Distributed. #14839 (Pavel Kovalenko).Construct query_start_time and query_start_time_microseconds from the same timespec. #14831 (Bharat Nallan).Support for disabling persistency for StorageJoin and StorageSet, this feature is controlled by setting disable_set_and_join_persistency. And this PR solved issue #6318. #14776 (vxider).Now COLUMNS can be used to wrap over a list of columns and apply column transformers afterwards. #14775 (Amos Bird).Add merge_algorithm to system.merges table to improve merging inspections. #14705 (Amos Bird).Fix potential memory leak caused by zookeeper exists watch. #14693 (hustnn).Allow parallel execution of distributed DDL. #14684 (Azat Khuzhin).Add QueryMemoryLimitExceeded event counter. This closes #14589. #14647 (fastio).Fix some trailing whitespaces in query formatting. #14595 (Azat Khuzhin).ClickHouse treats partition expr and key expr differently. Partition expr is used to construct an minmax index containing related columns, while primary key expr is stored as an expr. Sometimes user might partition a table at coarser levels, such as partition by i / 1000. However, binary operators are not monotonic and this PR tries to fix that. It might also benifit other use cases. #14513 (Amos Bird).Add an option to skip access checks for DiskS3. s3 disk is an experimental feature. #14497 (Pavel Kovalenko).Speed up server shutdown process if there are ongoing S3 requests. #14496 (Pavel Kovalenko).SYSTEM RELOAD CONFIG now throws an exception if failed to reload and continues using the previous users.xml. The background periodic reloading also continues using the previous users.xml if failed to reload. #14492 (Vitaly Baranov).For INSERTs with inline data in VALUES format in the script mode of clickhouse-client, support semicolon as the data terminator, in addition to the new line. Closes #12288. #13192 (Alexander Kuzmenkov).Support custom codecs in compact parts. #12183 (Anton Popov). Performance Improvement​ Enable compact parts by default for small parts. This will allow to process frequent inserts slightly more efficiently (4..100 times). #11913 (alexey-milovidov).Improve quantileTDigest performance. This fixes #2668. #15542 (Kruglov Pavel).Significantly reduce memory usage in AggregatingInOrderTransform/optimize_aggregation_in_order. #15543 (Azat Khuzhin).Faster 256-bit multiplication. #15418 (Artem Zuikov).Improve performance of 256-bit types using (u)int64_t as base type for wide integers. Original wide integers use 8-bit types as base. #14859 (Artem Zuikov).Explicitly use a temporary disk to store vertical merge temporary data. #15639 (Grigory Pervakov).Use one S3 DeleteObjects request instead of multiple DeleteObject in a loop. No any functionality changes, so covered by existing tests like integration/test_log_family_s3. #15238 (ianton-ru).Fix DateTime &lt;op&gt; DateTime mistakenly choosing the slow generic implementation. This fixes #15153. #15178 (Amos Bird).Improve performance of GROUP BY key of type FixedString. #15034 (Amos Bird).Only mlock code segment when starting clickhouse-server. In previous versions, all mapped regions were locked in memory, including debug info. Debug info is usually splitted to a separate file but if it isn't, it led to +2..3 GiB memory usage. #14929 (alexey-milovidov).ClickHouse binary become smaller due to link time optimization. Build/Testing/Packaging Improvement​ Now we use clang-11 for production ClickHouse build. #15239 (alesapin).Now we use clang-11 to build ClickHouse in CI. #14846 (alesapin).Switch binary builds (Linux, Darwin, AArch64, FreeDSD) to clang-11. #15622 (Ilya Yatsishin).Now all test images use llvm-symbolizer-11. #15069 (alesapin).Allow to build with llvm-11. #15366 (alexey-milovidov).Switch from clang-tidy-10 to clang-tidy-11. #14922 (alexey-milovidov).Use LLVM's experimental pass manager by default. #15608 (Danila Kutenin).Don't allow any C++ translation unit to build more than 10 minutes or to use more than 10 GB or memory. This fixes #14925. #15060 (alexey-milovidov).Make performance test more stable and representative by splitting test runs and profile runs. #15027 (alexey-milovidov).Attempt to make performance test more reliable. It is done by remapping the executable memory of the process on the fly with madvise to use transparent huge pages - it can lower the number of iTLB misses which is the main source of instabilities in performance tests. #14685 (alexey-milovidov).Convert to python3. This closes #14886. #15007 (Azat Khuzhin).Fail early in functional tests if server failed to respond. This closes #15262. #15267 (alexey-milovidov).Allow to run AArch64 version of clickhouse-server without configs. This facilitates #15174. #15266 (alexey-milovidov).Improvements in CI docker images: get rid of ZooKeeper and single script for test configs installation. #15215 (alesapin).Fix CMake options forwarding in fast test script. Fixes error in #14711. #15155 (alesapin).Added a script to perform hardware benchmark in a single command. #15115 (alexey-milovidov).Splitted huge test test_dictionaries_all_layouts_and_sources into smaller ones. #15110 (Nikita Mikhaylov).Maybe fix MSan report in base64 (on servers with AVX-512). This fixes #14006. #15030 (alexey-milovidov).Reformat and cleanup code in all integration test *.py files. #14864 (Bharat Nallan).Fix MaterializeMySQL empty transaction unstable test case found in CI. #14854 (Winter Zhang).Attempt to speed up build a little. #14808 (alexey-milovidov).Speed up build a little by removing unused headers. #14714 (alexey-milovidov).Fix build failure in OSX. #14761 (Winter Zhang).Enable ccache by default in cmake if it's found in OS. #14575 (alesapin).Control CI builds configuration from the ClickHouse repository. #14547 (alesapin).In CMake files: - Moved some options' descriptions' parts to comments above. - Replace 0 -&gt; OFF, 1 -&gt; ON in options default values. - Added some descriptions and links to docs to the options. - Replaced FUZZER option (there is another option ENABLE_FUZZING which also enables same functionality). - Removed ENABLE_GTEST_LIBRARY option as there is ENABLE_TESTS. See the full description in PR: #14711 (Mike).Make binary a bit smaller (~50 Mb for debug version). #14555 (Artem Zuikov).Use std::filesystem::path in ConfigProcessor for concatenating file paths. #14558 (Bharat Nallan).Fix debug assertion in bitShiftLeft() when called with negative big integer. #14697 (Artem Zuikov). "},{"title":"ClickHouse release 20.9​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-209","content":""},{"title":"ClickHouse release v20.9.7.11-stable, 2020-12-07​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v209711-stable-2020-12-07","content":"Performance Improvement​ Fix performance of reading from Merge tables over huge number of MergeTree tables. Fixes #7748. #16988 (Anton Popov). Bug Fix​ Do not restore parts from WAL if in_memory_parts_enable_wal is disabled. #17802 (detailyang).Fixed segfault when there is not enough space when inserting into Distributed table. #17737 (tavplubix).Fixed problem when ClickHouse fails to resume connection to MySQL servers. #17681 (Alexander Kazakov).Fixed Function not implemented error when executing RENAME query in Atomic database with ClickHouse running on Windows Subsystem for Linux. Fixes #17661. #17664 (tavplubix).When clickhouse-client is used in interactive mode with multiline queries, single line comment was erronously extended till the end of query. This fixes #13654. #17565 (alexey-milovidov).Fix the issue when server can stop accepting connections in very rare cases. #17542 (alexey-milovidov).Fix alter query hang when the corresponding mutation was killed on the different replica. Fixes #16953. #17499 (alesapin).Fix bug when mark cache size was underestimated by clickhouse. It may happen when there are a lot of tiny files with marks. #17496 (alesapin).Fix ORDER BY with enabled setting optimize_redundant_functions_in_order_by. #17471 (Anton Popov).Fix duplicates after DISTINCT which were possible because of incorrect optimization. Fixes #17294. #17296 (li chengxiang). #17439 (Nikolai Kochetov).Fix crash while reading from JOIN table with LowCardinality types. Fixes #17228. #17397 (Nikolai Kochetov).Fix set index invalidation when there are const columns in the subquery. This fixes #17246 . #17249 (Amos Bird).Fix ColumnConst comparison which leads to crash. This fixed #17088 . #17135 (Amos Bird).Fixed crash on CREATE TABLE ... AS some_table query when some_table was created AS table_function() Fixes #16944. #17072 (tavplubix).Bug fix for funciton fuzzBits, related issue: #16980. #17051 (hexiaoting).Avoid unnecessary network errors for remote queries which may be cancelled while execution, like queries with LIMIT. #17006 (Azat Khuzhin).TODO. #16866 (tavplubix).Return number of affected rows for INSERT queries via MySQL protocol. Previously ClickHouse used to always return 0, it's fixed. Fixes #16605. #16715 (Winter Zhang). Build/Testing/Packaging Improvement​ Update embedded timezone data to version 2020d (also update cctz to the latest master). #17204 (filimonov). "},{"title":"ClickHouse release v20.9.6.14-stable, 2020-11-20​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v209614-stable-2020-11-20","content":"Improvement​ Make it possible to connect to clickhouse-server secure endpoint which requires SNI. This is possible when clickhouse-server is hosted behind TLS proxy. #16938 (filimonov).Conditional aggregate functions (for example: avgIf, sumIf, maxIf) should return NULL when miss rows and use nullable arguments. #13964 (Winter Zhang). Bug Fix​ Fix bug when ON CLUSTER queries may hang forever for non-leader ReplicatedMergeTreeTables. #17089 (alesapin).Reresolve the IP of the format_avro_schema_registry_url in case of errors. #16985 (filimonov).Fix possible server crash after ALTER TABLE ... MODIFY COLUMN ... NewType when SELECT have WHERE expression on altering column and alter does not finished yet. #16968 (Amos Bird).Install script should always create subdirs in config folders. This is only relevant for Docker build with custom config. #16936 (filimonov).Fix possible error Illegal type of argument for queries with ORDER BY. Fixes #16580. #16928 (Nikolai Kochetov).Abort multipart upload if no data was written to WriteBufferFromS3. #16840 (Pavel Kovalenko).Fix crash when using any without any arguments. This is for #16803 . cc @azat. #16826 (Amos Bird).Fix IN operator over several columns and tuples with enabled transform_null_in setting. Fixes #15310. #16722 (Anton Popov).This will fix optimize_read_in_order/optimize_aggregation_in_order with max_threads&gt;0 and expression in ORDER BY. #16637 (Azat Khuzhin).fixes #16574 fixes #16231 fix remote query failure when using 'if' suffix aggregate function. #16610 (Winter Zhang).Query is finished faster in case of exception. Cancel execution on remote replicas if exception happens. #15578 (Azat Khuzhin). "},{"title":"ClickHouse release v20.9.5.5-stable, 2020-11-13​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v20955-stable-2020-11-13","content":"Bug Fix​ Fix rare silent crashes when query profiler is on and ClickHouse is installed on OS with glibc version that has (supposedly) broken asynchronous unwind tables for some functions. This fixes #15301. This fixes #13098. #16846 (alexey-milovidov).Now when parsing AVRO from input the LowCardinality is removed from type. Fixes #16188. #16521 (Mike).Fix rapid growth of metadata when using MySQL Master -&gt; MySQL Slave -&gt; ClickHouse MaterializeMySQL Engine, and slave_parallel_worker enabled on MySQL Slave, by properly shrinking GTID sets. This fixes #15951. #16504 (TCeason).Fix DROP TABLE for Distributed (racy with INSERT). #16409 (Azat Khuzhin).Fix processing of very large entries in replication queue. Very large entries may appear in ALTER queries if table structure is extremely large (near 1 MB). This fixes #16307. #16332 (alexey-milovidov).Fixed the inconsistent behaviour when a part of return data could be dropped because the set for its filtration wasn't created. #16308 (Nikita Mikhaylov).Fix bug with MySQL database. When MySQL server used as database engine is down some queries raise Exception, because they try to get tables from disabled server, while it's unnecessary. For example, query SELECT ... FROM system.parts should work only with MergeTree tables and don't touch MySQL database at all. #16032 (Kruglov Pavel). "},{"title":"ClickHouse release v20.9.4.76-stable (2020-10-29)​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v209476-stable-2020-10-29","content":"Bug Fix​ Fix double free in case of exception in function dictGet. It could have happened if dictionary was loaded with error. #16429 (Nikolai Kochetov).Fix group by with totals/rollup/cube modifers and min/max functions over group by keys. Fixes #16393. #16397 (Anton Popov).Fix async Distributed INSERT w/ prefer_localhost_replica=0 and internal_replication. #16358 (Azat Khuzhin).Fix a very wrong code in TwoLevelStringHashTable implementation, which might lead to memory leak. I'm suprised how this bug can lurk for so long.... #16264 (Amos Bird).Fix the case when memory can be overallocated regardless to the limit. This closes #14560. #16206 (alexey-milovidov).Fix ALTER MODIFY ... ORDER BY query hang for ReplicatedVersionedCollapsingMergeTree. This fixes #15980. #16011 (alesapin).Fix collate name &amp; charset name parser and support length = 0 for string type. #16008 (Winter Zhang).Allow to use direct layout for dictionaries with complex keys. #16007 (Anton Popov).Prevent replica hang for 5-10 mins when replication error happens after a period of inactivity. #15987 (filimonov).Fix rare segfaults when inserting into or selecting from MaterializedView and concurrently dropping target table (for Atomic database engine). #15984 (tavplubix).Fix ambiguity in parsing of settings profiles: CREATE USER ... SETTINGS profile readonly is now considered as using a profile named readonly, not a setting named profile with the readonly constraint. This fixes #15628. #15982 (Vitaly Baranov).Fix a crash when database creation fails. #15954 (Winter Zhang).Fixed DROP TABLE IF EXISTS failure with Table ... does not exist error when table is concurrently renamed (for Atomic database engine). Fixed rare deadlock when concurrently executing some DDL queries with multiple tables (like DROP DATABASE and RENAME TABLE) Fixed DROP/DETACH DATABASE failure with Table ... does not exist when concurrently executing DROP/DETACH TABLE. #15934 (tavplubix).Fix incorrect empty result for query from Distributed table if query has WHERE, PREWHERE and GLOBAL IN. Fixes #15792. #15933 (Nikolai Kochetov).Fix possible deadlocks in RBAC. #15875 (Vitaly Baranov).Fix exception Block structure mismatch in SELECT ... ORDER BY DESC queries which were executed after ALTER MODIFY COLUMN query. Fixes #15800. #15852 (alesapin).Fix select count() inaccuracy for MaterializeMySQL. #15767 (tavplubix).Fix some cases of queries, in which only virtual columns are selected. Previously Not found column _nothing in block exception may be thrown. Fixes #12298. #15756 (Anton Popov).Fixed too low default value of max_replicated_logs_to_keep setting, which might cause replicas to become lost too often. Improve lost replica recovery process by choosing the most up-to-date replica to clone. Also do not remove old parts from lost replica, detach them instead. #15701 (tavplubix).Fix error Cannot add simple transform to empty Pipe which happened while reading from Buffer table which has different structure than destination table. It was possible if destination table returned empty result for query. Fixes #15529. #15662 (Nikolai Kochetov).Fixed bug with globs in S3 table function, region from URL was not applied to S3 client configuration. #15646 (Vladimir Chebotarev).Decrement the ReadonlyReplica metric when detaching read-only tables. This fixes #15598. #15592 (sundyli).Throw an error when a single parameter is passed to ReplicatedMergeTree instead of ignoring it. #15516 (nvartolomei). Improvement​ Now it's allowed to execute ALTER ... ON CLUSTER queries regardless of the &lt;internal_replication&gt; setting in cluster config. #16075 (alesapin).Unfold {database}, {table} and {uuid} macros in ReplicatedMergeTree arguments on table creation. #16160 (tavplubix). "},{"title":"ClickHouse release v20.9.3.45-stable (2020-10-09)​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v209345-stable-2020-10-09","content":"Bug Fix​ Fix error Cannot find column which may happen at insertion into MATERIALIZED VIEW in case if query for MV containes ARRAY JOIN. #15717 (Nikolai Kochetov).Fix race condition in AMQP-CPP. #15667 (alesapin).Fix the order of destruction for resources in ReadFromStorage step of query plan. It might cause crashes in rare cases. Possibly connected with #15610. #15645 (Nikolai Kochetov).Fixed Element ... is not a constant expression error when using JSON* function result in VALUES, LIMIT or right side of IN operator. #15589 (tavplubix).Prevent the possibility of error message Could not calculate available disk space (statvfs), errno: 4, strerror: Interrupted system call. This fixes #15541. #15557 (alexey-milovidov).Significantly reduce memory usage in AggregatingInOrderTransform/optimize_aggregation_in_order. #15543 (Azat Khuzhin).Mutation might hang waiting for some non-existent part after MOVE or REPLACE PARTITION or, in rare cases, after DETACH or DROP PARTITION. It's fixed. #15537 (tavplubix).Fix bug when ILIKE operator stops being case insensitive if LIKE with the same pattern was executed. #15536 (alesapin).Fix Missing columns errors when selecting columns which absent in data, but depend on other columns which also absent in data. Fixes #15530. #15532 (alesapin).Fix bug with event subscription in DDLWorker which rarely may lead to query hangs in ON CLUSTER. Introduced in #13450. #15477 (alesapin).Report proper error when the second argument of boundingRatio aggregate function has a wrong type. #15407 (detailyang).Fix bug where queries like SELECT toStartOfDay(today()) fail complaining about empty time_zone argument. #15319 (Bharat Nallan).Fix race condition during MergeTree table rename and background cleanup. #15304 (alesapin).Fix rare race condition on server startup when system.logs are enabled. #15300 (alesapin).Fix MSan report in QueryLog. Uninitialized memory can be used for the field memory_usage. #15258 (alexey-milovidov).Fix instance crash when using joinGet with LowCardinality types. This fixes #15214. #15220 (Amos Bird).Fix bug in table engine Buffer which does not allow to insert data of new structure into Buffer after ALTER query. Fixes #15117. #15192 (alesapin).Adjust decimals field size in mysql column definition packet. #15152 (maqroll).Fixed Cannot rename ... errno: 22, strerror: Invalid argument error on DDL query execution in Atomic database when running clickhouse-server in docker on Mac OS. #15024 (tavplubix).Fix to make predicate push down work when subquery contains finalizeAggregation function. Fixes #14847. #14937 (filimonov).Fix a problem where the server may get stuck on startup while talking to ZooKeeper, if the configuration files have to be fetched from ZK (using the from_zk include option). This fixes #14814. #14843 (Alexander Kuzmenkov). Improvement​ Now it's possible to change the type of version column for VersionedCollapsingMergeTree with ALTER query. #15442 (alesapin). "},{"title":"ClickHouse release v20.9.2.20, 2020-09-22​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v209220-2020-09-22","content":"Backward Incompatible Change​ When upgrading from versions older than 20.5, if rolling update is performed and cluster contains both versions 20.5 or greater and less than 20.5, if ClickHouse nodes with old versions are restarted and old version has been started up in presence of newer versions, it may lead to Part ... intersects previous part errors. To prevent this error, first install newer clickhouse-server packages on all cluster nodes and then do restarts (so, when clickhouse-server is restarted, it will start up with the new version). New Feature​ Added column transformers EXCEPT, REPLACE, APPLY, which can be applied to the list of selected columns (after * or COLUMNS(...)). For example, you can write SELECT * EXCEPT(URL) REPLACE(number + 1 AS number). Another example: select * apply(length) apply(max) from wide_string_table to find out the maxium length of all string columns. #14233 (Amos Bird).Added an aggregate function rankCorr which computes a rank correlation coefficient. #11769 (antikvist) #14411 (Nikita Mikhaylov).Added table function view which turns a subquery into a table object. This helps passing queries around. For instance, it can be used in remote/cluster table functions. #12567 (Amos Bird). Bug Fix​ Fix bug when ALTER UPDATE mutation with Nullable column in assignment expression and constant value (like UPDATE x = 42) leads to incorrect value in column or segfault. Fixes #13634, #14045. #14646 (alesapin).Fix wrong Decimal multiplication result caused wrong decimal scale of result column. #14603 (Artem Zuikov).Fixed the incorrect sorting order of Nullable column. This fixes #14344. #14495 (Nikita Mikhaylov).Fixed inconsistent comparison with primary key of type FixedString on index analysis if they're compered with a string of less size. This fixes #14908. #15033 (Amos Bird).Fix bug which leads to wrong merges assignment if table has partitions with a single part. #14444 (alesapin).If function bar was called with specifically crafted arguments, buffer overflow was possible. This closes #13926. #15028 (alexey-milovidov).Publish CPU frequencies per logical core in system.asynchronous_metrics. This fixes #14923. #14924 (Alexander Kuzmenkov).Fixed .metadata.tmp File exists error when using MaterializeMySQL database engine. #14898 (Winter Zhang).Fix the issue when some invocations of extractAllGroups function may trigger &quot;Memory limit exceeded&quot; error. This fixes #13383. #14889 (alexey-milovidov).Fix SIGSEGV for an attempt to INSERT into StorageFile(fd). #14887 (Azat Khuzhin).Fix rare error in SELECT queries when the queried column has DEFAULT expression which depends on the other column which also has DEFAULT and not present in select query and not exists on disk. Partially fixes #14531. #14845 (alesapin).Fix wrong monotonicity detection for shrunk Int -&gt; Int cast of signed types. It might lead to incorrect query result. This bug is unveiled in #14513. #14783 (Amos Bird).Fixed missed default database name in metadata of materialized view when executing ALTER ... MODIFY QUERY. #14664 (tavplubix).Fix possibly incorrect result of function has when LowCardinality and Nullable types are involved. #14591 (Mike).Cleanup data directory after Zookeeper exceptions during CREATE query for tables with ReplicatedMergeTree Engine. #14563 (Bharat Nallan).Fix rare segfaults in functions with combinator -Resample, which could appear in result of overflow with very large parameters. #14562 (Anton Popov).Check for array size overflow in topK aggregate function. Without this check the user may send a query with carefully crafted parameters that will lead to server crash. This closes #14452. #14467 (alexey-milovidov).Proxy restart/start/stop/reload of SysVinit to systemd (if it is used). #14460 (Azat Khuzhin).Stop query execution if exception happened in PipelineExecutor itself. This could prevent rare possible query hung. #14334 #14402 (Nikolai Kochetov).Fix crash during ALTER query for table which was created AS table_function. Fixes #14212. #14326 (alesapin).Fix exception during ALTER LIVE VIEW query with REFRESH command. LIVE VIEW is an experimental feature. #14320 (Bharat Nallan).Fix QueryPlan lifetime (for EXPLAIN PIPELINE graph=1) for queries with nested interpreter. #14315 (Azat Khuzhin).Better check for tuple size in SSD cache complex key external dictionaries. This fixes #13981. #14313 (alexey-milovidov).Disallows CODEC on ALIAS column type. Fixes #13911. #14263 (Bharat Nallan).Fix GRANT ALL statement when executed on a non-global level. #13987 (Vitaly Baranov).Fix arrayJoin() capturing in lambda (exception with logical error message was thrown). #13792 (Azat Khuzhin). Experimental Feature​ Added db-generator tool for random database generation by given SELECT queries. It may faciliate reproducing issues when there is only incomplete bug report from the user. #14442 (Nikita Mikhaylov) #10973 (ZeDRoman). Improvement​ Allow using multi-volume storage configuration in storage Distributed. #14839 (Pavel Kovalenko).Disallow empty time_zone argument in toStartOf* type of functions. #14509 (Bharat Nallan).MySQL handler returns OK for queries like SET @@var = value. Such statement is ignored. It is needed because some MySQL drivers send SET @@ query for setup after handshake https://github.com/ClickHouse/ClickHouse/issues/9336#issuecomment-686222422 . #14469 (BohuTANG).Now TTLs will be applied during merge if they were not previously materialized. #14438 (alesapin).Now clickhouse-obfuscator supports UUID type as proposed in #13163. #14409 (dimarub2000).Added new setting system_events_show_zero_values as proposed in #11384. #14404 (dimarub2000).Implicitly convert primary key to not null in MaterializeMySQL (Same as MySQL). Fixes #14114. #14397 (Winter Zhang).Replace wide integers (256 bit) from boost multiprecision with implementation from https://github.com/cerevra/int. 256bit integers are experimental. #14229 (Artem Zuikov).Add default compression codec for parts in system.part_log with the name default_compression_codec. #14116 (alesapin).Add precision argument for DateTime type. It allows to use DateTime name instead of DateTime64. #13761 (Winter Zhang).Added requirepass authorization for Redis external dictionary. #13688 (Ivan Torgashov).Improvements in RabbitMQ engine: added connection and channels failure handling, proper commits, insert failures handling, better exchanges, queue durability and queue resume opportunity, new queue settings. Fixed tests. #12761 (Kseniia Sumarokova).Support custom codecs in compact parts. #12183 (Anton Popov). Performance Improvement​ Optimize queries with LIMIT/LIMIT BY/ORDER BY for distributed with GROUP BY sharding_key (under optimize_skip_unused_shards and optimize_distributed_group_by_sharding_key). #10373 (Azat Khuzhin).Creating sets for multiple JOIN and IN in parallel. It may slightly improve performance for queries with several different IN subquery expressions. #14412 (Nikolai Kochetov).Improve Kafka engine performance by providing independent thread for each consumer. Separate thread pool for streaming engines (like Kafka). #13939 (fastio). Build/Testing/Packaging Improvement​ Lower binary size in debug build by removing debug info from Functions. This is needed only for one internal project in Yandex who is using very old linker. #14549 (alexey-milovidov).Prepare for build with clang 11. #14455 (alexey-milovidov).Fix the logic in backport script. In previous versions it was triggered for any labels of 100% red color. It was strange. #14433 (alexey-milovidov).Integration tests use default base config. All config changes are explicit with main_configs, user_configs and dictionaries parameters for instance. #13647 (Ilya Yatsishin). "},{"title":"ClickHouse release 20.8​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-208","content":""},{"title":"ClickHouse release v20.8.12.2-lts, 2021-01-16​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v208122-lts-2021-01-16","content":"Bug Fix​ Fix *If combinator with unary function and Nullable types. #18806 (Azat Khuzhin).Restrict merges from wide to compact parts. In case of vertical merge it led to broken result part. #18381 (Anton Popov). "},{"title":"ClickHouse release v20.8.11.17-lts, 2020-12-25​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v2081117-lts-2020-12-25","content":"Bug Fix​ Disable write with AIO during merges because it can lead to extremely rare data corruption of primary key columns during merge. #18481 (alesapin).Fixed value is too short error when executing toType(...) functions (toDate, toUInt32, etc) with argument of type Nullable(String). Now such functions return NULL on parsing errors instead of throwing exception. Fixes #7673. #18445 (tavplubix).Fix possible crashes in aggregate functions with combinator Distinct, while using two-level aggregation. Fixes #17682. #18365 (Anton Popov). "},{"title":"ClickHouse release v20.8.10.13-lts, 2020-12-24​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v2081013-lts-2020-12-24","content":"Bug Fix​ When server log rotation was configured using logger.size parameter with numeric value larger than 2^32, the logs were not rotated properly. #17905 (Alexander Kuzmenkov).Fixed incorrect initialization of max_compress_block_size in MergeTreeWriterSettings with min_compress_block_size. #17833 (flynn).Fixed problem when ClickHouse fails to resume connection to MySQL servers. #17681 (Alexander Kazakov).Fixed ALTER query hang when the corresponding mutation was killed on the different replica. This fixes #16953. #17499 (alesapin).Fixed a bug when mark cache size was underestimated by ClickHouse. It may happen when there are a lot of tiny files with marks. #17496 (alesapin).Fixed ORDER BY with enabled setting optimize_redundant_functions_in_order_by. #17471 (Anton Popov).Fixed ColumnConst comparison which leads to crash. This fixed #17088 . #17135 (Amos Bird).Fixed bug when ON CLUSTER queries may hang forever for non-leader ReplicatedMergeTreeTables. #17089 (alesapin).Avoid unnecessary network errors for remote queries which may be cancelled while execution, like queries with LIMIT. #17006 (Azat Khuzhin).Reresolve the IP of the format_avro_schema_registry_url in case of errors. #16985 (filimonov).Fixed possible server crash after ALTER TABLE ... MODIFY COLUMN ... NewType when SELECT have WHERE expression on altering column and alter does not finished yet. #16968 (Amos Bird).Install script should always create subdirs in config folders. This is only relevant for Docker build with custom config. #16936 (filimonov).Fixed possible error Illegal type of argument for queries with ORDER BY. Fixes #16580. #16928 (Nikolai Kochetov).Abort multipart upload if no data was written to WriteBufferFromS3. #16840 (Pavel Kovalenko).Fixed crash when using any without any arguments. This fixes #16803. #16826 (Amos Bird).Fixed IN operator over several columns and tuples with enabled transform_null_in setting. Fixes #15310. #16722 (Anton Popov).Fixed inconsistent behaviour of optimize_read_in_order/optimize_aggregation_in_order with max_threads &gt; 0 and expression in ORDER BY. #16637 (Azat Khuzhin).Fixed the issue when query optimization was producing wrong result if query contains ARRAY JOIN. #17887 (sundyli).Query is finished faster in case of exception. Cancel execution on remote replicas if exception happens. #15578 (Azat Khuzhin). "},{"title":"ClickHouse release v20.8.6.6-lts, 2020-11-13​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v20866-lts-2020-11-13","content":"Bug Fix​ Fix rare silent crashes when query profiler is on and ClickHouse is installed on OS with glibc version that has (supposedly) broken asynchronous unwind tables for some functions. This fixes #15301. This fixes #13098. #16846 (alexey-milovidov).Now when parsing AVRO from input the LowCardinality is removed from type. Fixes #16188. #16521 (Mike).Fix rapid growth of metadata when using MySQL Master -&gt; MySQL Slave -&gt; ClickHouse MaterializeMySQL Engine, and slave_parallel_worker enabled on MySQL Slave, by properly shrinking GTID sets. This fixes #15951. #16504 (TCeason).Fix DROP TABLE for Distributed (racy with INSERT). #16409 (Azat Khuzhin).Fix processing of very large entries in replication queue. Very large entries may appear in ALTER queries if table structure is extremely large (near 1 MB). This fixes #16307. #16332 (alexey-milovidov).Fixed the inconsistent behaviour when a part of return data could be dropped because the set for its filtration wasn't created. #16308 (Nikita Mikhaylov).Fix bug with MySQL database. When MySQL server used as database engine is down some queries raise Exception, because they try to get tables from disabled server, while it's unnecessary. For example, query SELECT ... FROM system.parts should work only with MergeTree tables and don't touch MySQL database at all. #16032 (Kruglov Pavel). "},{"title":"ClickHouse release v20.8.5.45-lts, 2020-10-29​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v208545-lts-2020-10-29","content":"Bug Fix​ Fix double free in case of exception in function dictGet. It could have happened if dictionary was loaded with error. #16429 (Nikolai Kochetov).Fix group by with totals/rollup/cube modifers and min/max functions over group by keys. Fixes #16393. #16397 (Anton Popov).Fix async Distributed INSERT w/ prefer_localhost_replica=0 and internal_replication. #16358 (Azat Khuzhin).Fix a possible memory leak during GROUP BY with string keys, caused by an error in TwoLevelStringHashTable implementation. #16264 (Amos Bird).Fix the case when memory can be overallocated regardless to the limit. This closes #14560. #16206 (alexey-milovidov).Fix ALTER MODIFY ... ORDER BY query hang for ReplicatedVersionedCollapsingMergeTree. This fixes #15980. #16011 (alesapin).Fix collate name &amp; charset name parser and support length = 0 for string type. #16008 (Winter Zhang).Allow to use direct layout for dictionaries with complex keys. #16007 (Anton Popov).Prevent replica hang for 5-10 mins when replication error happens after a period of inactivity. #15987 (filimonov).Fix rare segfaults when inserting into or selecting from MaterializedView and concurrently dropping target table (for Atomic database engine). #15984 (tavplubix).Fix ambiguity in parsing of settings profiles: CREATE USER ... SETTINGS profile readonly is now considered as using a profile named readonly, not a setting named profile with the readonly constraint. This fixes #15628. #15982 (Vitaly Baranov).Fix a crash when database creation fails. #15954 (Winter Zhang).Fixed DROP TABLE IF EXISTS failure with Table ... does not exist error when table is concurrently renamed (for Atomic database engine). Fixed rare deadlock when concurrently executing some DDL queries with multiple tables (like DROP DATABASE and RENAME TABLE) Fixed DROP/DETACH DATABASE failure with Table ... does not exist when concurrently executing DROP/DETACH TABLE. #15934 (tavplubix).Fix incorrect empty result for query from Distributed table if query has WHERE, PREWHERE and GLOBAL IN. Fixes #15792. #15933 (Nikolai Kochetov).Fix possible deadlocks in RBAC. #15875 (Vitaly Baranov).Fix exception Block structure mismatch in SELECT ... ORDER BY DESC queries which were executed after ALTER MODIFY COLUMN query. Fixes #15800. #15852 (alesapin).Fix some cases of queries, in which only virtual columns are selected. Previously Not found column _nothing in block exception may be thrown. Fixes #12298. #15756 (Anton Popov).Fix error Cannot find column which may happen at insertion into MATERIALIZED VIEW in case if query for MV containes ARRAY JOIN. #15717 (Nikolai Kochetov).Fixed too low default value of max_replicated_logs_to_keep setting, which might cause replicas to become lost too often. Improve lost replica recovery process by choosing the most up-to-date replica to clone. Also do not remove old parts from lost replica, detach them instead. #15701 (tavplubix).Fix error Cannot add simple transform to empty Pipe which happened while reading from Buffer table which has different structure than destination table. It was possible if destination table returned empty result for query. Fixes #15529. #15662 (Nikolai Kochetov).Fixed bug with globs in S3 table function, region from URL was not applied to S3 client configuration. #15646 (Vladimir Chebotarev).Decrement the ReadonlyReplica metric when detaching read-only tables. This fixes #15598. #15592 (sundyli).Throw an error when a single parameter is passed to ReplicatedMergeTree instead of ignoring it. #15516 (nvartolomei). Improvement​ Now it's allowed to execute ALTER ... ON CLUSTER queries regardless of the &lt;internal_replication&gt; setting in cluster config. #16075 (alesapin).Unfold {database}, {table} and {uuid} macros in ReplicatedMergeTree arguments on table creation. #16159 (tavplubix). "},{"title":"ClickHouse release v20.8.4.11-lts, 2020-10-09​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v208411-lts-2020-10-09","content":"Bug Fix​ Fix the order of destruction for resources in ReadFromStorage step of query plan. It might cause crashes in rare cases. Possibly connected with #15610. #15645 (Nikolai Kochetov).Fixed Element ... is not a constant expression error when using JSON* function result in VALUES, LIMIT or right side of IN operator. #15589 (tavplubix).Prevent the possibility of error message Could not calculate available disk space (statvfs), errno: 4, strerror: Interrupted system call. This fixes #15541. #15557 (alexey-milovidov).Significantly reduce memory usage in AggregatingInOrderTransform/optimize_aggregation_in_order. #15543 (Azat Khuzhin).Mutation might hang waiting for some non-existent part after MOVE or REPLACE PARTITION or, in rare cases, after DETACH or DROP PARTITION. It's fixed. #15537 (tavplubix).Fix bug when ILIKE operator stops being case insensitive if LIKE with the same pattern was executed. #15536 (alesapin).Fix Missing columns errors when selecting columns which absent in data, but depend on other columns which also absent in data. Fixes #15530. #15532 (alesapin).Fix bug with event subscription in DDLWorker which rarely may lead to query hangs in ON CLUSTER. Introduced in #13450. #15477 (alesapin).Report proper error when the second argument of boundingRatio aggregate function has a wrong type. #15407 (detailyang).Fix race condition during MergeTree table rename and background cleanup. #15304 (alesapin).Fix rare race condition on server startup when system.logs are enabled. #15300 (alesapin).Fix MSan report in QueryLog. Uninitialized memory can be used for the field memory_usage. #15258 (alexey-milovidov).Fix instance crash when using joinGet with LowCardinality types. This fixes #15214. #15220 (Amos Bird).Fix bug in table engine Buffer which does not allow to insert data of new structure into Buffer after ALTER query. Fixes #15117. #15192 (alesapin).Adjust decimals field size in mysql column definition packet. #15152 (maqroll).We already use padded comparison between String and FixedString (https://github.com/ClickHouse/ClickHouse/blob/master/src/Functions/FunctionsComparison.h#L333). This PR applies the same logic to field comparison which corrects the usage of FixedString as primary keys. This fixes #14908. #15033 (Amos Bird).If function bar was called with specifically crafted arguments, buffer overflow was possible. This closes #13926. #15028 (alexey-milovidov).Fixed Cannot rename ... errno: 22, strerror: Invalid argument error on DDL query execution in Atomic database when running clickhouse-server in docker on Mac OS. #15024 (tavplubix).Now settings number_of_free_entries_in_pool_to_execute_mutation and number_of_free_entries_in_pool_to_lower_max_size_of_merge can be equal to background_pool_size. #14975 (alesapin).Fix to make predicate push down work when subquery contains finalizeAggregation function. Fixes #14847. #14937 (filimonov).Publish CPU frequencies per logical core in system.asynchronous_metrics. This fixes #14923. #14924 (Alexander Kuzmenkov).Fixed .metadata.tmp File exists error when using MaterializeMySQL database engine. #14898 (Winter Zhang).Fix a problem where the server may get stuck on startup while talking to ZooKeeper, if the configuration files have to be fetched from ZK (using the from_zk include option). This fixes #14814. #14843 (Alexander Kuzmenkov).Fix wrong monotonicity detection for shrunk Int -&gt; Int cast of signed types. It might lead to incorrect query result. This bug is unveiled in #14513. #14783 (Amos Bird).Fixed the incorrect sorting order of Nullable column. This fixes #14344. #14495 (Nikita Mikhaylov). Improvement​ Now it's possible to change the type of version column for VersionedCollapsingMergeTree with ALTER query. #15442 (alesapin). "},{"title":"ClickHouse release v20.8.3.18-stable, 2020-09-18​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v208318-stable-2020-09-18","content":"Bug Fix​ Fix the issue when some invocations of extractAllGroups function may trigger &quot;Memory limit exceeded&quot; error. This fixes #13383. #14889 (alexey-milovidov).Fix SIGSEGV for an attempt to INSERT into StorageFile(fd). #14887 (Azat Khuzhin).Fix rare error in SELECT queries when the queried column has DEFAULT expression which depends on the other column which also has DEFAULT and not present in select query and not exists on disk. Partially fixes #14531. #14845 (alesapin).Fixed missed default database name in metadata of materialized view when executing ALTER ... MODIFY QUERY. #14664 (tavplubix).Fix bug when ALTER UPDATE mutation with Nullable column in assignment expression and constant value (like UPDATE x = 42) leads to incorrect value in column or segfault. Fixes #13634, #14045. #14646 (alesapin).Fix wrong Decimal multiplication result caused wrong decimal scale of result column. #14603 (Artem Zuikov).Added the checker as neither calling lc-&gt;isNullable() nor calling ls-&gt;getDictionaryPtr()-&gt;isNullable() would return the correct result. #14591 (myrrc).Cleanup data directory after Zookeeper exceptions during CreateQuery for StorageReplicatedMergeTree Engine. #14563 (Bharat Nallan).Fix rare segfaults in functions with combinator -Resample, which could appear in result of overflow with very large parameters. #14562 (Anton Popov). Improvement​ Speed up server shutdown process if there are ongoing S3 requests. #14858 (Pavel Kovalenko).Allow using multi-volume storage configuration in storage Distributed. #14839 (Pavel Kovalenko).Speed up server shutdown process if there are ongoing S3 requests. #14496 (Pavel Kovalenko).Support custom codecs in compact parts. #12183 (Anton Popov). "},{"title":"ClickHouse release v20.8.2.3-stable, 2020-09-08​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v20823-stable-2020-09-08","content":"Backward Incompatible Change​ Now OPTIMIZE FINAL query does not recalculate TTL for parts that were added before TTL was created. Use ALTER TABLE ... MATERIALIZE TTL once to calculate them, after that OPTIMIZE FINAL will evaluate TTL's properly. This behavior never worked for replicated tables. #14220 (alesapin).Extend parallel_distributed_insert_select setting, adding an option to run INSERT into local table. The setting changes type from Bool to UInt64, so the values false and true are no longer supported. If you have these values in server configuration, the server will not start. Please replace them with 0 and 1, respectively. #14060 (Azat Khuzhin).Remove support for the ODBCDriver input/output format. This was a deprecated format once used for communication with the ClickHouse ODBC driver, now long superseded by the ODBCDriver2 format. Resolves #13629. #13847 (hexiaoting).When upgrading from versions older than 20.5, if rolling update is performed and cluster contains both versions 20.5 or greater and less than 20.5, if ClickHouse nodes with old versions are restarted and old version has been started up in presence of newer versions, it may lead to Part ... intersects previous part errors. To prevent this error, first install newer clickhouse-server packages on all cluster nodes and then do restarts (so, when clickhouse-server is restarted, it will start up with the new version). New Feature​ Add the ability to specify Default compression codec for columns that correspond to settings specified in config.xml. Implements: #9074. #14049 (alesapin).Support Kerberos authentication in Kafka, using krb5 and cyrus-sasl libraries. #12771 (Ilya Golshtein).Add function normalizeQuery that replaces literals, sequences of literals and complex aliases with placeholders. Add function normalizedQueryHash that returns identical 64bit hash values for similar queries. It helps to analyze query log. This closes #11271. #13816 (alexey-milovidov).Add time_zones table. #13880 (Bharat Nallan).Add function defaultValueOfTypeName that returns the default value for a given type. #13877 (hcz).Add countDigits(x) function that count number of decimal digits in integer or decimal column. Add isDecimalOverflow(d, [p]) function that checks if the value in Decimal column is out of its (or specified) precision. #14151 (Artem Zuikov).Add quantileExactLow and quantileExactHigh implementations with respective aliases for medianExactLow and medianExactHigh. #13818 (Bharat Nallan).Added date_trunc function that truncates a date/time value to a specified date/time part. #13888 (Vladimir Golovchenko).Add new optional section &lt;user_directories&gt; to the main config. #13425 (Vitaly Baranov).Add ALTER SAMPLE BY statement that allows to change table sample clause. #13280 (Amos Bird).Function position now supports optional start_pos argument. #13237 (vdimir). Bug Fix​ Fix visible data clobbering by progress bar in client in interactive mode. This fixes #12562 and #13369 and #13584 and fixes #12964. #13691 (alexey-milovidov).Fixed incorrect sorting order if LowCardinality column when sorting by multiple columns. This fixes #13958. #14223 (Nikita Mikhaylov).Check for array size overflow in topK aggregate function. Without this check the user may send a query with carefully crafted parameters that will lead to server crash. This closes #14452. #14467 (alexey-milovidov).Fix bug which can lead to wrong merges assignment if table has partitions with a single part. #14444 (alesapin).Stop query execution if exception happened in PipelineExecutor itself. This could prevent rare possible query hung. Continuation of #14334. #14402 #14334 (Nikolai Kochetov).Fix crash during ALTER query for table which was created AS table_function. Fixes #14212. #14326 (alesapin).Fix exception during ALTER LIVE VIEW query with REFRESH command. Live view is an experimental feature. #14320 (Bharat Nallan).Fix QueryPlan lifetime (for EXPLAIN PIPELINE graph=1) for queries with nested interpreter. #14315 (Azat Khuzhin).Fix segfault in clickhouse-odbc-bridge during schema fetch from some external sources. This PR fixes #13861. #14267 (Vitaly Baranov).Fix crash in mark inclusion search introduced in #12277. #14225 (Amos Bird).Fix creation of tables with named tuples. This fixes #13027. #14143 (alexey-milovidov).Fix formatting of minimal negative decimal numbers. This fixes #14111. #14119 (Alexander Kuzmenkov).Fix DistributedFilesToInsert metric (zeroed when it should not). #14095 (Azat Khuzhin).Fix pointInPolygon with const 2d array as polygon. #14079 (Alexey Ilyukhov).Fixed wrong mount point in extra info for Poco::Exception: no space left on device. #14050 (tavplubix).Fix GRANT ALL statement when executed on a non-global level. #13987 (Vitaly Baranov).Fix parser to reject create table as table function with engine. #13940 (hcz).Fix wrong results in select queries with DISTINCT keyword and subqueries with UNION ALL in case optimize_duplicate_order_by_and_distinct setting is enabled. #13925 (Artem Zuikov).Fixed potential deadlock when renaming Distributed table. #13922 (tavplubix).Fix incorrect sorting for FixedString columns when sorting by multiple columns. Fixes #13182. #13887 (Nikolai Kochetov).Fix potentially imprecise result of topK/topKWeighted merge (with non-default parameters). #13817 (Azat Khuzhin).Fix reading from MergeTree table with INDEX of type SET fails when comparing against NULL. This fixes #13686. #13793 (Amos Bird).Fix arrayJoin capturing in lambda (LOGICAL_ERROR). #13792 (Azat Khuzhin).Add step overflow check in function range. #13790 (Azat Khuzhin).Fixed Directory not empty error when concurrently executing DROP DATABASE and CREATE TABLE. #13756 (alexey-milovidov).Add range check for h3KRing function. This fixes #13633. #13752 (alexey-milovidov).Fix race condition between DETACH and background merges. Parts may revive after detach. This is continuation of #8602 that did not fix the issue but introduced a test that started to fail in very rare cases, demonstrating the issue. #13746 (alexey-milovidov).Fix logging Settings.Names/Values when log_queries_min_type &gt; QUERY_START. #13737 (Azat Khuzhin).Fixes /replicas_status endpoint response status code when verbose=1. #13722 (javi santana).Fix incorrect message in clickhouse-server.init while checking user and group. #13711 (ylchou).Do not optimize any(arrayJoin()) -&gt; arrayJoin() under optimize_move_functions_out_of_any setting. #13681 (Azat Khuzhin).Fix crash in JOIN with StorageMerge and set enable_optimize_predicate_expression=1. #13679 (Artem Zuikov).Fix typo in error message about The value of 'number_of_free_entries_in_pool_to_lower_max_size_of_merge' setting. #13678 (alexey-milovidov).Concurrent ALTER ... REPLACE/MOVE PARTITION ... queries might cause deadlock. It's fixed. #13626 (tavplubix).Fixed the behaviour when sometimes cache-dictionary returned default value instead of present value from source. #13624 (Nikita Mikhaylov).Fix secondary indices corruption in compact parts. Compact parts are experimental feature. #13538 (Anton Popov).Fix premature ON CLUSTER timeouts for queries that must be executed on a single replica. Fixes #6704, #7228, #13361, #11884. #13450 (alesapin).Fix wrong code in function netloc. This fixes #13335. #13446 (alexey-milovidov).Fix possible race in StorageMemory. #13416 (Nikolai Kochetov).Fix missing or excessive headers in TSV/CSVWithNames formats in HTTP protocol. This fixes #12504. #13343 (Azat Khuzhin).Fix parsing row policies from users.xml when names of databases or tables contain dots. This fixes #5779, #12527. #13199 (Vitaly Baranov).Fix access to redis dictionary after connection was dropped once. It may happen with cache and direct dictionary layouts. #13082 (Anton Popov).Removed wrong auth access check when using ClickHouseDictionarySource to query remote tables. #12756 (sundyli).Properly distinguish subqueries in some cases for common subexpression elimination. #8333. #8367 (Amos Bird). Improvement​ Disallows CODEC on ALIAS column type. Fixes #13911. #14263 (Bharat Nallan).When waiting for a dictionary update to complete, use the timeout specified by query_wait_timeout_milliseconds setting instead of a hard-coded value. #14105 (Nikita Mikhaylov).Add setting min_index_granularity_bytes that protects against accidentally creating a table with very low index_granularity_bytes setting. #14139 (Bharat Nallan).Now it's possible to fetch partitions from clusters that use different ZooKeeper: ALTER TABLE table_name FETCH PARTITION partition_expr FROM 'zk-name:/path-in-zookeeper'. It's useful for shipping data to new clusters. #14155 (Amos Bird).Slightly better performance of Memory table if it was constructed from a huge number of very small blocks (that's unlikely). Author of the idea: Mark Papadakis. Closes #14043. #14056 (alexey-milovidov).Conditional aggregate functions (for example: avgIf, sumIf, maxIf) should return NULL when miss rows and use nullable arguments. #13964 (Winter Zhang).Increase limit in -Resample combinator to 1M. #13947 (Mikhail f. Shiryaev).Corrected an error in AvroConfluent format that caused the Kafka table engine to stop processing messages when an abnormally small, malformed, message was received. #13941 (Gervasio Varela).Fix wrong error for long queries. It was possible to get syntax error other than Max query size exceeded for correct query. #13928 (Nikolai Kochetov).Better error message for null value of TabSeparated format. #13906 (jiang tao).Function arrayCompact will compare NaNs bitwise if the type of array elements is Float32/Float64. In previous versions NaNs were always not equal if the type of array elements is Float32/Float64 and were always equal if the type is more complex, like Nullable(Float64). This closes #13857. #13868 (alexey-milovidov).Fix data race in lgamma function. This race was caught only in tsan, no side effects a really happened. #13842 (Nikolai Kochetov).Avoid too slow queries when arrays are manipulated as fields. Throw exception instead. #13753 (alexey-milovidov).Added Redis requirepass authorization (for redis dictionary source). #13688 (Ivan Torgashov).Add MergeTree Write-Ahead-Log (WAL) dump tool. WAL is an experimental feature. #13640 (BohuTANG).In previous versions lcm function may produce assertion violation in debug build if called with specifically crafted arguments. This fixes #13368. #13510 (alexey-milovidov).Provide monotonicity for toDate/toDateTime functions in more cases. Monotonicity information is used for index analysis (more complex queries will be able to use index). Now the input arguments are saturated more naturally and provides better monotonicity. #13497 (Amos Bird).Support compound identifiers for custom settings. Custom settings is an integration point of ClickHouse codebase with other codebases (no benefits for ClickHouse itself) #13496 (Vitaly Baranov).Move parts from DiskLocal to DiskS3 in parallel. DiskS3 is an experimental feature. #13459 (Pavel Kovalenko).Enable mixed granularity parts by default. #13449 (alesapin).Proper remote host checking in S3 redirects (security-related thing). #13404 (Vladimir Chebotarev).Add QueryTimeMicroseconds, SelectQueryTimeMicroseconds and InsertQueryTimeMicroseconds to system.events. #13336 (ianton-ru).Fix debug assertion when Decimal has too large negative exponent. Fixes #13188. #13228 (alexey-milovidov).Added cache layer for DiskS3 (cache to local disk mark and index files). DiskS3 is an experimental feature. #13076 (Pavel Kovalenko).Fix readline so it dumps history to file now. #13600 (Amos Bird).Create system database with Atomic engine by default (a preparation to enable Atomic database engine by default everywhere). #13680 (tavplubix). Performance Improvement​ Slightly optimize very short queries with LowCardinality. #14129 (Anton Popov).Enable parallel INSERTs for table engines Null, Memory, Distributed and Buffer when the setting max_insert_threads is set. #14120 (alexey-milovidov).Fail fast if max_rows_to_read limit is exceeded on parts scan. The motivation behind this change is to skip ranges scan for all selected parts if it is clear that max_rows_to_read is already exceeded. The change is quite noticeable for queries over big number of parts. #13677 (Roman Khavronenko).Slightly improve performance of aggregation by UInt8/UInt16 keys. #13099 (alexey-milovidov).Optimize has(), indexOf() and countEqual() functions for Array(LowCardinality(T)) and constant right arguments. #12550 (myrrc).When performing trivial INSERT SELECT queries, automatically set max_threads to 1 or max_insert_threads, and set max_block_size to min_insert_block_size_rows. Related to #5907. #12195 (flynn). Experimental Feature​ ClickHouse can work as MySQL replica - it is implemented by MaterializeMySQL database engine. Implements #4006. #10851 (Winter Zhang).Add types Int128, Int256, UInt256 and related functions for them. Extend Decimals with Decimal256 (precision up to 76 digits). New types are under the setting allow_experimental_bigint_types. It is working extremely slow and bad. The implementation is incomplete. Please don't use this feature. #13097 (Artem Zuikov). Build/Testing/Packaging Improvement​ Added clickhouse install script, that is useful if you only have a single binary. #13528 (alexey-milovidov).Allow to run clickhouse binary without configuration. #13515 (alexey-milovidov).Enable check for typos in code with codespell. #13513 #13511 (alexey-milovidov).Enable Shellcheck in CI as a linter of .sh tests. This closes #13168. #13530 #13529 (alexey-milovidov).Add a CMake option to fail configuration instead of auto-reconfiguration, enabled by default. #13687 (Konstantin).Expose version of embedded tzdata via TZDATA_VERSION in system.build_options. #13648 (filimonov).Improve generation of system.time_zones table during build. Closes #14209. #14215 (filimonov).Build ClickHouse with the most fresh tzdata from package repository. #13623 (alexey-milovidov).Add the ability to write js-style comments in skip_list.json. #14159 (alesapin).Ensure that there is no copy-pasted GPL code. #13514 (alexey-milovidov).Switch tests docker images to use test-base parent. #14167 (Ilya Yatsishin).Adding retry logic when bringing up docker-compose cluster; Increasing COMPOSE_HTTP_TIMEOUT. #14112 (vzakaznikov).Enabled system.text_log in stress test to find more bugs. #13855 (Nikita Mikhaylov).Testflows LDAP module: adding missing certificates and dhparam.pem for openldap4. #13780 (vzakaznikov).ZooKeeper cannot work reliably in unit tests in CI infrastructure. Using unit tests for ZooKeeper interaction with real ZooKeeper is bad idea from the start (unit tests are not supposed to verify complex distributed systems). We already using integration tests for this purpose and they are better suited. #13745 (alexey-milovidov).Added docker image for style check. Added style check that all docker and docker compose files are located in docker directory. #13724 (Ilya Yatsishin).Fix cassandra build on Mac OS. #13708 (Ilya Yatsishin).Fix link error in shared build. #13700 (Amos Bird).Updating LDAP user authentication suite to check that it works with RBAC. #13656 (vzakaznikov).Removed -DENABLE_CURL_CLIENT for contrib/aws. #13628 (Vladimir Chebotarev).Increasing health-check timeouts for ClickHouse nodes and adding support to dump docker-compose logs if unhealthy containers found. #13612 (vzakaznikov).Make sure #10977 is invalid. #13539 (Amos Bird).Skip PR's from robot-clickhouse. #13489 (Nikita Mikhaylov).Move Dockerfiles from integration tests to docker/test directory. docker_compose files are available in runner docker container. Docker images are built in CI and not in integration tests. #13448 (Ilya Yatsishin). "},{"title":"ClickHouse release 20.7​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-207","content":""},{"title":"ClickHouse release v20.7.2.30-stable, 2020-08-31​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v207230-stable-2020-08-31","content":"Backward Incompatible Change​ Function modulo (operator %) with at least one floating point number as argument will calculate remainder of division directly on floating point numbers without converting both arguments to integers. It makes behaviour compatible with most of DBMS. This also applicable for Date and DateTime data types. Added alias mod. This closes #7323. #12585 (alexey-milovidov).Deprecate special printing of zero Date/DateTime values as 0000-00-00 and 0000-00-00 00:00:00. #12442 (alexey-milovidov).The function groupArrayMoving* was not working for distributed queries. It's result was calculated within incorrect data type (without promotion to the largest type). The function groupArrayMovingAvg was returning integer number that was inconsistent with the avg function. This fixes #12568. #12622 (alexey-milovidov).Add sanity check for MergeTree settings. If the settings are incorrect, the server will refuse to start or to create a table, printing detailed explanation to the user. #13153 (alexey-milovidov).Protect from the cases when user may set background_pool_size to value lower than number_of_free_entries_in_pool_to_execute_mutation or number_of_free_entries_in_pool_to_lower_max_size_of_merge. In these cases ALTERs won't work or the maximum size of merge will be too limited. It will throw exception explaining what to do. This closes #10897. #12728 (alexey-milovidov).When upgrading from versions older than 20.5, if rolling update is performed and cluster contains both versions 20.5 or greater and less than 20.5, if ClickHouse nodes with old versions are restarted and old version has been started up in presence of newer versions, it may lead to Part ... intersects previous part errors. To prevent this error, first install newer clickhouse-server packages on all cluster nodes and then do restarts (so, when clickhouse-server is restarted, it will start up with the new version). New Feature​ Polygon dictionary type that provides efficient &quot;reverse geocoding&quot; lookups - to find the region by coordinates in a dictionary of many polygons (world map). It is using carefully optimized algorithm with recursive grids to maintain low CPU and memory usage. #9278 (achulkov2).Added support of LDAP authentication for preconfigured users (&quot;Simple Bind&quot; method). #11234 (Denis Glazachev).Introduce setting alter_partition_verbose_result which outputs information about touched parts for some types of ALTER TABLE ... PARTITION ... queries (currently ATTACH and FREEZE). Closes #8076. #13017 (alesapin).Add bayesAB function for bayesian-ab-testing. #12327 (achimbab).Added system.crash_log table into which stack traces for fatal errors are collected. This table should be empty. #12316 (alexey-milovidov).Added http headers X-ClickHouse-Database and X-ClickHouse-Format which may be used to set default database and output format. #12981 (hcz).Add minMap and maxMap functions support to SimpleAggregateFunction. #12662 (Ildus Kurbangaliev).Add setting allow_non_metadata_alters which restricts to execute ALTER queries which modify data on disk. Disabled be default. Closes #11547. #12635 (alesapin).A function formatRow is added to support turning arbitrary expressions into a string via given format. It's useful for manipulating SQL outputs and is quite versatile combined with the columns function. #12574 (Amos Bird).Add FROM_UNIXTIME function for compatibility with MySQL, related to 12149. #12484 (flynn).Allow Nullable types as keys in MergeTree tables if allow_nullable_key table setting is enabled. Closes #5319. #12433 (Amos Bird).Integration with COS. #12386 (fastio).Add mapAdd and mapSubtract functions for adding/subtracting key-mapped values. #11735 (Ildus Kurbangaliev). Bug Fix​ Fix premature ON CLUSTER timeouts for queries that must be executed on a single replica. Fixes #6704, #7228, #13361, #11884. #13450 (alesapin).Fix crash in mark inclusion search introduced in #12277. #14225 (Amos Bird).Fix race condition in external dictionaries with cache layout which can lead server crash. #12566 (alesapin).Fix visible data clobbering by progress bar in client in interactive mode. This fixes #12562 and #13369 and #13584 and fixes #12964. #13691 (alexey-milovidov).Fixed incorrect sorting order for LowCardinality columns when ORDER BY multiple columns is used. This fixes #13958. #14223 (Nikita Mikhaylov).Removed hardcoded timeout, which wrongly overruled query_wait_timeout_milliseconds setting for cache-dictionary. #14105 (Nikita Mikhaylov).Fixed wrong mount point in extra info for Poco::Exception: no space left on device. #14050 (tavplubix).Fix wrong query optimization of select queries with DISTINCT keyword when subqueries also have DISTINCT in case optimize_duplicate_order_by_and_distinct setting is enabled. #13925 (Artem Zuikov).Fixed potential deadlock when renaming Distributed table. #13922 (tavplubix).Fix incorrect sorting for FixedString columns when ORDER BY multiple columns is used. Fixes #13182. #13887 (Nikolai Kochetov).Fix potentially lower precision of topK/topKWeighted aggregations (with non-default parameters). #13817 (Azat Khuzhin).Fix reading from MergeTree table with INDEX of type SET fails when compared against NULL. This fixes #13686. #13793 (Amos Bird).Fix step overflow in function range(). #13790 (Azat Khuzhin).Fixed Directory not empty error when concurrently executing DROP DATABASE and CREATE TABLE. #13756 (alexey-milovidov).Add range check for h3KRing function. This fixes #13633. #13752 (alexey-milovidov).Fix race condition between DETACH and background merges. Parts may revive after detach. This is continuation of #8602 that did not fix the issue but introduced a test that started to fail in very rare cases, demonstrating the issue. #13746 (alexey-milovidov).Fix logging Settings.Names/Values when log_queries_min_type greater than QUERY_START. #13737 (Azat Khuzhin).Fix incorrect message in clickhouse-server.init while checking user and group. #13711 (ylchou).Do not optimize any(arrayJoin()) to arrayJoin() under optimize_move_functions_out_of_any. #13681 (Azat Khuzhin).Fixed possible deadlock in concurrent ALTER ... REPLACE/MOVE PARTITION ... queries. #13626 (tavplubix).Fixed the behaviour when sometimes cache-dictionary returned default value instead of present value from source. #13624 (Nikita Mikhaylov).Fix secondary indices corruption in compact parts (compact parts is an experimental feature). #13538 (Anton Popov).Fix wrong code in function netloc. This fixes #13335. #13446 (alexey-milovidov).Fix error in parseDateTimeBestEffort function when unix timestamp was passed as an argument. This fixes #13362. #13441 (alexey-milovidov).Fix invalid return type for comparison of tuples with NULL elements. Fixes #12461. #13420 (Nikolai Kochetov).Fix wrong optimization caused aggregate function any(x) is found inside another aggregate function in query error with SET optimize_move_functions_out_of_any = 1 and aliases inside any(). #13419 (Artem Zuikov).Fix possible race in StorageMemory. #13416 (Nikolai Kochetov).Fix empty output for Arrow and Parquet formats in case if query return zero rows. It was done because empty output is not valid for this formats. #13399 (hcz).Fix select queries with constant columns and prefix of primary key in ORDER BY clause. #13396 (Anton Popov).Fix PrettyCompactMonoBlock for clickhouse-local. Fix extremes/totals with PrettyCompactMonoBlock. Fixes #7746. #13394 (Azat Khuzhin).Fixed deadlock in system.text_log. #12452 (alexey-milovidov). It is a part of #12339. This fixes #12325. #13386 (Nikita Mikhaylov).Fixed File(TSVWithNames*) (header was written multiple times), fixed clickhouse-local --format CSVWithNames* (lacks header, broken after #12197), fixed clickhouse-local --format CSVWithNames* with zero rows (lacks header). #13343 (Azat Khuzhin).Fix segfault when function groupArrayMovingSum deserializes empty state. Fixes #13339. #13341 (alesapin).Throw error on arrayJoin() function in JOIN ON section. #13330 (Artem Zuikov).Fix crash in LEFT ASOF JOIN with join_use_nulls=1. #13291 (Artem Zuikov).Fix possible error Totals having transform was already added to pipeline in case of a query from delayed replica. #13290 (Nikolai Kochetov).The server may crash if user passed specifically crafted arguments to the function h3ToChildren. This fixes #13275. #13277 (alexey-milovidov).Fix potentially low performance and slightly incorrect result for uniqExact, topK, sumDistinct and similar aggregate functions called on Float types with NaN values. It also triggered assert in debug build. This fixes #12491. #13254 (alexey-milovidov).Fix assertion in KeyCondition when primary key contains expression with monotonic function and query contains comparison with constant whose type is different. This fixes #12465. #13251 (alexey-milovidov).Return passed number for numbers with MSB set in function roundUpToPowerOfTwoOrZero(). It prevents potential errors in case of overflow of array sizes. #13234 (Azat Khuzhin).Fix function if with nullable constexpr as cond that is not literal NULL. Fixes #12463. #13226 (alexey-milovidov).Fix assert in arrayElement function in case of array elements are Nullable and array subscript is also Nullable. This fixes #12172. #13224 (alexey-milovidov).Fix DateTime64 conversion functions with constant argument. #13205 (Azat Khuzhin).Fix parsing row policies from users.xml when names of databases or tables contain dots. This fixes #5779, #12527. #13199 (Vitaly Baranov).Fix access to redis dictionary after connection was dropped once. It may happen with cache and direct dictionary layouts. #13082 (Anton Popov).Fix wrong index analysis with functions. It could lead to some data parts being skipped when reading from MergeTree tables. Fixes #13060. Fixes #12406. #13081 (Anton Popov).Fix error Cannot convert column because it is constant but values of constants are different in source and result for remote queries which use deterministic functions in scope of query, but not deterministic between queries, like now(), now64(), randConstant(). Fixes #11327. #13075 (Nikolai Kochetov).Fix crash which was possible for queries with ORDER BY tuple and small LIMIT. Fixes #12623. #13009 (Nikolai Kochetov).Fix Block structure mismatch error for queries with UNION and JOIN. Fixes #12602. #12989 (Nikolai Kochetov).Corrected merge_with_ttl_timeout logic which did not work well when expiration affected more than one partition over one time interval. (Authored by @excitoon). #12982 (Alexander Kazakov).Fix columns duplication for range hashed dictionary created from DDL query. This fixes #10605. #12857 (alesapin).Fix unnecessary limiting for the number of threads for selects from local replica. #12840 (Nikolai Kochetov).Fix rare bug when ALTER DELETE and ALTER MODIFY COLUMN queries executed simultaneously as a single mutation. Bug leads to an incorrect amount of rows in count.txt and as a consequence incorrect data in part. Also, fix a small bug with simultaneous ALTER RENAME COLUMN and ALTER ADD COLUMN. #12760 (alesapin).Wrong credentials being used when using clickhouse dictionary source to query remote tables. #12756 (sundyli).Fix CAST(Nullable(String), Enum()). #12745 (Azat Khuzhin).Fix performance with large tuples, which are interpreted as functions in IN section. The case when user writes WHERE x IN tuple(1, 2, ...) instead of WHERE x IN (1, 2, ...) for some obscure reason. #12700 (Anton Popov).Fix memory tracking for input_format_parallel_parsing (by attaching thread to group). #12672 (Azat Khuzhin).Fix wrong optimization optimize_move_functions_out_of_any=1 in case of any(func(&lt;lambda&gt;)). #12664 (Artem Zuikov).Fixed #10572 fix bloom filter index with const expression. #12659 (Winter Zhang).Fix SIGSEGV in StorageKafka when broker is unavailable (and not only). #12658 (Azat Khuzhin).Add support for function if with Array(UUID) arguments. This fixes #11066. #12648 (alexey-milovidov).CREATE USER IF NOT EXISTS now does not throw exception if the user exists. This fixes #12507. #12646 (Vitaly Baranov).Exception There is no supertype... can be thrown during ALTER ... UPDATE in unexpected cases (e.g. when subtracting from UInt64 column). This fixes #7306. This fixes #4165. #12633 (alexey-milovidov).Fix possible Pipeline stuck error for queries with external sorting. Fixes #12617. #12618 (Nikolai Kochetov).Fix error Output of TreeExecutor is not sorted for OPTIMIZE DEDUPLICATE. Fixes #11572. #12613 (Nikolai Kochetov).Fix the issue when alias on result of function any can be lost during query optimization. #12593 (Anton Popov).Remove data for Distributed tables (blocks from async INSERTs) on DROP TABLE. #12556 (Azat Khuzhin).Now ClickHouse will recalculate checksums for parts when file checksums.txt is absent. Broken since #9827. #12545 (alesapin).Fix bug which lead to broken old parts after ALTER DELETE query when enable_mixed_granularity_parts=1. Fixes #12536. #12543 (alesapin).Fixing race condition in live view tables which could cause data duplication. LIVE VIEW is an experimental feature. #12519 (vzakaznikov).Fix backwards compatibility in binary format of AggregateFunction(avg, ...) values. This fixes #12342. #12486 (alexey-milovidov).Fix crash in JOIN with dictionary when we are joining over expression of dictionary key: t JOIN dict ON expr(dict.id) = t.id. Disable dictionary join optimisation for this case. #12458 (Artem Zuikov).Fix overflow when very large LIMIT or OFFSET is specified. This fixes #10470. This fixes #11372. #12427 (alexey-milovidov).kafka: fix SIGSEGV if there is a message with error in the middle of the batch. #12302 (Azat Khuzhin). Improvement​ Keep smaller amount of logs in ZooKeeper. Avoid excessive growing of ZooKeeper nodes in case of offline replicas when having many servers/tables/inserts. #13100 (alexey-milovidov).Now exceptions forwarded to the client if an error happened during ALTER or mutation. Closes #11329. #12666 (alesapin).Add QueryTimeMicroseconds, SelectQueryTimeMicroseconds and InsertQueryTimeMicroseconds to system.events, along with system.metrics, processes, query_log, etc. #13028 (ianton-ru).Added SelectedRows and SelectedBytes to system.events, along with system.metrics, processes, query_log, etc. #12638 (ianton-ru).Added current_database information to system.query_log. #12652 (Amos Bird).Allow TabSeparatedRaw as input format. #12009 (hcz).Now joinGet supports multi-key lookup. #12418 (Amos Bird).Allow *Map aggregate functions to work on Arrays with NULLs. Fixes #13157. #13225 (alexey-milovidov).Avoid overflow in parsing of DateTime values that will lead to negative unix timestamp in their timezone (for example, 1970-01-01 00:00:00 in Moscow). Saturate to zero instead. This fixes #3470. This fixes #4172. #12443 (alexey-milovidov).AvroConfluent: Skip Kafka tombstone records - Support skipping broken records #13203 (Andrew Onyshchuk).Fix wrong error for long queries. It was possible to get syntax error other than Max query size exceeded for correct query. #13928 (Nikolai Kochetov).Fix data race in lgamma function. This race was caught only in tsan, no side effects really happened. #13842 (Nikolai Kochetov).Fix a 'Week'-interval formatting for ATTACH/ALTER/CREATE QUOTA-statements. #13417 (vladimir-golovchenko).Now broken parts are also reported when encountered in compact part processing. Compact parts is an experimental feature. #13282 (Amos Bird).Fix assert in geohashesInBox. This fixes #12554. #13229 (alexey-milovidov).Fix assert in parseDateTimeBestEffort. This fixes #12649. #13227 (alexey-milovidov).Minor optimization in Processors/PipelineExecutor: breaking out of a loop because it makes sense to do so. #13058 (Mark Papadakis).Support TRUNCATE table without TABLE keyword. #12653 (Winter Zhang).Fix explain query format overwrite by default. This fixes #12541. #12541 (BohuTANG).Allow to set JOIN kind and type in more standad way: LEFT SEMI JOIN instead of SEMI LEFT JOIN. For now both are correct. #12520 (Artem Zuikov).Changes default value for multiple_joins_rewriter_version to 2. It enables new multiple joins rewriter that knows about column names. #12469 (Artem Zuikov).Add several metrics for requests to S3 storages. #12464 (ianton-ru).Use correct default secure port for clickhouse-benchmark with --secure argument. This fixes #11044. #12440 (alexey-milovidov).Rollback insertion errors in Log, TinyLog, StripeLog engines. In previous versions insertion error lead to inconsisent table state (this works as documented and it is normal for these table engines). This fixes #12402. #12426 (alexey-milovidov).Implement RENAME DATABASE and RENAME DICTIONARY for Atomic database engine - Add implicit {uuid} macro, which can be used in ZooKeeper path for ReplicatedMergeTree. It works with CREATE ... ON CLUSTER ... queries. Set show_table_uuid_in_table_create_query_if_not_nil to true to use it. - Make ReplicatedMergeTree engine arguments optional, /clickhouse/tables/{uuid}/{shard}/ and {replica} are used by default. Closes #12135. - Minor fixes. - These changes break backward compatibility of Atomic database engine. Previously created Atomic databases must be manually converted to new format. Atomic database is an experimental feature. #12343 (tavplubix).Separated AWSAuthV4Signer into different logger, removed excessive AWSClient: AWSClient from log messages. #12320 (Vladimir Chebotarev).Better exception message in disk access storage. #12625 (alesapin).Better exception for function in with invalid number of arguments. #12529 (Anton Popov).Fix error message about adaptive granularity. #12624 (alesapin).Fix SETTINGS parse after FORMAT. #12480 (Azat Khuzhin).If MergeTree table does not contain ORDER BY or PARTITION BY, it was possible to request ALTER to CLEAR all the columns and ALTER will stuck. Fixed #7941. #12382 (alexey-milovidov).Avoid re-loading completion from the history file after each query (to avoid history overlaps with other client sessions). #13086 (Azat Khuzhin). Performance Improvement​ Lower memory usage for some operations up to 2 times. #12424 (alexey-milovidov).Optimize PK lookup for queries that match exact PK range. #12277 (Ivan Babrou).Slightly optimize very short queries with LowCardinality. #14129 (Anton Popov).Slightly improve performance of aggregation by UInt8/UInt16 keys. #13091 and #13055 (alexey-milovidov).Push down LIMIT step for query plan (inside subqueries). #13016 (Nikolai Kochetov).Parallel primary key lookup and skipping index stages on parts, as described in #11564. #12589 (Ivan Babrou).Converting String-type arguments of function &quot;if&quot; and &quot;transform&quot; into enum if set optimize_if_transform_strings_to_enum = 1. #12515 (Artem Zuikov).Replaces monotonic functions with its argument in ORDER BY if set optimize_monotonous_functions_in_order_by=1. #12467 (Artem Zuikov).Add order by optimization that rewrites ORDER BY x, f(x) with ORDER by x if set optimize_redundant_functions_in_order_by = 1. #12404 (Artem Zuikov).Allow pushdown predicate when subquery contains WITH clause. This fixes #12293 #12663 (Winter Zhang).Improve performance of reading from compact parts. Compact parts is an experimental feature. #12492 (Anton Popov).Attempt to implement streaming optimization in DiskS3. DiskS3 is an experimental feature. #12434 (Vladimir Chebotarev). Build/Testing/Packaging Improvement​ Use shellcheck for sh tests linting. #13200 #13207 (alexey-milovidov).Add script which set labels for pull requests in GitHub hook. #13183 (alesapin).Remove some of recursive submodules. See #13378. #13379 (alexey-milovidov).Ensure that all the submodules are from proper URLs. Continuation of #13379. This fixes #13378. #13397 (alexey-milovidov).Added support for user-declared settings, which can be accessed from inside queries. This is needed when ClickHouse engine is used as a component of another system. #13013 (Vitaly Baranov).Added testing for RBAC functionality of INSERT privilege in TestFlows. Expanded tables on which SELECT is being tested. Added Requirements to match new table engine tests. #13340 (MyroTk).Fix timeout error during server restart in the stress test. #13321 (alesapin).Now fast test will wait server with retries. #13284 (alesapin).Function materialize() (the function for ClickHouse testing) will work for NULL as expected - by transforming it to non-constant column. #13212 (alexey-milovidov).Fix libunwind build in AArch64. This fixes #13204. #13208 (alexey-milovidov).Even more retries in zkutil gtest to prevent test flakiness. #13165 (alexey-milovidov).Small fixes to the RBAC TestFlows. #13152 (vzakaznikov).Fixing 00960_live_view_watch_events_live.py test. #13108 (vzakaznikov).Improve cache purge in documentation deploy script. #13107 (alesapin).Rewrote some orphan tests to gtest. Removed useless includes from tests. #13073 (Nikita Mikhaylov).Added tests for RBAC functionality of SELECT privilege in TestFlows. #13061 (Ritaank Tiwari).Rerun some tests in fast test check. #12992 (alesapin).Fix MSan error in &quot;rdkafka&quot; library. This closes #12990. Updated rdkafka to version 1.5 (master). #12991 (alexey-milovidov).Fix UBSan report in base64 if tests were run on server with AVX-512. This fixes #12318. Author: @qoega. #12441 (alexey-milovidov).Fix UBSan report in HDFS library. This closes #12330. #12453 (alexey-milovidov).Check an ability that we able to restore the backup from an old version to the new version. This closes #8979. #12959 (alesapin).Do not build helper_container image inside integrational tests. Build docker container in CI and use pre-built helper_container in integration tests. #12953 (Ilya Yatsishin).Add a test for ALTER TABLE CLEAR COLUMN query for primary key columns. #12951 (alesapin).Increased timeouts in testflows tests. #12949 (vzakaznikov).Fix build of test under Mac OS X. This closes #12767. #12772 (alexey-milovidov).Connector-ODBC updated to mysql-connector-odbc-8.0.21. #12739 (Ilya Yatsishin).Adding RBAC syntax tests in TestFlows. #12642 (vzakaznikov).Improve performance of TestKeeper. This will speedup tests with heavy usage of Replicated tables. #12505 (alexey-milovidov).Now we check that server is able to start after stress tests run. This fixes #12473. #12496 (alesapin).Update fmtlib to master (7.0.1). #12446 (alexey-milovidov).Add docker image for fast tests. #12294 (alesapin).Rework configuration paths for integration tests. #12285 (Ilya Yatsishin).Add compiler option to control that stack frames are not too large. This will help to run the code in fibers with small stack size. #11524 (alexey-milovidov).Update gitignore-files. #13447 (vladimir-golovchenko). "},{"title":"ClickHouse release 20.6​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-206","content":""},{"title":"ClickHouse release v20.6.3.28-stable​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v206328-stable","content":"Backward Incompatible Change​ When upgrading from versions older than 20.5, if rolling update is performed and cluster contains both versions 20.5 or greater and less than 20.5, if ClickHouse nodes with old versions are restarted and old version has been started up in presence of newer versions, it may lead to Part ... intersects previous part errors. To prevent this error, first install newer clickhouse-server packages on all cluster nodes and then do restarts (so, when clickhouse-server is restarted, it will start up with the new version). New Feature​ Added an initial implementation of EXPLAIN query. Syntax: EXPLAIN SELECT .... This fixes #1118. #11873 (Nikolai Kochetov).Added storage RabbitMQ. #11069 (Kseniia Sumarokova).Implemented PostgreSQL-like ILIKE operator for #11710. #12125 (Mike).Supported RIGHT and FULL JOIN with SET join_algorithm = 'partial_merge'. Only ALL strictness is allowed (ANY, SEMI, ANTI, ASOF are not). #12118 (Artem Zuikov).Added a function initializeAggregation to initialize an aggregation based on a single value. #12109 (Guillaume Tassery).Supported ALTER TABLE ... [ADD|MODIFY] COLUMN ... FIRST #4006. #12073 (Winter Zhang).Added function parseDateTimeBestEffortUS. #12028 (flynn).Support format ORC for output (was supported only for input). #11662 (Kruglov Pavel). Bug Fix​ Fixed aggregate function any(x) is found inside another aggregate function in query error with SET optimize_move_functions_out_of_any = 1 and aliases inside any(). #13419 (Artem Zuikov).Fixed PrettyCompactMonoBlock for clickhouse-local. Fixed extremes/totals with PrettyCompactMonoBlock. This fixes #7746. #13394 (Azat Khuzhin).Fixed possible error Totals having transform was already added to pipeline in case of a query from delayed replica. #13290 (Nikolai Kochetov).The server may crash if user passed specifically crafted arguments to the function h3ToChildren. This fixes #13275. #13277 (alexey-milovidov).Fixed potentially low performance and slightly incorrect result for uniqExact, topK, sumDistinct and similar aggregate functions called on Float types with NaN values. It also triggered assert in debug build. This fixes #12491. #13254 (alexey-milovidov).Fixed function if with nullable constexpr as cond that is not literal NULL. Fixes #12463. #13226 (alexey-milovidov).Fixed assert in arrayElement function in case of array elements are Nullable and array subscript is also Nullable. This fixes #12172. #13224 (alexey-milovidov).Fixed DateTime64 conversion functions with constant argument. #13205 (Azat Khuzhin).Fixed wrong index analysis with functions. It could lead to pruning wrong parts, while reading from MergeTree tables. Fixes #13060. Fixes #12406. #13081 (Anton Popov).Fixed error Cannot convert column because it is constant but values of constants are different in source and result for remote queries which use deterministic functions in scope of query, but not deterministic between queries, like now(), now64(), randConstant(). Fixes #11327. #13075 (Nikolai Kochetov).Fixed unnecessary limiting for the number of threads for selects from local replica. #12840 (Nikolai Kochetov).Fixed rare bug when ALTER DELETE and ALTER MODIFY COLUMN queries executed simultaneously as a single mutation. Bug leads to an incorrect amount of rows in count.txt and as a consequence incorrect data in part. Also, fix a small bug with simultaneous ALTER RENAME COLUMN and ALTER ADD COLUMN. #12760 (alesapin).Fixed CAST(Nullable(String), Enum()). #12745 (Azat Khuzhin).Fixed a performance with large tuples, which are interpreted as functions in IN section. The case when user write WHERE x IN tuple(1, 2, ...) instead of WHERE x IN (1, 2, ...) for some obscure reason. #12700 (Anton Popov).Fixed memory tracking for input_format_parallel_parsing (by attaching thread to group). #12672 (Azat Khuzhin).Fixed bloom filter index with const expression. This fixes #10572. #12659 (Winter Zhang).Fixed SIGSEGV in StorageKafka when broker is unavailable (and not only). #12658 (Azat Khuzhin).Added support for function if with Array(UUID) arguments. This fixes #11066. #12648 (alexey-milovidov).CREATE USER IF NOT EXISTS now does not throw exception if the user exists. This fixes #12507. #12646 (Vitaly Baranov).Better exception message in disk access storage. #12625 (alesapin).The function groupArrayMoving* was not working for distributed queries. It's result was calculated within incorrect data type (without promotion to the largest type). The function groupArrayMovingAvg was returning integer number that was inconsistent with the avg function. This fixes #12568. #12622 (alexey-milovidov).Fixed lack of aliases with function any. #12593 (Anton Popov).Fixed race condition in external dictionaries with cache layout which can lead server crash. #12566 (alesapin).Remove data for Distributed tables (blocks from async INSERTs) on DROP TABLE. #12556 (Azat Khuzhin).Fixed bug which lead to broken old parts after ALTER DELETE query when enable_mixed_granularity_parts=1. Fixes #12536. #12543 (alesapin).Better exception for function in with invalid number of arguments. #12529 (Anton Popov).Fixing race condition in live view tables which could cause data duplication. #12519 (vzakaznikov).Fixed performance issue, while reading from compact parts. #12492 (Anton Popov).Fixed backwards compatibility in binary format of AggregateFunction(avg, ...) values. This fixes #12342. #12486 (alexey-milovidov).Fixed SETTINGS parse after FORMAT. #12480 (Azat Khuzhin).Fixed the deadlock if text_log is enabled. #12452 (alexey-milovidov).Fixed overflow when very large LIMIT or OFFSET is specified. This fixes #10470. This fixes #11372. #12427 (alexey-milovidov).Fixed possible segfault if StorageMerge. This fixes #12054. #12401 (tavplubix).Reverted change introduced in #11079 to resolve #12098. #12397 (Mike).Additional check for arguments of bloom filter index. This fixes #11408. #12388 (alexey-milovidov).Avoid exception when negative or floating point constant is used in WHERE condition for indexed tables. This fixes #11905. #12384 (alexey-milovidov).Allowed to CLEAR column even if there are depending DEFAULT expressions. This fixes #12333. #12378 (alexey-milovidov).Fix TOTALS/ROLLUP/CUBE for aggregate functions with -State and Nullable arguments. This fixes #12163. #12376 (alexey-milovidov).Fixed error message and exit codes for ALTER RENAME COLUMN queries, when RENAME is not allowed. Fixes #12301 and #12303. #12335 (alesapin).Fixed very rare race condition in ReplicatedMergeTreeQueue. #12315 (alexey-milovidov).When using codec Delta or DoubleDelta with non fixed width types, exception with code LOGICAL_ERROR was returned instead of exception with code BAD_ARGUMENTS (we ensure that exceptions with code logical error never happen). This fixes #12110. #12308 (alexey-milovidov).Fixed order of columns in WITH FILL modifier. Previously order of columns of ORDER BY statement wasn't respected. #12306 (Anton Popov).Avoid &quot;bad cast&quot; exception when there is an expression that filters data by virtual columns (like _table in Merge tables) or by &quot;index&quot; columns in system tables such as filtering by database name when querying from system.tables, and this expression returns Nullable type. This fixes #12166. #12305 (alexey-milovidov).Fixed TTL after renaming column, on which depends TTL expression. #12304 (Anton Popov).Fixed SIGSEGV if there is an message with error in the middle of the batch in Kafka Engine. #12302 (Azat Khuzhin).Fixed the situation when some threads might randomly hang for a few seconds during DNS cache updating. #12296 (tavplubix).Fixed typo in setting name. #12292 (alexey-milovidov).Show error after TrieDictionary failed to load. #12290 (Vitaly Baranov).The function arrayFill worked incorrectly for empty arrays that may lead to crash. This fixes #12263. #12279 (alexey-milovidov).Implement conversions to the common type for LowCardinality types. This allows to execute UNION ALL of tables with columns of LowCardinality and other columns. This fixes #8212. This fixes #4342. #12275 (alexey-milovidov).Fixed the behaviour on reaching redirect limit in request to S3 storage. #12256 (ianton-ru).Fixed the behaviour when during multiple sequential inserts in StorageFile header for some special types was written more than once. This fixed #6155. #12197 (Nikita Mikhaylov).Fixed logical functions for UInt8 values when they are not equal to 0 or 1. #12196 (Alexander Kazakov).Cap max_memory_usage* limits to the process resident memory. #12182 (Azat Khuzhin).Fix dictGet arguments check during GROUP BY injective functions elimination. #12179 (Azat Khuzhin).Fixed the behaviour when SummingMergeTree engine sums up columns from partition key. Added an exception in case of explicit definition of columns to sum which intersects with partition key columns. This fixes #7867. #12173 (Nikita Mikhaylov).Don't split the dictionary source's table name into schema and table name itself if ODBC connection does not support schema. #12165 (Vitaly Baranov).Fixed wrong logic in ALTER DELETE that leads to deleting of records when condition evaluates to NULL. This fixes #9088. This closes #12106. #12153 (alexey-milovidov).Fixed transform of query to send to external DBMS (e.g. MySQL, ODBC) in presense of aliases. This fixes #12032. #12151 (alexey-milovidov).Fixed bad code in redundant ORDER BY optimization. The bug was introduced in #10067. #12148 (alexey-milovidov).Fixed potential overflow in integer division. This fixes #12119. #12140 (alexey-milovidov).Fixed potential infinite loop in greatCircleDistance, geoDistance. This fixes #12117. #12137 (alexey-milovidov).Normalize &quot;pid&quot; file handling. In previous versions the server may refuse to start if it was killed without proper shutdown and if there is another process that has the same pid as previously runned server. Also pid file may be removed in unsuccessful server startup even if there is another server running. This fixes #3501. #12133 (alexey-milovidov).Fixed bug which leads to incorrect table metadata in ZooKeepeer for ReplicatedVersionedCollapsingMergeTree tables. Fixes #12093. #12121 (alesapin).Avoid &quot;There is no query&quot; exception for materialized views with joins or with subqueries attached to system logs (system.query_log, metric_log, etc) or to engine=Buffer underlying table. #12120 (filimonov).Fixed handling dependency of table with ENGINE=Dictionary on dictionary. This fixes #10994. This fixes #10397. #12116 (Vitaly Baranov).Format Parquet now properly works with LowCardinality and LowCardinality(Nullable) types. Fixes #12086, #8406. #12108 (Nikolai Kochetov).Fixed performance for selects with UNION caused by wrong limit for the total number of threads. Fixes #12030. #12103 (Nikolai Kochetov).Fixed segfault with -StateResample combinators. #12092 (Anton Popov).Fixed empty result_rows and result_bytes metrics in system.quey_log for selects. Fixes #11595. #12089 (Nikolai Kochetov).Fixed unnecessary limiting the number of threads for selects from VIEW. Fixes #11937. #12085 (Nikolai Kochetov).Fixed SIGSEGV in StorageKafka on DROP TABLE. #12075 (Azat Khuzhin).Fixed possible crash while using wrong type for PREWHERE. Fixes #12053, #12060. #12060 (Nikolai Kochetov).Fixed error Cannot capture column for higher-order functions with Tuple(LowCardinality) argument. Fixes #9766. #12055 (Nikolai Kochetov).Fixed constraints check if constraint is a constant expression. This fixes #11360. #12042 (alexey-milovidov).Fixed wrong result and potential crash when invoking function if with arguments of type FixedString with different sizes. This fixes #11362. #12021 (alexey-milovidov). Improvement​ Allowed to set JOIN kind and type in more standard way: LEFT SEMI JOIN instead of SEMI LEFT JOIN. For now both are correct. #12520 (Artem Zuikov).lifetime_rows/lifetime_bytes for Buffer engine. #12421 (Azat Khuzhin).Write the detail exception message to the client instead of 'MySQL server has gone away'. #12383 (BohuTANG).Allows to change a charset which is used for printing grids borders. Available charsets are following: UTF-8, ASCII. Setting output_format_pretty_grid_charset enables this feature. #12372 (Sabyanin Maxim).Supported MySQL 'SELECT DATABASE()' #9336 2. Add MySQL replacement query integration test. #12314 (BohuTANG).Added KILL QUERY [connection_id] for the MySQL client/driver to cancel the long query, issue #12038. #12152 (BohuTANG).Added support for %g (two digit ISO year) and %G (four digit ISO year) substitutions in formatDateTime function. #12136 (vivarum).Added 'type' column in system.disks. #12115 (ianton-ru).Improved REVOKE command: now it requires grant/admin option for only access which will be revoked. For example, to execute REVOKE ALL ON *.* FROM user1 now it does not require to have full access rights granted with grant option. Added command REVOKE ALL FROM user1 - it revokes all granted roles from user1. #12083 (Vitaly Baranov).Added replica priority for load_balancing (for manual prioritization of the load balancing). #11995 (Azat Khuzhin).Switched paths in S3 metadata to relative which allows to handle S3 blobs more easily. #11892 (Vladimir Chebotarev). Performance Improvement​ Improved performace of 'ORDER BY' and 'GROUP BY' by prefix of sorting key (enabled with optimize_aggregation_in_order setting, disabled by default). #11696 (Anton Popov).Removed injective functions inside uniq*() if set optimize_injective_functions_inside_uniq=1. #12337 (Ruslan Kamalov).Index not used for IN operator with literals, performance regression introduced around v19.3. This fixes #10574. #12062 (nvartolomei).Implemented single part uploads for DiskS3 (experimental feature). #12026 (Vladimir Chebotarev). Experimental Feature​ Added new in-memory format of parts in MergeTree-family tables, which stores data in memory. Parts are written on disk at first merge. Part will be created in in-memory format if its size in rows or bytes is below thresholds min_rows_for_compact_part and min_bytes_for_compact_part. Also optional support of Write-Ahead-Log is available, which is enabled by default and is controlled by setting in_memory_parts_enable_wal. #10697 (Anton Popov). Build/Testing/Packaging Improvement​ Implement AST-based query fuzzing mode for clickhouse-client. See this label for the list of issues we recently found by fuzzing. Most of them were found by this tool, and a couple by SQLancer and 00746_sql_fuzzy.pl. #12111 (Alexander Kuzmenkov).Add new type of tests based on Testflows framework. #12090 (vzakaznikov).Added S3 HTTPS integration test. #12412 (Pavel Kovalenko).Log sanitizer trap messages from separate thread. This will prevent possible deadlock under thread sanitizer. #12313 (alexey-milovidov).Now functional and stress tests will be able to run with old version of clickhouse-test script. #12287 (alesapin).Remove strange file creation during build in orc. #12258 (Nikita Mikhaylov).Place common docker compose files to integration docker container. #12168 (Ilya Yatsishin).Fix warnings from CodeQL. CodeQL is another static analyzer that we will use along with clang-tidy and PVS-Studio that we use already. #12138 (alexey-milovidov).Minor CMake fixes for UNBUNDLED build. #12131 (Matwey V. Kornilov).Added a showcase of the minimal Docker image without using any Linux distribution. #12126 (alexey-milovidov).Perform an upgrade of system packages in the clickhouse-server docker image. #12124 (Ivan Blinkov).Add UNBUNDLED flag to system.build_options table. Move skip lists for clickhouse-test to clickhouse repo. #12107 (alesapin).Regular check by Anchore Container Analysis security analysis tool that looks for CVE in clickhouse-server Docker image. Also confirms that Dockerfile is buildable. Runs daily on master and on pull-requests to Dockerfile. #12102 (Ivan Blinkov).Daily check by GitHub CodeQL security analysis tool that looks for CWE. #12101 (Ivan Blinkov).Install ca-certificates before the first apt-get update in Dockerfile. #12095 (Ivan Blinkov). "},{"title":"ClickHouse release 20.5​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-205","content":""},{"title":"ClickHouse release v20.5.4.40-stable 2020-08-10​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v205440-stable-2020-08-10","content":"Bug Fix​ Fixed wrong index analysis with functions. It could lead to pruning wrong parts, while reading from MergeTree tables. Fixes #13060. Fixes #12406. #13081 (Anton Popov).Fixed unnecessary limiting for the number of threads for selects from local replica. #12840 (Nikolai Kochetov).Fixed performance with large tuples, which are interpreted as functions in IN section. The case when user write WHERE x IN tuple(1, 2, ...) instead of WHERE x IN (1, 2, ...) for some obscure reason. #12700 (Anton Popov).Fixed memory tracking for input_format_parallel_parsing (by attaching thread to group). #12672 (Azat Khuzhin).Fixed bloom filter index with const expression. This fixes #10572. #12659 (Winter Zhang).Fixed SIGSEGV in StorageKafka when broker is unavailable (and not only). #12658 (Azat Khuzhin).Added support for function if with Array(UUID) arguments. This fixes #11066. #12648 (alexey-milovidov).Fixed lack of aliases with function any. #12593 (Anton Popov).Fixed race condition in external dictionaries with cache layout which can lead server crash. #12566 (alesapin).Remove data for Distributed tables (blocks from async INSERTs) on DROP TABLE. #12556 (Azat Khuzhin).Fixed bug which lead to broken old parts after ALTER DELETE query when enable_mixed_granularity_parts=1. Fixes #12536. #12543 (alesapin).Better exception for function in with invalid number of arguments. #12529 (Anton Popov).Fixed race condition in live view tables which could cause data duplication. #12519 (vzakaznikov).Fixed performance issue, while reading from compact parts. #12492 (Anton Popov).Fixed backwards compatibility in binary format of AggregateFunction(avg, ...) values. This fixes #12342. #12486 (alexey-milovidov).Fixed the deadlock if text_log is enabled. #12452 (alexey-milovidov).Fixed overflow when very large LIMIT or OFFSET is specified. This fixes #10470. This fixes #11372. #12427 (alexey-milovidov).Fixed possible segfault if StorageMerge. Closes #12054. #12401 (tavplubix).Reverts change introduced in #11079 to resolve #12098. #12397 (Mike).Avoid exception when negative or floating point constant is used in WHERE condition for indexed tables. This fixes #11905. #12384 (alexey-milovidov).Allow to CLEAR column even if there are depending DEFAULT expressions. This fixes #12333. #12378 (alexey-milovidov).Fixed TOTALS/ROLLUP/CUBE for aggregate functions with -State and Nullable arguments. This fixes #12163. #12376 (alexey-milovidov).Fixed SIGSEGV if there is an message with error in the middle of the batch in Kafka Engine. #12302 (Azat Khuzhin).Fixed the behaviour when SummingMergeTree engine sums up columns from partition key. Added an exception in case of explicit definition of columns to sum which intersects with partition key columns. This fixes #7867. #12173 (Nikita Mikhaylov).Fixed transform of query to send to external DBMS (e.g. MySQL, ODBC) in presense of aliases. This fixes #12032. #12151 (alexey-milovidov).Fixed bug which leads to incorrect table metadata in ZooKeepeer for ReplicatedVersionedCollapsingMergeTree tables. Fixes #12093. #12121 (alesapin).Fixed unnecessary limiting the number of threads for selects from VIEW. Fixes #11937. #12085 (Nikolai Kochetov).Fixed crash in JOIN with LowCardinality type with join_algorithm=partial_merge. #12035 (Artem Zuikov).Fixed wrong result for if() with NULLs in condition. #11807 (Artem Zuikov). Performance Improvement​ Index not used for IN operator with literals, performance regression introduced around v19.3. This fixes #10574. #12062 (nvartolomei). Build/Testing/Packaging Improvement​ Install ca-certificates before the first apt-get update in Dockerfile. #12095 (Ivan Blinkov). "},{"title":"ClickHouse release v20.5.2.7-stable 2020-07-02​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v20527-stable-2020-07-02","content":"Backward Incompatible Change​ Return non-Nullable result from COUNT(DISTINCT), and uniq aggregate functions family. If all passed values are NULL, return zero instead. This improves SQL compatibility. #11661 (alexey-milovidov).Added a check for the case when user-level setting is specified in a wrong place. User-level settings should be specified in users.xml inside &lt;profile&gt; section for specific user profile (or in &lt;default&gt; for default settings). The server won't start with exception message in log. This fixes #9051. If you want to skip the check, you can either move settings to the appropriate place or add &lt;skip_check_for_incorrect_settings&gt;1&lt;/skip_check_for_incorrect_settings&gt; to config.xml. #11449 (alexey-milovidov).The setting input_format_with_names_use_header is enabled by default. It will affect parsing of input formats -WithNames and -WithNamesAndTypes. #10937 (alexey-milovidov).Remove experimental_use_processors setting. It is enabled by default. #10924 (Nikolai Kochetov).Update zstd to 1.4.4. It has some minor improvements in performance and compression ratio. If you run replicas with different versions of ClickHouse you may see reasonable error messages Data after merge is not byte-identical to data on another replicas. with explanation. These messages are Ok and you should not worry. This change is backward compatible but we list it here in changelog in case you will wonder about these messages. #10663 (alexey-milovidov).Added a check for meaningless codecs and a setting allow_suspicious_codecs to control this check. This closes #4966. #10645 (alexey-milovidov).Several Kafka setting changes their defaults. See #11388.When upgrading from versions older than 20.5, if rolling update is performed and cluster contains both versions 20.5 or greater and less than 20.5, if ClickHouse nodes with old versions are restarted and old version has been started up in presence of newer versions, it may lead to Part ... intersects previous part errors. To prevent this error, first install newer clickhouse-server packages on all cluster nodes and then do restarts (so, when clickhouse-server is restarted, it will start up with the new version). New Feature​ TTL DELETE WHERE and TTL GROUP BY for automatic data coarsening and rollup in tables. #10537 (expl0si0nn).Implementation of PostgreSQL wire protocol. #10242 (Movses).Added system tables for users, roles, grants, settings profiles, quotas, row policies; added commands SHOW USER, SHOW [CURRENT|ENABLED] ROLES, SHOW SETTINGS PROFILES. #10387 (Vitaly Baranov).Support writes in ODBC Table function #10554 (ageraab). #10901 (tavplubix).Add query performance metrics based on Linux perf_events (these metrics are calculated with hardware CPU counters and OS counters). It is optional and requires CAP_SYS_ADMIN to be set on clickhouse binary. #9545 Andrey Skobtsov. #11226 (Alexander Kuzmenkov).Now support NULL and NOT NULL modifiers for data types in CREATE query. #11057 (Павел Потемкин).Add ArrowStream input and output format. #11088 (hcz).Support Cassandra as external dictionary source. #4978 (favstovol).Added a new layout direct which loads all the data directly from the source for each query, without storing or caching data. #10622 (Artem Streltsov).Added new complex_key_direct layout to dictionaries, that does not store anything locally during query execution. #10850 (Artem Streltsov).Added support for MySQL style global variables syntax (stub). This is needed for compatibility of MySQL protocol. #11832 (alexey-milovidov).Added syntax highligting to clickhouse-client using replxx. #11422 (Tagir Kuskarov).minMap and maxMap functions were added. #11603 (Ildus Kurbangaliev).Add the system.asynchronous_metric_log table that logs historical metrics from system.asynchronous_metrics. #11588 (Alexander Kuzmenkov).Add functions extractAllGroupsHorizontal(haystack, re) and extractAllGroupsVertical(haystack, re). #11554 (Vasily Nemkov).Add SHOW CLUSTER(S) queries. #11467 (hexiaoting).Add netloc function for extracting network location, similar to urlparse(url), netloc in python. #11356 (Guillaume Tassery).Add 2 more virtual columns for engine=Kafka to access message headers. #11283 (filimonov).Add _timestamp_ms virtual column for Kafka engine (type is Nullable(DateTime64(3))). #11260 (filimonov).Add function randomFixedString. #10866 (Andrei Nekrashevich).Add function fuzzBits that randomly flips bits in a string with given probability. #11237 (Andrei Nekrashevich).Allow comparison of numbers with constant string in comparison operators, IN and VALUES sections. #11647 (alexey-milovidov).Add round_robin load_balancing mode. #11645 (Azat Khuzhin).Add cast_keep_nullable setting. If set CAST(something_nullable AS Type) return Nullable(Type). #11733 (Artem Zuikov).Added column position to system.columns table and column_position to system.parts_columns table. It contains ordinal position of a column in a table starting with 1. This closes #7744. #11655 (alexey-milovidov).ON CLUSTER support for SYSTEM {FLUSH DISTRIBUTED,STOP/START DISTRIBUTED SEND}. #11415 (Azat Khuzhin).Add system.distribution_queue table. #11394 (Azat Khuzhin).Support for all format settings in Kafka, expose some setting on table level, adjust the defaults for better performance. #11388 (filimonov).Add port function (to extract port from URL). #11120 (Azat Khuzhin).Now dictGet* functions accept table names. #11050 (Vitaly Baranov).The clickhouse-format tool is now able to format multiple queries when the -n argument is used. #10852 (Darío).Possibility to configure proxy-resolver for DiskS3. #10744 (Pavel Kovalenko).Make pointInPolygon work with non-constant polygon. PointInPolygon now can take Array(Array(Tuple(..., ...))) as second argument, array of polygon and holes. #10623 (Alexey Ilyukhov) #11421 (Alexey Ilyukhov).Added move_ttl_info to system.parts in order to provide introspection of move TTL functionality. #10591 (Vladimir Chebotarev).Possibility to work with S3 through proxies. #10576 (Pavel Kovalenko).Add NCHAR and NVARCHAR synonims for data types. #11025 (alexey-milovidov).Resolved #7224: added FailedQuery, FailedSelectQuery and FailedInsertQuery metrics to system.events table. #11151 (Nikita Orlov).Add more jemalloc statistics to system.asynchronous_metrics, and ensure that we see up-to-date values for them. #11748 (Alexander Kuzmenkov).Allow to specify default S3 credentials and custom auth headers. #11134 (Grigory Pervakov).Added new functions to import/export DateTime64 as Int64 with various precision: to-/fromUnixTimestamp64Milli/-Micro/-Nano. #10923 (Vasily Nemkov).Allow specifying mongodb:// URI for MongoDB dictionaries. #10915 (Alexander Kuzmenkov).OFFSET keyword can now be used without an affiliated LIMIT clause. #10802 (Guillaume Tassery).Added system.licenses table. This table contains licenses of third-party libraries that are located in contrib directory. This closes #2890. #10795 (alexey-milovidov).New function function toStartOfSecond(DateTime64) -&gt; DateTime64 that nullifies sub-second part of DateTime64 value. #10722 (Vasily Nemkov).Add new input format JSONAsString that accepts a sequence of JSON objects separated by newlines, spaces and/or commas. #10607 (Kruglov Pavel).Allowed to profile memory with finer granularity steps than 4 MiB. Added sampling memory profiler to capture random allocations/deallocations. #10598 (alexey-milovidov).SimpleAggregateFunction now also supports sumMap. #10000 (Ildus Kurbangaliev).Support ALTER RENAME COLUMN for the distributed table engine. Continuation of #10727. Fixes #10747. #10887 (alesapin). Bug Fix​ Fix UBSan report in Decimal parse. This fixes #7540. #10512 (alexey-milovidov).Fix potential floating point exception when parsing DateTime64. This fixes #11374. #11875 (alexey-milovidov).Fix rare crash caused by using Nullable column in prewhere condition. #11895 #11608 #11869 (Nikolai Kochetov).Don't allow arrayJoin inside higher order functions. It was leading to broken protocol synchronization. This closes #3933. #11846 (alexey-milovidov).Fix wrong result of comparison of FixedString with constant String. This fixes #11393. This bug appeared in version 20.4. #11828 (alexey-milovidov).Fix wrong result for if with NULLs in condition. #11807 (Artem Zuikov).Fix using too many threads for queries. #11788 (Nikolai Kochetov).Fixed Scalar does not exist exception when using WITH &lt;scalar subquery&gt; ... in SELECT ... FROM merge_tree_table ... #11621. #11767 (Amos Bird).Fix unexpected behaviour of queries like SELECT *, xyz.* which were success while an error expected. #11753 (hexiaoting).Now replicated fetches will be cancelled during metadata alter. #11744 (alesapin).Parse metadata stored in zookeeper before checking for equality. #11739 (Azat Khuzhin).Fixed LOGICAL_ERROR caused by wrong type deduction of complex literals in Values input format. #11732 (tavplubix).Fix ORDER BY ... WITH FILL over const columns. #11697 (Anton Popov).Fix very rare race condition in SYSTEM SYNC REPLICA. If the replicated table is created and at the same time from the separate connection another client is issuing SYSTEM SYNC REPLICA command on that table (this is unlikely, because another client should be aware that the table is created), it's possible to get nullptr dereference. #11691 (alexey-milovidov).Pass proper timeouts when communicating with XDBC bridge. Recently timeouts were not respected when checking bridge liveness and receiving meta info. #11690 (alexey-milovidov).Fix LIMIT n WITH TIES usage together with ORDER BY statement, which contains aliases. #11689 (Anton Popov).Fix possible Pipeline stuck for selects with parallel FINAL. Fixes #11636. #11682 (Nikolai Kochetov).Fix error which leads to an incorrect state of system.mutations. It may show that whole mutation is already done but the server still has MUTATE_PART tasks in the replication queue and tries to execute them. This fixes #11611. #11681 (alesapin).Fix syntax hilite in CREATE USER query. #11664 (alexey-milovidov).Add support for regular expressions with case-insensitive flags. This fixes #11101 and fixes #11506. #11649 (alexey-milovidov).Remove trivial count query optimization if row-level security is set. In previous versions the user get total count of records in a table instead filtered. This fixes #11352. #11644 (alexey-milovidov).Fix bloom filters for String (data skipping indices). #11638 (Azat Khuzhin).Without -q option the database does not get created at startup. #11604 (giordyb).Fix error Block structure mismatch for queries with sampling reading from Buffer table. #11602 (Nikolai Kochetov).Fix wrong exit code of the clickhouse-client, when exception.code() % 256 == 0. #11601 (filimonov).Fix race conditions in CREATE/DROP of different replicas of ReplicatedMergeTree. Continue to work if the table was not removed completely from ZooKeeper or not created successfully. This fixes #11432. #11592 (alexey-milovidov).Fix trivial error in log message about &quot;Mark cache size was lowered&quot; at server startup. This closes #11399. #11589 (alexey-milovidov).Fix error Size of offsets does not match size of column for queries with PREWHERE column in (subquery) and ARRAY JOIN. #11580 (Nikolai Kochetov).Fixed rare segfault in SHOW CREATE TABLE Fixes #11490. #11579 (tavplubix).All queries in HTTP session have had the same query_id. It is fixed. #11578 (tavplubix).Now clickhouse-server docker container will prefer IPv6 checking server aliveness. #11550 (Ivan Starkov).Fix the error Data compressed with different methods that can happen if min_bytes_to_use_direct_io is enabled and PREWHERE is active and using SAMPLE or high number of threads. This fixes #11539. #11540 (alexey-milovidov).Fix shard_num/replica_num for &lt;node&gt; (breaks use_compact_format_in_distributed_parts_names). #11528 (Azat Khuzhin).Fix async INSERT into Distributed for prefer_localhost_replica=0 and w/o internal_replication. #11527 (Azat Khuzhin).Fix memory leak when exception is thrown in the middle of aggregation with -State functions. This fixes #8995. #11496 (alexey-milovidov).Fix Pipeline stuck exception for INSERT SELECT FINAL where SELECT (max_threads&gt;1) has multiple streams but INSERT has only one (max_insert_threads==0). #11455 (Azat Khuzhin).Fix wrong result in queries like select count() from t, u. #11454 (Artem Zuikov).Fix return compressed size for codecs. #11448 (Nikolai Kochetov).Fix server crash when a column has compression codec with non-literal arguments. Fixes #11365. #11431 (alesapin).Fix potential uninitialized memory read in MergeTree shutdown if table was not created successfully. #11420 (alexey-milovidov).Fix crash in JOIN over LowCarinality(T) and Nullable(T). #11380. #11414 (Artem Zuikov).Fix error code for wrong USING key. #11373. #11404 (Artem Zuikov).Fixed geohashesInBox with arguments outside of latitude/longitude range. #11403 (Vasily Nemkov).Better errors for joinGet() functions. #11389 (Artem Zuikov).Fix possible Pipeline stuck error for queries with external sort and limit. Fixes #11359. #11366 (Nikolai Kochetov).Remove redundant lock during parts send in ReplicatedMergeTree. #11354 (alesapin).Fix support for \\G (vertical output) in clickhouse-client in multiline mode. This closes #9933. #11350 (alexey-milovidov).Fix potential segfault when using Lazy database. #11348 (alexey-milovidov).Fix crash in direct selects from Join table engine (without JOIN) and wrong nullability. #11340 (Artem Zuikov).Fix crash in quantilesExactWeightedArray. #11337 (Nikolai Kochetov).Now merges stopped before change metadata in ALTER queries. #11335 (alesapin).Make writing to MATERIALIZED VIEW with setting parallel_view_processing = 1 parallel again. Fixes #10241. #11330 (Nikolai Kochetov).Fix visitParamExtractRaw when extracted JSON has strings with unbalanced { or [. #11318 (Ewout).Fix very rare race condition in ThreadPool. #11314 (alexey-milovidov).Fix insignificant data race in clickhouse-copier. Found by integration tests. #11313 (alexey-milovidov).Fix potential uninitialized memory in conversion. Example: SELECT toIntervalSecond(now64()). #11311 (alexey-milovidov).Fix the issue when index analysis cannot work if a table has Array column in primary key and if a query is filtering by this column with empty or notEmpty functions. This fixes #11286. #11303 (alexey-milovidov).Fix bug when query speed estimation can be incorrect and the limit of min_execution_speed may not work or work incorrectly if the query is throttled by max_network_bandwidth, max_execution_speed or priority settings. Change the default value of timeout_before_checking_execution_speed to non-zero, because otherwise the settings min_execution_speed and max_execution_speed have no effect. This fixes #11297. This fixes #5732. This fixes #6228. Usability improvement: avoid concatenation of exception message with progress bar in clickhouse-client. #11296 (alexey-milovidov).Fix crash when SET DEFAULT ROLE is called with wrong arguments. This fixes #10586. #11278 (Vitaly Baranov).Fix crash while reading malformed data in Protobuf format. This fixes #5957, fixes #11203. #11258 (Vitaly Baranov).Fixed a bug when cache dictionary could return default value instead of normal (when there are only expired keys). This affects only string fields. #11233 (Nikita Mikhaylov).Fix error Block structure mismatch in QueryPipeline while reading from VIEW with constants in inner query. Fixes #11181. #11205 (Nikolai Kochetov).Fix possible exception Invalid status for associated output. #11200 (Nikolai Kochetov).Now primary.idx will be checked if it's defined in CREATE query. #11199 (alesapin).Fix possible error Cannot capture column for higher-order functions with Array(Array(LowCardinality)) captured argument. #11185 (Nikolai Kochetov).Fixed S3 globbing which could fail in case of more than 1000 keys and some backends. #11179 (Vladimir Chebotarev).If data skipping index is dependent on columns that are going to be modified during background merge (for SummingMergeTree, AggregatingMergeTree as well as for TTL GROUP BY), it was calculated incorrectly. This issue is fixed by moving index calculation after merge so the index is calculated on merged data. #11162 (Azat Khuzhin).Fix for the hang which was happening sometimes during DROP of table engine=Kafka (or during server restarts). #11145 (filimonov).Fix excessive reserving of threads for simple queries (optimization for reducing the number of threads, which was partly broken after changes in pipeline). #11114 (Azat Khuzhin).Remove logging from mutation finalization task if nothing was finalized. #11109 (alesapin).Fixed deadlock during server startup after update with changes in structure of system log tables. #11106 (alesapin).Fixed memory leak in registerDiskS3. #11074 (Pavel Kovalenko).Fix error No such name in Block::erase() when JOIN appears with PREWHERE or optimize_move_to_prewhere makes PREWHERE from WHERE. #11051 (Artem Zuikov).Fixes the potential missed data during termination of Kafka engine table. #11048 (filimonov).Fixed parseDateTime64BestEffort argument resolution bugs. #10925. #11038 (Vasily Nemkov).Now it's possible to ADD/DROP and RENAME the same one column in a single ALTER query. Exception message for simultaneous MODIFY and RENAME became more clear. Partially fixes #10669. #11037 (alesapin).Fixed parsing of S3 URLs. #11036 (Vladimir Chebotarev).Fix memory tracking for two-level GROUP BY when there is a LIMIT. #11022 (Azat Khuzhin).Fix very rare potential use-after-free error in MergeTree if table was not created successfully. #10986 (alexey-milovidov).Fix metadata (relative path for rename) and data (relative path for symlink) handling for Atomic database. #10980 (Azat Khuzhin).Fix server crash on concurrent ALTER and DROP DATABASE queries with Atomic database engine. #10968 (tavplubix).Fix incorrect raw data size in method getRawData(). #10964 (Igr).Fix incompatibility of two-level aggregation between versions 20.1 and earlier. This incompatibility happens when different versions of ClickHouse are used on initiator node and remote nodes and the size of GROUP BY result is large and aggregation is performed by a single String field. It leads to several unmerged rows for a single key in result. #10952 (alexey-milovidov).Avoid sending partially written files by the DistributedBlockOutputStream. #10940 (Azat Khuzhin).Fix crash in SELECT count(notNullIn(NULL, [])). #10920 (Nikolai Kochetov).Fix for the hang which was happening sometimes during DROP of table engine=Kafka (or during server restarts). #10910 (filimonov).Now it's possible to execute multiple ALTER RENAME like a TO b, c TO a. #10895 (alesapin).Fix possible race which could happen when you get result from aggregate function state from multiple thread for the same column. The only way (which I found) it can happen is when you use finalizeAggregation function while reading from table with Memory engine which stores AggregateFunction state for quanite* function. #10890 (Nikolai Kochetov).Fix backward compatibility with tuples in Distributed tables. #10889 (Anton Popov).Fix SIGSEGV in StringHashTable (if such key does not exist). #10870 (Azat Khuzhin).Fixed WATCH hangs after LiveView table was dropped from database with Atomic engine. #10859 (tavplubix).Fixed bug in ReplicatedMergeTree which might cause some ALTER on OPTIMIZE query to hang waiting for some replica after it become inactive. #10849 (tavplubix).Now constraints are updated if the column participating in CONSTRAINT expression was renamed. Fixes #10844. #10847 (alesapin).Fix potential read of uninitialized memory in cache dictionary. #10834 (alexey-milovidov).Fix columns order after Block::sortColumns() (also add a test that shows that it affects some real use case - Buffer engine). #10826 (Azat Khuzhin).Fix the issue with ODBC bridge when no quoting of identifiers is requested. This fixes #7984. #10821 (alexey-milovidov).Fix UBSan and MSan report in DateLUT. #10798 (alexey-milovidov).Make use of src_type for correct type conversion in key conditions. Fixes #6287. #10791 (Andrew Onyshchuk).Get rid of old libunwind patches. https://github.com/ClickHouse-Extras/libunwind/commit/500aa227911bd185a94bfc071d68f4d3b03cb3b1#r39048012 This allows to disable -fno-omit-frame-pointer in clang builds that improves performance at least by 1% in average. #10761 (Amos Bird).Fix avgWeighted when using floating-point weight over multiple shards. #10758 (Baudouin Giard).Fix parallel_view_processing behavior. Now all insertions into MATERIALIZED VIEW without exception should be finished if exception happened. Fixes #10241. #10757 (Nikolai Kochetov).Fix combinator -OrNull and -OrDefault when combined with -State. #10741 (hcz).Fix crash in generateRandom with nested types. Fixes #10583. #10734 (Nikolai Kochetov).Fix data corruption for LowCardinality(FixedString) key column in SummingMergeTree which could have happened after merge. Fixes #10489. #10721 (Nikolai Kochetov).Fix usage of primary key wrapped into a function with 'FINAL' modifier and 'ORDER BY' optimization. #10715 (Anton Popov).Fix possible buffer overflow in function h3EdgeAngle. #10711 (alexey-milovidov).Fix disappearing totals. Totals could have being filtered if query had had join or subquery with external where condition. Fixes #10674. #10698 (Nikolai Kochetov).Fix atomicity of HTTP insert. This fixes #9666. #10687 (Andrew Onyshchuk).Fix multiple usages of IN operator with the identical set in one query. #10686 (Anton Popov).Fixed bug, which causes http requests stuck on client close when readonly=2 and cancel_http_readonly_queries_on_client_close=1. Fixes #7939, #7019, #7736, #7091. #10684 (tavplubix).Fix order of parameters in AggregateTransform constructor. #10667 (palasonic1).Fix the lack of parallel execution of remote queries with distributed_aggregation_memory_efficient enabled. Fixes #10655. #10664 (Nikolai Kochetov).Fix possible incorrect number of rows for queries with LIMIT. Fixes #10566, #10709. #10660 (Nikolai Kochetov).Fix bug which locks concurrent alters when table has a lot of parts. #10659 (alesapin).Fix nullptr dereference in StorageBuffer if server was shutdown before table startup. #10641 (alexey-milovidov).Fix predicates optimization for distributed queries (enable_optimize_predicate_expression=1) for queries with HAVING section (i.e. when filtering on the server initiator is required), by preserving the order of expressions (and this is enough to fix), and also force aggregator use column names over indexes. Fixes: #10613, #11413. #10621 (Azat Khuzhin).Fix optimize_skip_unused_shards with LowCardinality. #10611 (Azat Khuzhin).Fix segfault in StorageBuffer when exception on server startup. Fixes #10550. #10609 (tavplubix).On SYSTEM DROP DNS CACHE query also drop caches, which are used to check if user is allowed to connect from some IP addresses. #10608 (tavplubix).Fixed incorrect scalar results inside inner query of MATERIALIZED VIEW in case if this query contained dependent table. #10603 (Nikolai Kochetov).Fixed handling condition variable for synchronous mutations. In some cases signals to that condition variable could be lost. #10588 (Vladimir Chebotarev).Fixes possible crash createDictionary() is called before loadStoredObject() has finished. #10587 (Vitaly Baranov).Fix error the BloomFilter false positive must be a double number between 0 and 1 #10551. #10569 (Winter Zhang).Fix SELECT of column ALIAS which default expression type different from column type. #10563 (Azat Khuzhin).Implemented comparison between DateTime64 and String values (just like for DateTime). #10560 (Vasily Nemkov).Fix index corruption, which may occur in some cases after merge compact parts into another compact part. #10531 (Anton Popov).Disable GROUP BY sharding_key optimization by default (optimize_distributed_group_by_sharding_key had been introduced and turned of by default, due to trickery of sharding_key analyzing, simple example is if in sharding key) and fix it for WITH ROLLUP/CUBE/TOTALS. #10516 (Azat Khuzhin).Fixes: #10263 (after that PR dist send via INSERT had been postponing on each INSERT) Fixes: #8756 (that PR breaks distributed sends with all of the following conditions met (unlikely setup for now I guess): internal_replication == false, multiple local shards (activates the hardlinking code) and distributed_storage_policy (makes link(2) fails on EXDEV)). #10486 (Azat Khuzhin).Fixed error with &quot;max_rows_to_sort&quot; limit. #10268 (alexey-milovidov).Get dictionary and check access rights only once per each call of any function reading external dictionaries. #10928 (Vitaly Baranov). Improvement​ Apply TTL for old data, after ALTER MODIFY TTL query. This behaviour is controlled by setting materialize_ttl_after_modify, which is enabled by default. #11042 (Anton Popov).When parsing C-style backslash escapes in string literals, VALUES and various text formats (this is an extension to SQL standard that is endemic for ClickHouse and MySQL), keep backslash if unknown escape sequence is found (e.g. \\% or \\w) that will make usage of LIKE and match regular expressions more convenient (it's enough to write name LIKE 'used\\_cars' instead of name LIKE 'used\\\\_cars') and more compatible at the same time. This fixes #10922. #11208 (alexey-milovidov).When reading Decimal value, cut extra digits after point. This behaviour is more compatible with MySQL and PostgreSQL. This fixes #10202. #11831 (alexey-milovidov).Allow to DROP replicated table if the metadata in ZooKeeper was already removed and does not exist (this is also the case when using TestKeeper for testing and the server was restarted). Allow to RENAME replicated table even if there is an error communicating with ZooKeeper. This fixes #10720. #11652 (alexey-milovidov).Slightly improve diagnostic of reading decimal from string. This closes #10202. #11829 (alexey-milovidov).Fix sleep invocation in signal handler. It was sleeping for less amount of time than expected. #11825 (alexey-milovidov).(Only Linux) OS related performance metrics (for CPU and I/O) will work even without CAP_NET_ADMIN capability. #10544 (Alexander Kazakov).Added hostname as an alias to function hostName. This feature was suggested by Victor Tarnavskiy from Yandex.Metrica. #11821 (alexey-milovidov).Added support for distributed DDL (update/delete/drop partition) on cross replication clusters. #11703 (Nikita Mikhaylov).Emit warning instead of error in server log at startup if we cannot listen one of the listen addresses (e.g. IPv6 is unavailable inside Docker). Note that if server fails to listen all listed addresses, it will refuse to startup as before. This fixes #4406. #11687 (alexey-milovidov).Default user and database creation on docker image starting. #10637 (Paramtamtam).When multiline query is printed to server log, the lines are joined. Make it to work correct in case of multiline string literals, identifiers and single-line comments. This fixes #3853. #11686 (alexey-milovidov).Multiple names are now allowed in commands: CREATE USER, CREATE ROLE, ALTER USER, SHOW CREATE USER, SHOW GRANTS and so on. #11670 (Vitaly Baranov).Add support for distributed DDL (UPDATE/DELETE/DROP PARTITION) on cross replication clusters. #11508 (frank lee).Clear password from command line in clickhouse-client and clickhouse-benchmark if the user has specified it with explicit value. This prevents password exposure by ps and similar tools. #11665 (alexey-milovidov).Don't use debug info from ELF file if it does not correspond to the running binary. It is needed to avoid printing wrong function names and source locations in stack traces. This fixes #7514. #11657 (alexey-milovidov).Return NULL/zero when value is not parsed completely in parseDateTimeBestEffortOrNull/Zero functions. This fixes #7876. #11653 (alexey-milovidov).Skip empty parameters in requested URL. They may appear when you write http://localhost:8123/?&amp;a=b or http://localhost:8123/?a=b&amp;&amp;c=d. This closes #10749. #11651 (alexey-milovidov).Allow using groupArrayArray and groupUniqArrayArray as SimpleAggregateFunction. #11650 (Volodymyr Kuznetsov).Allow comparison with constant strings by implicit conversions when analysing index conditions on other types. This may close #11630. #11648 (alexey-milovidov).https://github.com/ClickHouse/ClickHouse/pull/7572#issuecomment-642815377 Support config default HTTPHandlers. #11628 (Winter Zhang).Make more input formats to work with Kafka engine. Fix the issue with premature flushes. Fix the performance issue when kafka_num_consumers is greater than number of partitions in topic. #11599 (filimonov).Improve multiple_joins_rewriter_version=2 logic. Fix unknown columns error for lambda aliases. #11587 (Artem Zuikov).Better exception message when cannot parse columns declaration list. This closes #10403. #11537 (alexey-milovidov).Improve enable_optimize_predicate_expression=1 logic for VIEW. #11513 (Artem Zuikov).Adding support for PREWHERE in live view tables. #11495 (vzakaznikov).Automatically update DNS cache, which is used to check if user is allowed to connect from an address. #11487 (tavplubix).OPTIMIZE FINAL will force merge even if concurrent merges are performed. This closes #11309 and closes #11322. #11346 (alexey-milovidov).Suppress output of cancelled queries in clickhouse-client. In previous versions result may continue to print in terminal even after you press Ctrl+C to cancel query. This closes #9473. #11342 (alexey-milovidov).Now history file is updated after each query and there is no race condition if multiple clients use one history file. This fixes #9897. #11453 (Tagir Kuskarov).Better log messages in while reloading configuration. #11341 (alexey-milovidov).Remove trailing whitespaces from formatted queries in clickhouse-client or clickhouse-format in some cases. #11325 (alexey-milovidov).Add setting &quot;output_format_pretty_max_value_width&quot;. If value is longer, it will be cut to avoid output of too large values in terminal. This closes #11140. #11324 (alexey-milovidov).Better exception message in case when there is shortage of memory mappings. This closes #11027. #11316 (alexey-milovidov).Support (U)Int8, (U)Int16, Date in ASOF JOIN. #11301 (Artem Zuikov).Support kafka_client_id parameter for Kafka tables. It also changes the default client.id used by ClickHouse when communicating with Kafka to be more verbose and usable. #11252 (filimonov).Keep the value of DistributedFilesToInsert metric on exceptions. In previous versions, the value was set when we are going to send some files, but it is zero, if there was an exception and some files are still pending. Now it corresponds to the number of pending files in filesystem. #11220 (alexey-milovidov).Add support for multi-word data type names (such as DOUBLE PRECISION and CHAR VARYING) for better SQL compatibility. #11214 (Павел Потемкин).Provide synonyms for some data types. #10856 (Павел Потемкин).The query log is now enabled by default. #11184 (Ivan Blinkov).Show authentication type in table system.users and while executing SHOW CREATE USER query. #11080 (Vitaly Baranov).Remove data on explicit DROP DATABASE for Memory database engine. Fixes #10557. #11021 (tavplubix).Set thread names for internal threads of rdkafka library. Make logs from rdkafka available in server logs. #10983 (Azat Khuzhin).Support for unicode whitespaces in queries. This helps when queries are copy-pasted from Word or from web page. This fixes #10896. #10903 (alexey-milovidov).Allow large UInt types as the index in function tupleElement. #10874 (hcz).Respect prefer_localhost_replica/load_balancing on INSERT into Distributed. #10867 (Azat Khuzhin).Introduce min_insert_block_size_rows_for_materialized_views, min_insert_block_size_bytes_for_materialized_views settings. This settings are similar to min_insert_block_size_rows and min_insert_block_size_bytes, but applied only for blocks inserted into MATERIALIZED VIEW. It helps to control blocks squashing while pushing to MVs and avoid excessive memory usage. #10858 (Azat Khuzhin).Get rid of exception from replicated queue during server shutdown. Fixes #10819. #10841 (alesapin).Ensure that varSamp, varPop cannot return negative results due to numerical errors and that stddevSamp, stddevPop cannot be calculated from negative variance. This fixes #10532. #10829 (alexey-milovidov).Better DNS exception message. This fixes #10813. #10828 (alexey-milovidov).Change HTTP response code in case of some parse errors to 400 Bad Request. This fix #10636. #10640 (alexey-milovidov).Print a message if clickhouse-client is newer than clickhouse-server. #10627 (alexey-milovidov).Adding support for INSERT INTO [db.]table WATCH query. #10498 (vzakaznikov).Allow to pass quota_key in clickhouse-client. This closes #10227. #10270 (alexey-milovidov). Performance Improvement​ Allow multiple replicas to assign merges, mutations, partition drop, move and replace concurrently. This closes #10367. #11639 (alexey-milovidov) #11795 (alexey-milovidov).Optimization of GROUP BY with respect to table sorting key, enabled with optimize_aggregation_in_order setting. #9113 (dimarub2000).Selects with final are executed in parallel. Added setting max_final_threads to limit the number of threads used. #10463 (Nikolai Kochetov).Improve performance for INSERT queries via INSERT SELECT or INSERT with clickhouse-client when small blocks are generated (typical case with parallel parsing). This fixes #11275. Fix the issue that CONSTRAINTs were not working for DEFAULT fields. This fixes #11273. Fix the issue that CONSTRAINTS were ignored for TEMPORARY tables. This fixes #11274. #11276 (alexey-milovidov).Optimization that eliminates min/max/any aggregators of GROUP BY keys in SELECT section, enabled with optimize_aggregators_of_group_by_keys setting. #11667 (xPoSx). #11806 (Azat Khuzhin).New optimization that takes all operations out of any function, enabled with optimize_move_functions_out_of_any #11529 (Ruslan).Improve performance of clickhouse-client in interactive mode when Pretty formats are used. In previous versions, significant amount of time can be spent calculating visible width of UTF-8 string. This closes #11323. #11323 (alexey-milovidov).Improved performance for queries with ORDER BY and small LIMIT (less, then max_block_size). #11171 (Albert Kidrachev).Add runtime CPU detection to select and dispatch the best function implementation. Add support for codegeneration for multiple targets. This closes #1017. #10058 (DimasKovas).Enable mlock of clickhouse binary by default. It will prevent clickhouse executable from being paged out under high IO load. #11139 (alexey-milovidov).Make queries with sum aggregate function and without GROUP BY keys to run multiple times faster. #10992 (alexey-milovidov).Improving radix sort (used in ORDER BY with simple keys) by removing some redundant data moves. #10981 (Arslan Gumerov).Sort bigger parts of the left table in MergeJoin. Buffer left blocks in memory. Add partial_merge_join_left_table_buffer_bytes setting to manage the left blocks buffers sizes. #10601 (Artem Zuikov).Remove duplicate ORDER BY and DISTINCT from subqueries, this optimization is enabled with optimize_duplicate_order_by_and_distinct #10067 (Mikhail Malafeev).This feature eliminates functions of other keys in GROUP BY section, enabled with optimize_group_by_function_keys #10051 (xPoSx).New optimization that takes arithmetic operations out of aggregate functions, enabled with optimize_arithmetic_operations_in_aggregate_functions #10047 (Ruslan).Use HTTP client for S3 based on Poco instead of curl. This will improve performance and lower memory usage of s3 storage and table functions. #11230 (Pavel Kovalenko).Fix Kafka performance issue related to reschedules based on limits, which were always applied. #11149 (filimonov).Enable percpu_arena:percpu for jemalloc (This will reduce memory fragmentation due to thread pool). #11084 (Azat Khuzhin).Optimize memory usage when reading a response from an S3 HTTP client. #11561 (Pavel Kovalenko).Adjust the default Kafka settings for better performance. #11388 (filimonov). Experimental Feature​ Add data type Point (Tuple(Float64, Float64)) and Polygon (Array(Array(Tuple(Float64, Float64))). #10678 (Alexey Ilyukhov).Add's a hasSubstr function that allows for look for subsequences in arrays. Note: this function is likely to be renamed without further notice. #11071 (Ryad Zenine).Added OpenCL support and bitonic sort algorithm, which can be used for sorting integer types of data in single column. Needs to be build with flag -DENABLE_OPENCL=1. For using bitonic sort algorithm instead of others you need to set bitonic_sort for Setting's option special_sort and make sure that OpenCL is available. This feature does not improve performance or anything else, it is only provided as an example and for demonstration purposes. It is likely to be removed in near future if there will be no further development in this direction. #10232 (Ri). Build/Testing/Packaging Improvement​ Enable clang-tidy for programs and utils. #10991 (alexey-milovidov).Remove dependency on tzdata: do not fail if /usr/share/zoneinfo directory does not exist. Note that all timezones work in ClickHouse even without tzdata installed in system. #11827 (alexey-milovidov).Added MSan and UBSan stress tests. Note that we already have MSan, UBSan for functional tests and &quot;stress&quot; test is another kind of tests. #10871 (alexey-milovidov).Print compiler build id in crash messages. It will make us slightly more certain about what binary has crashed. Added new function buildId. #11824 (alexey-milovidov).Added a test to ensure that mutations continue to work after FREEZE query. #11820 (alexey-milovidov).Don't allow tests with &quot;fail&quot; substring in their names because it makes looking at the tests results in browser less convenient when you type Ctrl+F and search for &quot;fail&quot;. #11817 (alexey-milovidov).Removes unused imports from HTTPHandlerFactory. #11660 (Bharat Nallan).Added a random sampling of instances where copier is executed. It is needed to avoid Too many simultaneous queries error. Also increased timeout and decreased fault probability. #11573 (Nikita Mikhaylov).Fix missed include. #11525 (Matwey V. Kornilov).Speed up build by removing old example programs. Also found some orphan functional test. #11486 (alexey-milovidov).Increase ccache size for builds in CI. #11450 (alesapin).Leave only unit_tests_dbms in deb build. #11429 (Ilya Yatsishin).Update librdkafka to version 1.4.2. #11256 (filimonov).Refactor CMake build files. #11390 (Ivan).Fix several flaky integration tests. #11355 (alesapin).Add support for unit tests run with UBSan. #11345 (alexey-milovidov).Remove redundant timeout from integration test test_insertion_sync_fails_with_timeout. #11343 (alesapin).Better check for hung queries in clickhouse-test. #11321 (alexey-milovidov).Emit a warning if server was build in debug or with sanitizers. #11304 (alexey-milovidov).Now clickhouse-test check the server aliveness before tests run. #11285 (alesapin).Fix potentially flacky test 00731_long_merge_tree_select_opened_files.sh. It does not fail frequently but we have discovered potential race condition in this test while experimenting with ThreadFuzzer: #9814 See link for the example. #11270 (alexey-milovidov).Repeat test in CI if curl invocation was timed out. It is possible due to system hangups for 10+ seconds that are typical in our CI infrastructure. This fixes #11267. #11268 (alexey-milovidov).Add a test for Join table engine from @donmikel. This closes #9158. #11265 (alexey-milovidov).Fix several non significant errors in unit tests. #11262 (alesapin).Now parts of linker command for cctz library will not be shuffled with other libraries. #11213 (alesapin).Split /programs/server into actual program and library. #11186 (Ivan).Improve build scripts for protobuf &amp; gRPC. #11172 (Vitaly Baranov).Enable performance test that was not working. #11158 (alexey-milovidov).Create root S3 bucket for tests before any CH instance is started. #11142 (Pavel Kovalenko).Add performance test for non-constant polygons. #11141 (alexey-milovidov).Fixing 00979_live_view_watch_continuous_aggregates test. #11024 (vzakaznikov).Add ability to run zookeeper in integration tests over tmpfs. #11002 (alesapin).Wait for odbc-bridge with exponential backoff. Previous wait time of 200 ms was not enough in our CI environment. #10990 (alexey-milovidov).Fix non-deterministic test. #10989 (alexey-milovidov).Added a test for empty external data. #10926 (alexey-milovidov).Database is recreated for every test. This improves separation of tests. #10902 (alexey-milovidov).Added more asserts in columns code. #10833 (alexey-milovidov).Better cooperation with sanitizers. Print information about query_id in the message of sanitizer failure. #10832 (alexey-milovidov).Fix obvious race condition in &quot;Split build smoke test&quot; check. #10820 (alexey-milovidov).Fix (false) MSan report in MergeTreeIndexFullText. The issue first appeared in #9968. #10801 (alexey-milovidov).Add MSan suppression for MariaDB Client library. #10800 (alexey-milovidov).GRPC make couldn't find protobuf files, changed make file by adding the right link. #10794 (mnkonkova).Enable extra warnings (-Weverything) for base, utils, programs. Note that we already have it for the most of the code. #10779 (alexey-milovidov).Suppressions of warnings from libraries was mistakenly declared as public in #10396. #10776 (alexey-milovidov).Restore a patch that was accidentially deleted in #10396. #10774 (alexey-milovidov).Fix performance tests errors, part 2. #10773 (alexey-milovidov).Fix performance test errors. #10766 (alexey-milovidov).Update cross-builds to use clang-10 compiler. #10724 (Ivan).Update instruction to install RPM packages. This was suggested by Denis (TG login @ldviolet) and implemented by Arkady Shejn. #10707 (alexey-milovidov).Trying to fix tests/queries/0_stateless/01246_insert_into_watch_live_view.py test. #10670 (vzakaznikov).Fixing and re-enabling 00979_live_view_watch_continuous_aggregates.py test. #10658 (vzakaznikov).Fix OOM in ASan stress test. #10646 (alexey-milovidov).Fix UBSan report (adding zero to nullptr) in HashTable that appeared after migration to clang-10. #10638 (alexey-milovidov).Remove external call to ld (bfd) linker during tzdata processing in compile time. #10634 (alesapin).Allow to use lld to link blobs (resources). #10632 (alexey-milovidov).Fix UBSan report in LZ4 library. #10631 (alexey-milovidov). See also https://github.com/lz4/lz4/issues/857Update LZ4 to the latest dev branch. #10630 (alexey-milovidov).Added auto-generated machine-readable file with the list of stable versions. #10628 (alexey-milovidov).Fix capnproto version check for capnp::UnalignedFlatArrayMessageReader. #10618 (Matwey V. Kornilov).Lower memory usage in tests. #10617 (alexey-milovidov).Fixing hard coded timeouts in new live view tests. #10604 (vzakaznikov).Increasing timeout when opening a client in tests/queries/0_stateless/helpers/client.py. #10599 (vzakaznikov).Enable ThinLTO for clang builds, continuation of #10435. #10585 (Amos Bird).Adding fuzzers and preparing for oss-fuzz integration. #10546 (kyprizel).Fix FreeBSD build. #10150 (Ivan).Add new build for query tests using pytest framework. #10039 (Ivan). "},{"title":"ClickHouse release v20.4​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v204","content":""},{"title":"ClickHouse release v20.4.8.99-stable 2020-08-10​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v204899-stable-2020-08-10","content":"Bug Fix​ Fixed error in parseDateTimeBestEffort function when unix timestamp was passed as an argument. This fixes #13362. #13441 (alexey-milovidov).Fixed potentially low performance and slightly incorrect result for uniqExact, topK, sumDistinct and similar aggregate functions called on Float types with NaN values. It also triggered assert in debug build. This fixes #12491. #13254 (alexey-milovidov).Fixed function if with nullable constexpr as cond that is not literal NULL. Fixes #12463. #13226 (alexey-milovidov).Fixed assert in arrayElement function in case of array elements are Nullable and array subscript is also Nullable. This fixes #12172. #13224 (alexey-milovidov).Fixed wrong index analysis with functions. It could lead to pruning wrong parts, while reading from MergeTree tables. Fixes #13060. Fixes #12406. #13081 (Anton Popov).Fixed unnecessary limiting for the number of threads for selects from local replica. #12840 (Nikolai Kochetov).Fixed possible extra overflow row in data which could appear for queries WITH TOTALS. #12747 (Nikolai Kochetov).Fixed performance with large tuples, which are interpreted as functions in IN section. The case when user write WHERE x IN tuple(1, 2, ...) instead of WHERE x IN (1, 2, ...) for some obscure reason. #12700 (Anton Popov).Fixed memory tracking for input_format_parallel_parsing (by attaching thread to group). #12672 (Azat Khuzhin).Fixed #12293 allow push predicate when subquery contains with clause. #12663 (Winter Zhang).Fixed #10572 fix bloom filter index with const expression. #12659 (Winter Zhang).Fixed SIGSEGV in StorageKafka when broker is unavailable (and not only). #12658 (Azat Khuzhin).Added support for function if with Array(UUID) arguments. This fixes #11066. #12648 (alexey-milovidov).Fixed race condition in external dictionaries with cache layout which can lead server crash. #12566 (alesapin).Removed data for Distributed tables (blocks from async INSERTs) on DROP TABLE. #12556 (Azat Khuzhin).Fixed bug which lead to broken old parts after ALTER DELETE query when enable_mixed_granularity_parts=1. Fixes #12536. #12543 (alesapin).Better exception for function in with invalid number of arguments. #12529 (Anton Popov).Fixed performance issue, while reading from compact parts. #12492 (Anton Popov).Fixed crash in JOIN with dictionary when we are joining over expression of dictionary key: t JOIN dict ON expr(dict.id) = t.id. Disable dictionary join optimisation for this case. #12458 (Artem Zuikov).Fixed possible segfault if StorageMerge. Closes #12054. #12401 (tavplubix).Fixed order of columns in WITH FILL modifier. Previously order of columns of ORDER BY statement wasn't respected. #12306 (Anton Popov).Avoid &quot;bad cast&quot; exception when there is an expression that filters data by virtual columns (like _table in Merge tables) or by &quot;index&quot; columns in system tables such as filtering by database name when querying from system.tables, and this expression returns Nullable type. This fixes #12166. #12305 (alexey-milovidov).Show error after TrieDictionary failed to load. #12290 (Vitaly Baranov).The function arrayFill worked incorrectly for empty arrays that may lead to crash. This fixes #12263. #12279 (alexey-milovidov).Implemented conversions to the common type for LowCardinality types. This allows to execute UNION ALL of tables with columns of LowCardinality and other columns. This fixes #8212. This fixes #4342. #12275 (alexey-milovidov).Fixed the behaviour when during multiple sequential inserts in StorageFile header for some special types was written more than once. This fixed #6155. #12197 (Nikita Mikhaylov).Fixed logical functions for UInt8 values when they are not equal to 0 or 1. #12196 (Alexander Kazakov).Cap max_memory_usage* limits to the process resident memory. #12182 (Azat Khuzhin).Fixed dictGet arguments check during GROUP BY injective functions elimination. #12179 (Azat Khuzhin).Don't split the dictionary source's table name into schema and table name itself if ODBC connection does not support schema. #12165 (Vitaly Baranov).Fixed wrong logic in ALTER DELETE that leads to deleting of records when condition evaluates to NULL. This fixes #9088. This closes #12106. #12153 (alexey-milovidov).Fixed transform of query to send to external DBMS (e.g. MySQL, ODBC) in presense of aliases. This fixes #12032. #12151 (alexey-milovidov).Fixed potential overflow in integer division. This fixes #12119. #12140 (alexey-milovidov).Fixed potential infinite loop in greatCircleDistance, geoDistance. This fixes #12117. #12137 (alexey-milovidov).Normalize &quot;pid&quot; file handling. In previous versions the server may refuse to start if it was killed without proper shutdown and if there is another process that has the same pid as previously runned server. Also pid file may be removed in unsuccessful server startup even if there is another server running. This fixes #3501. #12133 (alexey-milovidov).Fixed handling dependency of table with ENGINE=Dictionary on dictionary. This fixes #10994. This fixes #10397. #12116 (Vitaly Baranov).Fixed performance for selects with UNION caused by wrong limit for the total number of threads. Fixes #12030. #12103 (Nikolai Kochetov).Fixed segfault with -StateResample combinators. #12092 (Anton Popov).Fixed empty result_rows and result_bytes metrics in system.quey_log for selects. Fixes #11595. #12089 (Nikolai Kochetov).Fixed unnecessary limiting the number of threads for selects from VIEW. Fixes #11937. #12085 (Nikolai Kochetov).Fixed possible crash while using wrong type for PREWHERE. Fixes #12053, #12060. #12060 (Nikolai Kochetov).Fixed error Expected single dictionary argument for function for function defaultValueOfArgumentType with LowCardinality type. Fixes #11808. #12056 (Nikolai Kochetov).Fixed error Cannot capture column for higher-order functions with Tuple(LowCardinality) argument. Fixes #9766. #12055 (Nikolai Kochetov).Parse tables metadata in parallel when loading database. This fixes slow server startup when there are large number of tables. #12045 (tavplubix).Make topK aggregate function return Enum for Enum types. This fixes #3740. #12043 (alexey-milovidov).Fixed constraints check if constraint is a constant expression. This fixes #11360. #12042 (alexey-milovidov).Fixed incorrect comparison of tuples with Nullable columns. Fixes #11985. #12039 (Nikolai Kochetov).Fixed calculation of access rights when allow_introspection_functions=0. #12031 (Vitaly Baranov).Fixed wrong result and potential crash when invoking function if with arguments of type FixedString with different sizes. This fixes #11362. #12021 (alexey-milovidov).A query with function neighbor as the only returned expression may return empty result if the function is called with offset -9223372036854775808. This fixes #11367. #12019 (alexey-milovidov).Fixed calculation of access rights when allow_ddl=0. #12015 (Vitaly Baranov).Fixed potential array size overflow in generateRandom that may lead to crash. This fixes #11371. #12013 (alexey-milovidov).Fixed potential floating point exception. This closes #11378. #12005 (alexey-milovidov).Fixed wrong setting name in log message at server startup. #11997 (alexey-milovidov).Fixed Query parameter was not set in Values format. Fixes #11918. #11936 (tavplubix).Keep aliases for substitutions in query (parametrized queries). This fixes #11914. #11916 (alexey-milovidov).Fixed bug with no moves when changing storage policy from default one. #11893 (Vladimir Chebotarev).Fixed potential floating point exception when parsing DateTime64. This fixes #11374. #11875 (alexey-milovidov).Fixed memory accounting via HTTP interface (can be significant with wait_end_of_query=1). #11840 (Azat Khuzhin).Parse metadata stored in zookeeper before checking for equality. #11739 (Azat Khuzhin). Performance Improvement​ Index not used for IN operator with literals, performance regression introduced around v19.3. This fixes #10574. #12062 (nvartolomei). Build/Testing/Packaging Improvement​ Install ca-certificates before the first apt-get update in Dockerfile. #12095 (Ivan Blinkov). "},{"title":"ClickHouse release v20.4.6.53-stable 2020-06-25​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v204653-stable-2020-06-25","content":"Bug Fix​ Fix rare crash caused by using Nullable column in prewhere condition. Continuation of #11608. #11869 (Nikolai Kochetov).Don't allow arrayJoin inside higher order functions. It was leading to broken protocol synchronization. This closes #3933. #11846 (alexey-milovidov).Fix wrong result of comparison of FixedString with constant String. This fixes #11393. This bug appeared in version 20.4. #11828 (alexey-milovidov).Fix wrong result for if() with NULLs in condition. #11807 (Artem Zuikov).Fix using too many threads for queries. #11788 (Nikolai Kochetov).Fix unexpected behaviour of queries like SELECT *, xyz.* which were success while an error expected. #11753 (hexiaoting).Now replicated fetches will be cancelled during metadata alter. #11744 (alesapin).Fixed LOGICAL_ERROR caused by wrong type deduction of complex literals in Values input format. #11732 (tavplubix).Fix ORDER BY ... WITH FILL over const columns. #11697 (Anton Popov).Pass proper timeouts when communicating with XDBC bridge. Recently timeouts were not respected when checking bridge liveness and receiving meta info. #11690 (alexey-milovidov).Fix LIMIT n WITH TIES usage together with ORDER BY statement, which contains aliases. #11689 (Anton Popov).Fix error which leads to an incorrect state of system.mutations. It may show that whole mutation is already done but the server still has MUTATE_PART tasks in the replication queue and tries to execute them. This fixes #11611. #11681 (alesapin).Add support for regular expressions with case-insensitive flags. This fixes #11101 and fixes #11506. #11649 (alexey-milovidov).Remove trivial count query optimization if row-level security is set. In previous versions the user get total count of records in a table instead filtered. This fixes #11352. #11644 (alexey-milovidov).Fix bloom filters for String (data skipping indices). #11638 (Azat Khuzhin).Fix rare crash caused by using Nullable column in prewhere condition. (Probably it is connected with #11572 somehow). #11608 (Nikolai Kochetov).Fix error Block structure mismatch for queries with sampling reading from Buffer table. #11602 (Nikolai Kochetov).Fix wrong exit code of the clickhouse-client, when exception.code() % 256 = 0. #11601 (filimonov).Fix trivial error in log message about &quot;Mark cache size was lowered&quot; at server startup. This closes #11399. #11589 (alexey-milovidov).Fix error Size of offsets does not match size of column for queries with PREWHERE column in (subquery) and ARRAY JOIN. #11580 (Nikolai Kochetov).Fixed rare segfault in SHOW CREATE TABLE Fixes #11490. #11579 (tavplubix).All queries in HTTP session have had the same query_id. It is fixed. #11578 (tavplubix).Now clickhouse-server docker container will prefer IPv6 checking server aliveness. #11550 (Ivan Starkov).Fix shard_num/replica_num for &lt;node&gt; (breaks use_compact_format_in_distributed_parts_names). #11528 (Azat Khuzhin).Fix race condition which may lead to an exception during table drop. It's a bit tricky and not dangerous at all. If you want an explanation, just notice me in telegram. #11523 (alesapin).Fix memory leak when exception is thrown in the middle of aggregation with -State functions. This fixes #8995. #11496 (alexey-milovidov).If data skipping index is dependent on columns that are going to be modified during background merge (for SummingMergeTree, AggregatingMergeTree as well as for TTL GROUP BY), it was calculated incorrectly. This issue is fixed by moving index calculation after merge so the index is calculated on merged data. #11162 (Azat Khuzhin).Get rid of old libunwind patches. https://github.com/ClickHouse-Extras/libunwind/commit/500aa227911bd185a94bfc071d68f4d3b03cb3b1#r39048012 This allows to disable -fno-omit-frame-pointer in clang builds that improves performance at least by 1% in average. #10761 (Amos Bird).Fix usage of primary key wrapped into a function with 'FINAL' modifier and 'ORDER BY' optimization. #10715 (Anton Popov). Build/Testing/Packaging Improvement​ Fix several non significant errors in unit tests. #11262 (alesapin).Fix (false) MSan report in MergeTreeIndexFullText. The issue first appeared in #9968. #10801 (alexey-milovidov). "},{"title":"ClickHouse release v20.4.5.36-stable 2020-06-10​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v204536-stable-2020-06-10","content":"Bug Fix​ Fix the error Data compressed with different methods that can happen if min_bytes_to_use_direct_io is enabled and PREWHERE is active and using SAMPLE or high number of threads. This fixes #11539. #11540 (alexey-milovidov).Fix return compressed size for codecs. #11448 (Nikolai Kochetov).Fix server crash when a column has compression codec with non-literal arguments. Fixes #11365. #11431 (alesapin).Fix pointInPolygon with nan as point. Fixes #11375. #11421 (Alexey Ilyukhov).Fix potential uninitialized memory read in MergeTree shutdown if table was not created successfully. #11420 (alexey-milovidov).Fixed geohashesInBox with arguments outside of latitude/longitude range. #11403 (Vasily Nemkov).Fix possible Pipeline stuck error for queries with external sort and limit. Fixes #11359. #11366 (Nikolai Kochetov).Remove redundant lock during parts send in ReplicatedMergeTree. #11354 (alesapin).Fix support for \\G (vertical output) in clickhouse-client in multiline mode. This closes #9933. #11350 (alexey-milovidov).Fix potential segfault when using Lazy database. #11348 (alexey-milovidov).Fix crash in quantilesExactWeightedArray. #11337 (Nikolai Kochetov).Now merges stopped before change metadata in ALTER queries. #11335 (alesapin).Make writing to MATERIALIZED VIEW with setting parallel_view_processing = 1 parallel again. Fixes #10241. #11330 (Nikolai Kochetov).Fix visitParamExtractRaw when extracted JSON has strings with unbalanced { or [. #11318 (Ewout).Fix very rare race condition in ThreadPool. #11314 (alexey-milovidov).Fix insignificant data race in clickhouse-copier. Found by integration tests. #11313 (alexey-milovidov).Fix potential uninitialized memory in conversion. Example: SELECT toIntervalSecond(now64()). #11311 (alexey-milovidov).Fix the issue when index analysis cannot work if a table has Array column in primary key and if a query is filtering by this column with empty or notEmpty functions. This fixes #11286. #11303 (alexey-milovidov).Fix bug when query speed estimation can be incorrect and the limit of min_execution_speed may not work or work incorrectly if the query is throttled by max_network_bandwidth, max_execution_speed or priority settings. Change the default value of timeout_before_checking_execution_speed to non-zero, because otherwise the settings min_execution_speed and max_execution_speed have no effect. This fixes #11297. This fixes #5732. This fixes #6228. Usability improvement: avoid concatenation of exception message with progress bar in clickhouse-client. #11296 (alexey-milovidov).Fix crash when SET DEFAULT ROLE is called with wrong arguments. This fixes #10586. #11278 (Vitaly Baranov).Fix crash while reading malformed data in Protobuf format. This fixes #5957, fixes #11203. #11258 (Vitaly Baranov).Fixed a bug when cache-dictionary could return default value instead of normal (when there are only expired keys). This affects only string fields. #11233 (Nikita Mikhaylov).Fix error Block structure mismatch in QueryPipeline while reading from VIEW with constants in inner query. Fixes #11181. #11205 (Nikolai Kochetov).Fix possible exception Invalid status for associated output. #11200 (Nikolai Kochetov).Fix possible error Cannot capture column for higher-order functions with Array(Array(LowCardinality)) captured argument. #11185 (Nikolai Kochetov).Fixed S3 globbing which could fail in case of more than 1000 keys and some backends. #11179 (Vladimir Chebotarev).If data skipping index is dependent on columns that are going to be modified during background merge (for SummingMergeTree, AggregatingMergeTree as well as for TTL GROUP BY), it was calculated incorrectly. This issue is fixed by moving index calculation after merge so the index is calculated on merged data. #11162 (Azat Khuzhin).Fix Kafka performance issue related to reschedules based on limits, which were always applied. #11149 (filimonov).Fix for the hang which was happening sometimes during DROP of table engine=Kafka (or during server restarts). #11145 (filimonov).Fix excessive reserving of threads for simple queries (optimization for reducing the number of threads, which was partly broken after changes in pipeline). #11114 (Azat Khuzhin).Fix predicates optimization for distributed queries (enable_optimize_predicate_expression=1) for queries with HAVING section (i.e. when filtering on the server initiator is required), by preserving the order of expressions (and this is enough to fix), and also force aggregator use column names over indexes. Fixes: #10613, #11413. #10621 (Azat Khuzhin). Build/Testing/Packaging Improvement​ Fix several flaky integration tests. #11355 (alesapin). "},{"title":"ClickHouse release v20.4.4.18-stable 2020-05-26​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v204418-stable-2020-05-26","content":"No changes compared to v20.4.3.16-stable. "},{"title":"ClickHouse release v20.4.3.16-stable 2020-05-23​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v204316-stable-2020-05-23","content":"Bug Fix​ Removed logging from mutation finalization task if nothing was finalized. #11109 (alesapin).Fixed memory leak in registerDiskS3. #11074 (Pavel Kovalenko).Fixed the potential missed data during termination of Kafka engine table. #11048 (filimonov).Fixed parseDateTime64BestEffort argument resolution bugs. #11038 (Vasily Nemkov).Fixed very rare potential use-after-free error in MergeTree if table was not created successfully. #10986, #10970 (alexey-milovidov).Fixed metadata (relative path for rename) and data (relative path for symlink) handling for Atomic database. #10980 (Azat Khuzhin).Fixed server crash on concurrent ALTER and DROP DATABASE queries with Atomic database engine. #10968 (tavplubix).Fixed incorrect raw data size in getRawData() method. #10964 (Igr).Fixed incompatibility of two-level aggregation between versions 20.1 and earlier. This incompatibility happens when different versions of ClickHouse are used on initiator node and remote nodes and the size of GROUP BY result is large and aggregation is performed by a single String field. It leads to several unmerged rows for a single key in result. #10952 (alexey-milovidov).Fixed sending partially written files by the DistributedBlockOutputStream. #10940 (Azat Khuzhin).Fixed crash in SELECT count(notNullIn(NULL, [])). #10920 (Nikolai Kochetov).Fixed the hang which was happening sometimes during DROP of Kafka table engine. (or during server restarts). #10910 (filimonov).Fixed the impossibility of executing multiple ALTER RENAME like a TO b, c TO a. #10895 (alesapin).Fixed possible race which could happen when you get result from aggregate function state from multiple thread for the same column. The only way it can happen is when you use finalizeAggregation function while reading from table with Memory engine which stores AggregateFunction state for quantile* function. #10890 (Nikolai Kochetov).Fixed backward compatibility with tuples in Distributed tables. #10889 (Anton Popov).Fixed SIGSEGV in StringHashTable if such a key does not exist. #10870 (Azat Khuzhin).Fixed WATCH hangs after LiveView table was dropped from database with Atomic engine. #10859 (tavplubix).Fixed bug in ReplicatedMergeTree which might cause some ALTER on OPTIMIZE query to hang waiting for some replica after it become inactive. #10849 (tavplubix).Now constraints are updated if the column participating in CONSTRAINT expression was renamed. Fixes #10844. #10847 (alesapin).Fixed potential read of uninitialized memory in cache-dictionary. #10834 (alexey-milovidov).Fixed columns order after Block::sortColumns(). #10826 (Azat Khuzhin).Fixed the issue with ODBC bridge when no quoting of identifiers is requested. Fixes #7984. #10821 (alexey-milovidov).Fixed UBSan and MSan report in DateLUT. #10798 (alexey-milovidov).Fixed incorrect type conversion in key conditions. Fixes #6287. #10791 (Andrew Onyshchuk).Fixed parallel_view_processing behavior. Now all insertions into MATERIALIZED VIEW without exception should be finished if exception happened. Fixes #10241. #10757 (Nikolai Kochetov).Fixed combinator -OrNull and -OrDefault when combined with -State. #10741 (hcz).Fixed possible buffer overflow in function h3EdgeAngle. #10711 (alexey-milovidov).Fixed bug which locks concurrent alters when table has a lot of parts. #10659 (alesapin).Fixed nullptr dereference in StorageBuffer if server was shutdown before table startup. #10641 (alexey-milovidov).Fixed optimize_skip_unused_shards with LowCardinality. #10611 (Azat Khuzhin).Fixed handling condition variable for synchronous mutations. In some cases signals to that condition variable could be lost. #10588 (Vladimir Chebotarev).Fixed possible crash when createDictionary() is called before loadStoredObject() has finished. #10587 (Vitaly Baranov).Fixed SELECT of column ALIAS which default expression type different from column type. #10563 (Azat Khuzhin).Implemented comparison between DateTime64 and String values. #10560 (Vasily Nemkov).Disable GROUP BY sharding_key optimization by default (optimize_distributed_group_by_sharding_key had been introduced and turned of by default, due to trickery of sharding_key analyzing, simple example is if in sharding key) and fix it for WITH ROLLUP/CUBE/TOTALS. #10516 (Azat Khuzhin).Fixed #10263. #10486 (Azat Khuzhin).Added tests about max_rows_to_sort setting. #10268 (alexey-milovidov).Added backward compatibility for create bloom filter index. #10551. #10569 (Winter Zhang). "},{"title":"ClickHouse release v20.4.2.9, 2020-05-12​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v20429-2020-05-12","content":"Backward Incompatible Change​ System tables (e.g. system.query_log, system.trace_log, system.metric_log) are using compact data part format for parts smaller than 10 MiB in size. Compact data part format is supported since version 20.3. If you are going to downgrade to version less than 20.3, you should manually delete table data for system logs in /var/lib/clickhouse/data/system/.When string comparison involves FixedString and compared arguments are of different sizes, do comparison as if smaller string is padded to the length of the larger. This is intented for SQL compatibility if we imagine that FixedString data type corresponds to SQL CHAR. This closes #9272. #10363 (alexey-milovidov)Make SHOW CREATE TABLE multiline. Now it is more readable and more like MySQL. #10049 (Azat Khuzhin)Added a setting validate_polygons that is used in pointInPolygon function and enabled by default. #9857 (alexey-milovidov) New Feature​ Add support for secured connection from ClickHouse to Zookeeper #10184 (Konstantin Lebedev)Support custom HTTP handlers. See #5436 for description. #7572 (Winter Zhang)Add MessagePack Input/Output format. #9889 (Kruglov Pavel)Add Regexp input format. #9196 (Kruglov Pavel)Added output format Markdown for embedding tables in markdown documents. #10317 (Kruglov Pavel)Added support for custom settings section in dictionaries. Also fixes issue #2829. #10137 (Artem Streltsov)Added custom settings support in DDL-queries for CREATE DICTIONARY #10465 (Artem Streltsov)Add simple server-wide memory profiler that will collect allocation contexts when server memory usage becomes higher than the next allocation threshold. #10444 (alexey-milovidov)Add setting always_fetch_merged_part which restrict replica to merge parts by itself and always prefer dowloading from other replicas. #10379 (alesapin)Add function JSONExtractKeysAndValuesRaw which extracts raw data from JSON objects #10378 (hcz)Add memory usage from OS to system.asynchronous_metrics. #10361 (alexey-milovidov)Added generic variants for functions least and greatest. Now they work with arbitrary number of arguments of arbitrary types. This fixes #4767 #10318 (alexey-milovidov)Now ClickHouse controls timeouts of dictionary sources on its side. Two new settings added to cache dictionary configuration: strict_max_lifetime_seconds, which is max_lifetime by default, and query_wait_timeout_milliseconds, which is one minute by default. The first settings is also useful with allow_read_expired_keys settings (to forbid reading very expired keys). #10337 (Nikita Mikhaylov)Add log_queries_min_type to filter which entries will be written to query_log #10053 (Azat Khuzhin)Added function isConstant. This function checks whether its argument is constant expression and returns 1 or 0. It is intended for development, debugging and demonstration purposes. #10198 (alexey-milovidov)add joinGetOrNull to return NULL when key is missing instead of returning the default value. #10094 (Amos Bird)Consider NULL to be equal to NULL in IN operator, if the option transform_null_in is set. #10085 (achimbab)Add ALTER TABLE ... RENAME COLUMN for MergeTree table engines family. #9948 (alesapin)Support parallel distributed INSERT SELECT. #9759 (vxider)Add ability to query Distributed over Distributed (w/o distributed_group_by_no_merge) ... #9923 (Azat Khuzhin)Add function arrayReduceInRanges which aggregates array elements in given ranges. #9598 (hcz)Add Dictionary Status on prometheus exporter. #9622 (Guillaume Tassery)Add function arrayAUC #8698 (taiyang-li)Support DROP VIEW statement for better TPC-H compatibility. #9831 (Amos Bird)Add 'strict_order' option to windowFunnel() #9773 (achimbab)Support DATE and TIMESTAMP SQL operators, e.g. SELECT date '2001-01-01' #9691 (Artem Zuikov) Experimental Feature​ Added experimental database engine Atomic. It supports non-blocking DROP and RENAME TABLE queries and atomic EXCHANGE TABLES t1 AND t2 query #7512 (tavplubix)Initial support for ReplicatedMergeTree over S3 (it works in suboptimal way) #10126 (Pavel Kovalenko) Bug Fix​ Fixed incorrect scalar results inside inner query of MATERIALIZED VIEW in case if this query contained dependent table #10603 (Nikolai Kochetov)Fixed bug, which caused HTTP requests to get stuck on client closing connection when readonly=2 and cancel_http_readonly_queries_on_client_close=1. #10684 (tavplubix)Fix segfault in StorageBuffer when exception is thrown on server startup. Fixes #10550 #10609 (tavplubix)The querySYSTEM DROP DNS CACHE now also drops caches used to check if user is allowed to connect from some IP addresses #10608 (tavplubix)Fix usage of multiple IN operators with an identical set in one query. Fixes #10539 #10686 (Anton Popov)Fix crash in generateRandom with nested types. Fixes #10583. #10734 (Nikolai Kochetov)Fix data corruption for LowCardinality(FixedString) key column in SummingMergeTree which could have happened after merge. Fixes #10489. #10721 (Nikolai Kochetov)Fix logic for aggregation_memory_efficient_merge_threads setting. #10667 (palasonic1)Fix disappearing totals. Totals could have being filtered if query had JOIN or subquery with external WHERE condition. Fixes #10674 #10698 (Nikolai Kochetov)Fix the lack of parallel execution of remote queries with distributed_aggregation_memory_efficient enabled. Fixes #10655 #10664 (Nikolai Kochetov)Fix possible incorrect number of rows for queries with LIMIT. Fixes #10566, #10709 #10660 (Nikolai Kochetov)Fix index corruption, which may occur in some cases after merging compact parts into another compact part. #10531 (Anton Popov)Fix the situation, when mutation finished all parts, but hung up in is_done=0. #10526 (alesapin)Fix overflow at beginning of unix epoch for timezones with fractional offset from UTC. Fixes #9335. #10513 (alexey-milovidov)Better diagnostics for input formats. Fixes #10204 #10418 (tavplubix)Fix numeric overflow in simpleLinearRegression() over large integers #10474 (hcz)Fix use-after-free in Distributed shutdown, avoid waiting for sending all batches #10491 (Azat Khuzhin)Add CA certificates to clickhouse-server docker image #10476 (filimonov)Fix a rare endless loop that might have occurred when using the addressToLine function or AggregateFunctionState columns. #10466 (Alexander Kuzmenkov)Handle zookeeper &quot;no node error&quot; during distributed query #10050 (Daniel Chen)Fix bug when server cannot attach table after column's default was altered. #10441 (alesapin)Implicitly cast the default expression type to the column type for the ALIAS columns #10563 (Azat Khuzhin)Don't remove metadata directory if ATTACH DATABASE fails #10442 (Winter Zhang)Avoid dependency on system tzdata. Fixes loading of Africa/Casablanca timezone on CentOS 8. Fixes #10211 #10425 (alexey-milovidov)Fix some issues if data is inserted with quorum and then gets deleted (DROP PARTITION, TTL, etc.). It led to stuck of INSERTs or false-positive exceptions in SELECTs. Fixes #9946 #10188 (Nikita Mikhaylov)Check the number and type of arguments when creating BloomFilter index #9623 #10431 (Winter Zhang)Prefer fallback_to_stale_replicas over skip_unavailable_shards, otherwise when both settings specified and there are no up-to-date replicas the query will fail (patch from @alex-zaitsev ) #10422 (Azat Khuzhin)Fix the issue when a query with ARRAY JOIN, ORDER BY and LIMIT may return incomplete result. Fixes #10226. #10427 (Vadim Plakhtinskiy)Add database name to dictionary name after DETACH/ATTACH. Fixes system.dictionaries table and SYSTEM RELOAD query #10415 (Azat Khuzhin)Fix possible incorrect result for extremes in processors pipeline. #10131 (Nikolai Kochetov)Fix possible segfault when the setting distributed_group_by_no_merge is enabled (introduced in 20.3.7.46 by #10131). #10399 (Nikolai Kochetov)Fix wrong flattening of Array(Tuple(...)) data types. Fixes #10259 #10390 (alexey-milovidov)Fix column names of constants inside JOIN that may clash with names of constants outside of JOIN #9950 (Alexander Kuzmenkov)Fix order of columns after Block::sortColumns() #10826 (Azat Khuzhin)Fix possible Pipeline stuck error in ConcatProcessor which may happen in remote query. #10381 (Nikolai Kochetov)Don't make disk reservations for aggregations. Fixes #9241 #10375 (Azat Khuzhin)Fix wrong behaviour of datetime functions for timezones that has altered between positive and negative offsets from UTC (e.g. Pacific/Kiritimati). Fixes #7202 #10369 (alexey-milovidov)Avoid infinite loop in dictIsIn function. Fixes #515 #10365 (alexey-milovidov)Disable GROUP BY sharding_key optimization by default and fix it for WITH ROLLUP/CUBE/TOTALS #10516 (Azat Khuzhin)Check for error code when checking parts and don't mark part as broken if the error is like &quot;not enough memory&quot;. Fixes #6269 #10364 (alexey-milovidov)Show information about not loaded dictionaries in system tables. #10234 (Vitaly Baranov)Fix nullptr dereference in StorageBuffer if server was shutdown before table startup. #10641 (alexey-milovidov)Fixed DROP vs OPTIMIZE race in ReplicatedMergeTree. DROP could left some garbage in replica path in ZooKeeper if there was concurrent OPTIMIZE query. #10312 (tavplubix)Fix 'Logical error: CROSS JOIN has expressions' error for queries with comma and names joins mix. Fixes #9910 #10311 (Artem Zuikov)Fix queries with max_bytes_before_external_group_by. #10302 (Artem Zuikov)Fix the issue with limiting maximum recursion depth in parser in certain cases. This fixes #10283 This fix may introduce minor incompatibility: long and deep queries via clickhouse-client may refuse to work, and you should adjust settings max_query_size and max_parser_depth accordingly. #10295 (alexey-milovidov)Allow to use count(*) with multiple JOINs. Fixes #9853 #10291 (Artem Zuikov)Fix error Pipeline stuck with max_rows_to_group_by and group_by_overflow_mode = 'break'. #10279 (Nikolai Kochetov)Fix 'Cannot add column' error while creating range_hashed dictionary using DDL query. Fixes #10093. #10235 (alesapin)Fix rare possible exception Cannot drain connections: cancel first. #10239 (Nikolai Kochetov)Fixed bug where ClickHouse would throw &quot;Unknown function lambda.&quot; error message when user tries to run ALTER UPDATE/DELETE on tables with ENGINE = Replicated*. Check for nondeterministic functions now handles lambda expressions correctly. #10237 (Alexander Kazakov)Fixed reasonably rare segfault in StorageSystemTables that happens when SELECT ... FROM system.tables is run on a database with Lazy engine. #10209 (Alexander Kazakov)Fix possible infinite query execution when the query actually should stop on LIMIT, while reading from infinite source like system.numbers or system.zeros. #10206 (Nikolai Kochetov)Fixed &quot;generateRandom&quot; function for Date type. This fixes #9973. Fix an edge case when dates with year 2106 are inserted to MergeTree tables with old-style partitioning but partitions are named with year 1970. #10218 (alexey-milovidov)Convert types if the table definition of a View does not correspond to the SELECT query. This fixes #10180 and #10022 #10217 (alexey-milovidov)Fix parseDateTimeBestEffort for strings in RFC-2822 when day of week is Tuesday or Thursday. This fixes #10082 #10214 (alexey-milovidov)Fix column names of constants inside JOIN that may clash with names of constants outside of JOIN. #10207 (alexey-milovidov)Fix move-to-prewhere optimization in presense of arrayJoin functions (in certain cases). This fixes #10092 #10195 (alexey-milovidov)Fix issue with separator appearing in SCRAMBLE for native mysql-connector-java (JDBC) #10140 (BohuTANG)Fix using the current database for an access checking when the database isn't specified. #10192 (Vitaly Baranov)Fix ALTER of tables with compact parts. #10130 (Anton Popov)Add the ability to relax the restriction on non-deterministic functions usage in mutations with allow_nondeterministic_mutations setting. #10186 (filimonov)Fix DROP TABLE invoked for dictionary #10165 (Azat Khuzhin)Convert blocks if structure does not match when doing INSERT into Distributed table #10135 (Azat Khuzhin)The number of rows was logged incorrectly (as sum across all parts) when inserted block is split by parts with partition key. #10138 (alexey-milovidov)Add some arguments check and support identifier arguments for MySQL Database Engine #10077 (Winter Zhang)Fix incorrect index_granularity_bytes check while creating new replica. Fixes #10098. #10121 (alesapin)Fix bug in CHECK TABLE query when table contain skip indices. #10068 (alesapin)Fix Distributed-over-Distributed with the only one shard in a nested table #9997 (Azat Khuzhin)Fix possible rows loss for queries with JOIN and UNION ALL. Fixes #9826, #10113. ... #10099 (Nikolai Kochetov)Fix bug in dictionary when local clickhouse server is used as source. It may caused memory corruption if types in dictionary and source are not compatible. #10071 (alesapin)Fixed replicated tables startup when updating from an old ClickHouse version where /table/replicas/replica_name/metadata node does not exist. Fixes #10037. #10095 (alesapin)Fix error Cannot clone block with columns because block has 0 columns ... While executing GroupingAggregatedTransform. It happened when setting distributed_aggregation_memory_efficient was enabled, and distributed query read aggregating data with mixed single and two-level aggregation from different shards. #10063 (Nikolai Kochetov)Fix deadlock when database with materialized view failed attach at start #10054 (Azat Khuzhin)Fix a segmentation fault that could occur in GROUP BY over string keys containing trailing zero bytes (#8636, #8925). ... #10025 (Alexander Kuzmenkov)Fix wrong results of distributed queries when alias could override qualified column name. Fixes #9672 #9714 #9972 (Artem Zuikov)Fix possible deadlock in SYSTEM RESTART REPLICAS #9955 (tavplubix)Fix the number of threads used for remote query execution (performance regression, since 20.3). This happened when query from Distributed table was executed simultaneously on local and remote shards. Fixes #9965 #9971 (Nikolai Kochetov)Fixed DeleteOnDestroy logic in ATTACH PART which could lead to automatic removal of attached part and added few tests #9410 (Vladimir Chebotarev)Fix a bug with ON CLUSTER DDL queries freezing on server startup. #9927 (Gagan Arneja)Fix bug in which the necessary tables weren't retrieved at one of the processing stages of queries to some databases. Fixes #9699. #9949 (achulkov2)Fix 'Not found column in block' error when JOIN appears with TOTALS. Fixes #9839 #9939 (Artem Zuikov)Fix parsing multiple hosts set in the CREATE USER command #9924 (Vitaly Baranov)Fix TRUNCATE for Join table engine (#9917). #9920 (Amos Bird)Fix race condition between drop and optimize in ReplicatedMergeTree. #9901 (alesapin)Fix DISTINCT for Distributed when optimize_skip_unused_shards is set. #9808 (Azat Khuzhin)Fix &quot;scalar does not exist&quot; error in ALTERs (#9878). ... #9904 (Amos Bird)Fix error with qualified names in distributed_product_mode=\\'local\\'. Fixes #4756 #9891 (Artem Zuikov)For INSERT queries shards now do clamp the settings from the initiator to their constraints instead of throwing an exception. This fix allows to send INSERT queries to a shard with another constraints. This change improves fix #9447. #9852 (Vitaly Baranov)Add some retries when commiting offsets to Kafka broker, since it can reject commit if during offsets.commit.timeout.ms there were no enough replicas available for the __consumer_offsets topic #9884 (filimonov)Fix Distributed engine behavior when virtual columns of the underlying table used in WHERE #9847 (Azat Khuzhin)Fixed some cases when timezone of the function argument wasn't used properly. #9574 (Vasily Nemkov)Fix 'Different expressions with the same alias' error when query has PREWHERE and WHERE on distributed table and SET distributed_product_mode = 'local'. #9871 (Artem Zuikov)Fix mutations excessive memory consumption for tables with a composite primary key. This fixes #9850. #9860 (alesapin)Fix calculating grants for introspection functions from the setting allow_introspection_functions. #9840 (Vitaly Baranov)Fix max_distributed_connections (w/ and w/o Processors) #9673 (Azat Khuzhin)Fix possible exception Got 0 in totals chunk, expected 1 on client. It happened for queries with JOIN in case if right joined table had zero rows. Example: select * from system.one t1 join system.one t2 on t1.dummy = t2.dummy limit 0 FORMAT TabSeparated;. Fixes #9777. ... #9823 (Nikolai Kochetov)Fix 'COMMA to CROSS JOIN rewriter is not enabled or cannot rewrite query' error in case of subqueries with COMMA JOIN out of tables lists (i.e. in WHERE). Fixes #9782 #9830 (Artem Zuikov)Fix server crashing when optimize_skip_unused_shards is set and expression for key can't be converted to its field type #9804 (Azat Khuzhin)Fix empty string handling in splitByString. #9767 (hcz)Fix broken ALTER TABLE DELETE COLUMN query for compact parts. #9779 (alesapin)Fixed missing rows_before_limit_at_least for queries over http (with processors pipeline). Fixes #9730 #9757 (Nikolai Kochetov)Fix excessive memory consumption in ALTER queries (mutations). This fixes #9533 and #9670. #9754 (alesapin)Fix possible permanent &quot;Cannot schedule a task&quot; error. #9154 (Azat Khuzhin)Fix bug in backquoting in external dictionaries DDL. Fixes #9619. #9734 (alesapin)Fixed data race in text_log. It does not correspond to any real bug. #9726 (alexey-milovidov)Fix bug in a replication that does not allow replication to work if the user has executed mutations on the previous version. This fixes #9645. #9652 (alesapin)Fixed incorrect internal function names for sumKahan and sumWithOverflow. It led to exception while using this functions in remote queries. #9636 (Azat Khuzhin)Add setting use_compact_format_in_distributed_parts_names which allows to write files for INSERT queries into Distributed table with more compact format. This fixes #9647. #9653 (alesapin)Fix RIGHT and FULL JOIN with LowCardinality in JOIN keys. #9610 (Artem Zuikov)Fix possible exceptions Size of filter does not match size of column and Invalid number of rows in Chunk in MergeTreeRangeReader. They could appear while executing PREWHERE in some cases. #9612 (Anton Popov)Allow ALTER ON CLUSTER of Distributed tables with internal replication. This fixes #3268 #9617 (shinoi2)Fix issue when timezone was not preserved if you write a simple arithmetic expression like time + 1 (in contrast to an expression like time + INTERVAL 1 SECOND). This fixes #5743 #9323 (alexey-milovidov) Improvement​ Use time zone when comparing DateTime with string literal. This fixes #5206. #10515 (alexey-milovidov)Print verbose diagnostic info if Decimal value cannot be parsed from text input format. #10205 (alexey-milovidov)Add tasks/memory metrics for distributed/buffer schedule pools #10449 (Azat Khuzhin)Display result as soon as it's ready for SELECT DISTINCT queries in clickhouse-local and HTTP interface. This fixes #8951 #9559 (alexey-milovidov)Allow to use SAMPLE OFFSET query instead of cityHash64(PRIMARY KEY) % N == n for splitting in clickhouse-copier. To use this feature, pass --experimental-use-sample-offset 1 as a command line argument. #10414 (Nikita Mikhaylov)Allow to parse BOM in TSV if the first column cannot contain BOM in its value. This fixes #10301 #10424 (alexey-milovidov)Add Avro nested fields insert support #10354 (Andrew Onyshchuk)Allowed to alter column in non-modifying data mode when the same type is specified. #10382 (Vladimir Chebotarev)Auto distributed_group_by_no_merge on GROUP BY sharding key (if optimize_skip_unused_shards is set) #10341 (Azat Khuzhin)Optimize queries with LIMIT/LIMIT BY/ORDER BY for distributed with GROUP BY sharding_key #10373 (Azat Khuzhin)Added a setting max_server_memory_usage to limit total memory usage of the server. The metric MemoryTracking is now calculated without a drift. The setting max_memory_usage_for_all_queries is now obsolete and does nothing. This closes #10293. #10362 (alexey-milovidov)Add config option system_tables_lazy_load. If it's set to false, then system tables with logs are loaded at the server startup. Alexander Burmak, Svyatoslav Tkhon Il Pak, #9642 #10359 (alexey-milovidov)Use background thread pool (background_schedule_pool_size) for distributed sends #10263 (Azat Khuzhin)Use background thread pool for background buffer flushes. #10315 (Azat Khuzhin)Support for one special case of removing incompletely written parts. This fixes #9940. #10221 (alexey-milovidov)Use isInjective() over manual list of such functions for GROUP BY optimization. #10342 (Azat Khuzhin)Avoid printing error message in log if client sends RST packet immediately on connect. It is typical behaviour of IPVS balancer with keepalived and VRRP. This fixes #1851 #10274 (alexey-milovidov)Allow to parse +inf for floating point types. This closes #1839 #10272 (alexey-milovidov)Implemented generateRandom table function for Nested types. This closes #9903 #10219 (alexey-milovidov)Provide max_allowed_packed in MySQL compatibility interface that will help some clients to communicate with ClickHouse via MySQL protocol. #10199 (BohuTANG)Allow literals for GLOBAL IN (i.e. SELECT * FROM remote('localhost', system.one) WHERE dummy global in (0)) #10196 (Azat Khuzhin)Fix various small issues in interactive mode of clickhouse-client #10194 (alexey-milovidov)Avoid superfluous dictionaries load (system.tables, DROP/SHOW CREATE TABLE) #10164 (Azat Khuzhin)Update to RWLock: timeout parameter for getLock() + implementation reworked to be phase fair #10073 (Alexander Kazakov)Enhanced compatibility with native mysql-connector-java(JDBC) #10021 (BohuTANG)The function toString is considered monotonic and can be used for index analysis even when applied in tautological cases with String or LowCardinality(String) argument. #10110 (Amos Bird)Add ON CLUSTER clause support to commands {CREATE|DROP} USER/ROLE/ROW POLICY/SETTINGS PROFILE/QUOTA, GRANT. #9811 (Vitaly Baranov)Virtual hosted-style support for S3 URI #9998 (Pavel Kovalenko)Now layout type for dictionaries with no arguments can be specified without round brackets in dictionaries DDL-queries. Fixes #10057. #10064 (alesapin)Add ability to use number ranges with leading zeros in filepath #9989 (Olga Khvostikova)Better memory usage in CROSS JOIN. #10029 (Artem Zuikov)Try to connect to all shards in cluster when getting structure of remote table and skip_unavailable_shards is set. #7278 (nvartolomei)Add total_rows/total_bytes into the system.tables table. #9919 (Azat Khuzhin)System log tables now use polymorpic parts by default. #9905 (Anton Popov)Add type column into system.settings/merge_tree_settings #9909 (Azat Khuzhin)Check for available CPU instructions at server startup as early as possible. #9888 (alexey-milovidov)Remove ORDER BY stage from mutations because we read from a single ordered part in a single thread. Also add check that the rows in mutation are ordered by sorting key and this order is not violated. #9886 (alesapin)Implement operator LIKE for FixedString at left hand side. This is needed to better support TPC-DS queries. #9890 (alexey-milovidov)Add force_optimize_skip_unused_shards_no_nested that will disable force_optimize_skip_unused_shards for nested Distributed table #9812 (Azat Khuzhin)Now columns size is calculated only once for MergeTree data parts. #9827 (alesapin)Evaluate constant expressions for optimize_skip_unused_shards (i.e. SELECT * FROM foo_dist WHERE key=xxHash32(0)) #8846 (Azat Khuzhin)Check for using Date or DateTime column from TTL expressions was removed. #9967 (Vladimir Chebotarev)DiskS3 hard links optimal implementation. #9760 (Pavel Kovalenko)If set multiple_joins_rewriter_version = 2 enables second version of multiple JOIN rewrites that keeps not clashed column names as is. It supports multiple JOINs with USING and allow select * for JOINs with subqueries. #9739 (Artem Zuikov)Implementation of &quot;non-blocking&quot; alter for StorageMergeTree #9606 (alesapin)Add MergeTree full support for DiskS3 #9646 (Pavel Kovalenko)Extend splitByString to support empty strings as separators. #9742 (hcz)Add a timestamp_ns column to system.trace_log. It contains a high-definition timestamp of the trace event, and allows to build timelines of thread profiles (&quot;flame charts&quot;). #9696 (Alexander Kuzmenkov)When the setting send_logs_level is enabled, avoid intermixing of log messages and query progress. #9634 (Azat Khuzhin)Added support of MATERIALIZE TTL IN PARTITION. #9581 (Vladimir Chebotarev)Support complex types inside Avro nested fields #10502 (Andrew Onyshchuk) Performance Improvement​ Better insert logic for right table for Partial MergeJoin. #10467 (Artem Zuikov)Improved performance of row-oriented formats (more than 10% for CSV and more than 35% for Avro in case of narrow tables). #10503 (Andrew Onyshchuk)Improved performance of queries with explicitly defined sets at right side of IN operator and tuples on the left side. #10385 (Anton Popov)Use less memory for hash table in HashJoin. #10416 (Artem Zuikov)Special HashJoin over StorageDictionary. Allow rewrite dictGet() functions with JOINs. It's not backward incompatible itself but could uncover #8400 on some installations. #10133 (Artem Zuikov)Enable parallel insert of materialized view when its target table supports. #10052 (vxider)Improved performance of index analysis with monotonic functions. #9607#10026 (Anton Popov)Using SSE2 or SSE4.2 SIMD intrinsics to speed up tokenization in bloom filters. #9968 (Vasily Nemkov)Improved performance of queries with explicitly defined sets at right side of IN operator. This fixes performance regression in version 20.3. #9740 (Anton Popov)Now clickhouse-copier splits each partition in number of pieces and copies them independently. #9075 (Nikita Mikhaylov)Adding more aggregation methods. For example TPC-H query 1 will now pick FixedHashMap&lt;UInt16, AggregateDataPtr&gt; and gets 25% performance gain #9829 (Amos Bird)Use single row counter for multiple streams in pre-limit transform. This helps to avoid uniting pipeline streams in queries with limit but without order by (like select f(x) from (select x from t limit 1000000000)) and use multiple threads for further processing. #9602 (Nikolai Kochetov) Build/Testing/Packaging Improvement​ Use a fork of AWS SDK libraries from ClickHouse-Extras #10527 (Pavel Kovalenko)Add integration tests for new ALTER RENAME COLUMN query. #10654 (vzakaznikov)Fix possible signed integer overflow in invocation of function now64 with wrong arguments. This fixes #8973 #10511 (alexey-milovidov)Split fuzzer and sanitizer configurations to make build config compatible with Oss-fuzz. #10494 (kyprizel)Fixes for clang-tidy on clang-10. #10420 (alexey-milovidov)Display absolute paths in error messages. Otherwise KDevelop fails to navigate to correct file and opens a new file instead. #10434 (alexey-milovidov)Added ASAN_OPTIONS environment variable to investigate errors in CI stress tests with Address sanitizer. #10440 (Nikita Mikhaylov)Enable ThinLTO for clang builds (experimental). #10435 (alexey-milovidov)Remove accidential dependency on Z3 that may be introduced if the system has Z3 solver installed. #10426 (alexey-milovidov)Move integration tests docker files to docker/ directory. #10335 (Ilya Yatsishin)Allow to use clang-10 in CI. It ensures that #10238 is fixed. #10384 (alexey-milovidov)Update OpenSSL to upstream master. Fixed the issue when TLS connections may fail with the message OpenSSL SSL_read: error:14094438:SSL routines:ssl3_read_bytes:tlsv1 alert internal error and SSL Exception: error:2400006E:random number generator::error retrieving entropy. The issue was present in version 20.1. #8956 (alexey-milovidov)Fix clang-10 build. #10238 #10370 (Amos Bird)Add performance test for Parallel INSERT for materialized view. #10345 (vxider)Fix flaky test test_settings_constraints_distributed.test_insert_clamps_settings. #10346 (Vitaly Baranov)Add util to test results upload in CI ClickHouse #10330 (Ilya Yatsishin)Convert test results to JSONEachRow format in junit_to_html tool #10323 (Ilya Yatsishin)Update cctz. #10215 (alexey-milovidov)Allow to create HTML report from the purest JUnit XML report. #10247 (Ilya Yatsishin)Update the check for minimal compiler version. Fix the root cause of the issue #10250 #10256 (alexey-milovidov)Initial support for live view tables over distributed #10179 (vzakaznikov)Fix (false) MSan report in MergeTreeIndexFullText. The issue first appeared in #9968. #10801 (alexey-milovidov)clickhouse-docker-util #10151 (filimonov)Update pdqsort to recent version #10171 (Ivan)Update libdivide to v3.0 #10169 (Ivan)Add check with enabled polymorphic parts. #10086 (Anton Popov)Add cross-compile build for FreeBSD. This fixes #9465 #9643 (Ivan)Add performance test for #6924 #6980 (filimonov)Add support of /dev/null in the File engine for better performance testing #8455 (Amos Bird)Move all folders inside /dbms one level up #9974 (Ivan)Add a test that checks that read from MergeTree with single thread is performed in order. Addition to #9670 #9762 (alexey-milovidov)Fix the 00964_live_view_watch_events_heartbeat.py test to avoid race condition. #9944 (vzakaznikov)Fix integration test test_settings_constraints #9962 (Vitaly Baranov)Every function in its own file, part 12. #9922 (alexey-milovidov)Added performance test for the case of extremely slow analysis of array of tuples. #9872 (alexey-milovidov)Update zstd to 1.4.4. It has some minor improvements in performance and compression ratio. If you run replicas with different versions of ClickHouse you may see reasonable error messages Data after merge is not byte-identical to data on another replicas. with explanation. These messages are Ok and you should not worry. #10663 (alexey-milovidov)Fix TSan report in system.stack_trace. #9832 (alexey-milovidov)Removed dependency on clock_getres. #9833 (alexey-milovidov)Added identifier names check with clang-tidy. #9799 (alexey-milovidov)Update &quot;builder&quot; docker image. This image is not used in CI but is useful for developers. #9809 (alexey-milovidov)Remove old performance-test tool that is no longer used in CI. clickhouse-performance-test is great but now we are using way superior tool that is doing comparison testing with sophisticated statistical formulas to achieve confident results regardless to various changes in environment. #9796 (alexey-milovidov)Added most of clang-static-analyzer checks. #9765 (alexey-milovidov)Update Poco to 1.9.3 in preparation for MongoDB URI support. #6892 (Alexander Kuzmenkov)Fix build with -DUSE_STATIC_LIBRARIES=0 -DENABLE_JEMALLOC=0 #9651 (Artem Zuikov)For change log script, if merge commit was cherry-picked to release branch, take PR name from commit description. #9708 (Nikolai Kochetov)Support vX.X-conflicts tag in backport script. #9705 (Nikolai Kochetov)Fix auto-label for backporting script. #9685 (Nikolai Kochetov)Use libc++ in Darwin cross-build to make it consistent with native build. #9665 (Hui Wang)Fix flacky test 01017_uniqCombined_memory_usage. Continuation of #7236. #9667 (alexey-milovidov)Fix build for native MacOS Clang compiler #9649 (Ivan)Allow to add various glitches around pthread_mutex_lock, pthread_mutex_unlock functions. #9635 (alexey-milovidov)Add support for clang-tidy in packager script. #9625 (alexey-milovidov)Add ability to use unbundled msgpack. #10168 (Azat Khuzhin) "},{"title":"ClickHouse release v20.3​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v203","content":""},{"title":"ClickHouse release v20.3.21.2-lts, 2020-11-02​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v203212-lts-2020-11-02","content":"Bug Fix​ Fix dictGet in sharding_key (and similar places, i.e. when the function context is stored permanently). #16205 (Azat Khuzhin).Fix incorrect empty result for query from Distributed table if query has WHERE, PREWHERE and GLOBAL IN. Fixes #15792. #15933 (Nikolai Kochetov).Fix missing or excessive headers in TSV/CSVWithNames formats. This fixes #12504. #13343 (Azat Khuzhin). "},{"title":"ClickHouse release v20.3.20.6-lts, 2020-10-09​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v203206-lts-2020-10-09","content":"Bug Fix​ Mutation might hang waiting for some non-existent part after MOVE or REPLACE PARTITION or, in rare cases, after DETACH or DROP PARTITION. It's fixed. #15724, #15537 (tavplubix).Fix hang of queries with a lot of subqueries to same table of MySQL engine. Previously, if there were more than 16 subqueries to same MySQL table in query, it hang forever. #15299 (Anton Popov).Fix 'Unknown identifier' in GROUP BY when query has JOIN over Merge table. #15242 (Artem Zuikov).Fix to make predicate push down work when subquery contains finalizeAggregation function. Fixes #14847. #14937 (filimonov).Concurrent ALTER ... REPLACE/MOVE PARTITION ... queries might cause deadlock. It's fixed. #13626 (tavplubix). "},{"title":"ClickHouse release v20.3.19.4-lts, 2020-09-18​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v203194-lts-2020-09-18","content":"Bug Fix​ Fix rare error in SELECT queries when the queried column has DEFAULT expression which depends on the other column which also has DEFAULT and not present in select query and not exists on disk. Partially fixes #14531. #14845 (alesapin).Fix bug when ALTER UPDATE mutation with Nullable column in assignment expression and constant value (like UPDATE x = 42) leads to incorrect value in column or segfault. Fixes #13634, #14045. #14646 (alesapin).Fix wrong Decimal multiplication result caused wrong decimal scale of result column. #14603 (Artem Zuikov). Improvement​ Support custom codecs in compact parts. #12183 (Anton Popov). "},{"title":"ClickHouse release v20.3.18.10-lts, 2020-09-08​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v2031810-lts-2020-09-08","content":"Bug Fix​ Stop query execution if exception happened in PipelineExecutor itself. This could prevent rare possible query hung. Continuation of #14334. #14402 (Nikolai Kochetov).Fixed the behaviour when sometimes cache-dictionary returned default value instead of present value from source. #13624 (Nikita Mikhaylov).Fix parsing row policies from users.xml when names of databases or tables contain dots. This fixes #5779, #12527. #13199 (Vitaly Baranov).Fix CAST(Nullable(String), Enum()). #12745 (Azat Khuzhin).Fixed data race in text_log. It does not correspond to any real bug. #9726 (alexey-milovidov). Improvement​ Fix wrong error for long queries. It was possible to get syntax error other than Max query size exceeded for correct query. #13928 (Nikolai Kochetov).Return NULL/zero when value is not parsed completely in parseDateTimeBestEffortOrNull/Zero functions. This fixes #7876. #11653 (alexey-milovidov). Performance Improvement​ Slightly optimize very short queries with LowCardinality. #14129 (Anton Popov). Build/Testing/Packaging Improvement​ Fix UBSan report (adding zero to nullptr) in HashTable that appeared after migration to clang-10. #10638 (alexey-milovidov). "},{"title":"ClickHouse release v20.3.17.173-lts, 2020-08-15​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v20317173-lts-2020-08-15","content":"Bug Fix​ Fix crash in JOIN with StorageMerge and set enable_optimize_predicate_expression=1. #13679 (Artem Zuikov).Fix invalid return type for comparison of tuples with NULL elements. Fixes #12461. #13420 (Nikolai Kochetov).Fix queries with constant columns and ORDER BY prefix of primary key. #13396 (Anton Popov).Return passed number for numbers with MSB set in roundUpToPowerOfTwoOrZero(). #13234 (Azat Khuzhin). "},{"title":"ClickHouse release v20.3.16.165-lts 2020-08-10​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v20316165-lts-2020-08-10","content":"Bug Fix​ Fixed error in parseDateTimeBestEffort function when unix timestamp was passed as an argument. This fixes #13362. #13441 (alexey-milovidov).Fixed potentially low performance and slightly incorrect result for uniqExact, topK, sumDistinct and similar aggregate functions called on Float types with NaN values. It also triggered assert in debug build. This fixes #12491. #13254 (alexey-milovidov).Fixed function if with nullable constexpr as cond that is not literal NULL. Fixes #12463. #13226 (alexey-milovidov).Fixed assert in arrayElement function in case of array elements are Nullable and array subscript is also Nullable. This fixes #12172. #13224 (alexey-milovidov).Fixed unnecessary limiting for the number of threads for selects from local replica. #12840 (Nikolai Kochetov).Fixed possible extra overflow row in data which could appear for queries WITH TOTALS. #12747 (Nikolai Kochetov).Fixed performance with large tuples, which are interpreted as functions in IN section. The case when user write WHERE x IN tuple(1, 2, ...) instead of WHERE x IN (1, 2, ...) for some obscure reason. #12700 (Anton Popov).Fixed memory tracking for input_format_parallel_parsing (by attaching thread to group). #12672 (Azat Khuzhin).Fixed #12293 allow push predicate when subquery contains with clause. #12663 (Winter Zhang).Fixed #10572 fix bloom filter index with const expression. #12659 (Winter Zhang).Fixed SIGSEGV in StorageKafka when broker is unavailable (and not only). #12658 (Azat Khuzhin).Fixed race condition in external dictionaries with cache layout which can lead server crash. #12566 (alesapin).Fixed bug which lead to broken old parts after ALTER DELETE query when enable_mixed_granularity_parts=1. Fixes #12536. #12543 (alesapin).Better exception for function in with invalid number of arguments. #12529 (Anton Popov).Fixed performance issue, while reading from compact parts. #12492 (Anton Popov).Fixed the deadlock if text_log is enabled. #12452 (alexey-milovidov).Fixed possible segfault if StorageMerge. Closes #12054. #12401 (tavplubix).Fixed TOTALS/ROLLUP/CUBE for aggregate functions with -State and Nullable arguments. This fixes #12163. #12376 (alexey-milovidov).Fixed order of columns in WITH FILL modifier. Previously order of columns of ORDER BY statement wasn't respected. #12306 (Anton Popov).Avoid &quot;bad cast&quot; exception when there is an expression that filters data by virtual columns (like _table in Merge tables) or by &quot;index&quot; columns in system tables such as filtering by database name when querying from system.tables, and this expression returns Nullable type. This fixes #12166. #12305 (alexey-milovidov).Show error after TrieDictionary failed to load. #12290 (Vitaly Baranov).The function arrayFill worked incorrectly for empty arrays that may lead to crash. This fixes #12263. #12279 (alexey-milovidov).Implement conversions to the common type for LowCardinality types. This allows to execute UNION ALL of tables with columns of LowCardinality and other columns. This fixes #8212. This fixes #4342. #12275 (alexey-milovidov).Fixed the behaviour when during multiple sequential inserts in StorageFile header for some special types was written more than once. This fixed #6155. #12197 (Nikita Mikhaylov).Fixed logical functions for UInt8 values when they are not equal to 0 or 1. #12196 (Alexander Kazakov).Fixed dictGet arguments check during GROUP BY injective functions elimination. #12179 (Azat Khuzhin).Fixed wrong logic in ALTER DELETE that leads to deleting of records when condition evaluates to NULL. This fixes #9088. This closes #12106. #12153 (alexey-milovidov).Fixed transform of query to send to external DBMS (e.g. MySQL, ODBC) in presense of aliases. This fixes #12032. #12151 (alexey-milovidov).Fixed potential overflow in integer division. This fixes #12119. #12140 (alexey-milovidov).Fixed potential infinite loop in greatCircleDistance, geoDistance. This fixes #12117. #12137 (alexey-milovidov).Avoid There is no query exception for materialized views with joins or with subqueries attached to system logs (system.query_log, metric_log, etc) or to engine=Buffer underlying table. #12120 (filimonov).Fixed performance for selects with UNION caused by wrong limit for the total number of threads. Fixes #12030. #12103 (Nikolai Kochetov).Fixed segfault with -StateResample combinators. #12092 (Anton Popov).Fixed unnecessary limiting the number of threads for selects from VIEW. Fixes #11937. #12085 (Nikolai Kochetov).Fixed possible crash while using wrong type for PREWHERE. Fixes #12053, #12060. #12060 (Nikolai Kochetov).Fixed error Expected single dictionary argument for function for function defaultValueOfArgumentType with LowCardinality type. Fixes #11808. #12056 (Nikolai Kochetov).Fixed error Cannot capture column for higher-order functions with Tuple(LowCardinality) argument. Fixes #9766. #12055 (Nikolai Kochetov).Parse tables metadata in parallel when loading database. This fixes slow server startup when there are large number of tables. #12045 (tavplubix).Make topK aggregate function return Enum for Enum types. This fixes #3740. #12043 (alexey-milovidov).Fixed constraints check if constraint is a constant expression. This fixes #11360. #12042 (alexey-milovidov).Fixed incorrect comparison of tuples with Nullable columns. Fixes #11985. #12039 (Nikolai Kochetov).Fixed wrong result and potential crash when invoking function if with arguments of type FixedString with different sizes. This fixes #11362. #12021 (alexey-milovidov).A query with function neighbor as the only returned expression may return empty result if the function is called with offset -9223372036854775808. This fixes #11367. #12019 (alexey-milovidov).Fixed potential array size overflow in generateRandom that may lead to crash. This fixes #11371. #12013 (alexey-milovidov).Fixed potential floating point exception. This closes #11378. #12005 (alexey-milovidov).Fixed wrong setting name in log message at server startup. #11997 (alexey-milovidov).Fixed Query parameter was not set in Values format. Fixes #11918. #11936 (tavplubix).Keep aliases for substitutions in query (parametrized queries). This fixes #11914. #11916 (alexey-milovidov).Fixed potential floating point exception when parsing DateTime64. This fixes #11374. #11875 (alexey-milovidov).Fixed memory accounting via HTTP interface (can be significant with wait_end_of_query=1). #11840 (Azat Khuzhin).Fixed wrong result for if() with NULLs in condition. #11807 (Artem Zuikov).Parse metadata stored in zookeeper before checking for equality. #11739 (Azat Khuzhin).Fixed LIMIT n WITH TIES usage together with ORDER BY statement, which contains aliases. #11689 (Anton Popov).Fix potential read of uninitialized memory in cache dictionary. #10834 (alexey-milovidov). Performance Improvement​ Index not used for IN operator with literals, performance regression introduced around v19.3. This fixes #10574. #12062 (nvartolomei). "},{"title":"ClickHouse release v20.3.12.112-lts 2020-06-25​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v20312112-lts-2020-06-25","content":"Bug Fix​ Fix rare crash caused by using Nullable column in prewhere condition. Continuation of #11608. #11869 (Nikolai Kochetov).Don't allow arrayJoin inside higher order functions. It was leading to broken protocol synchronization. This closes #3933. #11846 (alexey-milovidov).Fix using too many threads for queries. #11788 (Nikolai Kochetov).Fix unexpected behaviour of queries like SELECT *, xyz.* which were success while an error expected. #11753 (hexiaoting).Now replicated fetches will be cancelled during metadata alter. #11744 (alesapin).Fixed LOGICAL_ERROR caused by wrong type deduction of complex literals in Values input format. #11732 (tavplubix).Fix ORDER BY ... WITH FILL over const columns. #11697 (Anton Popov).Pass proper timeouts when communicating with XDBC bridge. Recently timeouts were not respected when checking bridge liveness and receiving meta info. #11690 (alexey-milovidov).Fix error which leads to an incorrect state of system.mutations. It may show that whole mutation is already done but the server still has MUTATE_PART tasks in the replication queue and tries to execute them. This fixes #11611. #11681 (alesapin).Add support for regular expressions with case-insensitive flags. This fixes #11101 and fixes #11506. #11649 (alexey-milovidov).Remove trivial count query optimization if row-level security is set. In previous versions the user get total count of records in a table instead filtered. This fixes #11352. #11644 (alexey-milovidov).Fix bloom filters for String (data skipping indices). #11638 (Azat Khuzhin).Fix rare crash caused by using Nullable column in prewhere condition. (Probably it is connected with #11572 somehow). #11608 (Nikolai Kochetov).Fix error Block structure mismatch for queries with sampling reading from Buffer table. #11602 (Nikolai Kochetov).Fix wrong exit code of the clickhouse-client, when exception.code() % 256 = 0. #11601 (filimonov).Fix trivial error in log message about &quot;Mark cache size was lowered&quot; at server startup. This closes #11399. #11589 (alexey-milovidov).Fix error Size of offsets does not match size of column for queries with PREWHERE column in (subquery) and ARRAY JOIN. #11580 (Nikolai Kochetov).All queries in HTTP session have had the same query_id. It is fixed. #11578 (tavplubix).Now clickhouse-server docker container will prefer IPv6 checking server aliveness. #11550 (Ivan Starkov).Fix shard_num/replica_num for &lt;node&gt; (breaks use_compact_format_in_distributed_parts_names). #11528 (Azat Khuzhin).Fix memory leak when exception is thrown in the middle of aggregation with -State functions. This fixes #8995. #11496 (alexey-milovidov).Fix wrong results of distributed queries when alias could override qualified column name. Fixes #9672 #9714. #9972 (Artem Zuikov). "},{"title":"ClickHouse release v20.3.11.97-lts 2020-06-10​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v2031197-lts-2020-06-10","content":"New Feature​ Now ClickHouse controls timeouts of dictionary sources on its side. Two new settings added to cache dictionary configuration: strict_max_lifetime_seconds, which is max_lifetime by default and query_wait_timeout_milliseconds, which is one minute by default. The first settings is also useful with allow_read_expired_keys settings (to forbid reading very expired keys). #10337 (Nikita Mikhaylov). Bug Fix​ Fix the error Data compressed with different methods that can happen if min_bytes_to_use_direct_io is enabled and PREWHERE is active and using SAMPLE or high number of threads. This fixes #11539. #11540 (alexey-milovidov).Fix return compressed size for codecs. #11448 (Nikolai Kochetov).Fix server crash when a column has compression codec with non-literal arguments. Fixes #11365. #11431 (alesapin).Fix pointInPolygon with nan as point. Fixes #11375. #11421 (Alexey Ilyukhov).Fix crash in JOIN over LowCarinality(T) and Nullable(T). #11380. #11414 (Artem Zuikov).Fix error code for wrong USING key. #11373. #11404 (Artem Zuikov).Fixed geohashesInBox with arguments outside of latitude/longitude range. #11403 (Vasily Nemkov).Better errors for joinGet() functions. #11389 (Artem Zuikov).Fix possible Pipeline stuck error for queries with external sort and limit. Fixes #11359. #11366 (Nikolai Kochetov).Remove redundant lock during parts send in ReplicatedMergeTree. #11354 (alesapin).Fix support for \\G (vertical output) in clickhouse-client in multiline mode. This closes #9933. #11350 (alexey-milovidov).Fix crash in direct selects from StorageJoin (without JOIN) and wrong nullability. #11340 (Artem Zuikov).Fix crash in quantilesExactWeightedArray. #11337 (Nikolai Kochetov).Now merges stopped before change metadata in ALTER queries. #11335 (alesapin).Make writing to MATERIALIZED VIEW with setting parallel_view_processing = 1 parallel again. Fixes #10241. #11330 (Nikolai Kochetov).Fix visitParamExtractRaw when extracted JSON has strings with unbalanced { or [. #11318 (Ewout).Fix very rare race condition in ThreadPool. #11314 (alexey-milovidov).Fix potential uninitialized memory in conversion. Example: SELECT toIntervalSecond(now64()). #11311 (alexey-milovidov).Fix the issue when index analysis cannot work if a table has Array column in primary key and if a query is filtering by this column with empty or notEmpty functions. This fixes #11286. #11303 (alexey-milovidov).Fix bug when query speed estimation can be incorrect and the limit of min_execution_speed may not work or work incorrectly if the query is throttled by max_network_bandwidth, max_execution_speed or priority settings. Change the default value of timeout_before_checking_execution_speed to non-zero, because otherwise the settings min_execution_speed and max_execution_speed have no effect. This fixes #11297. This fixes #5732. This fixes #6228. Usability improvement: avoid concatenation of exception message with progress bar in clickhouse-client. #11296 (alexey-milovidov).Fix crash while reading malformed data in Protobuf format. This fixes #5957, fixes #11203. #11258 (Vitaly Baranov).Fixed a bug when cache-dictionary could return default value instead of normal (when there are only expired keys). This affects only string fields. #11233 (Nikita Mikhaylov).Fix error Block structure mismatch in QueryPipeline while reading from VIEW with constants in inner query. Fixes #11181. #11205 (Nikolai Kochetov).Fix possible exception Invalid status for associated output. #11200 (Nikolai Kochetov).Fix possible error Cannot capture column for higher-order functions with Array(Array(LowCardinality)) captured argument. #11185 (Nikolai Kochetov).Fixed S3 globbing which could fail in case of more than 1000 keys and some backends. #11179 (Vladimir Chebotarev).If data skipping index is dependent on columns that are going to be modified during background merge (for SummingMergeTree, AggregatingMergeTree as well as for TTL GROUP BY), it was calculated incorrectly. This issue is fixed by moving index calculation after merge so the index is calculated on merged data. #11162 (Azat Khuzhin).Fix excessive reserving of threads for simple queries (optimization for reducing the number of threads, which was partly broken after changes in pipeline). #11114 (Azat Khuzhin).Fix predicates optimization for distributed queries (enable_optimize_predicate_expression=1) for queries with HAVING section (i.e. when filtering on the server initiator is required), by preserving the order of expressions (and this is enough to fix), and also force aggregator use column names over indexes. Fixes: #10613, #11413. #10621 (Azat Khuzhin).Introduce commit retry logic to decrease the possibility of getting duplicates from Kafka in rare cases when offset commit was failed. #9884 (filimonov). Performance Improvement​ Get dictionary and check access rights only once per each call of any function reading external dictionaries. #10928 (Vitaly Baranov). Build/Testing/Packaging Improvement​ Fix several flaky integration tests. #11355 (alesapin). "},{"title":"ClickHouse release v20.3.10.75-lts 2020-05-23​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v2031075-lts-2020-05-23","content":"Bug Fix​ Removed logging from mutation finalization task if nothing was finalized. #11109 (alesapin).Fixed parseDateTime64BestEffort argument resolution bugs. #11038 (Vasily Nemkov).Fixed incorrect raw data size in method getRawData(). #10964 (Igr).Fixed incompatibility of two-level aggregation between versions 20.1 and earlier. This incompatibility happens when different versions of ClickHouse are used on initiator node and remote nodes and the size of GROUP BY result is large and aggregation is performed by a single String field. It leads to several unmerged rows for a single key in result. #10952 (alexey-milovidov).Fixed backward compatibility with tuples in Distributed tables. #10889 (Anton Popov).Fixed SIGSEGV in StringHashTable if such a key does not exist. #10870 (Azat Khuzhin).Fixed bug in ReplicatedMergeTree which might cause some ALTER on OPTIMIZE query to hang waiting for some replica after it become inactive. #10849 (tavplubix).Fixed columns order after Block::sortColumns(). #10826 (Azat Khuzhin).Fixed the issue with ODBC bridge when no quoting of identifiers is requested. Fixes #7984. #10821 (alexey-milovidov).Fixed UBSan and MSan report in DateLUT. #10798 (alexey-milovidov).Fixed incorrect type conversion in key conditions. Fixes #6287. #10791 (Andrew Onyshchuk)Fixed parallel_view_processing behavior. Now all insertions into MATERIALIZED VIEW without exception should be finished if exception happened. Fixes #10241. #10757 (Nikolai Kochetov).Fixed combinator -OrNull and -OrDefault when combined with -State. #10741 (hcz).Fixed crash in generateRandom with nested types. Fixes #10583. #10734 (Nikolai Kochetov).Fixed data corruption for LowCardinality(FixedString) key column in SummingMergeTree which could have happened after merge. Fixes #10489. #10721 (Nikolai Kochetov).Fixed possible buffer overflow in function h3EdgeAngle. #10711 (alexey-milovidov).Fixed disappearing totals. Totals could have being filtered if query had had join or subquery with external where condition. Fixes #10674. #10698 (Nikolai Kochetov).Fixed multiple usages of IN operator with the identical set in one query. #10686 (Anton Popov).Fixed bug, which causes http requests stuck on client close when readonly=2 and cancel_http_readonly_queries_on_client_close=1. Fixes #7939, #7019, #7736, #7091. #10684 (tavplubix).Fixed order of parameters in AggregateTransform constructor. #10667 (palasonic1).Fixed the lack of parallel execution of remote queries with distributed_aggregation_memory_efficient enabled. Fixes #10655. #10664 (Nikolai Kochetov).Fixed possible incorrect number of rows for queries with LIMIT. Fixes #10566, #10709. #10660 (Nikolai Kochetov).Fixed a bug which locks concurrent alters when table has a lot of parts. #10659 (alesapin).Fixed a bug when on SYSTEM DROP DNS CACHE query also drop caches, which are used to check if user is allowed to connect from some IP addresses. #10608 (tavplubix).Fixed incorrect scalar results inside inner query of MATERIALIZED VIEW in case if this query contained dependent table. #10603 (Nikolai Kochetov).Fixed SELECT of column ALIAS which default expression type different from column type. #10563 (Azat Khuzhin).Implemented comparison between DateTime64 and String values. #10560 (Vasily Nemkov).Fixed index corruption, which may occur in some cases after merge compact parts into another compact part. #10531 (Anton Popov).Fixed the situation, when mutation finished all parts, but hung up in is_done=0. #10526 (alesapin).Fixed overflow at beginning of unix epoch for timezones with fractional offset from UTC. This fixes #9335. #10513 (alexey-milovidov).Fixed improper shutdown of Distributed storage. #10491 (Azat Khuzhin).Fixed numeric overflow in simpleLinearRegression over large integers. #10474 (hcz). Build/Testing/Packaging Improvement​ Fix UBSan report in LZ4 library. #10631 (alexey-milovidov).Fix clang-10 build. #10238. #10370 (Amos Bird).Added failing tests about max_rows_to_sort setting. #10268 (alexey-milovidov).Added some improvements in printing diagnostic info in input formats. Fixes #10204. #10418 (tavplubix).Added CA certificates to clickhouse-server docker image. #10476 (filimonov). Bug fix​ Fix error the BloomFilter false positive must be a double number between 0 and 1 #10551. #10569 (Winter Zhang). "},{"title":"ClickHouse release v20.3.8.53, 2020-04-23​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v203853-2020-04-23","content":"Bug Fix​ Fixed wrong behaviour of datetime functions for timezones that has altered between positive and negative offsets from UTC (e.g. Pacific/Kiritimati). This fixes #7202 #10369 (alexey-milovidov)Fix possible segfault with distributed_group_by_no_merge enabled (introduced in 20.3.7.46 by #10131). #10399 (Nikolai Kochetov)Fix wrong flattening of Array(Tuple(...)) data types. This fixes #10259 #10390 (alexey-milovidov)Drop disks reservation in Aggregator. This fixes bug in disk space reservation, which may cause big external aggregation to fail even if it could be completed successfully #10375 (Azat Khuzhin)Fixed DROP vs OPTIMIZE race in ReplicatedMergeTree. DROP could left some garbage in replica path in ZooKeeper if there was concurrent OPTIMIZE query. #10312 (tavplubix)Fix bug when server cannot attach table after column default was altered. #10441 (alesapin)Do not remove metadata directory when attach database fails before loading tables. #10442 (Winter Zhang)Fixed several bugs when some data was inserted with quorum, then deleted somehow (DROP PARTITION, TTL) and this leaded to the stuck of INSERTs or false-positive exceptions in SELECTs. This fixes #9946 #10188 (Nikita Mikhaylov)Fix possible Pipeline stuck error in ConcatProcessor which could have happened in remote query. #10381 (Nikolai Kochetov)Fixed wrong behavior in HashTable that caused compilation error when trying to read HashMap from buffer. #10386 (palasonic1)Allow to use count(*) with multiple JOINs. Fixes #9853 #10291 (Artem Zuikov)Prefer fallback_to_stale_replicas over skip_unavailable_shards, otherwise when both settings specified and there are no up-to-date replicas the query will fail (patch from @alex-zaitsev). Fixes: #2564. #10422 (Azat Khuzhin)Fix the issue when a query with ARRAY JOIN, ORDER BY and LIMIT may return incomplete result. This fixes #10226. Author: Vadim Plakhtinskiy. #10427 (alexey-milovidov)Check the number and type of arguments when creating BloomFilter index #9623 #10431 (Winter Zhang) Performance Improvement​ Improved performance of queries with explicitly defined sets at right side of IN operator and tuples in the left side. This fixes performance regression in version 20.3. #9740, #10385 (Anton Popov) "},{"title":"ClickHouse release v20.3.7.46, 2020-04-17​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v203746-2020-04-17","content":"Bug Fix​ Fix Logical error: CROSS JOIN has expressions error for queries with comma and names joins mix. #10311 (Artem Zuikov).Fix queries with max_bytes_before_external_group_by. #10302 (Artem Zuikov).Fix move-to-prewhere optimization in presense of arrayJoin functions (in certain cases). This fixes #10092. #10195 (alexey-milovidov).Add the ability to relax the restriction on non-deterministic functions usage in mutations with allow_nondeterministic_mutations setting. #10186 (filimonov). "},{"title":"ClickHouse release v20.3.6.40, 2020-04-16​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v203640-2020-04-16","content":"New Feature​ Added function isConstant. This function checks whether its argument is constant expression and returns 1 or 0. It is intended for development, debugging and demonstration purposes. #10198 (alexey-milovidov). Bug Fix​ Fix error Pipeline stuck with max_rows_to_group_by and group_by_overflow_mode = 'break'. #10279 (Nikolai Kochetov).Fix rare possible exception Cannot drain connections: cancel first. #10239 (Nikolai Kochetov).Fixed bug where ClickHouse would throw &quot;Unknown function lambda.&quot; error message when user tries to run ALTER UPDATE/DELETE on tables with ENGINE = Replicated*. Check for nondeterministic functions now handles lambda expressions correctly. #10237 (Alexander Kazakov).Fixed &quot;generateRandom&quot; function for Date type. This fixes #9973. Fix an edge case when dates with year 2106 are inserted to MergeTree tables with old-style partitioning but partitions are named with year 1970. #10218 (alexey-milovidov).Convert types if the table definition of a View does not correspond to the SELECT query. This fixes #10180 and #10022. #10217 (alexey-milovidov).Fix parseDateTimeBestEffort for strings in RFC-2822 when day of week is Tuesday or Thursday. This fixes #10082. #10214 (alexey-milovidov).Fix column names of constants inside JOIN that may clash with names of constants outside of JOIN. #10207 (alexey-milovidov).Fix possible inifinite query execution when the query actually should stop on LIMIT, while reading from infinite source like system.numbers or system.zeros. #10206 (Nikolai Kochetov).Fix using the current database for access checking when the database isn't specified. #10192 (Vitaly Baranov).Convert blocks if structure does not match on INSERT into Distributed(). #10135 (Azat Khuzhin).Fix possible incorrect result for extremes in processors pipeline. #10131 (Nikolai Kochetov).Fix some kinds of alters with compact parts. #10130 (Anton Popov).Fix incorrect index_granularity_bytes check while creating new replica. Fixes #10098. #10121 (alesapin).Fix SIGSEGV on INSERT into Distributed table when its structure differs from the underlying tables. #10105 (Azat Khuzhin).Fix possible rows loss for queries with JOIN and UNION ALL. Fixes #9826, #10113. #10099 (Nikolai Kochetov).Fixed replicated tables startup when updating from an old ClickHouse version where /table/replicas/replica_name/metadata node does not exist. Fixes #10037. #10095 (alesapin).Add some arguments check and support identifier arguments for MySQL Database Engine. #10077 (Winter Zhang).Fix bug in clickhouse dictionary source from localhost clickhouse server. The bug may lead to memory corruption if types in dictionary and source are not compatible. #10071 (alesapin).Fix bug in CHECK TABLE query when table contain skip indices. #10068 (alesapin).Fix error Cannot clone block with columns because block has 0 columns ... While executing GroupingAggregatedTransform. It happened when setting distributed_aggregation_memory_efficient was enabled, and distributed query read aggregating data with different level from different shards (mixed single and two level aggregation). #10063 (Nikolai Kochetov).Fix a segmentation fault that could occur in GROUP BY over string keys containing trailing zero bytes (#8636, #8925). #10025 (Alexander Kuzmenkov).Fix the number of threads used for remote query execution (performance regression, since 20.3). This happened when query from Distributed table was executed simultaneously on local and remote shards. Fixes #9965. #9971 (Nikolai Kochetov).Fix bug in which the necessary tables weren't retrieved at one of the processing stages of queries to some databases. Fixes #9699. #9949 (achulkov2).Fix 'Not found column in block' error when JOIN appears with TOTALS. Fixes #9839. #9939 (Artem Zuikov).Fix a bug with ON CLUSTER DDL queries freezing on server startup. #9927 (Gagan Arneja).Fix parsing multiple hosts set in the CREATE USER command, e.g. CREATE USER user6 HOST NAME REGEXP 'lo.?*host', NAME REGEXP 'lo*host'. #9924 (Vitaly Baranov).Fix TRUNCATE for Join table engine (#9917). #9920 (Amos Bird).Fix &quot;scalar does not exist&quot; error in ALTERs (#9878). #9904 (Amos Bird).Fix race condition between drop and optimize in ReplicatedMergeTree. #9901 (alesapin).Fix error with qualified names in distributed_product_mode='local'. Fixes #4756. #9891 (Artem Zuikov).Fix calculating grants for introspection functions from the setting 'allow_introspection_functions'. #9840 (Vitaly Baranov). Build/Testing/Packaging Improvement​ Fix integration test test_settings_constraints. #9962 (Vitaly Baranov).Removed dependency on clock_getres. #9833 (alexey-milovidov). "},{"title":"ClickHouse release v20.3.5.21, 2020-03-27​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v203521-2020-03-27","content":"Bug Fix​ Fix 'Different expressions with the same alias' error when query has PREWHERE and WHERE on distributed table and SET distributed_product_mode = 'local'. #9871 (Artem Zuikov).Fix mutations excessive memory consumption for tables with a composite primary key. This fixes #9850. #9860 (alesapin).For INSERT queries shard now clamps the settings got from the initiator to the shard's constaints instead of throwing an exception. This fix allows to send INSERT queries to a shard with another constraints. This change improves fix #9447. #9852 (Vitaly Baranov).Fix 'COMMA to CROSS JOIN rewriter is not enabled or cannot rewrite query' error in case of subqueries with COMMA JOIN out of tables lists (i.e. in WHERE). Fixes #9782. #9830 (Artem Zuikov).Fix possible exception Got 0 in totals chunk, expected 1 on client. It happened for queries with JOIN in case if right joined table had zero rows. Example: select * from system.one t1 join system.one t2 on t1.dummy = t2.dummy limit 0 FORMAT TabSeparated;. Fixes #9777. #9823 (Nikolai Kochetov).Fix SIGSEGV with optimize_skip_unused_shards when type cannot be converted. #9804 (Azat Khuzhin).Fix broken ALTER TABLE DELETE COLUMN query for compact parts. #9779 (alesapin).Fix max_distributed_connections (w/ and w/o Processors). #9673 (Azat Khuzhin).Fixed a few cases when timezone of the function argument wasn't used properly. #9574 (Vasily Nemkov). Improvement​ Remove order by stage from mutations because we read from a single ordered part in a single thread. Also add check that the order of rows in mutation is ordered in sorting key order and this order is not violated. #9886 (alesapin). "},{"title":"ClickHouse release v20.3.4.10, 2020-03-20​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v203410-2020-03-20","content":"Bug Fix​ This release also contains all bug fixes from 20.1.8.41Fix missing rows_before_limit_at_least for queries over http (with processors pipeline). This fixes #9730. #9757 (Nikolai Kochetov) "},{"title":"ClickHouse release v20.3.3.6, 2020-03-17​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v20336-2020-03-17","content":"Bug Fix​ This release also contains all bug fixes from 20.1.7.38Fix bug in a replication that does not allow replication to work if the user has executed mutations on the previous version. This fixes #9645. #9652 (alesapin). It makes version 20.3 backward compatible again.Add setting use_compact_format_in_distributed_parts_names which allows to write files for INSERT queries into Distributed table with more compact format. This fixes #9647. #9653 (alesapin). It makes version 20.3 backward compatible again. "},{"title":"ClickHouse release v20.3.2.1, 2020-03-12​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v20321-2020-03-12","content":"Backward Incompatible Change​ Fixed the issue file name too long when sending data for Distributed tables for a large number of replicas. Fixed the issue that replica credentials were exposed in the server log. The format of directory name on disk was changed to [shard{shard_index}[_replica{replica_index}]]. #8911 (Mikhail Korotov) After you upgrade to the new version, you will not be able to downgrade without manual intervention, because old server version does not recognize the new directory format. If you want to downgrade, you have to manually rename the corresponding directories to the old format. This change is relevant only if you have used asynchronous INSERTs to Distributed tables. In the version 20.3.3 we will introduce a setting that will allow you to enable the new format gradually.Changed the format of replication log entries for mutation commands. You have to wait for old mutations to process before installing the new version.Implement simple memory profiler that dumps stacktraces to system.trace_log every N bytes over soft allocation limit #8765 (Ivan) #9472 (alexey-milovidov) The column of system.trace_log was renamed from timer_type to trace_type. This will require changes in third-party performance analysis and flamegraph processing tools.Use OS thread id everywhere instead of internal thread number. This fixes #7477 Old clickhouse-client cannot receive logs that are send from the server when the setting send_logs_level is enabled, because the names and types of the structured log messages were changed. On the other hand, different server versions can send logs with different types to each other. When you don't use the send_logs_level setting, you should not care. #8954 (alexey-milovidov)Remove indexHint function #9542 (alexey-milovidov)Remove findClusterIndex, findClusterValue functions. This fixes #8641. If you were using these functions, send an email to clickhouse-feedback@yandex-team.com #9543 (alexey-milovidov)Now it's not allowed to create columns or add columns with SELECT subquery as default expression. #9481 (alesapin)Require aliases for subqueries in JOIN. #9274 (Artem Zuikov)Improved ALTER MODIFY/ADD queries logic. Now you cannot ADD column without type, MODIFY default expression does not change type of column and MODIFY type does not loose default expression value. Fixes #8669. #9227 (alesapin)Require server to be restarted to apply the changes in logging configuration. This is a temporary workaround to avoid the bug where the server logs to a deleted log file (see #8696). #8707 (Alexander Kuzmenkov)The setting experimental_use_processors is enabled by default. This setting enables usage of the new query pipeline. This is internal refactoring and we expect no visible changes. If you will see any issues, set it to back zero. #8768 (alexey-milovidov) New Feature​ Add Avro and AvroConfluent input/output formats #8571 (Andrew Onyshchuk) #8957 (Andrew Onyshchuk) #8717 (alexey-milovidov)Multi-threaded and non-blocking updates of expired keys in cache dictionaries (with optional permission to read old ones). #8303 (Nikita Mikhaylov)Add query ALTER ... MATERIALIZE TTL. It runs mutation that forces to remove expired data by TTL and recalculates meta-information about TTL in all parts. #8775 (Anton Popov)Switch from HashJoin to MergeJoin (on disk) if needed #9082 (Artem Zuikov)Added MOVE PARTITION command for ALTER TABLE #4729 #6168 (Guillaume Tassery)Reloading storage configuration from configuration file on the fly. #8594 (Vladimir Chebotarev)Allowed to change storage_policy to not less rich one. #8107 (Vladimir Chebotarev)Added support for globs/wildcards for S3 storage and table function. #8851 (Vladimir Chebotarev)Implement bitAnd, bitOr, bitXor, bitNot for FixedString(N) datatype. #9091 (Guillaume Tassery)Added function bitCount. This fixes #8702. #8708 (alexey-milovidov) #8749 (ikopylov)Add generateRandom table function to generate random rows with given schema. Allows to populate arbitrary test table with data. #8994 (Ilya Yatsishin)JSONEachRowFormat: support special case when objects enclosed in top-level array. #8860 (Kruglov Pavel)Now it's possible to create a column with DEFAULT expression which depends on a column with default ALIAS expression. #9489 (alesapin)Allow to specify --limit more than the source data size in clickhouse-obfuscator. The data will repeat itself with different random seed. #9155 (alexey-milovidov)Added groupArraySample function (similar to groupArray) with reservior sampling algorithm. #8286 (Amos Bird)Now you can monitor the size of update queue in cache/complex_key_cache dictionaries via system metrics. #9413 (Nikita Mikhaylov)Allow to use CRLF as a line separator in CSV output format with setting output_format_csv_crlf_end_of_line is set to 1 #8934 #8935 #8963 (Mikhail Korotov)Implement more functions of the H3 API: h3GetBaseCell, h3HexAreaM2, h3IndexesAreNeighbors, h3ToChildren, h3ToString and stringToH3 #8938 (Nico Mandery)New setting introduced: max_parser_depth to control maximum stack size and allow large complex queries. This fixes #6681 and #7668. #8647 (Maxim Smirnov)Add a setting force_optimize_skip_unused_shards setting to throw if skipping of unused shards is not possible #8805 (Azat Khuzhin)Allow to configure multiple disks/volumes for storing data for send in Distributed engine #8756 (Azat Khuzhin)Support storage policy (&lt;tmp_policy&gt;) for storing temporary data. #8750 (Azat Khuzhin)Added X-ClickHouse-Exception-Code HTTP header that is set if exception was thrown before sending data. This implements #4971. #8786 (Mikhail Korotov)Added function ifNotFinite. It is just a syntactic sugar: ifNotFinite(x, y) = isFinite(x) ? x : y. #8710 (alexey-milovidov)Added last_successful_update_time column in system.dictionaries table #9394 (Nikita Mikhaylov)Add blockSerializedSize function (size on disk without compression) #8952 (Azat Khuzhin)Add function moduloOrZero #9358 (hcz)Added system tables system.zeros and system.zeros_mt as well as tale functions zeros() and zeros_mt(). Tables (and table functions) contain single column with name zero and type UInt8. This column contains zeros. It is needed for test purposes as the fastest method to generate many rows. This fixes #6604 #9593 (Nikolai Kochetov) Experimental Feature​ Add new compact format of parts in MergeTree-family tables in which all columns are stored in one file. It helps to increase performance of small and frequent inserts. The old format (one file per column) is now called wide. Data storing format is controlled by settings min_bytes_for_wide_part and min_rows_for_wide_part. #8290 (Anton Popov)Support for S3 storage for Log, TinyLog and StripeLog tables. #8862 (Pavel Kovalenko) Bug Fix​ Fixed inconsistent whitespaces in log messages. #9322 (alexey-milovidov)Fix bug in which arrays of unnamed tuples were flattened as Nested structures on table creation. #8866 (achulkov2)Fixed the issue when &quot;Too many open files&quot; error may happen if there are too many files matching glob pattern in File table or file table function. Now files are opened lazily. This fixes #8857 #8861 (alexey-milovidov)DROP TEMPORARY TABLE now drops only temporary table. #8907 (Vitaly Baranov)Remove outdated partition when we shutdown the server or DETACH/ATTACH a table. #8602 (Guillaume Tassery)For how the default disk calculates the free space from data subdirectory. Fixed the issue when the amount of free space is not calculated correctly if the data directory is mounted to a separate device (rare case). This fixes #7441 #9257 (Mikhail Korotov)Allow comma (cross) join with IN () inside. #9251 (Artem Zuikov)Allow to rewrite CROSS to INNER JOIN if there's [NOT] LIKE operator in WHERE section. #9229 (Artem Zuikov)Fix possible incorrect result after GROUP BY with enabled setting distributed_aggregation_memory_efficient. Fixes #9134. #9289 (Nikolai Kochetov)Found keys were counted as missed in metrics of cache dictionaries. #9411 (Nikita Mikhaylov)Fix replication protocol incompatibility introduced in #8598. #9412 (alesapin)Fixed race condition on queue_task_handle at the startup of ReplicatedMergeTree tables. #9552 (alexey-milovidov)The token NOT did not work in SHOW TABLES NOT LIKE query #8727 #8940 (alexey-milovidov)Added range check to function h3EdgeLengthM. Without this check, buffer overflow is possible. #8945 (alexey-milovidov)Fixed up a bug in batched calculations of ternary logical OPs on multiple arguments (more than 10). #8718 (Alexander Kazakov)Fix error of PREWHERE optimization, which could lead to segfaults or Inconsistent number of columns got from MergeTreeRangeReader exception. #9024 (Anton Popov)Fix unexpected Timeout exceeded while reading from socket exception, which randomly happens on secure connection before timeout actually exceeded and when query profiler is enabled. Also add connect_timeout_with_failover_secure_ms settings (default 100ms), which is similar to connect_timeout_with_failover_ms, but is used for secure connections (because SSL handshake is slower, than ordinary TCP connection) #9026 (tavplubix)Fix bug with mutations finalization, when mutation may hang in state with parts_to_do=0 and is_done=0. #9022 (alesapin)Use new ANY JOIN logic with partial_merge_join setting. It's possible to make ANY|ALL|SEMI LEFT and ALL INNER joins with partial_merge_join=1 now. #8932 (Artem Zuikov)Shard now clamps the settings got from the initiator to the shard's constaints instead of throwing an exception. This fix allows to send queries to a shard with another constraints. #9447 (Vitaly Baranov)Fixed memory management problem in MergeTreeReadPool. #8791 (Vladimir Chebotarev)Fix toDecimal*OrNull() functions family when called with string e. Fixes #8312 #8764 (Artem Zuikov)Make sure that FORMAT Null sends no data to the client. #8767 (Alexander Kuzmenkov)Fix bug that timestamp in LiveViewBlockInputStream will not updated. LIVE VIEW is an experimental feature. #8644 (vxider) #8625 (vxider)Fixed ALTER MODIFY TTL wrong behavior which did not allow to delete old TTL expressions. #8422 (Vladimir Chebotarev)Fixed UBSan report in MergeTreeIndexSet. This fixes #9250 #9365 (alexey-milovidov)Fixed the behaviour of match and extract functions when haystack has zero bytes. The behaviour was wrong when haystack was constant. This fixes #9160 #9163 (alexey-milovidov) #9345 (alexey-milovidov)Avoid throwing from destructor in Apache Avro 3rd-party library. #9066 (Andrew Onyshchuk)Don't commit a batch polled from Kafka partially as it can lead to holes in data. #8876 (filimonov)Fix joinGet with nullable return types. #8919 #9014 (Amos Bird)Fix data incompatibility when compressed with T64 codec. #9016 (Artem Zuikov) Fix data type ids in T64 compression codec that leads to wrong (de)compression in affected versions. #9033 (Artem Zuikov)Add setting enable_early_constant_folding and disable it in some cases that leads to errors. #9010 (Artem Zuikov)Fix pushdown predicate optimizer with VIEW and enable the test #9011 (Winter Zhang)Fix segfault in Merge tables, that can happen when reading from File storages #9387 (tavplubix)Added a check for storage policy in ATTACH PARTITION FROM, REPLACE PARTITION, MOVE TO TABLE. Otherwise it could make data of part inaccessible after restart and prevent ClickHouse to start. #9383 (Vladimir Chebotarev)Fix alters if there is TTL set for table. #8800 (Anton Popov)Fix race condition that can happen when SYSTEM RELOAD ALL DICTIONARIES is executed while some dictionary is being modified/added/removed. #8801 (Vitaly Baranov)In previous versions Memory database engine use empty data path, so tables are created in path directory (e.g. /var/lib/clickhouse/), not in data directory of database (e.g. /var/lib/clickhouse/db_name). #8753 (tavplubix)Fixed wrong log messages about missing default disk or policy. #9530 (Vladimir Chebotarev)Fix not(has()) for the bloom_filter index of array types. #9407 (achimbab)Allow first column(s) in a table with Log engine be an alias #9231 (Ivan)Fix order of ranges while reading from MergeTree table in one thread. It could lead to exceptions from MergeTreeRangeReader or wrong query results. #9050 (Anton Popov)Make reinterpretAsFixedString to return FixedString instead of String. #9052 (Andrew Onyshchuk)Avoid extremely rare cases when the user can get wrong error message (Success instead of detailed error description). #9457 (alexey-milovidov)Do not crash when using Template format with empty row template. #8785 (Alexander Kuzmenkov)Metadata files for system tables could be created in wrong place #8653 (tavplubix) Fixes #8581.Fix data race on exception_ptr in cache dictionary #8303. #9379 (Nikita Mikhaylov)Do not throw an exception for query ATTACH TABLE IF NOT EXISTS. Previously it was thrown if table already exists, despite the IF NOT EXISTS clause. #8967 (Anton Popov)Fixed missing closing paren in exception message. #8811 (alexey-milovidov)Avoid message Possible deadlock avoided at the startup of clickhouse-client in interactive mode. #9455 (alexey-milovidov)Fixed the issue when padding at the end of base64 encoded value can be malformed. Update base64 library. This fixes #9491, closes #9492 #9500 (alexey-milovidov)Prevent losing data in Kafka in rare cases when exception happens after reading suffix but before commit. Fixes #9378 #9507 (filimonov)Fixed exception in DROP TABLE IF EXISTS #8663 (Nikita Vasilev)Fix crash when a user tries to ALTER MODIFY SETTING for old-formated MergeTree table engines family. #9435 (alesapin)Support for UInt64 numbers that don't fit in Int64 in JSON-related functions. Update SIMDJSON to master. This fixes #9209 #9344 (alexey-milovidov)Fixed execution of inversed predicates when non-strictly monotinic functional index is used. #9223 (Alexander Kazakov)Don't try to fold IN constant in GROUP BY #8868 (Amos Bird)Fix bug in ALTER DELETE mutations which leads to index corruption. This fixes #9019 and #8982. Additionally fix extremely rare race conditions in ReplicatedMergeTree ALTER queries. #9048 (alesapin)When the setting compile_expressions is enabled, you can get unexpected column in LLVMExecutableFunction when we use Nullable type #8910 (Guillaume Tassery)Multiple fixes for Kafka engine: 1) fix duplicates that were appearing during consumer group rebalance. 2) Fix rare 'holes' appeared when data were polled from several partitions with one poll and committed partially (now we always process / commit the whole polled block of messages). 3) Fix flushes by block size (before that only flushing by timeout was working properly). 4) better subscription procedure (with assignment feedback). 5) Make tests work faster (with default intervals and timeouts). Due to the fact that data was not flushed by block size before (as it should according to documentation), that PR may lead to some performance degradation with default settings (due to more often &amp; tinier flushes which are less optimal). If you encounter the performance issue after that change - please increase kafka_max_block_size in the table to the bigger value ( for example CREATE TABLE ...Engine=Kafka ... SETTINGS ... kafka_max_block_size=524288). Fixes #7259 #8917 (filimonov)Fix Parameter out of bound exception in some queries after PREWHERE optimizations. #8914 (Baudouin Giard)Fixed the case of mixed-constness of arguments of function arrayZip. #8705 (alexey-milovidov)When executing CREATE query, fold constant expressions in storage engine arguments. Replace empty database name with current database. Fixes #6508, #3492 #9262 (tavplubix)Now it's not possible to create or add columns with simple cyclic aliases like a DEFAULT b, b DEFAULT a. #9603 (alesapin)Fixed a bug with double move which may corrupt original part. This is relevant if you use ALTER TABLE MOVE #8680 (Vladimir Chebotarev)Allow interval identifier to correctly parse without backticks. Fixed issue when a query cannot be executed even if the interval identifier is enclosed in backticks or double quotes. This fixes #9124. #9142 (alexey-milovidov)Fixed fuzz test and incorrect behaviour of bitTestAll/bitTestAny functions. #9143 (alexey-milovidov)Fix possible crash/wrong number of rows in LIMIT n WITH TIES when there are a lot of rows equal to n'th row. #9464 (tavplubix)Fix mutations with parts written with enabled insert_quorum. #9463 (alesapin)Fix data race at destruction of Poco::HTTPServer. It could happen when server is started and immediately shut down. #9468 (Anton Popov)Fix bug in which a misleading error message was shown when running SHOW CREATE TABLE a_table_that_does_not_exist. #8899 (achulkov2)Fixed Parameters are out of bound exception in some rare cases when we have a constant in the SELECT clause when we have an ORDER BY and a LIMIT clause. #8892 (Guillaume Tassery)Fix mutations finalization, when already done mutation can have status is_done=0. #9217 (alesapin)Prevent from executing ALTER ADD INDEX for MergeTree tables with old syntax, because it does not work. #8822 (Mikhail Korotov)During server startup do not access table, which LIVE VIEW depends on, so server will be able to start. Also remove LIVE VIEW dependencies when detaching LIVE VIEW. LIVE VIEW is an experimental feature. #8824 (tavplubix)Fix possible segfault in MergeTreeRangeReader, while executing PREWHERE. #9106 (Anton Popov)Fix possible mismatched checksums with column TTLs. #9451 (Anton Popov)Fixed a bug when parts were not being moved in background by TTL rules in case when there is only one volume. #8672 (Vladimir Chebotarev)Fixed the issue Method createColumn() is not implemented for data type Set. This fixes #7799. #8674 (alexey-milovidov)Now we will try finalize mutations more frequently. #9427 (alesapin)Fix intDiv by minus one constant #9351 (hcz)Fix possible race condition in BlockIO. #9356 (Nikolai Kochetov)Fix bug leading to server termination when trying to use / drop Kafka table created with wrong parameters. #9513 (filimonov)Added workaround if OS returns wrong result for timer_create function. #8837 (alexey-milovidov)Fixed error in usage of min_marks_for_seek parameter. Fixed the error message when there is no sharding key in Distributed table and we try to skip unused shards. #8908 (Azat Khuzhin) Improvement​ Implement ALTER MODIFY/DROP queries on top of mutations for ReplicatedMergeTree* engines family. Now ALTERS blocks only at the metadata update stage, and don't block after that. #8701 (alesapin)Add ability to rewrite CROSS to INNER JOINs with WHERE section containing unqialified names. #9512 (Artem Zuikov)Make SHOW TABLES and SHOW DATABASES queries support the WHERE expressions and FROM/IN #9076 (sundyli)Added a setting deduplicate_blocks_in_dependent_materialized_views. #9070 (urykhy)After recent changes MySQL client started to print binary strings in hex thereby making them not readable (#9032). The workaround in ClickHouse is to mark string columns as UTF-8, which is not always, but usually the case. #9079 (Yuriy Baranov)Add support of String and FixedString keys for sumMap #8903 (Baudouin Giard)Support string keys in SummingMergeTree maps #8933 (Baudouin Giard)Signal termination of thread to the thread pool even if the thread has thrown exception #8736 (Ding Xiang Fei)Allow to set query_id in clickhouse-benchmark #9416 (Anton Popov)Don't allow strange expressions in ALTER TABLE ... PARTITION partition query. This addresses #7192 #8835 (alexey-milovidov)The table system.table_engines now provides information about feature support (like supports_ttl or supports_sort_order). #8830 (Max Akhmedov)Enable system.metric_log by default. It will contain rows with values of ProfileEvents, CurrentMetrics collected with &quot;collect_interval_milliseconds&quot; interval (one second by default). The table is very small (usually in order of megabytes) and collecting this data by default is reasonable. #9225 (alexey-milovidov)Initialize query profiler for all threads in a group, e.g. it allows to fully profile insert-queries. Fixes #6964 #8874 (Ivan)Now temporary LIVE VIEW is created by CREATE LIVE VIEW name WITH TIMEOUT [42] ... instead of CREATE TEMPORARY LIVE VIEW ..., because the previous syntax was not consistent with CREATE TEMPORARY TABLE ... #9131 (tavplubix)Add text_log.level configuration parameter to limit entries that goes to system.text_log table #8809 (Azat Khuzhin)Allow to put downloaded part to a disks/volumes according to TTL rules #8598 (Vladimir Chebotarev)For external MySQL dictionaries, allow to mutualize MySQL connection pool to &quot;share&quot; them among dictionaries. This option significantly reduces the number of connections to MySQL servers. #9409 (Clément Rodriguez)Show nearest query execution time for quantiles in clickhouse-benchmark output instead of interpolated values. It's better to show values that correspond to the execution time of some queries. #8712 (alexey-milovidov)Possibility to add key &amp; timestamp for the message when inserting data to Kafka. Fixes #7198 #8969 (filimonov)If server is run from terminal, highlight thread number, query id and log priority by colors. This is for improved readability of correlated log messages for developers. #8961 (alexey-milovidov)Better exception message while loading tables for Ordinary database. #9527 (alexey-milovidov)Implement arraySlice for arrays with aggregate function states. This fixes #9388 #9391 (alexey-milovidov)Allow constant functions and constant arrays to be used on the right side of IN operator. #8813 (Anton Popov)If zookeeper exception has happened while fetching data for system.replicas, display it in a separate column. This implements #9137 #9138 (alexey-milovidov)Atomically remove MergeTree data parts on destroy. #8402 (Vladimir Chebotarev)Support row-level security for Distributed tables. #8926 (Ivan)Now we recognize suffix (like KB, KiB...) in settings values. #8072 (Mikhail Korotov)Prevent out of memory while constructing result of a large JOIN. #8637 (Artem Zuikov)Added names of clusters to suggestions in interactive mode in clickhouse-client. #8709 (alexey-milovidov)Initialize query profiler for all threads in a group, e.g. it allows to fully profile insert-queries #8820 (Ivan)Added column exception_code in system.query_log table. #8770 (Mikhail Korotov)Enabled MySQL compatibility server on port 9004 in the default server configuration file. Fixed password generation command in the example in configuration. #8771 (Yuriy Baranov)Prevent abort on shutdown if the filesystem is readonly. This fixes #9094 #9100 (alexey-milovidov)Better exception message when length is required in HTTP POST query. #9453 (alexey-milovidov)Add _path and _file virtual columns to HDFS and File engines and hdfs and file table functions #8489 (Olga Khvostikova)Fix error Cannot find column while inserting into MATERIALIZED VIEW in case if new column was added to view's internal table. #8766 #8788 (vzakaznikov) #8788 #8806 (Nikolai Kochetov) #8803 (Nikolai Kochetov)Fix progress over native client-server protocol, by send progress after final update (like logs). This may be relevant only to some third-party tools that are using native protocol. #9495 (Azat Khuzhin)Add a system metric tracking the number of client connections using MySQL protocol (#9013). #9015 (Eugene Klimov)From now on, HTTP responses will have X-ClickHouse-Timezone header set to the same timezone value that SELECT timezone() would report. #9493 (Denis Glazachev) Performance Improvement​ Improve performance of analysing index with IN #9261 (Anton Popov)Simpler and more efficient code in Logical Functions + code cleanups. A followup to #8718 #8728 (Alexander Kazakov)Overall performance improvement (in range of 5%..200% for affected queries) by ensuring even more strict aliasing with C++20 features. #9304 (Amos Bird)More strict aliasing for inner loops of comparison functions. #9327 (alexey-milovidov)More strict aliasing for inner loops of arithmetic functions. #9325 (alexey-milovidov)A ~3 times faster implementation for ColumnVector::replicate(), via which ColumnConst::convertToFullColumn() is implemented. Also will be useful in tests when materializing constants. #9293 (Alexander Kazakov)Another minor performance improvement to ColumnVector::replicate() (this speeds up the materialize function and higher order functions) an even further improvement to #9293 #9442 (Alexander Kazakov)Improved performance of stochasticLinearRegression aggregate function. This patch is contributed by Intel. #8652 (alexey-milovidov)Improve performance of reinterpretAsFixedString function. #9342 (alexey-milovidov)Do not send blocks to client for Null format in processors pipeline. #8797 (Nikolai Kochetov) #8767 (Alexander Kuzmenkov) Build/Testing/Packaging Improvement​ Exception handling now works correctly on Windows Subsystem for Linux. See https://github.com/ClickHouse-Extras/libunwind/pull/3 This fixes #6480 #9564 (sobolevsv)Replace readline with replxx for interactive line editing in clickhouse-client #8416 (Ivan)Better build time and less template instantiations in FunctionsComparison. #9324 (alexey-milovidov)Added integration with clang-tidy in CI. See also #6044 #9566 (alexey-milovidov)Now we link ClickHouse in CI using lld even for gcc. #9049 (alesapin)Allow to randomize thread scheduling and insert glitches when THREAD_FUZZER_* environment variables are set. This helps testing. #9459 (alexey-milovidov)Enable secure sockets in stateless tests #9288 (tavplubix)Make SPLIT_SHARED_LIBRARIES=OFF more robust #9156 (Azat Khuzhin)Make &quot;performance_introspection_and_logging&quot; test reliable to random server stuck. This may happen in CI environment. See also #9515 #9528 (alexey-milovidov)Validate XML in style check. #9550 (alexey-milovidov)Fixed race condition in test 00738_lock_for_inner_table. This test relied on sleep. #9555 (alexey-milovidov)Remove performance tests of type once. This is needed to run all performance tests in statistical comparison mode (more reliable). #9557 (alexey-milovidov)Added performance test for arithmetic functions. #9326 (alexey-milovidov)Added performance test for sumMap and sumMapWithOverflow aggregate functions. Follow-up for #8933 #8947 (alexey-milovidov)Ensure style of ErrorCodes by style check. #9370 (alexey-milovidov)Add script for tests history. #8796 (alesapin)Add GCC warning -Wsuggest-override to locate and fix all places where override keyword must be used. #8760 (kreuzerkrieg)Ignore weak symbol under Mac OS X because it must be defined #9538 (Deleted user)Normalize running time of some queries in performance tests. This is done in preparation to run all the performance tests in comparison mode. #9565 (alexey-milovidov)Fix some tests to support pytest with query tests #9062 (Ivan)Enable SSL in build with MSan, so server will not fail at startup when running stateless tests #9531 (tavplubix)Fix database substitution in test results #9384 (Ilya Yatsishin)Build fixes for miscellaneous platforms #9381 (proller) #8755 (proller) #8631 (proller)Added disks section to stateless-with-coverage test docker image #9213 (Pavel Kovalenko)Get rid of in-source-tree files when building with GRPC #9588 (Amos Bird)Slightly faster build time by removing SessionCleaner from Context. Make the code of SessionCleaner more simple. #9232 (alexey-milovidov)Updated checking for hung queries in clickhouse-test script #8858 (Alexander Kazakov)Removed some useless files from repository. #8843 (alexey-milovidov)Changed type of math perftests from once to loop. #8783 (Nikolai Kochetov)Add docker image which allows to build interactive code browser HTML report for our codebase. #8781 (alesapin) See Woboq Code BrowserSuppress some test failures under MSan. #8780 (Alexander Kuzmenkov)Speedup &quot;exception while insert&quot; test. This test often time out in debug-with-coverage build. #8711 (alexey-milovidov)Updated libcxx and libcxxabi to master. In preparation to #9304 #9308 (alexey-milovidov)Fix flacky test 00910_zookeeper_test_alter_compression_codecs. #9525 (alexey-milovidov)Clean up duplicated linker flags. Make sure the linker won't look up an unexpected symbol. #9433 (Amos Bird)Add clickhouse-odbc driver into test images. This allows to test interaction of ClickHouse with ClickHouse via its own ODBC driver. #9348 (filimonov)Fix several bugs in unit tests. #9047 (alesapin)Enable -Wmissing-include-dirs GCC warning to eliminate all non-existing includes - mostly as a result of CMake scripting errors #8704 (kreuzerkrieg)Describe reasons if query profiler cannot work. This is intended for #9049 #9144 (alexey-milovidov)Update OpenSSL to upstream master. Fixed the issue when TLS connections may fail with the message OpenSSL SSL_read: error:14094438:SSL routines:ssl3_read_bytes:tlsv1 alert internal error and SSL Exception: error:2400006E:random number generator::error retrieving entropy. The issue was present in version 20.1. #8956 (alexey-milovidov)Update Dockerfile for server #8893 (Ilya Mazaev)Minor fixes in build-gcc-from-sources script #8774 (Michael Nacharov)Replace numbers to zeros in perftests where number column is not used. This will lead to more clean test results. #9600 (Nikolai Kochetov)Fix stack overflow issue when using initializer_list in Column constructors. #9367 (Deleted user)Upgrade librdkafka to v1.3.0. Enable bundled rdkafka and gsasl libraries on Mac OS X. #9000 (Andrew Onyshchuk)build fix on GCC 9.2.0 #9306 (vxider) "},{"title":"ClickHouse release v20.1​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v201","content":""},{"title":"ClickHouse release v20.1.16.120-stable 2020-60-26​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v20116120-stable-2020-60-26","content":"Bug Fix​ Fix rare crash caused by using Nullable column in prewhere condition. Continuation of #11608. #11869 (Nikolai Kochetov).Don't allow arrayJoin inside higher order functions. It was leading to broken protocol synchronization. This closes #3933. #11846 (alexey-milovidov).Fix unexpected behaviour of queries like SELECT *, xyz.* which were success while an error expected. #11753 (hexiaoting).Fixed LOGICAL_ERROR caused by wrong type deduction of complex literals in Values input format. #11732 (tavplubix).Fix ORDER BY ... WITH FILL over const columns. #11697 (Anton Popov).Pass proper timeouts when communicating with XDBC bridge. Recently timeouts were not respected when checking bridge liveness and receiving meta info. #11690 (alexey-milovidov).Add support for regular expressions with case-insensitive flags. This fixes #11101 and fixes #11506. #11649 (alexey-milovidov).Fix bloom filters for String (data skipping indices). #11638 (Azat Khuzhin).Fix rare crash caused by using Nullable column in prewhere condition. (Probably it is connected with #11572 somehow). #11608 (Nikolai Kochetov).Fix wrong exit code of the clickhouse-client, when exception.code() % 256 = 0. #11601 (filimonov).Fix trivial error in log message about &quot;Mark cache size was lowered&quot; at server startup. This closes #11399. #11589 (alexey-milovidov).Now clickhouse-server docker container will prefer IPv6 checking server aliveness. #11550 (Ivan Starkov).Fix memory leak when exception is thrown in the middle of aggregation with -State functions. This fixes #8995. #11496 (alexey-milovidov).Fix usage of primary key wrapped into a function with 'FINAL' modifier and 'ORDER BY' optimization. #10715 (Anton Popov). "},{"title":"ClickHouse release v20.1.15.109-stable 2020-06-19​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v20115109-stable-2020-06-19","content":"Bug Fix​ Fix excess lock for structure during alter. #11790 (alesapin). "},{"title":"ClickHouse release v20.1.14.107-stable 2020-06-11​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v20114107-stable-2020-06-11","content":"Bug Fix​ Fix error Size of offsets does not match size of column for queries with PREWHERE column in (subquery) and ARRAY JOIN. #11580 (Nikolai Kochetov). "},{"title":"ClickHouse release v20.1.13.105-stable 2020-06-10​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v20113105-stable-2020-06-10","content":"Bug Fix​ Fix the error Data compressed with different methods that can happen if min_bytes_to_use_direct_io is enabled and PREWHERE is active and using SAMPLE or high number of threads. This fixes #11539. #11540 (alexey-milovidov).Fix return compressed size for codecs. #11448 (Nikolai Kochetov).Fix server crash when a column has compression codec with non-literal arguments. Fixes #11365. #11431 (alesapin).Fix pointInPolygon with nan as point. Fixes #11375. #11421 (Alexey Ilyukhov).Fixed geohashesInBox with arguments outside of latitude/longitude range. #11403 (Vasily Nemkov).Fix possible Pipeline stuck error for queries with external sort and limit. Fixes #11359. #11366 (Nikolai Kochetov).Fix crash in quantilesExactWeightedArray. #11337 (Nikolai Kochetov).Make writing to MATERIALIZED VIEW with setting parallel_view_processing = 1 parallel again. Fixes #10241. #11330 (Nikolai Kochetov).Fix visitParamExtractRaw when extracted JSON has strings with unbalanced { or [. #11318 (Ewout).Fix very rare race condition in ThreadPool. #11314 (alexey-milovidov).Fix potential uninitialized memory in conversion. Example: SELECT toIntervalSecond(now64()). #11311 (alexey-milovidov).Fix the issue when index analysis cannot work if a table has Array column in primary key and if a query is filtering by this column with empty or notEmpty functions. This fixes #11286. #11303 (alexey-milovidov).Fix bug when query speed estimation can be incorrect and the limit of min_execution_speed may not work or work incorrectly if the query is throttled by max_network_bandwidth, max_execution_speed or priority settings. Change the default value of timeout_before_checking_execution_speed to non-zero, because otherwise the settings min_execution_speed and max_execution_speed have no effect. This fixes #11297. This fixes #5732. This fixes #6228. Usability improvement: avoid concatenation of exception message with progress bar in clickhouse-client. #11296 (alexey-milovidov).Fix crash while reading malformed data in Protobuf format. This fixes #5957, fixes #11203. #11258 (Vitaly Baranov).Fix possible error Cannot capture column for higher-order functions with Array(Array(LowCardinality)) captured argument. #11185 (Nikolai Kochetov).If data skipping index is dependent on columns that are going to be modified during background merge (for SummingMergeTree, AggregatingMergeTree as well as for TTL GROUP BY), it was calculated incorrectly. This issue is fixed by moving index calculation after merge so the index is calculated on merged data. #11162 (Azat Khuzhin).Remove logging from mutation finalization task if nothing was finalized. #11109 (alesapin).Fixed parseDateTime64BestEffort argument resolution bugs. #10925. #11038 (Vasily Nemkov).Fix incorrect raw data size in method getRawData(). #10964 (Igr).Fix backward compatibility with tuples in Distributed tables. #10889 (Anton Popov).Fix SIGSEGV in StringHashTable (if such key does not exist). #10870 (Azat Khuzhin).Fixed bug in ReplicatedMergeTree which might cause some ALTER on OPTIMIZE query to hang waiting for some replica after it become inactive. #10849 (tavplubix).Fix columns order after Block::sortColumns() (also add a test that shows that it affects some real use case - Buffer engine). #10826 (Azat Khuzhin).Fix the issue with ODBC bridge when no quoting of identifiers is requested. This fixes #7984. #10821 (alexey-milovidov).Fix UBSan and MSan report in DateLUT. #10798 (alexey-milovidov). Make use of src_type for correct type conversion in key conditions. Fixes #6287. #10791 (Andrew Onyshchuk). Fix parallel_view_processing behavior. Now all insertions into MATERIALIZED VIEW without exception should be finished if exception happened. Fixes #10241. #10757 (Nikolai Kochetov).Fix combinator -OrNull and -OrDefault when combined with -State. #10741 (hcz).Fix disappearing totals. Totals could have being filtered if query had had join or subquery with external where condition. Fixes #10674. #10698 (Nikolai Kochetov).Fix multiple usages of IN operator with the identical set in one query. #10686 (Anton Popov).Fix order of parameters in AggregateTransform constructor. #10667 (palasonic1).Fix the lack of parallel execution of remote queries with distributed_aggregation_memory_efficient enabled. Fixes #10655. #10664 (Nikolai Kochetov).Fix predicates optimization for distributed queries (enable_optimize_predicate_expression=1) for queries with HAVING section (i.e. when filtering on the server initiator is required), by preserving the order of expressions (and this is enough to fix), and also force aggregator use column names over indexes. Fixes: #10613, #11413. #10621 (Azat Khuzhin).Fix error the BloomFilter false positive must be a double number between 0 and 1 #10551. #10569 (Winter Zhang).Fix SELECT of column ALIAS which default expression type different from column type. #10563 (Azat Khuzhin). Implemented comparison between DateTime64 and String values (just like for DateTime). #10560 (Vasily Nemkov). "},{"title":"ClickHouse release v20.1.12.86, 2020-05-26​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v2011286-2020-05-26","content":"Bug Fix​ Fixed incompatibility of two-level aggregation between versions 20.1 and earlier. This incompatibility happens when different versions of ClickHouse are used on initiator node and remote nodes and the size of GROUP BY result is large and aggregation is performed by a single String field. It leads to several unmerged rows for a single key in result. #10952 (alexey-milovidov).Fixed data corruption for LowCardinality(FixedString) key column in SummingMergeTree which could have happened after merge. Fixes #10489. #10721 (Nikolai Kochetov).Fixed bug, which causes http requests stuck on client close when readonly=2 and cancel_http_readonly_queries_on_client_close=1. Fixes #7939, #7019, #7736, #7091. #10684 (tavplubix).Fixed a bug when on SYSTEM DROP DNS CACHE query also drop caches, which are used to check if user is allowed to connect from some IP addresses. #10608 (tavplubix).Fixed incorrect scalar results inside inner query of MATERIALIZED VIEW in case if this query contained dependent table. #10603 (Nikolai Kochetov).Fixed the situation when mutation finished all parts, but hung up in is_done=0. #10526 (alesapin).Fixed overflow at beginning of unix epoch for timezones with fractional offset from UTC. This fixes #9335. #10513 (alexey-milovidov).Fixed improper shutdown of Distributed storage. #10491 (Azat Khuzhin).Fixed numeric overflow in simpleLinearRegression over large integers. #10474 (hcz).Fixed removing metadata directory when attach database fails. #10442 (Winter Zhang).Added a check of number and type of arguments when creating BloomFilter index #9623. #10431 (Winter Zhang).Fixed the issue when a query with ARRAY JOIN, ORDER BY and LIMIT may return incomplete result. This fixes #10226. #10427 (alexey-milovidov).Prefer fallback_to_stale_replicas over skip_unavailable_shards. #10422 (Azat Khuzhin).Fixed wrong flattening of Array(Tuple(...)) data types. This fixes #10259. #10390 (alexey-milovidov).Fixed wrong behavior in HashTable that caused compilation error when trying to read HashMap from buffer. #10386 (palasonic1).Fixed possible Pipeline stuck error in ConcatProcessor which could have happened in remote query. #10381 (Nikolai Kochetov).Fixed error Pipeline stuck with max_rows_to_group_by and group_by_overflow_mode = 'break'. #10279 (Nikolai Kochetov).Fixed several bugs when some data was inserted with quorum, then deleted somehow (DROP PARTITION, TTL) and this leaded to the stuck of INSERTs or false-positive exceptions in SELECTs. This fixes #9946. #10188 (Nikita Mikhaylov).Fixed incompatibility when versions prior to 18.12.17 are used on remote servers and newer is used on initiating server, and GROUP BY both fixed and non-fixed keys, and when two-level group by method is activated. #3254 (alexey-milovidov). Build/Testing/Packaging Improvement​ Added CA certificates to clickhouse-server docker image. #10476 (filimonov). "},{"title":"ClickHouse release v20.1.10.70, 2020-04-17​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v2011070-2020-04-17","content":"Bug Fix​ Fix rare possible exception Cannot drain connections: cancel first. #10239 (Nikolai Kochetov).Fixed bug where ClickHouse would throw 'Unknown function lambda.' error message when user tries to run ALTER UPDATE/DELETE on tables with ENGINE = Replicated*. Check for nondeterministic functions now handles lambda expressions correctly. #10237 (Alexander Kazakov).Fix parseDateTimeBestEffort for strings in RFC-2822 when day of week is Tuesday or Thursday. This fixes #10082. #10214 (alexey-milovidov).Fix column names of constants inside JOIN that may clash with names of constants outside of JOIN. #10207 (alexey-milovidov).Fix possible inifinite query execution when the query actually should stop on LIMIT, while reading from infinite source like system.numbers or system.zeros. #10206 (Nikolai Kochetov).Fix move-to-prewhere optimization in presense of arrayJoin functions (in certain cases). This fixes #10092. #10195 (alexey-milovidov).Add the ability to relax the restriction on non-deterministic functions usage in mutations with allow_nondeterministic_mutations setting. #10186 (filimonov).Convert blocks if structure does not match on INSERT into table with Distributed engine. #10135 (Azat Khuzhin).Fix SIGSEGV on INSERT into Distributed table when its structure differs from the underlying tables. #10105 (Azat Khuzhin).Fix possible rows loss for queries with JOIN and UNION ALL. Fixes #9826, #10113. #10099 (Nikolai Kochetov).Add arguments check and support identifier arguments for MySQL Database Engine. #10077 (Winter Zhang).Fix bug in clickhouse dictionary source from localhost clickhouse server. The bug may lead to memory corruption if types in dictionary and source are not compatible. #10071 (alesapin).Fix error Cannot clone block with columns because block has 0 columns ... While executing GroupingAggregatedTransform. It happened when setting distributed_aggregation_memory_efficient was enabled, and distributed query read aggregating data with different level from different shards (mixed single and two level aggregation). #10063 (Nikolai Kochetov).Fix a segmentation fault that could occur in GROUP BY over string keys containing trailing zero bytes (#8636, #8925). #10025 (Alexander Kuzmenkov).Fix bug in which the necessary tables weren't retrieved at one of the processing stages of queries to some databases. Fixes #9699. #9949 (achulkov2).Fix 'Not found column in block' error when JOIN appears with TOTALS. Fixes #9839. #9939 (Artem Zuikov).Fix a bug with ON CLUSTER DDL queries freezing on server startup. #9927 (Gagan Arneja).Fix TRUNCATE for Join table engine (#9917). #9920 (Amos Bird).Fix 'scalar does not exist' error in ALTER queries (#9878). #9904 (Amos Bird).Fix race condition between drop and optimize in ReplicatedMergeTree. #9901 (alesapin).Fixed DeleteOnDestroy logic in ATTACH PART which could lead to automatic removal of attached part and added few tests. #9410 (Vladimir Chebotarev). Build/Testing/Packaging Improvement​ Fix unit test collapsing_sorted_stream. #9367 (Deleted user). "},{"title":"ClickHouse release v20.1.9.54, 2020-03-28​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v201954-2020-03-28","content":"Bug Fix​ Fix 'Different expressions with the same alias' error when query has PREWHERE and WHERE on distributed table and SET distributed_product_mode = 'local'. #9871 (Artem Zuikov).Fix mutations excessive memory consumption for tables with a composite primary key. This fixes #9850. #9860 (alesapin).For INSERT queries shard now clamps the settings got from the initiator to the shard's constaints instead of throwing an exception. This fix allows to send INSERT queries to a shard with another constraints. This change improves fix #9447. #9852 (Vitaly Baranov).Fix possible exception Got 0 in totals chunk, expected 1 on client. It happened for queries with JOIN in case if right joined table had zero rows. Example: select * from system.one t1 join system.one t2 on t1.dummy = t2.dummy limit 0 FORMAT TabSeparated;. Fixes #9777. #9823 (Nikolai Kochetov).Fix SIGSEGV with optimize_skip_unused_shards when type cannot be converted. #9804 (Azat Khuzhin).Fixed a few cases when timezone of the function argument wasn't used properly. #9574 (Vasily Nemkov). Improvement​ Remove ORDER BY stage from mutations because we read from a single ordered part in a single thread. Also add check that the order of rows in mutation is ordered in sorting key order and this order is not violated. #9886 (alesapin). Build/Testing/Packaging Improvement​ Clean up duplicated linker flags. Make sure the linker won't look up an unexpected symbol. #9433 (Amos Bird). "},{"title":"ClickHouse release v20.1.8.41, 2020-03-20​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v201841-2020-03-20","content":"Bug Fix​ Fix possible permanent Cannot schedule a task error (due to unhandled exception in ParallelAggregatingBlockInputStream::Handler::onFinish/onFinishThread). This fixes #6833. #9154 (Azat Khuzhin)Fix excessive memory consumption in ALTER queries (mutations). This fixes #9533 and #9670. #9754 (alesapin)Fix bug in backquoting in external dictionaries DDL. This fixes #9619. #9734 (alesapin) "},{"title":"ClickHouse release v20.1.7.38, 2020-03-18​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v201738-2020-03-18","content":"Bug Fix​ Fixed incorrect internal function names for sumKahan and sumWithOverflow. I lead to exception while using this functions in remote queries. #9636 (Azat Khuzhin). This issue was in all ClickHouse releases.Allow ALTER ON CLUSTER of Distributed tables with internal replication. This fixes #3268. #9617 (shinoi2). This issue was in all ClickHouse releases.Fix possible exceptions Size of filter does not match size of column and Invalid number of rows in Chunk in MergeTreeRangeReader. They could appear while executing PREWHERE in some cases. Fixes #9132. #9612 (Anton Popov)Fixed the issue: timezone was not preserved if you write a simple arithmetic expression like time + 1 (in contrast to an expression like time + INTERVAL 1 SECOND). This fixes #5743. #9323 (alexey-milovidov). This issue was in all ClickHouse releases.Now it's not possible to create or add columns with simple cyclic aliases like a DEFAULT b, b DEFAULT a. #9603 (alesapin)Fixed the issue when padding at the end of base64 encoded value can be malformed. Update base64 library. This fixes #9491, closes #9492 #9500 (alexey-milovidov)Fix data race at destruction of Poco::HTTPServer. It could happen when server is started and immediately shut down. #9468 (Anton Popov)Fix possible crash/wrong number of rows in LIMIT n WITH TIES when there are a lot of rows equal to n'th row. #9464 (tavplubix)Fix possible mismatched checksums with column TTLs. #9451 (Anton Popov)Fix crash when a user tries to ALTER MODIFY SETTING for old-formated MergeTree table engines family. #9435 (alesapin)Now we will try finalize mutations more frequently. #9427 (alesapin)Fix replication protocol incompatibility introduced in #8598. #9412 (alesapin)Fix not(has()) for the bloom_filter index of array types. #9407 (achimbab)Fixed the behaviour of match and extract functions when haystack has zero bytes. The behaviour was wrong when haystack was constant. This fixes #9160 #9163 (alexey-milovidov) #9345 (alexey-milovidov) Build/Testing/Packaging Improvement​ Exception handling now works correctly on Windows Subsystem for Linux. See https://github.com/ClickHouse-Extras/libunwind/pull/3 This fixes #6480 #9564 (sobolevsv) "},{"title":"ClickHouse release v20.1.6.30, 2020-03-05​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v201630-2020-03-05","content":"Bug Fix​ Fix data incompatibility when compressed with T64 codec.#9039 (abyss7)Fix order of ranges while reading from MergeTree table in one thread. Fixes #8964.#9050 (CurtizJ)Fix possible segfault in MergeTreeRangeReader, while executing PREWHERE. Fixes #9064.#9106 (CurtizJ)Fix reinterpretAsFixedString to return FixedString instead of String.#9052 (oandrew)Fix joinGet with nullable return types. Fixes #8919#9014 (amosbird)Fix fuzz test and incorrect behaviour of bitTestAll/bitTestAny functions.#9143 (alexey-milovidov)Fix the behaviour of match and extract functions when haystack has zero bytes. The behaviour was wrong when haystack was constant. Fixes #9160#9163 (alexey-milovidov)Fixed execution of inversed predicates when non-strictly monotinic functional index is used. Fixes #9034#9223 (Akazz)Allow to rewrite CROSS to INNER JOIN if there's [NOT] LIKE operator in WHERE section. Fixes #9191#9229 (4ertus2)Allow first column(s) in a table with Log engine be an alias.#9231 (abyss7)Allow comma join with IN() inside. Fixes #7314.#9251 (4ertus2)Improve ALTER MODIFY/ADD queries logic. Now you cannot ADD column without type, MODIFY default expression does not change type of column and MODIFY type does not loose default expression value. Fixes #8669.#9227 (alesapin)Fix mutations finalization, when already done mutation can have status is_done=0.#9217 (alesapin)Support &quot;Processors&quot; pipeline for system.numbers and system.numbers_mt. This also fixes the bug when max_execution_time is not respected.#7796 (KochetovNicolai)Fix wrong counting of DictCacheKeysRequestedFound metric.#9411 (nikitamikhaylov)Added a check for storage policy in ATTACH PARTITION FROM, REPLACE PARTITION, MOVE TO TABLE which otherwise could make data of part inaccessible after restart and prevent ClickHouse to start.#9383 (excitoon)Fixed UBSan report in MergeTreeIndexSet. This fixes #9250#9365 (alexey-milovidov)Fix possible datarace in BlockIO.#9356 (KochetovNicolai)Support for UInt64 numbers that don't fit in Int64 in JSON-related functions. Update SIMDJSON to master. This fixes #9209#9344 (alexey-milovidov)Fix the issue when the amount of free space is not calculated correctly if the data directory is mounted to a separate device. For default disk calculate the free space from data subdirectory. This fixes #7441#9257 (millb)Fix the issue when TLS connections may fail with the message OpenSSL SSL_read: error:14094438:SSL routines:ssl3_read_bytes:tlsv1 alert internal error and SSL Exception: error:2400006E:random number generator::error retrieving entropy. Update OpenSSL to upstream master.#8956 (alexey-milovidov)When executing CREATE query, fold constant expressions in storage engine arguments. Replace empty database name with current database. Fixes #6508, #3492. Also fix check for local address in ClickHouseDictionarySource.#9262 (tabplubix)Fix segfault in StorageMerge, which can happen when reading from StorageFile.#9387 (tabplubix)Prevent losing data in Kafka in rare cases when exception happens after reading suffix but before commit. Fixes #9378. Related: #7175#9507 (filimonov)Fix bug leading to server termination when trying to use / drop Kafka table created with wrong parameters. Fixes #9494. Incorporates #9507.#9513 (filimonov) New Feature​ Add deduplicate_blocks_in_dependent_materialized_views option to control the behaviour of idempotent inserts into tables with materialized views. This new feature was added to the bugfix release by a special request from Altinity.#9070 (urykhy) "},{"title":"ClickHouse release v20.1.2.4, 2020-01-22​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#clickhouse-release-v20124-2020-01-22","content":"Backward Incompatible Change​ Make the setting merge_tree_uniform_read_distribution obsolete. The server still recognizes this setting but it has no effect. #8308 (alexey-milovidov)Changed return type of the function greatCircleDistance to Float32 because now the result of calculation is Float32. #7993 (alexey-milovidov)Now it's expected that query parameters are represented in &quot;escaped&quot; format. For example, to pass string a&lt;tab&gt;b you have to write a\\tb or a\\&lt;tab&gt;b and respectively, a%5Ctb or a%5C%09b in URL. This is needed to add the possibility to pass NULL as \\N. This fixes #7488. #8517 (alexey-milovidov)Enable use_minimalistic_part_header_in_zookeeper setting for ReplicatedMergeTree by default. This will significantly reduce amount of data stored in ZooKeeper. This setting is supported since version 19.1 and we already use it in production in multiple services without any issues for more than half a year. Disable this setting if you have a chance to downgrade to versions older than 19.1. #6850 (alexey-milovidov)Data skipping indices are production ready and enabled by default. The settings allow_experimental_data_skipping_indices, allow_experimental_cross_to_join_conversion and allow_experimental_multiple_joins_emulation are now obsolete and do nothing. #7974 (alexey-milovidov)Add new ANY JOIN logic for StorageJoin consistent with JOIN operation. To upgrade without changes in behaviour you need add SETTINGS any_join_distinct_right_table_keys = 1 to Engine Join tables metadata or recreate these tables after upgrade. #8400 (Artem Zuikov)Require server to be restarted to apply the changes in logging configuration. This is a temporary workaround to avoid the bug where the server logs to a deleted log file (see #8696). #8707 (Alexander Kuzmenkov) New Feature​ Added information about part paths to system.merges. #8043 (Vladimir Chebotarev)Add ability to execute SYSTEM RELOAD DICTIONARY query in ON CLUSTER mode. #8288 (Guillaume Tassery)Add ability to execute CREATE DICTIONARY queries in ON CLUSTER mode. #8163 (alesapin)Now user's profile in users.xml can inherit multiple profiles. #8343 (Mikhail f. Shiryaev)Added system.stack_trace table that allows to look at stack traces of all server threads. This is useful for developers to introspect server state. This fixes #7576. #8344 (alexey-milovidov)Add DateTime64 datatype with configurable sub-second precision. #7170 (Vasily Nemkov)Add table function clusterAllReplicas which allows to query all the nodes in the cluster. #8493 (kiran sunkari)Add aggregate function categoricalInformationValue which calculates the information value of a discrete feature. #8117 (hcz)Speed up parsing of data files in CSV, TSV and JSONEachRow format by doing it in parallel. #7780 (Alexander Kuzmenkov)Add function bankerRound which performs banker's rounding. #8112 (hcz)Support more languages in embedded dictionary for region names: 'ru', 'en', 'ua', 'uk', 'by', 'kz', 'tr', 'de', 'uz', 'lv', 'lt', 'et', 'pt', 'he', 'vi'. #8189 (alexey-milovidov)Improvements in consistency of ANY JOIN logic. Now t1 ANY LEFT JOIN t2 equals t2 ANY RIGHT JOIN t1. #7665 (Artem Zuikov)Add setting any_join_distinct_right_table_keys which enables old behaviour for ANY INNER JOIN. #7665 (Artem Zuikov)Add new SEMI and ANTI JOIN. Old ANY INNER JOIN behaviour now available as SEMI LEFT JOIN. #7665 (Artem Zuikov)Added Distributed format for File engine and file table function which allows to read from .bin files generated by asynchronous inserts into Distributed table. #8535 (Nikolai Kochetov)Add optional reset column argument for runningAccumulate which allows to reset aggregation results for each new key value. #8326 (Sergey Kononenko)Add ability to use ClickHouse as Prometheus endpoint. #7900 (vdimir)Add section &lt;remote_url_allow_hosts&gt; in config.xml which restricts allowed hosts for remote table engines and table functions URL, S3, HDFS. #7154 (Mikhail Korotov)Added function greatCircleAngle which calculates the distance on a sphere in degrees. #8105 (alexey-milovidov)Changed Earth radius to be consistent with H3 library. #8105 (alexey-milovidov)Added JSONCompactEachRow and JSONCompactEachRowWithNamesAndTypes formats for input and output. #7841 (Mikhail Korotov)Added feature for file-related table engines and table functions (File, S3, URL, HDFS) which allows to read and write gzip files based on additional engine parameter or file extension. #7840 (Andrey Bodrov)Added the randomASCII(length) function, generating a string with a random set of ASCII printable characters. #8401 (BayoNet)Added function JSONExtractArrayRaw which returns an array on unparsed json array elements from JSON string. #8081 (Oleg Matrokhin)Add arrayZip function which allows to combine multiple arrays of equal lengths into one array of tuples. #8149 (Winter Zhang)Add ability to move data between disks according to configured TTL-expressions for *MergeTree table engines family. #8140 (Vladimir Chebotarev)Added new aggregate function avgWeighted which allows to calculate weighted average. #7898 (Andrey Bodrov)Now parallel parsing is enabled by default for TSV, TSKV, CSV and JSONEachRow formats. #7894 (Nikita Mikhaylov)Add several geo functions from H3 library: h3GetResolution, h3EdgeAngle, h3EdgeLength, h3IsValid and h3kRing. #8034 (Konstantin Malanchev)Added support for brotli (br) compression in file-related storages and table functions. This fixes #8156. #8526 (alexey-milovidov)Add groupBit* functions for the SimpleAggregationFunction type. #8485 (Guillaume Tassery) Bug Fix​ Fix rename of tables with Distributed engine. Fixes issue #7868. #8306 (tavplubix)Now dictionaries support EXPRESSION for attributes in arbitrary string in non-ClickHouse SQL dialect. #8098 (alesapin)Fix broken INSERT SELECT FROM mysql(...) query. This fixes #8070 and #7960. #8234 (tavplubix)Fix error &quot;Mismatch column sizes&quot; when inserting default Tuple from JSONEachRow. This fixes #5653. #8606 (tavplubix)Now an exception will be thrown in case of using WITH TIES alongside LIMIT BY. Also add ability to use TOP with LIMIT BY. This fixes #7472. #7637 (Nikita Mikhaylov)Fix unintendent dependency from fresh glibc version in clickhouse-odbc-bridge binary. #8046 (Amos Bird)Fix bug in check function of *MergeTree engines family. Now it does not fail in case when we have equal amount of rows in last granule and last mark (non-final). #8047 (alesapin)Fix insert into Enum* columns after ALTER query, when underlying numeric type is equal to table specified type. This fixes #7836. #7908 (Anton Popov)Allowed non-constant negative &quot;size&quot; argument for function substring. It was not allowed by mistake. This fixes #4832. #7703 (alexey-milovidov)Fix parsing bug when wrong number of arguments passed to (O|J)DBC table engine. #7709 (alesapin)Using command name of the running clickhouse process when sending logs to syslog. In previous versions, empty string was used instead of command name. #8460 (Michael Nacharov)Fix check of allowed hosts for localhost. This PR fixes the solution provided in #8241. #8342 (Vitaly Baranov)Fix rare crash in argMin and argMax functions for long string arguments, when result is used in runningAccumulate function. This fixes #8325 #8341 (dinosaur)Fix memory overcommit for tables with Buffer engine. #8345 (Azat Khuzhin)Fixed potential bug in functions that can take NULL as one of the arguments and return non-NULL. #8196 (alexey-milovidov)Better metrics calculations in thread pool for background processes for MergeTree table engines. #8194 (Vladimir Chebotarev)Fix function IN inside WHERE statement when row-level table filter is present. Fixes #6687 #8357 (Ivan)Now an exception is thrown if the integral value is not parsed completely for settings values. #7678 (Mikhail Korotov)Fix exception when aggregate function is used in query to distributed table with more than two local shards. #8164 (小路)Now bloom filter can handle zero length arrays and does not perform redundant calculations. #8242 (achimbab)Fixed checking if a client host is allowed by matching the client host to host_regexp specified in users.xml. #8241 (Vitaly Baranov)Relax ambiguous column check that leads to false positives in multiple JOIN ON section. #8385 (Artem Zuikov)Fixed possible server crash (std::terminate) when the server cannot send or write data in JSON or XML format with values of String data type (that require UTF-8 validation) or when compressing result data with Brotli algorithm or in some other rare cases. This fixes #7603 #8384 (alexey-milovidov)Fix race condition in StorageDistributedDirectoryMonitor found by CI. This fixes #8364. #8383 (Nikolai Kochetov)Now background merges in *MergeTree table engines family preserve storage policy volume order more accurately. #8549 (Vladimir Chebotarev)Now table engine Kafka works properly with Native format. This fixes #6731 #7337 #8003. #8016 (filimonov)Fixed formats with headers (like CSVWithNames) which were throwing exception about EOF for table engine Kafka. #8016 (filimonov)Fixed a bug with making set from subquery in right part of IN section. This fixes #5767 and #2542. #7755 (Nikita Mikhaylov)Fix possible crash while reading from storage File. #7756 (Nikolai Kochetov)Fixed reading of the files in Parquet format containing columns of type list. #8334 (maxulan)Fix error Not found column for distributed queries with PREWHERE condition dependent on sampling key if max_parallel_replicas &gt; 1. #7913 (Nikolai Kochetov)Fix error Not found column if query used PREWHERE dependent on table's alias and the result set was empty because of primary key condition. #7911 (Nikolai Kochetov)Fixed return type for functions rand and randConstant in case of Nullable argument. Now functions always return UInt32 and never Nullable(UInt32). #8204 (Nikolai Kochetov)Disabled predicate push-down for WITH FILL expression. This fixes #7784. #7789 (Winter Zhang)Fixed incorrect count() result for SummingMergeTree when FINAL section is used. #3280 #7786 (Nikita Mikhaylov)Fix possible incorrect result for constant functions from remote servers. It happened for queries with functions like version(), uptime(), etc. which returns different constant values for different servers. This fixes #7666. #7689 (Nikolai Kochetov)Fix complicated bug in push-down predicate optimization which leads to wrong results. This fixes a lot of issues on push-down predicate optimization. #8503 (Winter Zhang)Fix crash in CREATE TABLE .. AS dictionary query. #8508 (Azat Khuzhin)Several improvements ClickHouse grammar in .g4 file. #8294 (taiyang-li)Fix bug that leads to crashes in JOINs with tables with engine Join. This fixes #7556 #8254 #7915 #8100. #8298 (Artem Zuikov)Fix redundant dictionaries reload on CREATE DATABASE. #7916 (Azat Khuzhin)Limit maximum number of streams for read from StorageFile and StorageHDFS. Fixes #7650. #7981 (alesapin)Fix bug in ALTER ... MODIFY ... CODEC query, when user specify both default expression and codec. Fixes 8593. #8614 (alesapin)Fix error in background merge of columns with SimpleAggregateFunction(LowCardinality) type. #8613 (Nikolai Kochetov)Fixed type check in function toDateTime64. #8375 (Vasily Nemkov)Now server do not crash on LEFT or FULL JOIN with and Join engine and unsupported join_use_nulls settings. #8479 (Artem Zuikov)Now DROP DICTIONARY IF EXISTS db.dict query does not throw exception if db does not exist. #8185 (Vitaly Baranov)Fix possible crashes in table functions (file, mysql, remote) caused by usage of reference to removed IStorage object. Fix incorrect parsing of columns specified at insertion into table function. #7762 (tavplubix)Ensure network be up before starting clickhouse-server. This fixes #7507. #8570 (Zhichang Yu)Fix timeouts handling for secure connections, so queries does not hang indefenitely. This fixes #8126. #8128 (alexey-milovidov)Fix clickhouse-copier's redundant contention between concurrent workers. #7816 (Ding Xiang Fei)Now mutations does not skip attached parts, even if their mutation version were larger than current mutation version. #7812 (Zhichang Yu) #8250 (alesapin)Ignore redundant copies of *MergeTree data parts after move to another disk and server restart. #7810 (Vladimir Chebotarev)Fix crash in FULL JOIN with LowCardinality in JOIN key. #8252 (Artem Zuikov)Forbidden to use column name more than once in insert query like INSERT INTO tbl (x, y, x). This fixes #5465, #7681. #7685 (alesapin)Added fallback for detection the number of physical CPU cores for unknown CPUs (using the number of logical CPU cores). This fixes #5239. #7726 (alexey-milovidov)Fix There's no column error for materialized and alias columns. #8210 (Artem Zuikov)Fixed sever crash when EXISTS query was used without TABLE or DICTIONARY qualifier. Just like EXISTS t. This fixes #8172. This bug was introduced in version 19.17. #8213 (alexey-milovidov)Fix rare bug with error &quot;Sizes of columns does not match&quot; that might appear when using SimpleAggregateFunction column. #7790 (Boris Granveaud)Fix bug where user with empty allow_databases got access to all databases (and same for allow_dictionaries). #7793 (DeifyTheGod)Fix client crash when server already disconnected from client. #8071 (Azat Khuzhin)Fix ORDER BY behaviour in case of sorting by primary key prefix and non primary key suffix. #7759 (Anton Popov)Check if qualified column present in the table. This fixes #6836. #7758 (Artem Zuikov)Fixed behavior with ALTER MOVE ran immediately after merge finish moves superpart of specified. Fixes #8103. #8104 (Vladimir Chebotarev)Fix possible server crash while using UNION with different number of columns. Fixes #7279. #7929 (Nikolai Kochetov)Fix size of result substring for function substr with negative size. #8589 (Nikolai Kochetov)Now server does not execute part mutation in MergeTree if there are not enough free threads in background pool. #8588 (tavplubix)Fix a minor typo on formatting UNION ALL AST. #7999 (litao91)Fixed incorrect bloom filter results for negative numbers. This fixes #8317. #8566 (Winter Zhang)Fixed potential buffer overflow in decompress. Malicious user can pass fabricated compressed data that will cause read after buffer. This issue was found by Eldar Zaitov from Yandex information security team. #8404 (alexey-milovidov)Fix incorrect result because of integers overflow in arrayIntersect. #7777 (Nikolai Kochetov)Now OPTIMIZE TABLE query will not wait for offline replicas to perform the operation. #8314 (javi santana)Fixed ALTER TTL parser for Replicated*MergeTree tables. #8318 (Vladimir Chebotarev)Fix communication between server and client, so server read temporary tables info after query failure. #8084 (Azat Khuzhin)Fix bitmapAnd function error when intersecting an aggregated bitmap and a scalar bitmap. #8082 (Yue Huang)Refine the definition of ZXid according to the ZooKeeper Programmer's Guide which fixes bug in clickhouse-cluster-copier. #8088 (Ding Xiang Fei)odbc table function now respects external_table_functions_use_nulls setting. #7506 (Vasily Nemkov)Fixed bug that lead to a rare data race. #8143 (Alexander Kazakov)Now SYSTEM RELOAD DICTIONARY reloads a dictionary completely, ignoring update_field. This fixes #7440. #8037 (Vitaly Baranov)Add ability to check if dictionary exists in create query. #8032 (alesapin)Fix Float* parsing in Values format. This fixes #7817. #7870 (tavplubix)Fix crash when we cannot reserve space in some background operations of *MergeTree table engines family. #7873 (Vladimir Chebotarev)Fix crash of merge operation when table contains SimpleAggregateFunction(LowCardinality) column. This fixes #8515. #8522 (Azat Khuzhin)Restore support of all ICU locales and add the ability to apply collations for constant expressions. Also add language name to system.collations table. #8051 (alesapin)Fix bug when external dictionaries with zero minimal lifetime (LIFETIME(MIN 0 MAX N), LIFETIME(N)) don't update in background. #7983 (alesapin)Fix crash when external dictionary with ClickHouse source has subquery in query. #8351 (Nikolai Kochetov)Fix incorrect parsing of file extension in table with engine URL. This fixes #8157. #8419 (Andrey Bodrov)Fix CHECK TABLE query for *MergeTree tables without key. Fixes #7543. #7979 (alesapin)Fixed conversion of Float64 to MySQL type. #8079 (Yuriy Baranov)Now if table was not completely dropped because of server crash, server will try to restore and load it. #8176 (tavplubix)Fixed crash in table function file while inserting into file that does not exist. Now in this case file would be created and then insert would be processed. #8177 (Olga Khvostikova)Fix rare deadlock which can happen when trace_log is in enabled. #7838 (filimonov)Add ability to work with different types besides Date in RangeHashed external dictionary created from DDL query. Fixes 7899. #8275 (alesapin)Fixes crash when now64() is called with result of another function. #8270 (Vasily Nemkov)Fixed bug with detecting client IP for connections through mysql wire protocol. #7743 (Dmitry Muzyka)Fix empty array handling in arraySplit function. This fixes #7708. #7747 (hcz)Fixed the issue when pid-file of another running clickhouse-server may be deleted. #8487 (Weiqing Xu)Fix dictionary reload if it has invalidate_query, which stopped updates and some exception on previous update tries. #8029 (alesapin)Fixed error in function arrayReduce that may lead to &quot;double free&quot; and error in aggregate function combinator Resample that may lead to memory leak. Added aggregate function aggThrow. This function can be used for testing purposes. #8446 (alexey-milovidov) Improvement​ Improved logging when working with S3 table engine. #8251 (Grigory Pervakov)Printed help message when no arguments are passed when calling clickhouse-local. This fixes #5335. #8230 (Andrey Nagorny)Add setting mutations_sync which allows to wait ALTER UPDATE/DELETE queries synchronously. #8237 (alesapin)Allow to set up relative user_files_path in config.xml (in the way similar to format_schema_path). #7632 (hcz)Add exception for illegal types for conversion functions with -OrZero postfix. #7880 (Andrey Konyaev)Simplify format of the header of data sending to a shard in a distributed query. #8044 (Vitaly Baranov)Live View table engine refactoring. #8519 (vzakaznikov)Add additional checks for external dictionaries created from DDL-queries. #8127 (alesapin)Fix error Column ... already exists while using FINAL and SAMPLE together, e.g. select count() from table final sample 1/2. Fixes #5186. #7907 (Nikolai Kochetov)Now table the first argument of joinGet function can be table identifier. #7707 (Amos Bird)Allow using MaterializedView with subqueries above Kafka tables. #8197 (filimonov)Now background moves between disks run it the seprate thread pool. #7670 (Vladimir Chebotarev)SYSTEM RELOAD DICTIONARY now executes synchronously. #8240 (Vitaly Baranov)Stack traces now display physical addresses (offsets in object file) instead of virtual memory addresses (where the object file was loaded). That allows the use of addr2line when binary is position independent and ASLR is active. This fixes #8360. #8387 (alexey-milovidov)Support new syntax for row-level security filters: &lt;table name='table_name'&gt;…&lt;/table&gt;. Fixes #5779. #8381 (Ivan)Now cityHash function can work with Decimal and UUID types. Fixes #5184. #7693 (Mikhail Korotov)Removed fixed index granularity (it was 1024) from system logs because it's obsolete after implementation of adaptive granularity. #7698 (alexey-milovidov)Enabled MySQL compatibility server when ClickHouse is compiled without SSL. #7852 (Yuriy Baranov)Now server checksums distributed batches, which gives more verbose errors in case of corrupted data in batch. #7914 (Azat Khuzhin)Support DROP DATABASE, DETACH TABLE, DROP TABLE and ATTACH TABLE for MySQL database engine. #8202 (Winter Zhang)Add authentication in S3 table function and table engine. #7623 (Vladimir Chebotarev)Added check for extra parts of MergeTree at different disks, in order to not allow to miss data parts at undefined disks. #8118 (Vladimir Chebotarev)Enable SSL support for Mac client and server. #8297 (Ivan)Now ClickHouse can work as MySQL federated server (see https://dev.mysql.com/doc/refman/5.7/en/federated-create-server.html). #7717 (Maxim Fedotov)clickhouse-client now only enable bracketed-paste when multiquery is on and multiline is off. This fixes #7757. #7761 (Amos Bird)Support Array(Decimal) in if function. #7721 (Artem Zuikov)Support Decimals in arrayDifference, arrayCumSum and arrayCumSumNegative functions. #7724 (Artem Zuikov)Added lifetime column to system.dictionaries table. #6820 #7727 (kekekekule)Improved check for existing parts on different disks for *MergeTree table engines. Addresses #7660. #8440 (Vladimir Chebotarev)Integration with AWS SDK for S3 interactions which allows to use all S3 features out of the box. #8011 (Pavel Kovalenko)Added support for subqueries in Live View tables. #7792 (vzakaznikov)Check for using Date or DateTime column from TTL expressions was removed. #7920 (Vladimir Chebotarev)Information about disk was added to system.detached_parts table. #7833 (Vladimir Chebotarev)Now settings max_(table|partition)_size_to_drop can be changed without a restart. #7779 (Grigory Pervakov)Slightly better usability of error messages. Ask user not to remove the lines below Stack trace:. #7897 (alexey-milovidov)Better reading messages from Kafka engine in various formats after #7935. #8035 (Ivan)Better compatibility with MySQL clients which don't support sha2_password auth plugin. #8036 (Yuriy Baranov)Support more column types in MySQL compatibility server. #7975 (Yuriy Baranov)Implement ORDER BY optimization for Merge, Buffer and Materilized View storages with underlying MergeTree tables. #8130 (Anton Popov)Now we always use POSIX implementation of getrandom to have better compatibility with old kernels (&lt; 3.17). #7940 (Amos Bird)Better check for valid destination in a move TTL rule. #8410 (Vladimir Chebotarev)Better checks for broken insert batches for Distributed table engine. #7933 (Azat Khuzhin)Add column with array of parts name which mutations must process in future to system.mutations table. #8179 (alesapin)Parallel merge sort optimization for processors. #8552 (Nikolai Kochetov)The settings mark_cache_min_lifetime is now obsolete and does nothing. In previous versions, mark cache can grow in memory larger than mark_cache_size to accomodate data within mark_cache_min_lifetime seconds. That was leading to confusion and higher memory usage than expected, that is especially bad on memory constrained systems. If you will see performance degradation after installing this release, you should increase the mark_cache_size. #8484 (alexey-milovidov)Preparation to use tid everywhere. This is needed for #7477. #8276 (alexey-milovidov) Performance Improvement​ Performance optimizations in processors pipeline. #7988 (Nikolai Kochetov)Non-blocking updates of expired keys in cache dictionaries (with permission to read old ones). #8303 (Nikita Mikhaylov)Compile ClickHouse without -fno-omit-frame-pointer globally to spare one more register. #8097 (Amos Bird)Speedup greatCircleDistance function and add performance tests for it. #7307 (Olga Khvostikova)Improved performance of function roundDown. #8465 (alexey-milovidov)Improved performance of max, min, argMin, argMax for DateTime64 data type. #8199 (Vasily Nemkov)Improved performance of sorting without a limit or with big limit and external sorting. #8545 (alexey-milovidov)Improved performance of formatting floating point numbers up to 6 times. #8542 (alexey-milovidov)Improved performance of modulo function. #7750 (Amos Bird)Optimized ORDER BY and merging with single column key. #8335 (alexey-milovidov)Better implementation for arrayReduce, -Array and -State combinators. #7710 (Amos Bird)Now PREWHERE should be optimized to be at least as efficient as WHERE. #7769 (Amos Bird)Improve the way round and roundBankers handling negative numbers. #8229 (hcz)Improved decoding performance of DoubleDelta and Gorilla codecs by roughly 30-40%. This fixes #7082. #8019 (Vasily Nemkov)Improved performance of base64 related functions. #8444 (alexey-milovidov)Added a function geoDistance. It is similar to greatCircleDistance but uses approximation to WGS-84 ellipsoid model. The performance of both functions are near the same. #8086 (alexey-milovidov)Faster min and max aggregation functions for Decimal data type. #8144 (Artem Zuikov)Vectorize processing arrayReduce. #7608 (Amos Bird)if chains are now optimized as multiIf. #8355 (kamalov-ruslan)Fix performance regression of Kafka table engine introduced in 19.15. This fixes #7261. #7935 (filimonov)Removed &quot;pie&quot; code generation that gcc from Debian packages occasionally brings by default. #8483 (alexey-milovidov)Parallel parsing data formats #6553 (Nikita Mikhaylov)Enable optimized parser of Values with expressions by default (input_format_values_deduce_templates_of_expressions=1). #8231 (tavplubix) Build/Testing/Packaging Improvement​ Build fixes for ARM and in minimal mode. #8304 (proller)Add coverage file flush for clickhouse-server when std::atexit is not called. Also slightly improved logging in stateless tests with coverage. #8267 (alesapin)Update LLVM library in contrib. Avoid using LLVM from OS packages. #8258 (alexey-milovidov)Make bundled curl build fully quiet. #8232 #8203 (Pavel Kovalenko)Fix some MemorySanitizer warnings. #8235 (Alexander Kuzmenkov)Use add_warning and no_warning macros in CMakeLists.txt. #8604 (Ivan)Add support of Minio S3 Compatible object (https://min.io/) for better integration tests. #7863 #7875 (Pavel Kovalenko)Imported libc headers to contrib. It allows to make builds more consistent across various systems (only for x86_64-linux-gnu). #5773 (alexey-milovidov)Remove -fPIC from some libraries. #8464 (alexey-milovidov)Clean CMakeLists.txt for curl. See https://github.com/ClickHouse/ClickHouse/pull/8011#issuecomment-569478910 #8459 (alexey-milovidov)Silent warnings in CapNProto library. #8220 (alexey-milovidov)Add performance tests for short string optimized hash tables. #7679 (Amos Bird)Now ClickHouse will build on AArch64 even if MADV_FREE is not available. This fixes #8027. #8243 (Amos Bird)Update zlib-ng to fix memory sanitizer problems. #7182 #8206 (Alexander Kuzmenkov)Enable internal MySQL library on non-Linux system, because usage of OS packages is very fragile and usually does not work at all. This fixes #5765. #8426 (alexey-milovidov)Fixed build on some systems after enabling libc++. This supersedes #8374. #8380 (alexey-milovidov)Make Field methods more type-safe to find more errors. #7386 #8209 (Alexander Kuzmenkov)Added missing files to the libc-headers submodule. #8507 (alexey-milovidov)Fix wrong JSON quoting in performance test output. #8497 (Nikolai Kochetov)Now stack trace is displayed for std::exception and Poco::Exception. In previous versions it was available only for DB::Exception. This improves diagnostics. #8501 (alexey-milovidov)Porting clock_gettime and clock_nanosleep for fresh glibc versions. #8054 (Amos Bird)Enable part_log in example config for developers. #8609 (alexey-milovidov)Fix async nature of reload in 01036_no_superfluous_dict_reload_on_create_database*. #8111 (Azat Khuzhin)Fixed codec performance tests. #8615 (Vasily Nemkov)Add install scripts for .tgz build and documentation for them. #8612 #8591 (alesapin)Removed old ZSTD test (it was created in year 2016 to reproduce the bug that pre 1.0 version of ZSTD has had). This fixes #8618. #8619 (alexey-milovidov)Fixed build on Mac OS Catalina. #8600 (meo)Increased number of rows in codec performance tests to make results noticeable. #8574 (Vasily Nemkov)In debug builds, treat LOGICAL_ERROR exceptions as assertion failures, so that they are easier to notice. #8475 (Alexander Kuzmenkov)Make formats-related performance test more deterministic. #8477 (alexey-milovidov)Update lz4 to fix a MemorySanitizer failure. #8181 (Alexander Kuzmenkov)Suppress a known MemorySanitizer false positive in exception handling. #8182 (Alexander Kuzmenkov)Update gcc and g++ to version 9 in build/docker/build.sh #7766 (TLightSky)Add performance test case to test that PREWHERE is worse than WHERE. #7768 (Amos Bird)Progress towards fixing one flacky test. #8621 (alexey-milovidov)Avoid MemorySanitizer report for data from libunwind. #8539 (alexey-milovidov)Updated libc++ to the latest version. #8324 (alexey-milovidov)Build ICU library from sources. This fixes #6460. #8219 (alexey-milovidov)Switched from libressl to openssl. ClickHouse should support TLS 1.3 and SNI after this change. This fixes #8171. #8218 (alexey-milovidov)Fixed UBSan report when using chacha20_poly1305 from SSL (happens on connect to https://yandex.ru/). #8214 (alexey-milovidov)Fix mode of default password file for .deb linux distros. #8075 (proller)Improved expression for getting clickhouse-server PID in clickhouse-test. #8063 (Alexander Kazakov)Updated contrib/googletest to v1.10.0. #8587 (Alexander Burmak)Fixed ThreadSaninitizer report in base64 library. Also updated this library to the latest version, but it does not matter. This fixes #8397. #8403 (alexey-milovidov)Fix 00600_replace_running_query for processors. #8272 (Nikolai Kochetov)Remove support for tcmalloc to make CMakeLists.txt simpler. #8310 (alexey-milovidov)Release gcc builds now use libc++ instead of libstdc++. Recently libc++ was used only with clang. This will improve consistency of build configurations and portability. #8311 (alexey-milovidov)Enable ICU library for build with MemorySanitizer. #8222 (alexey-milovidov)Suppress warnings from CapNProto library. #8224 (alexey-milovidov)Removed special cases of code for tcmalloc, because it's no longer supported. #8225 (alexey-milovidov)In CI coverage task, kill the server gracefully to allow it to save the coverage report. This fixes incomplete coverage reports we've been seeing lately. #8142 (alesapin)Performance tests for all codecs against Float64 and UInt64 values. #8349 (Vasily Nemkov)termcap is very much deprecated and lead to various problems (f.g. missing &quot;up&quot; cap and echoing ^J instead of multi line) . Favor terminfo or bundled ncurses. #7737 (Amos Bird)Fix test_storage_s3 integration test. #7734 (Nikolai Kochetov)Support StorageFile(&lt;format&gt;, null) to insert block into given format file without actually write to disk. This is required for performance tests. #8455 (Amos Bird)Added argument --print-time to functional tests which prints execution time per test. #8001 (Nikolai Kochetov)Added asserts to KeyCondition while evaluating RPN. This will fix warning from gcc-9. #8279 (alexey-milovidov)Dump cmake options in CI builds. #8273 (Alexander Kuzmenkov)Don't generate debug info for some fat libraries. #8271 (alexey-milovidov)Make log_to_console.xml always log to stderr, regardless of is it interactive or not. #8395 (Alexander Kuzmenkov)Removed some unused features from clickhouse-performance-test tool. #8555 (alexey-milovidov)Now we will also search for lld-X with corresponding clang-X version. #8092 (alesapin)Parquet build improvement. #8421 (maxulan)More GCC warnings #8221 (kreuzerkrieg)Package for Arch Linux now allows to run ClickHouse server, and not only client. #8534 (Vladimir Chebotarev)Fix test with processors. Tiny performance fixes. #7672 (Nikolai Kochetov)Update contrib/protobuf. #8256 (Matwey V. Kornilov)In preparation of switching to c++20 as a new year celebration. &quot;May the C++ force be with ClickHouse.&quot; #8447 (Amos Bird) Experimental Feature​ Added experimental setting min_bytes_to_use_mmap_io. It allows to read big files without copying data from kernel to userspace. The setting is disabled by default. Recommended threshold is about 64 MB, because mmap/munmap is slow. #8520 (alexey-milovidov)Reworked quotas as a part of access control system. Added new table system.quotas, new functions currentQuota, currentQuotaKey, new SQL syntax CREATE QUOTA, ALTER QUOTA, DROP QUOTA, SHOW QUOTA. #7257 (Vitaly Baranov)Allow skipping unknown settings with warnings instead of throwing exceptions. #7653 (Vitaly Baranov)Reworked row policies as a part of access control system. Added new table system.row_policies, new function currentRowPolicies(), new SQL syntax CREATE POLICY, ALTER POLICY, DROP POLICY, SHOW CREATE POLICY, SHOW POLICIES. #7808 (Vitaly Baranov) Security Fix​ Fixed the possibility of reading directories structure in tables with File table engine. This fixes #8536. #8537 (alexey-milovidov) "},{"title":"Changelog for 2019​","type":1,"pageTitle":"2020","url":"docs/en/whats-new/changelog/2020#changelog-for-2019","content":""},{"title":"Using Kafka Connect","type":0,"sectionRef":"#","url":"docs/integrations/kafka/kafka-connect-intro","content":"","keywords":""},{"title":"Pre-requisites​","type":1,"pageTitle":"Using Kafka Connect","url":"docs/integrations/kafka/kafka-connect-intro#pre-requisites","content":"Download and install the Confluent platform [https://www.confluent.io/installation](https://www.confluent.io/installation). This main Confluent package contains the tested version of Kafka Connect v7.0.1. Java is required for the Confluent Platform. Refer to their documentation for the currently supported java versions.Ensure you have a ClickHouse instance available.Kafka instance - Confluent cloud is the easiest for this; otherwise, set up a self-managed instance using the above Confluent package. The setup of Kafka is beyond the scope of these docs.Test dataset. A small GitHub JSON-based dataset with an insertion script is provided for convenience here. This will automatically apply a Kafka schema to the data to ensure it is compatible with the JDBC connector.  "},{"title":"Assumptions​","type":1,"pageTitle":"Using Kafka Connect","url":"docs/integrations/kafka/kafka-connect-intro#assumptions","content":"We assume you are familiar with the Confluent Platform, specifically Kafka Connect. We recommend the Getting Started guide for Kafka Connect and the Kafka Connect 101 guide.We assume your source data is in JSON. Other data formats and the relevant configuration parameters are highlighted. "},{"title":"JDBC Connector","type":0,"sectionRef":"#","url":"docs/integrations/kafka/kafka-connect-jdbc","content":"","keywords":""},{"title":"Steps​","type":1,"pageTitle":"JDBC Connector","url":"docs/integrations/kafka/kafka-connect-jdbc#steps","content":""},{"title":"1. Install Kafka Connect and Connector​","type":1,"pageTitle":"JDBC Connector","url":"docs/integrations/kafka/kafka-connect-jdbc#1-install-kafka-connect-and-connector","content":"We assume you have downloaded the Confluent package and installed it locally. Follow the installation instructions for installing the connector as documented here. If you use the confluent-hub installation method, your local configuration files will be updated. For sending data to ClickHouse from Kafka, we use the Sink component of the connector. "},{"title":"2. Download and install the JDBC Driver​","type":1,"pageTitle":"JDBC Connector","url":"docs/integrations/kafka/kafka-connect-jdbc#2-download-and-install-the-jdbc-driver","content":"Download and install the ClickHouse JDBC driver clickhouse-jdbc-&amp;lt;version&gt;-shaded.jar from here. Install this into Kafka Connect following the details here. Other drivers may work but have not been tested. note Common Issue: the docs suggest copying the jar to share/java/kafka-connect-jdbc/. If you experience issues with Connect finding the driver, copy the driver to share/confluent-hub-components/confluentinc-kafka-connect-jdbc/lib/. Or modify plugin.path to include the driver - see below. "},{"title":"3. Prepare Configuration​","type":1,"pageTitle":"JDBC Connector","url":"docs/integrations/kafka/kafka-connect-jdbc#3-prepare-configuration","content":"Follow these instructions for setting up a Connect relevant to your installation type, noting the differences between a standalone and distributed cluster. If using Confluent Cloud the distributed setup is relevant. The following parameters are relevant to using the JDBC connector with ClickHouse. A full parameter list can be found here: _connection.url_ - this should take the form of jdbc:clickhouse://&amp;lt;clickhouse host&gt;:&amp;lt;clickhouse http port&gt;/&amp;lt;target database&gt;connection.user - a user with write access to the target databasetable.name.format- Clickhouse table to insert data. This must exist.batch.size - The number of rows to send in a single batch. Ensure this set is to an appropriately large number. Per ClickHouse recommendations a value of 1000 is should be considered a minimum.tasks.max - The JDBC Sink connector supports running one or more tasks. This can be used to increase performance. Along with batch size this represents your primary means of improving performance.value.converter.schemas.enable - Set to false if using a schema registry, true if you embed your schemas in the messages.value.converter - Set according to your datatype e.g. for JSON, “io.confluent.connect.json.JsonSchemaConverter”.key.converter - Set to “org.apache.kafka.connect.storage.StringConverter”. We utilise String keys.pk.mode - Not relevant to ClickHouse. Set to none.auto.create - Not supported and must be false.auto.evolve - We recommend false for this setting although it may be supported in the future.insert.mode - Set to “insert”. Other modes are not currently supported.key.converter - Set according to the types of your keys.value.converter - Set based on the type of data on your topic. This data must have a supported schema - JSON, Avro or Protobuf formats. If using our sample dataset for testing, ensure the following are set: value.converter.schemas.enable - Set to false as we utilize a schema registry. Set to true if you are embedding the schema in each message.key.converter - Set to “org.apache.kafka.connect.storage.StringConverter”. We utilise String keys.value.converter - Set “io.confluent.connect.json.JsonSchemaConverter”.value.converter.schema.registry.url - Set to the schema server url along with the credentials for the schema server via the parameter value.converter.schema.registry.basic.auth.user.info. Example configuration files for the Github sample data can be found here, assuming Connect is run in standalone mode and Kafka is hosted in Confluent Cloud. "},{"title":"4. Create the ClickHouse table​","type":1,"pageTitle":"JDBC Connector","url":"docs/integrations/kafka/kafka-connect-jdbc#4-create-the-clickhouse-table","content":"Ensure the table has been created, dropping it if it already exists from previous examples. An example compatible with the reduced Github dataset is shown below. Not the absence of any Array or Map types that are not currently not supported: CREATE TABLE github ( file_time DateTime, event_type Enum('CommitCommentEvent' = 1, 'CreateEvent' = 2, 'DeleteEvent' = 3, 'ForkEvent' = 4, 'GollumEvent' = 5, 'IssueCommentEvent' = 6, 'IssuesEvent' = 7, 'MemberEvent' = 8, 'PublicEvent' = 9, 'PullRequestEvent' = 10, 'PullRequestReviewCommentEvent' = 11, 'PushEvent' = 12, 'ReleaseEvent' = 13, 'SponsorshipEvent' = 14, 'WatchEvent' = 15, 'GistEvent' = 16, 'FollowEvent' = 17, 'DownloadEvent' = 18, 'PullRequestReviewEvent' = 19, 'ForkApplyEvent' = 20, 'Event' = 21, 'TeamAddEvent' = 22), actor_login LowCardinality(String), repo_name LowCardinality(String), created_at DateTime, updated_at DateTime, action Enum('none' = 0, 'created' = 1, 'added' = 2, 'edited' = 3, 'deleted' = 4, 'opened' = 5, 'closed' = 6, 'reopened' = 7, 'assigned' = 8, 'unassigned' = 9, 'labeled' = 10, 'unlabeled' = 11, 'review_requested' = 12, 'review_request_removed' = 13, 'synchronize' = 14, 'started' = 15, 'published' = 16, 'update' = 17, 'create' = 18, 'fork' = 19, 'merged' = 20), comment_id UInt64, path String, ref LowCardinality(String), ref_type Enum('none' = 0, 'branch' = 1, 'tag' = 2, 'repository' = 3, 'unknown' = 4), creator_user_login LowCardinality(String), number UInt32, title String, state Enum('none' = 0, 'open' = 1, 'closed' = 2), assignee LowCardinality(String), closed_at DateTime, merged_at DateTime, merge_commit_sha String, merged_by LowCardinality(String), review_comments UInt32, member_login LowCardinality(String) ) ENGINE = MergeTree ORDER BY (event_type, repo_name, created_at)  "},{"title":"5. Start Kafka Connect​","type":1,"pageTitle":"JDBC Connector","url":"docs/integrations/kafka/kafka-connect-jdbc#5-start-kafka-connect","content":"Start Kafka Connect in either standalone or distributed mode. For standalone mode, using the sample configurations, this is as simple as: ./bin/connect-standalone connect.properties.ini github-jdbc-sink.properties.ini  "},{"title":"6. Add data to Kafka​","type":1,"pageTitle":"JDBC Connector","url":"docs/integrations/kafka/kafka-connect-jdbc#6-add-data-to-kafka","content":"Insert messages to Kafka using the script and config provided. You will need to modify github.config to include your Kafka credentials. The script is currently configured for use with Confluent Cloud. python producer.py -c github.config  This script can be used to insert any ndjson file into a Kafka topic. This will attempt to infer a schema for you automatically. The sample config provided will only insert 10k messages - modify here if required. This configuration also removes any incompatible Array fields from the dataset during insertion to Kafka. This is required for the JDBC connector to convert messages to INSERT statements. If you are using your own data, ensure you either insert a schema with every message (setting _value.converter.schemas.enable _to true) or ensure your client publishes messages referencing a schema to the registry. Kafka Connect should begin consuming messages and inserting rows into ClickHouse. Note that warnings regards “[JDBC Compliant Mode] Transaction is not supported.” are expected and can be ignored. A simple read on the target table “Github” should confirm data insertion. SELECT count() FROM default.github;  | count\\(\\) | | :--- | | 10000 |  "},{"title":"Recommended Further Reading​","type":1,"pageTitle":"JDBC Connector","url":"docs/integrations/kafka/kafka-connect-jdbc#recommended-further-reading","content":"Kafka Sink Configuration ParametersKafka Connect Deep Dive – JDBC Source ConnectorKafka Connect JDBC Sink deep-dive: Working with Primary KeysKafka Connect in Action: JDBC Sink - for those who prefer to watch over read.Kafka Connect Deep Dive – Converters and Serialization Explained "},{"title":"Connection Options","type":0,"sectionRef":"#","url":"docs/integrations/kafka/kafka-connect-options","content":"Connection Options Kafka Connect uses Sink Connectors to deliver data from Kafka topics into other data stores such as ClickHouse. Two Sink connectors provided by Confluent are compatible with ClickHouse: JDBC Connector - This Connector is both a Sink and Source Connector (for pushing data to Kafka) via the JDBC interface.HTTP Sink Connector - A connector for pulling data from Kafka and inserting it via its HTTP interface. Limitations Each of these has benefits and limitations: The JDBC connector relies on the user providing a JDBC driver. This driver has several versions, including the official ClickHouse distribution. This version uses the HTTP interface, although native support is planned. Until the native interface is not supported, it provides no performance benefit over the HTTP Sink other than ease of configuration. Other drivers support the native protocol, but these have not been tested.The JDBC connector requires a Kafka schema defining the types of the fields. It uses this schema, defined in JSON schema, to formulate insert statements. Whilst this is effective on primitive types, the connector does not support ClickHouse specific types, e.g., Arrays and Maps. Furthermore, this connector will not support several configuration options which rely on DDL queries - highlighted in the section JDBC Connector below.The HTTP Sink Connector does not require a data schema. Our example assumes the data is in JSON format - although this approach should be compatible with any formats that the ClickHouse HTTP interface can consume. The HTTP Sink Connector is also deployed natively in Confluent Cloud and has been tested with ClickHouse Cloud, unlike the JDBC, which must be self-managed. We provide instructions for both scenarios below.The JDBC connector is not currently hosted in Confluent Cloud. This must be self-managed.Both connectors have at-least-once delivery semantics. Duplicates may therefore occur in ClickHouse. The JDBC Connector is distributed under the Confluent Community License. The HTTP Connector conversely requires a Confluent Enterprise License.","keywords":""},{"title":"Using the Kafka table engine","type":0,"sectionRef":"#","url":"docs/integrations/kafka/kafka-table-engine","content":"","keywords":""},{"title":"Kafka to ClickHouse​","type":1,"pageTitle":"Using the Kafka table engine","url":"docs/integrations/kafka/kafka-table-engine#kafka-to-clickhouse","content":""},{"title":"Overview​","type":1,"pageTitle":"Using the Kafka table engine","url":"docs/integrations/kafka/kafka-table-engine#overview","content":"Initially, we focus on the most common use case: using the Kafka table engine to insert data into ClickHouse from Kafka. The Kafka table engine allows ClickHouse to read from a Kafka topic directly. Whilst useful for viewing messages on a topic, the engine by design only permits one-time retrieval, i.e. when a query is issued to the table, it consumes data from the queue and increases the consumer offset before returning results to the caller. Data cannot, in effect, be re-read without resetting these offsets. To persist this data from a read of the table engine, we need a means of capturing the data and inserting it into another table. Trigger-based materialized views natively provide this functionality. A materialized view initiates a read on the table engine, receiving batches of documents. The TO clause determines the destination of the data - typically a table of the Merge Tree family. This process is visualized below:  "},{"title":"Steps​","type":1,"pageTitle":"Using the Kafka table engine","url":"docs/integrations/kafka/kafka-table-engine#steps","content":"1. Prepare​ If you have data populated on a target topic, you can adapt the following for use in your dataset. Alternatively, a sample Github dataset is provided here. This dataset is used in the examples below and uses a reduced schema and subset of the rows (specifically, we limit to Github events concerning the ClickHouse repository), compared to the full dataset available here, for brevity. This is still sufficient for most of the queries published with the dataset to work. 2. Configure ClickHouse​ This step is required if you are connecting to a secure Kafka. These settings cannot be passed through the SQL DDL commands and must be configured in the ClickHouse config.xml. We assume you are connecting to a SASL secured instance. This is the simplest method when interacting with Confluent Cloud. &lt;clickhouse&gt; &lt;kafka&gt; &lt;sasl_username&gt;username&lt;/sasl_username&gt; &lt;sasl_password&gt;password&lt;/sasl_password&gt; &lt;security_protocol&gt;sasl_ssl&lt;/security_protocol&gt; &lt;sasl_mechanisms&gt;PLAIN&lt;/sasl_mechanisms&gt; &lt;/kafka&gt; &lt;/clickhouse&gt;  Either place the above snippet inside a new file under your conf.d/ directory or merge it into existing configuration files. For settings that can be configured, see here. 3. Create the destination table​ Prepare your destination table. In the example below we use the reduced GitHub schema for purposes of brevity. Note that although we use a MergeTree table engine, this example could easily be adapted for any member of the MergeTree family. CREATE TABLE github ( file_time DateTime, event_type Enum('CommitCommentEvent' = 1, 'CreateEvent' = 2, 'DeleteEvent' = 3, 'ForkEvent' = 4, 'GollumEvent' = 5, 'IssueCommentEvent' = 6, 'IssuesEvent' = 7, 'MemberEvent' = 8, 'PublicEvent' = 9, 'PullRequestEvent' = 10, 'PullRequestReviewCommentEvent' = 11, 'PushEvent' = 12, 'ReleaseEvent' = 13, 'SponsorshipEvent' = 14, 'WatchEvent' = 15, 'GistEvent' = 16, 'FollowEvent' = 17, 'DownloadEvent' = 18, 'PullRequestReviewEvent' = 19, 'ForkApplyEvent' = 20, 'Event' = 21, 'TeamAddEvent' = 22), actor_login LowCardinality(String), repo_name LowCardinality(String), created_at DateTime, updated_at DateTime, action Enum('none' = 0, 'created' = 1, 'added' = 2, 'edited' = 3, 'deleted' = 4, 'opened' = 5, 'closed' = 6, 'reopened' = 7, 'assigned' = 8, 'unassigned' = 9, 'labeled' = 10, 'unlabeled' = 11, 'review_requested' = 12, 'review_request_removed' = 13, 'synchronize' = 14, 'started' = 15, 'published' = 16, 'update' = 17, 'create' = 18, 'fork' = 19, 'merged' = 20), comment_id UInt64, path String, ref LowCardinality(String), ref_type Enum('none' = 0, 'branch' = 1, 'tag' = 2, 'repository' = 3, 'unknown' = 4), creator_user_login LowCardinality(String), number UInt32, title String, labels Array(LowCardinality(String)), state Enum('none' = 0, 'open' = 1, 'closed' = 2), assignee LowCardinality(String), assignees Array(LowCardinality(String)), closed_at DateTime, merged_at DateTime, merge_commit_sha String, requested_reviewers Array(LowCardinality(String)), merged_by LowCardinality(String), review_comments UInt32, member_login LowCardinality(String) ) ENGINE = MergeTree ORDER BY (event_type, repo_name, created_at)  4. Create and populate the topic​ Kcat is recommended as a simple means of publishing data to a topic. Using the provided dataset with Confluent Cloud is as simple as modifying the configuration file and running the below example. The following assumes you have created the topic “github”. cat github_all_columns.ndjson | kafkacat -b &lt;host&gt;:&lt;port&gt; -X security.protocol=sasl_ssl -X sasl.mechanisms=PLAIN -X sasl.username=&lt;username&gt; -X sasl.password=&lt;password&gt; -t github  Note that this dataset is deliberately small, with only 200,000 rows. This should take only a few seconds to insert on most Kafka clusters, although this may depend on network connectivity. We include instructions to produce larger datasets should you need e.g. for performance testing. 5. Create the Kafka table engine​ The below example creates a table engine with the same schema as the merge tree table. Note that this isn’t required, e.g. you can have an alias or ephemeral columns in the target table. The settings are important; however - note the use of JSONEachRow as the data type for consuming JSON from a Kafka topic. The values “github” and “clickhouse” represent the name of the topic and consumer group names, respectively. Note that the topics can actually be a list of values. CREATE TABLE default.github_queue ( file_time DateTime, event_type Enum('CommitCommentEvent' = 1, 'CreateEvent' = 2, 'DeleteEvent' = 3, 'ForkEvent' = 4, 'GollumEvent' = 5, 'IssueCommentEvent' = 6, 'IssuesEvent' = 7, 'MemberEvent' = 8, 'PublicEvent' = 9, 'PullRequestEvent' = 10, 'PullRequestReviewCommentEvent' = 11, 'PushEvent' = 12, 'ReleaseEvent' = 13, 'SponsorshipEvent' = 14, 'WatchEvent' = 15, 'GistEvent' = 16, 'FollowEvent' = 17, 'DownloadEvent' = 18, 'PullRequestReviewEvent' = 19, 'ForkApplyEvent' = 20, 'Event' = 21, 'TeamAddEvent' = 22), actor_login LowCardinality(String), repo_name LowCardinality(String), created_at DateTime, updated_at DateTime, action Enum('none' = 0, 'created' = 1, 'added' = 2, 'edited' = 3, 'deleted' = 4, 'opened' = 5, 'closed' = 6, 'reopened' = 7, 'assigned' = 8, 'unassigned' = 9, 'labeled' = 10, 'unlabeled' = 11, 'review_requested' = 12, 'review_request_removed' = 13, 'synchronize' = 14, 'started' = 15, 'published' = 16, 'update' = 17, 'create' = 18, 'fork' = 19, 'merged' = 20), comment_id UInt64, path String, ref LowCardinality(String), ref_type Enum('none' = 0, 'branch' = 1, 'tag' = 2, 'repository' = 3, 'unknown' = 4), creator_user_login LowCardinality(String), number UInt32, title String, labels Array(LowCardinality(String)), state Enum('none' = 0, 'open' = 1, 'closed' = 2), assignee LowCardinality(String), assignees Array(LowCardinality(String)), closed_at DateTime, merged_at DateTime, merge_commit_sha String, requested_reviewers Array(LowCardinality(String)), merged_by LowCardinality(String), review_comments UInt32, member_login LowCardinality(String) ) ENGINE = Kafka('kafka_host:9092', 'github', 'clickhouse', 'JSONEachRow') settings kafka_thread_per_consumer = 0, kafka_num_consumers = 1;  We discuss engine settings and performance tuning below. At this point, a simple select on the table github_queue should read some rows. Note that this will move the consumer offsets forward, preventing these rows from being re-read without a reset. Note the limit and required parameter stream_like_engine_allow_direct_select. 6. Create the materialized view​ The materialized view will connect the two previously created tables, reading data from the Kafka table engine and inserting it into the target merge tree table. We can do a number of data transformations. We will do a simple read and insert. The use of * assumes column names are identical (case sensitive). CREATE MATERIALIZED VIEW default.github_mv TO default.github AS SELECT * FROM default.github_queue;  At the point of creation, the materialized view connects to the Kafka engine and commences reading: inserting rows into the target table. This process will continue indefinitely, with subsequent message inserts into Kafka being consumed. Feel free to re-run the insertion script to insert further messages to Kafka. 7. Confirm rows have been inserted​ Confirm data exists in the target table: SELECT count() FROM default.github;  You should see 200,000 rows: | count\\(\\) | | :--- | | 200000 |  "},{"title":"Common Operations​","type":1,"pageTitle":"Using the Kafka table engine","url":"docs/integrations/kafka/kafka-table-engine#common-operations","content":"Stopping &amp; restarting message consumption​ To stop message consumption, simply detach the Kafka engine table e.g. DETACH TABLE github_queue;  Note that this will not impact the offsets of the consumer group. To restart consumption, and continue from the previous offset, simply reattach the table. ATTACH TABLE github_queue;  We can use this operation to make setting and schema changes - see below. Adding Kafka Metadata​ It is not uncommon for users to need to identify the coordinates of the original Kafka messages for the rows in ClickHouse. For example, we may want to know how much of a specific topic or partition we have consumed. For this purpose, the Kafka table engine exposes several virtual columns. These can be persisted as columns in our target table by modifying our schema and materialized view’s select statement. First, we perform the stop operation described above before adding columns to our target table. DETACH TABLE github_queue;  Below we add information columns to identify the source topic and the partition from which the row originated. ALTER TABLE github ADD COLUMN topic String, ADD COLUMN partition UInt64;  Next, we need to ensure virtual columns are mapped as required. This requires us to drop and recreate our materialized view. Note those prefixed with _. A complete listing of virtual columns can be found here. DROP VIEW default.github_mv; CREATE MATERIALIZED VIEW default.github_mv TO default.github AS SELECT *, _topic as topic, _partition as partition FROM default.github_queue;  Finally, we are good to reattach our Kafka engine table github_queue and restart message consumption. ATTACH TABLE github_queue;  Newly consumed rows should have the metadata. SELECT actor_login, event_type, created_at, topic, partition FROM default.github LIMIT 10;  The result looks like: actor_login\tevent_type\tcreated_at\ttopic\tpartitionIgorMinar\tCommitCommentEvent\t2011-02-12 02:22:00\tgithub\t0 queeup\tCommitCommentEvent\t2011-02-12 02:23:23\tgithub\t0 IgorMinar\tCommitCommentEvent\t2011-02-12 02:23:24\tgithub\t0 IgorMinar\tCommitCommentEvent\t2011-02-12 02:24:50\tgithub\t0 IgorMinar\tCommitCommentEvent\t2011-02-12 02:25:20\tgithub\t0 dapi\tCommitCommentEvent\t2011-02-12 06:18:36\tgithub\t0 sourcerebels\tCommitCommentEvent\t2011-02-12 06:34:10\tgithub\t0 jamierumbelow\tCommitCommentEvent\t2011-02-12 12:21:40\tgithub\t0 jpn\tCommitCommentEvent\t2011-02-12 12:24:31\tgithub\t0 Oxonium\tCommitCommentEvent\t2011-02-12 12:31:28\tgithub\t0 Modify Kafka Engine Settings​ We recommend dropping the Kafka engine table and recreating it with the new settings. The materialized view does not need to be modified during this process - message consumption will resume once the Kafka engine table is recreated. Debugging Issues​ Errors such as authentication issues are not reported in responses to Kafka engine DDL. For diagnosing issues, we recommend using the main ClickHouse log file clickhouse-server.err.log. Further trace logging for the underlying Kafka client library librdkafka can be enabled through configuration. &lt;kafka&gt; &lt;debug&gt;all&lt;/debug&gt; &lt;/kafka&gt;  Handling malformed messages​ Kafka is often used as a &quot;dumping ground&quot; for data. This leads to topics containing mixed message formats and inconsistent field names. Avoid this and utilize Kafka features such Kafka Streams or ksqlDB to ensure messages are well-formed and consistent before insertion into Kafka. If these options are not possible, we can assist: Treat the message field as strings. Functions can be used in the materialized view statement to perform cleansing and casting if required. This should not represent a production solution but might assist in one-off ingestions.If you’re consuming JSON from a topic, using the JSONEachRow format, consider the setting input_format_skip_unknown_fields. Normally, when writing data, ClickHouse throws an exception if input data contains columns that do not exist in the target table. If this option is enabled, these excess columns will be ignored. Again this is not a production-level solution and might confuse others.Consider the setting kafka_skip_broken_messages. This requires the user to specify the level of tolerance per block for malformed messages - considered in the context of kafka_max_block_size. If this tolerance is exceeded (measured in absolute messages) the usual exception behaviour will revert, and other messages will be skipped.  Delivery Semantics and challenges with duplicates​ The Kafka table engine has at-least-once semantics. Duplicates are possible in several known rare circumstances. For example, messages could be read from Kafka and successfully inserted into ClickHouse. Before the new offset can be committed, the connection to Kafka is lost. A retry of the block in this situation is required. The block may be de-duplicated using a distributed table or ReplicatedMergeTree as the target table. While this reduces the chance of duplicate rows, it relies on identical blocks. Events such as a Kafka rebalancing may invalidate this assumption, causing duplicates in rare circumstances. Quorum based Inserts​ Users often need quorum-based inserts for cases where higher delivery guarantees are required in ClickHouse. This can’t be set on the materialized view or the target table. It can, however, be set for user profiles e.g. &lt;profiles&gt; &lt;default&gt; &lt;insert_quorum&gt;2&lt;/insert_quorum&gt; &lt;/default&gt; &lt;/profiles&gt;  "},{"title":"ClickHouse to Kafka​","type":1,"pageTitle":"Using the Kafka table engine","url":"docs/integrations/kafka/kafka-table-engine#clickhouse-to-kafka","content":"Although a rarer use case, ClickHouse data can also be persisted in Kafka. For example, we will insert rows manually into a Kafka table engine. This data will be read by the same Kafka engine, whose materialized view will place the data into a Merge Tree table. Finally, we demonstrate the application of materialized views in inserts to Kafka to read tables from existing source tables. "},{"title":"Steps​","type":1,"pageTitle":"Using the Kafka table engine","url":"docs/integrations/kafka/kafka-table-engine#steps-1","content":"Our initial objective is best illustrated:  We assume you have the tables and views created under steps for Kafka to ClickHouse and that the topic has been fully consumed. 1. Inserting rows directly​ First, confirm the count of the target table. SELECT count() FROM default.github;  You should have 200,000 rows: | count\\(\\) | | :--- | | 200000 |  Now insert rows from the GitHub target table back into the Kafka table engine github_queue. Note how we utilize JSONEachRow format and LIMIT the select to 100. INSERT INTO default.github_queue (*) SELECT file_time, event_type, actor_login, repo_name, created_at, updated_at, action, comment_id, path, ref, ref_type, creator_user_login, number, title, labels, state, assignee, assignees, closed_at, merged_at, merge_commit_sha, requested_reviewers, merged_by, review_comments, member_login FROM default.github LIMIT 100 FORMAT JSONEachRow; Recount the row in GitHub to confirm it has increased by 100. As shown in the above diagram, rows have been inserted into Kafka via the Kafka table engine before being re-read by the same engine and inserted into the GitHub target table by our materialized view! SELECT count() FROM default.github;  You should see 100 additional rows: | count\\(\\) | | :--- | | 200100 |  2. Utilizing materialized views​ We can utilize materialized views to push messages to a Kafka engine (and a topic) when documents are inserted into a table. When rows are inserted into the GitHub table, a materialized view is triggered, which causes the rows to be inserted back into a Kafka engine and into a new topic. Again this is best illustrated:  Create a new Kafka topic github_out or equivalent. Ensure a Kafka table engine github_out_queue points to this topic. ( file_time DateTime, event_type Enum('CommitCommentEvent' = 1, 'CreateEvent' = 2, 'DeleteEvent' = 3, 'ForkEvent' = 4, 'GollumEvent' = 5, 'IssueCommentEvent' = 6, 'IssuesEvent' = 7, 'MemberEvent' = 8, 'PublicEvent' = 9, 'PullRequestEvent' = 10, 'PullRequestReviewCommentEvent' = 11, 'PushEvent' = 12, 'ReleaseEvent' = 13, 'SponsorshipEvent' = 14, 'WatchEvent' = 15, 'GistEvent' = 16, 'FollowEvent' = 17, 'DownloadEvent' = 18, 'PullRequestReviewEvent' = 19, 'ForkApplyEvent' = 20, 'Event' = 21, 'TeamAddEvent' = 22), actor_login LowCardinality(String), repo_name LowCardinality(String), created_at DateTime, updated_at DateTime, action Enum('none' = 0, 'created' = 1, 'added' = 2, 'edited' = 3, 'deleted' = 4, 'opened' = 5, 'closed' = 6, 'reopened' = 7, 'assigned' = 8, 'unassigned' = 9, 'labeled' = 10, 'unlabeled' = 11, 'review_requested' = 12, 'review_request_removed' = 13, 'synchronize' = 14, 'started' = 15, 'published' = 16, 'update' = 17, 'create' = 18, 'fork' = 19, 'merged' = 20), comment_id UInt64, path String, ref LowCardinality(String), ref_type Enum('none' = 0, 'branch' = 1, 'tag' = 2, 'repository' = 3, 'unknown' = 4), creator_user_login LowCardinality(String), number UInt32, title String, labels Array(LowCardinality(String)), state Enum('none' = 0, 'open' = 1, 'closed' = 2), assignee LowCardinality(String), assignees Array(LowCardinality(String)), closed_at DateTime, merged_at DateTime, merge_commit_sha String, requested_reviewers Array(LowCardinality(String)), merged_by LowCardinality(String), review_comments UInt32, member_login LowCardinality(String) ) ENGINE = Kafka('host:port', 'github_out', 'clickhouse_out', 'JSONEachRow') settings kafka_thread_per_consumer = 0, kafka_num_consumers = 1;  Now create a new materialized view github_out_mv to point at the GitHub table, inserting rows to the above engine when it triggers. Additions to the GitHub table will, as a result, be pushed to our new Kafka topic. CREATE MATERIALIZED VIEW default.github_out_mv TO default.github_out_queue AS SELECT file_time, event_type, actor_login, repo_name, created_at, updated_at, action, comment_id, path, ref, ref_type, creator_user_login, number, title, labels, state, assignee, assignees, closed_at, merged_at, merge_commit_sha, requested_reviewers, merged_by, review_comments, member_login FROM default.github FORMAT JsonEachRow;  Should you insert into the original github topic, created as part of Kafka to ClickHouse, documents will magically appear in the “github_clickhouse” topic. Confirm this with native Kafka tooling. For example, below, we insert 100 rows onto the github topic using kcat for a Confluent Cloud hosted topic: head -n 10 github_all_columns.ndjson | kafkacat -b &lt;host&gt;:&lt;port&gt; -X security.protocol=sasl_ssl -X sasl.mechanisms=PLAIN -X sasl.username=&lt;username&gt; -X sasl.password=&lt;password&gt; -t github  A read on the github_out topic should confirm delivery of the messages. kafkacat -b &lt;host&gt;:&lt;port&gt; -X security.protocol=sasl_ssl -X sasl.mechanisms=PLAIN -X sasl.username=&lt;username&gt; -X sasl.password=&lt;password&gt; -t github_out -C -e -q | wc -l  Although an elaborate example, this illustrates the power of materialized views when used in conjunction with the Kafka engine. "},{"title":"Clusters and Performance​","type":1,"pageTitle":"Using the Kafka table engine","url":"docs/integrations/kafka/kafka-table-engine#clusters-and-performance","content":""},{"title":"Working with ClickHouse Clusters​","type":1,"pageTitle":"Using the Kafka table engine","url":"docs/integrations/kafka/kafka-table-engine#working-with-clickhouse-clusters","content":"Through Kafka consumer groups, multiple ClickHouse instances can potentially read from the same topic. Each consumer will be assigned to a topic partition in a 1:1 mapping. When scaling ClickHouse consumption using the Kafka table engine, consider that the total number of consumers within a cluster cannot exceed the number of partitions on the topic. Therefore ensure partitioning is appropriately configured for the topic in advance. Multiple ClickHouse instances can all be configured to read from a topic using the same consumer group id - specified during the Kafka table engine creation. Therefore, each instance will read from one or more partitions, inserting segments to their local target table. The target tables can, in turn, be configured to use a ReplicatedMergeTree to handle duplication of the data. This approach allows Kafka reads to be scaled with the ClickHouse cluster, provided there are sufficient Kafka partitions.  "},{"title":"Tuning Performance​","type":1,"pageTitle":"Using the Kafka table engine","url":"docs/integrations/kafka/kafka-table-engine#tuning-performance","content":"Consider the following when looking to increase Kafka Engine table throughput performance: The performance will vary depending on the message size, format, and target table types. 100k rows/sec on a single table engine should be considered obtainable. By default, messages are read in blocks, controlled by the parameter kafka_max_block_size. By default, this is set to the max_block_size, defaulting to 65,536. Unless messages are extremely large, this should nearly always be increased. Values between 500k to 1M are not uncommon. Test and evaluate the effect on throughput performance.The number of consumers for a table engine can be increased using kafka_num_consumers. However, by default, inserts will be linearized in a single thread unless kafka_thread_per_consumer is changed from the default value of 1. Set this to 1 to ensure flushes are performed in parallel. Note that creating a Kafka engine table with N consumers (and kafka_thread_per_consumer=1) is logically equivalent to creating N Kafka engines, each with a materialized view and kafka_thread_per_consumer=0.Increasing consumers is not a free operation. Each consumer maintains its own buffers and threads, increasing the overhead on the server. Be conscious of the overhead of consumers and scale linearly across your cluster first and if possible.If the throughput of Kafka messages is variable and delays are acceptable, consider increasing the stream_flush_interval_ms to ensure larger blocks are flushed. background_schedule_pool_size sets the number of threads performing background tasks. These threads are used for Kafka streaming. This setting is applied at the ClickHouse server start and can’t be changed in a user session, defaulting to 128. It is unlikely you should ever need to change this as sufficient threads are available for the number of Kafka engines you will create on a single host. If you see timeouts in the logs, it may be appropriate to increase this.For communication with Kafka, the librdkafka library is used, which itself creates threads. Large numbers of Kafka tables, or consumers, can thus result in large numbers of context switches. Either distribute this load across the cluster, only replicating the target tables if possible, or consider using a table engine to read from multiple topics - a list of values is supported. Multiple materialized views can be read from a single table, each filtering to the data from a specific topic. Any settings changes should be tested. We recommend monitoring Kafka consumer lags to ensure you are properly scaled. "},{"title":"Additional Settings​","type":1,"pageTitle":"Using the Kafka table engine","url":"docs/integrations/kafka/kafka-table-engine#additional-settings","content":"Aside from the settings discussed above, the following may be of interest: Kafka_max_wait_ms - The wait time in milliseconds for reading messages from Kafka before retry. Set at a user profile level and defaults to 5000. All settings from the underlying librdkafka can also be placed in the ClickHouse configuration files inside a kafka element - setting names should be XML elements with periods replaced with underscores e.g. &lt;clickhouse&gt; &lt;kafka&gt; &lt;enable_ssl_certificate_verification&gt;false&lt;/enable_ssl_certificate_verification&gt; &lt;/kafka&gt; &lt;/clickhouse&gt;  These are expert settings for which the user is referred to the Kafka documentation. "},{"title":"Connecting Kafka","type":0,"sectionRef":"#","url":"docs/integrations/kafka/kakfa-intro","content":"","keywords":""},{"title":"Assumptions​","type":1,"pageTitle":"Connecting Kafka","url":"docs/integrations/kafka/kakfa-intro#assumptions","content":"You are familiar with the Kafka fundamentals, such as producers, consumers and topics.You have a topic prepared for these examples. We assume all data is stored in Kafka as JSON, although the principles remain the same if using Avro.We utilise the excellent kcat (formerly kafkacat) in our examples to publish and consume Kafka data.Whilst we reference some python scripts for loading sample data, feel free to adapt the examples to your dataset.You are broadly familiar with ClickHouse materialized views. "},{"title":"Connecting ClickHouse to MySQL using the MySQL Table Engine","type":0,"sectionRef":"#","url":"docs/integrations/mysql/mysql-with-clickhouse","content":"","keywords":"clickhouse mysql connect integrate table engine"},{"title":"1. Configure MySQL​","type":1,"pageTitle":"Connecting ClickHouse to MySQL using the MySQL Table Engine","url":"docs/integrations/mysql/mysql-with-clickhouse#1-configure-mysql","content":"Create a database in MySQL: CREATE DATABASE db1; Create a table: CREATE TABLE db1.table1 ( id INT, column1 VARCHAR(255) ); Insert sample rows: INSERT INTO db1.table1 (id, column1) VALUES (1, 'abc'), (2, 'def'), (3, 'ghi'); Create a user to connect from ClickHouse: CREATE USER 'mysql_clickhouse'@'%' IDENTIFIED BY 'Password123!'; Grant privileges as needed. (For demonstration purposes, the mysql_clickhouse user is granted admin prvileges.) GRANT ALL PRIVILEGES ON *.* TO 'mysql_clickhouse'@'%';  "},{"title":"2. Define a Table in ClickHouse​","type":1,"pageTitle":"Connecting ClickHouse to MySQL using the MySQL Table Engine","url":"docs/integrations/mysql/mysql-with-clickhouse#2-define-a-table-in-clickhouse","content":"Now let's create a ClickHouse table that uses the MySQL table engine: CREATE TABLE mysql_table1 ( id UInt64, column1 String ) ENGINE = MySQL('mysql-host.domain.com','db1','table1','mysql_clickhouse','Password123!') The minimum parameters are: parameter\tDescription\texamplehost\thostname or IP\tmysql-host.domain.com database\tmysql database name\tdb1 table\tmysql table name\ttable1 user\tusername to connect to mysql\tmysql_clickhouse password\tpassword to connect to mysql\tPassword123! note View the MySQL table engine doc page for a complete list of parameters. "},{"title":"3. Test the Integration​","type":1,"pageTitle":"Connecting ClickHouse to MySQL using the MySQL Table Engine","url":"docs/integrations/mysql/mysql-with-clickhouse#3-test-the-integration","content":"In MySQL, insert a sample row: INSERT INTO db1.table1 (id, column1) VALUES (4, 'jkl'); Notice the existing rows from the MySQL table are in the ClickHouse table, along with the new row you just added: SELECT id, column1 FROM mysql_table1 You should see 4 rows: Query id: 6d590083-841e-4e95-8715-ef37d3e95197 ┌─id─┬─column1─┐ │ 1 │ abc │ │ 2 │ def │ │ 3 │ ghi │ │ 4 │ jkl │ └────┴─────────┘ 4 rows in set. Elapsed: 0.044 sec. Let's add a row to the ClickHouse table: INSERT INTO mysql_table1 (id, column1) VALUES (5,'mno') Notice the new row appears in MySQL: mysql&gt; select id,column1 from db1.table1; You should see the new row: +------+---------+ | id | column1 | +------+---------+ | 1 | abc | | 2 | def | | 3 | ghi | | 4 | jkl | | 5 | mno | +------+---------+ 5 rows in set (0.01 sec)  "},{"title":"Summary​","type":1,"pageTitle":"Connecting ClickHouse to MySQL using the MySQL Table Engine","url":"docs/integrations/mysql/mysql-with-clickhouse#summary","content":"The MySQL table engine allows you to connect ClickHouse to MySQL to exchange data back and forth. For more details, be sure to check out the documentation page for the MySQL table engine. "},{"title":"Connecting ClickHouse to PostgreSQL using the PostgreSQL Table Engine","type":0,"sectionRef":"#","url":"docs/integrations/postgresql/postgres-with-clickhouse","content":"","keywords":"clickhouse postgres postgresql connect integrate table engine"},{"title":"1. Setting up PostgreSQL​","type":1,"pageTitle":"Connecting ClickHouse to PostgreSQL using the PostgreSQL Table Engine","url":"docs/integrations/postgresql/postgres-with-clickhouse#1-setting-up-postgresql","content":"In postgresql.conf, add the following entry to enable PostgreSQL to listen on the network interfaces: listen_addresses = '*' Create a user to connect from ClickHouse. For demonstration purposes, this example grants full superuser rights. CREATE ROLE clickhouse_user SUPERUSER LOGIN PASSWORD 'ClickHouse_123'; Create a new database in PostgreSQL: CREATE DATABASE db_in_psg; Create a new table: CREATE TABLE table1 ( id integer primary key, column1 varchar(10) ); Let's add a few rows for testing: INSERT INTO table1 (id, column1) VALUES (1, 'abc'), (2, 'def'); To configure PostgreSQL to allow connections to the new database with the new user for replication, add the following entry to the pg_hba.conf file. Update the address line with either the subnet or IP address of your PostgreSQL server: # TYPE DATABASE USER ADDRESS METHOD host db_in_psg clickhouse_user 192.168.1.0/24 password Reload the pg_hba.conf configuration (adjust this command depending on your version): /usr/pgsql-12/bin/pg_ctl reload Verify the new clickhouse_user can login: psql -U clickhouse_user -W -d db_in_psg -h &lt;your_postgresql_host&gt;  "},{"title":"2. Define a Table in ClickHouse​","type":1,"pageTitle":"Connecting ClickHouse to PostgreSQL using the PostgreSQL Table Engine","url":"docs/integrations/postgresql/postgres-with-clickhouse#2-define-a-table-in-clickhouse","content":"Login to the clickhouse-client: clickhouse-client --user default --password ClickHouse123! Let's create a new database: CREATE DATABASE db_in_ch; Create a table that uses the PostgreSQL: CREATE TABLE db_in_ch.table1 ( id UInt64, column1 String ) ENGINE = PostgreSQL('postgres-host.domain.com:5432', 'db_in_psg', 'table1', 'clickhouse_user', 'ClickHouse_123'); The minimum parameters needed are: parameter\tDescription\texamplehost:port\thostname or IP and port\tpostgres-host.domain.com:5432 database\tPostgreSQL database name\tdb_in_psg user\tusername to connect to postgres\tclickhouse_user password\tpassword to connect to postgres\tClickHouse_123 note View the PostgreSQL table engine doc page for a complete list of parameters. "},{"title":"3 Test the Integration​","type":1,"pageTitle":"Connecting ClickHouse to PostgreSQL using the PostgreSQL Table Engine","url":"docs/integrations/postgresql/postgres-with-clickhouse#3-test-the-integration","content":"In ClickHouse, view initial rows: SELECT * FROM db_in_ch.table1 The ClickHouse table should automatically be populated with the two rows that already existed in the table in PostgreSQL: Query id: 34193d31-fe21-44ac-a182-36aaefbd78bf ┌─id─┬─column1─┐ │ 1 │ abc │ │ 2 │ def │ └────┴─────────┘ Back in PostgreSQL, add a couple of rows to the table: INSERT INTO table1 (id, column1) VALUES (3, 'ghi'), (4, 'jkl'); Those two new rows should appear in your ClickHouse table: SELECT * FROM db_in_ch.table1 The response should be: Query id: 86fa2c62-d320-4e47-b564-47ebf3d5d27b ┌─id─┬─column1─┐ │ 1 │ abc │ │ 2 │ def │ │ 3 │ ghi │ │ 4 │ jkl │ └────┴─────────┘ Let's see what happens when you add rows to the ClickHouse table: INSERT INTO db_in_ch.table1 (id, column1) VALUES (5, 'mno'), (6, 'pqr'); The rows added in ClickHouse should appear in the table in PostgreSQL: db_in_psg=# SELECT * FROM table1; id | column1 ----+--------- 1 | abc 2 | def 3 | ghi 4 | jkl 5 | mno 6 | pqr (6 rows)  "},{"title":"Summary​","type":1,"pageTitle":"Connecting ClickHouse to PostgreSQL using the PostgreSQL Table Engine","url":"docs/integrations/postgresql/postgres-with-clickhouse#summary","content":"This example demonstrated the basic integration between PostgreSQL and ClickHouse using the PostrgeSQL table engine. Check out the doc page for the PostgreSQL table engine for more features, such as specifying schemas, returning only a subset of columns, and connecting to multiple replicas. "},{"title":"Using MinIO","type":0,"sectionRef":"#","url":"docs/integrations/s3/s3-minio","content":"Using MinIO All S3 functions and tables and compatible with MinIO. Users may experience superior throughput on self-hosted MinIO stores, especially in the event of optimal network locality.","keywords":""},{"title":"Connnecting S3","type":0,"sectionRef":"#","url":"docs/integrations/s3/s3-intro","content":"Connnecting S3 Amazon S3 or Amazon Simple Storage Service is a service offered by Amazon Web Services (AWS) that provides object storage through a web service interface. Users can insert S3 based data into ClickHouse and use S3 as an export destination, thus allowing interaction with “Data Lake” architectures. Furthermore, s3 can provide “cold” storage tiers and assist with separating storage and compute. Below we outline the approach for these use cases: identifying key configuration parameters and any current limitations and providing hints on optimizing performance. We utilize a subset of the new york taxi public dataset for read-orientated examples. We assume you have s3 buckets available for insert examples into which data can be written.","keywords":""},{"title":"Connecting ClickHouse to PostgreSQL using the MaterializedPostgreSQL database engine","type":0,"sectionRef":"#","url":"docs/integrations/postgresql/postgres-with-clickhouse-database-engine","content":"","keywords":"clickhouse postgres postgresql connect integrate"},{"title":"1. In PostgreSQL​","type":1,"pageTitle":"Connecting ClickHouse to PostgreSQL using the MaterializedPostgreSQL database engine","url":"docs/integrations/postgresql/postgres-with-clickhouse-database-engine#1-in-postgresql","content":"In postgresql.conf, set minimum listen levels, replication wal level and replication slots: add the following entries: listen_addresses = '*' max_replication_slots = 10 wal_level = logical  *ClickHouse needs minimum of logical wal level and minimum 2 replication slots Using an admin account, create a user to connect from ClickHouse: CREATE ROLE clickhouse_user SUPERUSER LOGIN PASSWORD 'ClickHouse_123';  *for demonstration purposes, full superuser rights have been granted. create a new database: CREATE DATABASE db1;  connect to the new database in psql: \\connect db1  create a new table: CREATE TABLE table1 ( id integer primary key, column1 varchar(10) );  add initial rows: INSERT INTO table1 (id, column1) VALUES (1, 'abc'), (2, 'def');  Configure the PostgreSQLto allow connections to the new database with the new user for replication: below is the minimum entry to add to the pg_hba.conf file: # TYPE DATABASE USER ADDRESS METHOD host db1 clickhouse_user 192.168.1.0/24 password  *for demonstration purposes, this is using clear text password authentication method. update the address line with either the subnet or the address of the server per PostgreSQL documentation reload the pg_hba.conf configuration with something like this (adjust for your version): /usr/pgsql-12/bin/pg_ctl reload  Test the login with new clickhouse_user:  psql -U clickhouse_user -W -d db1 -h &lt;your_postgresql_host&gt;  "},{"title":"2. In ClickHouse​","type":1,"pageTitle":"Connecting ClickHouse to PostgreSQL using the MaterializedPostgreSQL database engine","url":"docs/integrations/postgresql/postgres-with-clickhouse-database-engine#2-in-clickhouse","content":"log into the ClickHouse CLI clickhouse-client --user default --password ClickHouse123!  Enable the PosgreSQL experimental feature for the database engine: SET allow_experimental_database_materialized_postgresql=1  Create the new database to be replicated and define the intitial table: CREATE DATABASE db1_postgres ENGINE = MaterializedPostgreSQL('postgres-host.domain.com:5432', 'db1', 'clickhouse_user', 'ClickHouse_123') SETTINGS materialized_postgresql_tables_list = 'table1';  minimum options: parameter\tDescription\texamplehost:port\thostname or IP and port\tpostgres-host.domain.com:5432 database\tPostgreSQL database name\tdb1 user\tusername to connect to postgres\tclickhouse_user password\tpassword to connect to postgres\tClickHouse_123 settings\tadditional settings for the engine\tmaterialized_postgresql_tables_list = 'table1' For complete guide to the PostgreSQL database engine, refer to: https://clickhouse.com/docs/en/engines/database-engines/materialized-postgresql/#settings Verify initial table has data ch_env_2 :) select * from db1_postgres.table1; SELECT * FROM db1_postgres.table1 Query id: df2381ac-4e30-4535-b22e-8be3894aaafc ┌─id─┬─column1─┐ │ 1 │ abc │ └────┴─────────┘ ┌─id─┬─column1─┐ │ 2 │ def │ └────┴─────────┘  "},{"title":"3. Test basic replication​","type":1,"pageTitle":"Connecting ClickHouse to PostgreSQL using the MaterializedPostgreSQL database engine","url":"docs/integrations/postgresql/postgres-with-clickhouse-database-engine#3-test-basic-replication","content":"In PostgreSQL, add new rows: INSERT INTO table1 (id, column1) VALUES (3, 'ghi'), (4, 'jkl');  In ClickHouse, verify new rows are visible: ch_env_2 :) select * from db1_postgres.table1; SELECT * FROM db1_postgres.table1 Query id: b0729816-3917-44d3-8d1a-fed912fb59ce ┌─id─┬─column1─┐ │ 1 │ abc │ └────┴─────────┘ ┌─id─┬─column1─┐ │ 4 │ jkl │ └────┴─────────┘ ┌─id─┬─column1─┐ │ 3 │ ghi │ └────┴─────────┘ ┌─id─┬─column1─┐ │ 2 │ def │ └────┴─────────┘  "},{"title":"4. Summary​","type":1,"pageTitle":"Connecting ClickHouse to PostgreSQL using the MaterializedPostgreSQL database engine","url":"docs/integrations/postgresql/postgres-with-clickhouse-database-engine#4-summary","content":"This integration guide focused on a simple example on how to replicate a database with a table, however, there exist more advanced options which include replicating the whole database or adding new tables and schemas to the existing replications. Although DDL commands are not supported for this replication, the engine can be set to detect changes and reload the tables when there are structural changes made. For more features available for advanced options, please see the reference documenation:https://clickhouse.com/docs/en/engines/database-engines/materialized-postgresql/ "},{"title":"S3 Backed MergeTree","type":0,"sectionRef":"#","url":"docs/integrations/s3/s3-merge-tree","content":"","keywords":""},{"title":"Storage Tiers​","type":1,"pageTitle":"S3 Backed MergeTree","url":"docs/integrations/s3/s3-merge-tree#storage-tiers","content":"ClickHouse storage volumes allow physical disks to be abstracted from the MergeTree table engine. Any single volume can be composed of an ordered set of disks. Whilst principally allowing multiple block devices to be potentially used for data storage, this abstraction also allows other storage types, including S3. ClickHouse data parts can be moved between volumes and fill rates according to storage policies, thus creating the concept of storage tiers. Storage tiers unlock hot-cold architectures where the most recent data, which is typically also the most queried, requires only a small amount of space on high-performing storage, e.g., NVMe SSDs. As the data ages, SLAs for query times increase, as does query frequency. This fat tail of data can be stored on slower, less performant storage such as HDD or object storage such as S3. "},{"title":"Creating a Disk​","type":1,"pageTitle":"S3 Backed MergeTree","url":"docs/integrations/s3/s3-merge-tree#creating-a-disk","content":"To utilize an S3 bucket as a disk, we must first declare it within the ClickHouse configuration file. Either extend config.xml or preferably provide a new file under conf.d. An example of an S3 disk declaration is shown below: &lt;clickhouse&gt; &lt;storage_configuration&gt; ... &lt;disks&gt; &lt;s3&gt; &lt;type&gt;s3&lt;/type&gt; &lt;endpoint&gt;https://sample-bucket.s3.us-east-2.amazonaws.com/tables/&lt;/endpoint&gt; &lt;access_key_id&gt;your_access_key_id&lt;/access_key_id&gt; &lt;secret_access_key&gt;your_secret_access_key&lt;/secret_access_key&gt; &lt;region&gt;&lt;/region&gt; &lt;metadata_path&gt;/var/lib/clickhouse/disks/s3/&lt;/metadata_path&gt; &lt;cache_enabled&gt;true&lt;/cache_enabled&gt; &lt;data_cache_enabled&gt;true&lt;/data_cache_enabled&gt; &lt;cache_path&gt;/var/lib/clickhouse/disks/s3/cache/&lt;/cache_path&gt; &lt;/s3&gt; &lt;/disks&gt; ... &lt;/storage_configuration&gt; &lt;/clickhouse&gt;  A complete list of settings relevant to this disk declaration can be found here. Note that credentials can be managed here using the same approaches described in Managing credentials, i.e., the use_environment_credentials can be set to true in the above settings block to use IAM roles. Further information on the cache settings can be found under Internals. "},{"title":"Creating a Storage Policy​","type":1,"pageTitle":"S3 Backed MergeTree","url":"docs/integrations/s3/s3-merge-tree#creating-a-storage-policy","content":"Once configured, this “disk” can be used by a storage volume declared within a policy. For the example below, we assume s3 is our only storage. This ignores more complex hot-cold architectures where data can be relocated based on TTLs and fill rates. &lt;clickhouse&gt; &lt;storage_configuration&gt; &lt;disks&gt; &lt;s3&gt; ... &lt;/s3&gt; &lt;/disks&gt; &lt;policies&gt; &lt;s3_main&gt; &lt;volumes&gt; &lt;main&gt; &lt;disk&gt;s3&lt;/disk&gt; &lt;/main&gt; &lt;/volumes&gt; &lt;/s3_main&gt; &lt;/policies&gt; &lt;/storage_configuration&gt; &lt;/clickhouse&gt;  "},{"title":"Creating a table​","type":1,"pageTitle":"S3 Backed MergeTree","url":"docs/integrations/s3/s3-merge-tree#creating-a-table","content":"Assuming you have configured your disk to use a bucket with write access, you should be able to create a table such as in the example below. For purposes of brevity, we use a subset of the NYC taxi columns and stream data directly to the s3 backed table: CREATE TABLE trips_s3 ( `trip_id` UInt32, `pickup_date` Date, `pickup_datetime` DateTime, `dropoff_datetime` DateTime, `pickup_longitude` Float64, `pickup_latitude` Float64, `dropoff_longitude` Float64, `dropoff_latitude` Float64, `passenger_count` UInt8, `trip_distance` Float64, `tip_amount` Float32, `total_amount` Float32, `payment_type` Enum8('UNK' = 0, 'CSH' = 1, 'CRE' = 2, 'NOC' = 3, 'DIS' = 4) ) ENGINE = MergeTree PARTITION BY toYYYYMM(pickup_date) ORDER BY pickup_datetime SETTINGS index_granularity = 8192, storage_policy='s3_main'  INSERT INTO trips_s3 SELECT trip_id, pickup_date, pickup_datetime, dropoff_datetime, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, passenger_count, trip_distance, tip_amount, total_amount, payment_type FROM s3('https://ch-nyc-taxi.s3.eu-west-3.amazonaws.com/tsv/trips_{0..9}.tsv.gz', 'TabSeparatedWithNames') LIMIT 1000000;  Depending on the hardware, this latter insert of 1m rows may take a few minutes to execute. You can confirm the progress via the system.processes table. Feel free to adjust the row count up to the limit of 10m and explore some sample queries. SELECT passenger_count, avg(tip_amount) as avg_tip, avg(total_amount) as avg_amount FROM trips_s3 GROUP BY passenger_count;  "},{"title":"Modifying a Table​","type":1,"pageTitle":"S3 Backed MergeTree","url":"docs/integrations/s3/s3-merge-tree#modifying-a-table","content":"Occasionally users may need to modify the storage policy of a specific table. Whilst this is possible, it comes with limitations. The new target policy must contain all of the disks and volumes of the previous policy, i.e., data will not be migrated to satisfy a policy change. When validating these constraints, volumes and disks will be identified by their name, with attempts to violate resulting in an error. However, assuming you use the previous examples, the following changes are valid. &lt;policies&gt; &lt;s3_main&gt; &lt;volumes&gt; &lt;main&gt; &lt;disk&gt;s3&lt;/disk&gt; &lt;/main&gt; &lt;/volumes&gt; &lt;/s3_main&gt; &lt;s3_tiered&gt; &lt;volumes&gt; &lt;hot&gt; &lt;disk&gt;default&lt;/disk&gt; &lt;/hot&gt; &lt;main&gt; &lt;disk&gt;s3&lt;/disk&gt; &lt;/main&gt; &lt;/volumes&gt; &lt;move_factor&gt;0.2&lt;/move_factor&gt; &lt;/s3_tiered&gt; &lt;/policies&gt;  ALTER TABLE trips_s3 MODIFY SETTING storage_policy='s3_tiered'  Here we reuse the main volume in our new s3_tiered policy and introduce a new hot volume. This uses the default disk, which consists of only one disk configured via the parameter &lt;path&gt;. Note that our volume names and disks do not change. New inserts to our table will reside on the default disk until this reaches move_factor * disk_size - at which data will be relocated to S3. "},{"title":"Handling Replication​","type":1,"pageTitle":"S3 Backed MergeTree","url":"docs/integrations/s3/s3-merge-tree#handling-replication","content":"For traditional disk-backed tables, we rely on ClickHouse to handle data replication via the ReplicatedTableEngine. Whilst for S3, this replication is inherently handled at the storage layer, local files are still held for the table on disk. Specifically, ClickHouse stores metadata data files on disk (see Internals) for further details. These files will be replicated if using a ReplicatedMergeTree in a process known as Zero Copy Replication. This is enabled by default through the setting allow_remote_fs_zero_copy_replication. This is best illustrated below where the table exists on 2 ClickHouse nodes:  "},{"title":"Internals​","type":1,"pageTitle":"S3 Backed MergeTree","url":"docs/integrations/s3/s3-merge-tree#internals","content":""},{"title":"Read & Writes​","type":1,"pageTitle":"S3 Backed MergeTree","url":"docs/integrations/s3/s3-merge-tree#read--writes","content":"The following notes cover the implementation of S3 interactions with ClickHouse. Whilst generally only informative, it may help the readers when Optimizing for Performance: By default, the maximum number of query processing threads used by any stage of the query processing pipeline is equal to the number of cores. Some stages are more parallelizable than others, so this value provides an upper bound. Multiple query stages may execute at once since data is streamed from the disk. The exact number of threads used for a query may thus exceed this. Modify through the setting max_threads.Reads on S3 are asynchronous by default. This behavior is determined by setting remote_filesystem_read_method, set to the value “threadpool” by default. When serving a request, ClickHouse reads granules in stripes. Each of these stripes potentially contain many columns. A thread will read the columns for their granules one by one. Rather than doing this synchronously, a prefetch is made for all columns before waiting for the data. This offers significant performance improvements over synchronous waits on each column. Users will not need to change this setting in most cases - see Optimizing for Performance.Writes are performed in parallel, with a maximum of 100 concurrent file writing threads. max_insert_delayed_streams_for_parallel_write, which has a default value of 1000, controls the number of S3 blobs written in parallel. Since a buffer is required for each file being written (~1MB), this effectively limits the memory consumption of an INSERT. It may be appropriate to lower this value in low server memory scenarios. For further information on tuning threads, see Optimizing for Performance. Important: as of 22.3.1, there are two settings to enable the cache. data_cache_enabled and cache_enabled. The former enables the new cache, which supports the eviction of index files. The latter setting enables a legacy cache. As of 22.3.1, we recommend enabling both settings. To accelerate reads, S3 files are cached on the local filesystem by breaking files into segments. Any contiguous read segments are saved in the cache, with overlapping segments reused. Writes resulting from INSERTs or merges can also optionally be stored in the cache. Where possible, the cache is reused for file reads. ClickHouse’s linear reads lend themselves to this caching strategy. Should a contiguous read result in a cache miss, the segment is downloaded and cached. Eviction occurs on an LRU basis per segment. The removal of a file also causes its removal from the cache. The metadata for the cache (entries and last used time) is held in memory for fast access. On restarts of ClickHouse, this metadata is reconstructed from the files on disk with the loss of the last used time. In this case, the value is set to 0, causing random eviction until the values are fully populated. The max cache size can be specified in bytes through the setting max_cache_size. This defaults to 1GB (subject to change). Index and mark files can be evicted from the cache. The FS page cache can efficiently cache all files. Finally, merges on data residing in S3 are potentially a performant bottleneck if not performed intelligently. Cached versions of files minimize merges performed directly on the remote storage. By default, this is enabled and can be turned off by setting read_from_cache_if_exists_orthersize_bypass_cache to 0. "},{"title":"S3 Table Engines","type":0,"sectionRef":"#","url":"docs/integrations/s3/s3-table-engine","content":"","keywords":""},{"title":"Reading Data​","type":1,"pageTitle":"S3 Table Engines","url":"docs/integrations/s3/s3-table-engine#reading-data","content":"In the following example, we create a table trips_raw using the first ten tsv files located within the nyc-taxi bucket. Each of these contains 1m rows each. CREATE TABLE trips_raw ( `trip_id` UInt32, `vendor_id` Enum8('1' = 1, '2' = 2, '3' = 3, '4' = 4, 'CMT' = 5, 'VTS' = 6, 'DDS' = 7, 'B02512' = 10, 'B02598' = 11, 'B02617' = 12, 'B02682' = 13, 'B02764' = 14, '' = 15), `pickup_date` Date, `pickup_datetime` DateTime, `dropoff_date` Date, `dropoff_datetime` DateTime, `store_and_fwd_flag` UInt8, `rate_code_id` UInt8, `pickup_longitude` Float64, `pickup_latitude` Float64, `dropoff_longitude` Float64, `dropoff_latitude` Float64, `passenger_count` UInt8, `trip_distance` Float64, `fare_amount` Float32, `extra` Float32, `mta_tax` Float32, `tip_amount` Float32, `tolls_amount` Float32, `ehail_fee` Float32, `improvement_surcharge` Float32, `total_amount` Float32, `payment_type_` Enum8('UNK' = 0, 'CSH' = 1, 'CRE' = 2, 'NOC' = 3, 'DIS' = 4), `trip_type` UInt8, `pickup` FixedString(25), `dropoff` FixedString(25), `cab_type` Enum8('yellow' = 1, 'green' = 2, 'uber' = 3), `pickup_nyct2010_gid` Int8, `pickup_ctlabel` Float32, `pickup_borocode` Int8, `pickup_ct2010` String, `pickup_boroct2010` FixedString(7), `pickup_cdeligibil` String, `pickup_ntacode` FixedString(4), `pickup_ntaname` String, `pickup_puma` UInt16, `dropoff_nyct2010_gid` UInt8, `dropoff_ctlabel` Float32, `dropoff_borocode` UInt8, `dropoff_ct2010` String, `dropoff_boroct2010` FixedString(7), `dropoff_cdeligibil` String, `dropoff_ntacode` FixedString(4), `dropoff_ntaname` String, `dropoff_puma` UInt16 ) ENGINE = S3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_{0..9}.tsv.gz', 'TabSeparatedWithNames', 'gzip');  Notice the use of the {0..9} pattern to limit to the first ten files. Once created, we can query this table like any other e.g. _path\t_file\ttrip_id\tpickup_date\ttotal_amountdatasets-documentation/nyc-taxi/trips_0.gz\ttrips_0.gz\t1199999902\t2015-07-07\t19.56 datasets-documentation/nyc-taxi/trips_0.gz\ttrips_0.gz\t1199999919\t2015-07-07\t10.3 datasets-documentation/nyc-taxi/trips_0.gz\ttrips_0.gz\t1199999944\t2015-07-07\t24.3 datasets-documentation/nyc-taxi/trips_0.gz\ttrips_0.gz\t1199999969\t2015-07-07\t9.95 datasets-documentation/nyc-taxi/trips_0.gz\ttrips_0.gz\t1199999990\t2015-07-08\t9.8 SELECT payment_type, max(tip_amount) as max_tip FROM trips_raw GROUP BY payment_type;  payment_type\tmax_tipUNK\t0 CSH\t800 CRE\t53.06 NOC\t100 DIS\t100 "},{"title":"Inserting Data​","type":1,"pageTitle":"S3 Table Engines","url":"docs/integrations/s3/s3-table-engine#inserting-data","content":"Whilst this table engine supports parallel reads, writes are only supported if the table definition does not contain glob patterns. The above table, therefore, would block writes. To illustrate writes, create the following table: CREATE TABLE trips_dest ( `trip_id` UInt32, `pickup_date` Date, `pickup_datetime` DateTime, `dropoff_datetime` DateTime, `tip_amount` Float32, `total_amount` Float32 ) ENGINE = S3('&lt;bucket path&gt;/trips.bin', 'Native');  This query requires write access to the bucket INSERT INTO trips_dest SELECT trip_id, pickup_date, pickup_datetime, dropoff_datetime, tip_amount, total_amount FROM trips LIMIT 10;  SELECT * FROM trips_dest LIMIT 5;  trip_id\tpickup_date\tpickup_datetime\tdropoff_datetime\ttip_amount\ttotal_amount14\t2013-08-02\t2013-08-02 09:43:58\t2013-08-02 09:44:13\t0\t2 15\t2013-08-02\t2013-08-02 09:44:43\t2013-08-02 09:45:15\t0\t2 21\t2013-08-02\t2013-08-02 11:30:00\t2013-08-02 17:08:00\t0\t172 21\t2013-08-02\t2013-08-02 12:30:00\t2013-08-02 18:08:00\t0\t172 23\t2013-08-02\t2013-08-02 18:00:50\t2013-08-02 18:01:55\t0\t6.5 Note that rows can only be inserted into new files. There are no merge cycles or file split operations. Once a file is written, subsequent inserts will fail. Users have two options here: Specify the setting s3_create_new_file_on_insert=1. This will cause the creation of new files on each insert. A numeric suffix will be appended to the end of each file that will monotonically increase for each insert operation. For the above example, a subsequent insert would cause the creation of a trips_1.bin file.Specify the setting s3_truncate_on_insert=1. This will cause a truncation of the file, i.e. it will only contain the newly inserted rows once complete. Both of these settings default to 0 - thus forcing the user to set one of them. s3_truncate_on_insert will take precedence if both are set. "},{"title":"Miscellaneous​","type":1,"pageTitle":"S3 Table Engines","url":"docs/integrations/s3/s3-table-engine#miscellaneous","content":"Unlike a traditional merge tree family table, dropping an s3 table will not delete the underlying data. Full settings for this table type can be found here. Be aware of the following caveats when using this engine: ALTER queries are not supportedSAMPLE operations are not supportedThere is no notion of indexes, i.e. primary or skip. "},{"title":"Managing Credentials​","type":1,"pageTitle":"S3 Table Engines","url":"docs/integrations/s3/s3-table-engine#managing-credentials","content":"In the previous examples, we have passed credentials in the s3 function or table definition. Whilst this may be acceptable for occasional usage, users require less explicit authentication mechanisms in production. To address this, ClickHouse has several options: In the previous examples, we have passed credentials in the s3 function or table definition. Whilst this may be acceptable for occasional usage, users require less explicit authentication mechanisms in production. To address this, ClickHouse has several options: Specify the connection details in the config.xml or an equivalent configuration file under conf.d. The contents of an example file are shown below, assuming installation using the debian package. ubuntu@single-node-clickhouse:/etc/clickhouse-server/config.d$ cat s3.xml &lt;clickhouse&gt; &lt;s3&gt; &lt;endpoint-name&gt; &lt;endpoint&gt;https://dalem-files.s3.amazonaws.com/test/&lt;/endpoint&gt; &lt;access_key_id&gt;key&lt;/access_key_id&gt; &lt;secret_access_key&gt;secret&lt;/secret_access_key&gt; &lt;!-- &lt;use_environment_credentials&gt;false&lt;/use_environment_credentials&gt; --&gt; &lt;!-- &lt;header&gt;Authorization: Bearer SOME-TOKEN&lt;/header&gt; --&gt; &lt;/endpoint-name&gt; &lt;/s3&gt; &lt;/clickhouse&gt; These credentials will be used for any requests where the endpoint above is an exact prefix match for the requested URL. Also, note the ability in this example to declare an authorization header as an alternative to access and secret keys. A complete list of supported settings can be found here. The example above highlights the availability of the configuration parameter use_environment_credentials. This configuration parameter can also be set globally at the s3 level i.e. &lt;clickhouse&gt; &lt;s3&gt; &lt;use_environment_credentials&gt;true&lt;/use_environment_credentials&gt; &lt;s3&gt; &lt;/clickhouse&gt; This setting turns on an attempt to retrieve s3 credentials from the environment, thus allowing access through IAM roles. Specifically, the following order of retrieval is performed: * A lookup for the environment variables AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN * Check performed in $HOME/.aws * Temporary credentials obtained via the AWS Security Token Service - i.e. vi[a AssumeRole](https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html) API * Checks for credentials in the ECS environment variables AWS_CONTAINER_CREDENTIALS_RELATIVE_URI or AWS_CONTAINER_CREDENTIALS_FULL_URI and AWS_ECS_CONTAINER_AUTHORIZATION_TOKEN. * Obtains the credentials via [Amazon EC2 instance metadata](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-metadata.html) provided [AWS_EC2_METADATA_DISABLED](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html#envvars-list-AWS_EC2_METADATA_DISABLED) is not set to true.  These same settings can also be set for a specific endpoint, using the same prefix matching rule. "},{"title":"Optimizing for Performance","type":0,"sectionRef":"#","url":"docs/integrations/s3/s3-optimizing-performance","content":"","keywords":""},{"title":"Measuring Performance​","type":1,"pageTitle":"Optimizing for Performance","url":"docs/integrations/s3/s3-optimizing-performance#measuring-performance","content":"Before making any changes to improve performance, ensure you measure appropriately. As S3 API calls are sensitive to latency and may impact client timings, use the query log for performance metrics, i.e., system.query_log. For further details on how to analyze query performance, see here. If measuring the performance of SELECT queries, where large volumes of data are returned to the client, either utilize the null format for queries or direct results to the Null engine. This should avoid the client being overwhelmed with data and network saturation. "},{"title":"Region Locality​","type":1,"pageTitle":"Optimizing for Performance","url":"docs/integrations/s3/s3-optimizing-performance#region-locality","content":"Ensure your buckets are located in the same region as your ClickHouse instances. This simple optimization can dramatically improve throughput performance, especially if you deploy your ClickHouse instances on AWS infrastructure. "},{"title":"Using Threads​","type":1,"pageTitle":"Optimizing for Performance","url":"docs/integrations/s3/s3-optimizing-performance#using-threads","content":"Read performance on s3 will scale linearly with the number of cores, provided you are not limited by network bandwidth or local IO. Increasing the number of threads also has memory overhead permutations that users should be aware of. The following can be modified to improve throughput performance potentially: Usually, the default value of max_threads is sufficient, i.e., the number of cores. If the amount of memory used for a query is high, and this needs to be reduced, or the LIMIT on results is low, this value can be set lower. Users with plenty of memory may wish to experiment with increasing this value for possible higher read throughput from s3. Typically this is only beneficial on machines with lower core counts, i.e., &lt; 10. The benefit from further parallelization typically diminishes as other resources act as a bottleneck, e.g., network.If performing an INSERT INTO x SELECT request, note that the number of threads will be set to 1 as dictated by the setting max_insert_threads. Provided max_threads is greater than 1 (confirm with SELECT * FROM system.settings WHERE name='max_threads'), increasing this will improve insert performance at the expense of memory. Increase with caution due to memory consumption overheads. This value should not be as high as the max_threads as resources are consumed on background merges. Furthermore, not all target engines (MergeTree does) support parallel inserts. Finally, parallel inserts invariably cause more parts, slowing down subsequent reads. Increase with caution.For low memory scenarios, consider lowering max_insert_delayed_streams_for_parallel_write if inserting into s3.Versions of ClickHouse before 22.3.1 only parallelized reads across multiple files when using the s3 function or s3 table engine. This required the user to ensure files were split into chunks on s3 and read using a glob pattern to achieve optimal read performance. Later versions now parallelize downloads within a file. Assuming sufficient memory (test!), increasing min_insert_block_size_rows can improve insert throughput.In low thread count scenarios, users may benefit from setting remote_filesystem_read_method to &quot;read&quot; to cause the synchronous reading of files from s3. "},{"title":"Formats​","type":1,"pageTitle":"Optimizing for Performance","url":"docs/integrations/s3/s3-optimizing-performance#formats","content":"ClickHouse can read files stored in s3 buckets in the supported formats using the s3 function and s3 engine. If reading raw files, some of these formats have distinct advantages: Formats with encoded column names such as Native, Parquet, CSVWithNames, and TabSeparatedWithNames will be less verbose to query since the user will not be required to specify the column name is the s3 function. The column names allow this information to be inferred.Formats will differ in performance with respect to read and write throughputs. Native and parquet represent the most optimal formats for read performance since they are already column orientated and more compact. The native format additionally benefits from alignment with how ClickHouse stores data in memory - thus reducing processing overhead as data is streamed into ClickHouse.The block size will often impact the latency of reads on large files. This is very apparent if you only sample the data, e.g., returning the top N rows. In the case of formats such as CSV and TSV, files must be parsed to return a set of rows. Formats such as Native and Parquet will allow faster sampling as a result.Each compression format brings pros and cons, often balancing the compression level for speed and biasing compression or decompression performance. If compressing raw files such as CSV or TSV, lz4 offers the fastest decompression performance, sacrificing the compression level. Gzip typically compresses better at the expense of slightly slower read speeds. Xz takes this further by usually offering the best compression with the slowest compression and decompression performance. If exporting, Gz and lz4 offer comparable compression speeds. Balance this against your connection speeds. Any gains from faster decompression or compression will be easily negated by a slower connection to your s3 buckets.Formats such as native or parquet do not typically justify the overhead of compression. Any savings in data size are likely to be minimal since these formats are inherently compact. The time spent compressing and decompressing will rarely offset network transfer times - especially since s3 is globally available with higher network bandwidth. Internally the ClickHouse merge tree uses two primary storage formats: Wide and Compact. Whilst the current implementation uses the default behavior of ClickHouse - controlled through the settings min_bytes_for_wide_part and min_rows_for_wide_part; we expect behavior to diverge for s3 in the future releases, e.g., a larger default value of min_bytes_for_wide_part encouraging a more Compact format and thus fewer files. Users may now wish to tune these settings when using exclusively s3 storage. "},{"title":"Scaling with Nodes​","type":1,"pageTitle":"Optimizing for Performance","url":"docs/integrations/s3/s3-optimizing-performance#scaling-with-nodes","content":"Users will have often have more than one node of ClickHouse available. While users can scale vertically, improving s3 throughput linearly with the number of cores, horizontal scaling is often necessary due to hardware availability and cost-efficiency. The replication of an s3 backed Merge Tree is supported through zero copy replication. Utilizing a cluster for s3 reads requires using the s3Cluster function as described in Utilizing Clusters. While this allows reads to be distributed across nodes, thread settings will not currently be sent to all nodes as of 22.3.1. For example, if the following query was executed against a node, only the receiving initiator node will respect the max_insert_threads setting. INSERT INTO default.trips_all SELECT * FROM s3Cluster('events', 'https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_*.gz', 'TabSeparatedWithNames') SETTINGS max_insert_threads=8;  To ensure this setting is used, the following would need to be added to each nodes config.xml file (or under conf.d): &lt;clickhouse&gt; &lt;profiles&gt; &lt;default&gt; &lt;max_insert_threads&gt;8&lt;/max_insert_threads&gt; &lt;/default&gt; &lt;/profiles&gt; &lt;/clickhouse&gt;  "},{"title":"Integrating Vector with ClickHouse","type":0,"sectionRef":"#","url":"docs/integrations/vector-to-clickhouse","content":"","keywords":""},{"title":"1. Create a database and table​","type":1,"pageTitle":"Integrating Vector with ClickHouse","url":"docs/integrations/vector-to-clickhouse#1-create-a-database-and-table","content":"Let's define a table to store the log events: We will start with a new database named nginxdb: CREATE DATABASE IF NOT EXISTS nginxdb For starters, we are just going to insert the entire log event as a single string. Obviously this is not a great format for performing analytics on the log data, but we will figure that part out below using materialized views. CREATE TABLE IF NOT EXISTS nginxdb.access_logs ( message String ) ENGINE = MergeTree() ORDER BY tuple() note There is not really a need for a primary key yet, so that is why ORDER BY is set to tuple(). "},{"title":"2. Configure Nginx​","type":1,"pageTitle":"Integrating Vector with ClickHouse","url":"docs/integrations/vector-to-clickhouse#2--configure-nginx","content":"We certainly do not want to spend too much time explaining Nginx, but we also do not want to hide all the details, so in this step we will provide you with enough details to get Nginx logging configured. The following access_log property sends logs to /var/log/nginx/my_access.log in the combined format. This value goes in the http section of your nginx.conf file: http { include /etc/nginx/mime.types; default_type application/octet-stream; access_log /var/log/nginx/my_access.log combined; sendfile on; keepalive_timeout 65; include /etc/nginx/conf.d/*.conf; } Be sure to restart Nginx if you had to modify nginx.conf. Generate some log events in the access log by visiting pages on your web server. Logs in the combined format have the following format: 192.168.208.1 - - [12/Oct/2021:03:31:44 +0000] &quot;GET / HTTP/1.1&quot; 200 615 &quot;-&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36&quot; 192.168.208.1 - - [12/Oct/2021:03:31:44 +0000] &quot;GET /favicon.ico HTTP/1.1&quot; 404 555 &quot;http://localhost/&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36&quot; 192.168.208.1 - - [12/Oct/2021:03:31:49 +0000] &quot;GET / HTTP/1.1&quot; 304 0 &quot;-&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36&quot;  "},{"title":"3. Configure Vector​","type":1,"pageTitle":"Integrating Vector with ClickHouse","url":"docs/integrations/vector-to-clickhouse#3-configure-vector","content":"Vector collects, transforms and routes logs, metrics, and traces (referred to as sources) to lots of different vendors (referred to as sinks), including out-of-the-box compatibility with ClickHouse. Sources and sinks are defined in a configuration file named vector.toml. The following vector.toml defines a source of type file that tails the end of my_access.log, and it also defines a sink as the access_logs table defined above: [sources.nginx_logs] type = &quot;file&quot; include = [ &quot;/var/log/nginx/my_access.log&quot; ] read_from = &quot;end&quot; [sinks.clickhouse] type = &quot;clickhouse&quot; inputs = [&quot;nginx_logs&quot;] endpoint = &quot;http://clickhouse-server:8123&quot; database = &quot;nginxdb&quot; table = &quot;access_logs&quot; skip_unknown_fields = true Start up Vector using the configuration above. Visit the Vector documentation for more details on defining sources and sinks. Verify the access logs are being inserted into ClickHouse. Run the following query and you should see the access logs in your table: SELECT * FROM nginxdb.access_logs  "},{"title":"4. Parse the Logs​","type":1,"pageTitle":"Integrating Vector with ClickHouse","url":"docs/integrations/vector-to-clickhouse#4-parse-the-logs","content":"Having the logs in ClickHouse is great, but storing each event as a single string does not allow for much data analysis. Let's see how to parse the log events using a materialized view. A materialized view (MV, for short) is a new table based on an existing table, and when inserts are made to the existing table, the new data is also added to the materialized view. Let's see how to define a MV that contains a parsed representation of the log events in access_logs, in other words: 192.168.208.1 - - [12/Oct/2021:15:32:43 +0000] &quot;GET / HTTP/1.1&quot; 304 0 &quot;-&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36&quot; There are various functions in ClickHouse to parse the string, but for starters let's take a look at splitByWhitespace - which parses a string by whitespace and returns each token in an array. To demonstrate, run the following command: SELECT splitByWhitespace('192.168.208.1 - - [12/Oct/2021:15:32:43 +0000] &quot;GET / HTTP/1.1&quot; 304 0 &quot;-&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36&quot;') Notice the response is pretty close to what we want! A few of the strings have some extra characters, and the user agent (the browser details) did not need to be parsed, but we will resolve that in the next step: [&quot;192.168.208.1&quot;,&quot;-&quot;,&quot;-&quot;,&quot;[12/Oct/2021:15:32:43&quot;,&quot;+0000]&quot;,&quot;\\&quot;GET&quot;,&quot;/&quot;,&quot;HTTP/1.1\\&quot;&quot;,&quot;304&quot;,&quot;0&quot;,&quot;\\&quot;-\\&quot;&quot;,&quot;\\&quot;Mozilla/5.0&quot;,&quot;(Macintosh;&quot;,&quot;Intel&quot;,&quot;Mac&quot;,&quot;OS&quot;,&quot;X&quot;,&quot;10_15_7)&quot;,&quot;AppleWebKit/537.36&quot;,&quot;(KHTML,&quot;,&quot;like&quot;,&quot;Gecko)&quot;,&quot;Chrome/93.0.4577.63&quot;,&quot;Safari/537.36\\&quot;&quot;] Similar to splitByWhitespace, the splitByRegexp function splits a string into an array based on a regular expression. Run the following command, which returns two strings. SELECT splitByRegexp('\\S \\d+ &quot;([^&quot;]*)&quot;', '192.168.208.1 - - [12/Oct/2021:15:32:43 +0000] &quot;GET / HTTP/1.1&quot; 304 0 &quot;-&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36&quot;') Notice the second string returned is the user agent successfully parsed from the log: [&quot;192.168.208.1 - - [12/Oct/2021:15:32:43 +0000] \\&quot;GET / HTTP/1.1\\&quot; 30&quot;,&quot; \\&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\\&quot;&quot;] Before looking at the final CREATE MATERIALIZED VIEW command, let's view a couple more functions used to cleanup the data. For example, the RequestMethod looks like &quot;GET with an unwanted double-quote. Run the following trim function, which removes the double quote: SELECT trim(LEADING '&quot;' FROM '&quot;GET') The time string has a leading square bracket, and also is not in a format that ClickHouse can parse into a date. However, if we change the separater from a colon (:) to a comma (,) then the parsing works great: SELECT parseDateTimeBestEffort(replaceOne(trim(LEADING '[' FROM '[12/Oct/2021:15:32:43'), ':', ' ')) We are now ready to define our materialized view. Our definition includes POPULATE, which means the existing rows in access_logs will be processed and inserted right away. Run the following SQL statement: CREATE MATERIALIZED VIEW nginxdb.access_logs_view ( RemoteAddr String, Client String, RemoteUser String, TimeLocal DateTime, RequestMethod String, Request String, HttpVersion String, Status Int32, BytesSent Int64, UserAgent String ) ENGINE = MergeTree() ORDER BY RemoteAddr POPULATE AS WITH splitByWhitespace(message) as split, splitByRegexp('\\S \\d+ &quot;([^&quot;]*)&quot;', message) as referer SELECT split[1] AS RemoteAddr, split[2] AS Client, split[3] AS RemoteUser, parseDateTimeBestEffort(replaceOne(trim(LEADING '[' FROM split[4]), ':', ' ')) AS TimeLocal, trim(LEADING '&quot;' FROM split[6]) AS RequestMethod, split[7] AS Request, trim(TRAILING '&quot;' FROM split[8]) AS HttpVersion, split[9] AS Status, split[10] AS BytesSent, trim(BOTH '&quot;' from referer[2]) AS UserAgent FROM (SELECT message FROM nginxdb.access_logs) Now verify it worked. You should see the access logs nicely parsed into columns: SELECT * FROM nginxdb.access_logs_view note The lesson above stored the data in two tables, but you could change the initial nginxdb.access_logs table to use the Null table engine - the parsed data will still end up in the nginxdb.access_logs_view table, but the raw data will not be stored in a table. Summary: By using Vector, which only required a simple install and quick configuration, we can send logs from an Nginx server to a table in ClickHouse. By using a clever materialized view, we can parse those logs into columns for easier analytics. "},{"title":"Replicate a MySQL Database in ClickHouse","type":0,"sectionRef":"#","url":"docs/integrations/mysql/mysql-with-clickhouse-database-engine","content":"","keywords":"clickhouse mysql connect integrate replicate database MaterializedMySQL"},{"title":"1. Configure MySQL​","type":1,"pageTitle":"Replicate a MySQL Database in ClickHouse","url":"docs/integrations/mysql/mysql-with-clickhouse-database-engine#1-configure-mysql","content":"Configure the MySQL database to allow for replication and native authentication. ClickHouse only works with native password authentication. Add the following entries to /etc/my.cnf: default-authentication-plugin = mysql_native_password gtid-mode = ON enforce-gtid-consistency = ON Create a user to connect from ClickHouse: CREATE USER clickhouse_user IDENTIFIED BY 'ClickHouse_123'; Grant the needed permissions to the new user. For demonstration purposes, full admin rights have been granted here: GRANT ALL PRIVILEGES ON *.* TO 'clickhouse_user'@'%'; note The minimal permissions needed for the MySQL user are RELOAD, REPLICATION SLAVE, REPLICATION CLIENT and SELECT PRIVILEGE. Create a database in MySQL: CREATE DATABASE db1; Create a table: CREATE TABLE db1.table_1 ( id INT, column1 VARCHAR(10), PRIMARY KEY (`id`) ) ENGINE = InnoDB; Insert a few sample rows: INSERT INTO db1.table_1 (id, column1) VALUES (1, 'abc'), (2, 'def'), (3, 'ghi');  "},{"title":"2. Configure ClickHouse​","type":1,"pageTitle":"Replicate a MySQL Database in ClickHouse","url":"docs/integrations/mysql/mysql-with-clickhouse-database-engine#2-configure-clickhouse","content":"Set parameter to allow use of experimental feature: set allow_experimental_database_materialized_mysql = 1; Create a database that uses the MaterializedMySQL database engine: CREATE DATABASE db1_mysql ENGINE = MaterializedMySQL( 'mysql-host.domain.com:3306', 'db1', 'clickhouse_user', 'ClickHouse_123' ); The minimum parameters are: parameter\tDescription\texamplehost:port\thostname or IP and port\tmysql-host.domain.com database\tmysql database name\tdb1 user\tusername to connect to mysql\tclickhouse_user password\tpassword to connect to mysql\tClickHouse_123 note View the MaterializedMySQL database engine doc page for a complete list of parameters. "},{"title":"3. Test the Integration​","type":1,"pageTitle":"Replicate a MySQL Database in ClickHouse","url":"docs/integrations/mysql/mysql-with-clickhouse-database-engine#3-test-the-integration","content":"In MySQL, insert a sample row: INSERT INTO db1.table_1 (id, column1) VALUES (4, 'jkl'); Notice the new row appears in the ClickHouse table: SELECT id, column1 FROM db1_mysql.table_1 The response looks like: Query id: d61a5840-63ca-4a3d-8fac-c93235985654 ┌─id─┬─column1─┐ │ 1 │ abc │ └────┴─────────┘ ┌─id─┬─column1─┐ │ 4 │ jkl │ └────┴─────────┘ ┌─id─┬─column1─┐ │ 2 │ def │ └────┴─────────┘ ┌─id─┬─column1─┐ │ 3 │ ghi │ └────┴─────────┘ 4 rows in set. Elapsed: 0.030 sec. Suppose the table in MySQL is modified. Let's a column to db1.table_1 in MySQL: alter table db1.table_1 add column column2 varchar(10) after column1; Now let's insert a row to the modified table: INSERT INTO db1.table_1 (id, column1, column2) VALUES (5, 'mno', 'pqr'); Notice the talbe in ClickHouse now has the new column and the new row: SELECT id, column1, column2 FROM db1_mysql.table_1 The previous rows will have NULL for column2: Query id: 2c32fd15-3c83-480b-9bfc-cba5d932d674 Connecting to localhost:9000 as user default. Connected to ClickHouse server version 22.2.2 revision 54455. ┌─id─┬─column1─┬─column2─┐ │ 3 │ ghi │ ᴺᵁᴸᴸ │ └────┴─────────┴─────────┘ ┌─id─┬─column1─┬─column2─┐ │ 2 │ def │ ᴺᵁᴸᴸ │ └────┴─────────┴─────────┘ ┌─id─┬─column1─┬─column2─┐ │ 1 │ abc │ ᴺᵁᴸᴸ │ │ 5 │ mno │ pqr │ └────┴─────────┴─────────┘ ┌─id─┬─column1─┬─column2─┐ │ 4 │ jkl │ ᴺᵁᴸᴸ │ └────┴─────────┴─────────┘ 5 rows in set. Elapsed: 0.017 sec.  "},{"title":"Summary​","type":1,"pageTitle":"Replicate a MySQL Database in ClickHouse","url":"docs/integrations/mysql/mysql-with-clickhouse-database-engine#summary","content":"That's it! The MaterializedMySQL database engine will keep the MySQL database synced on ClickHouse. There are a few details and limitations, so be sure to read the doc page for MaterializedMySQL for more details. note If you just want to move data between MySQL and ClickHouse, check out the MySQL table engine. "},{"title":"Quick Start","type":0,"sectionRef":"#","url":"docs/quick-start","content":"","keywords":"clickhouse install getting started quick start"},{"title":"1. Start ClickHouse​","type":1,"pageTitle":"Quick Start","url":"docs/quick-start#1-start-clickhouse","content":"The simplest way to download ClickHouse locally is to run the following command. If your operating system is supported, an appropriate ClickHouse binary will be downloaded and made executable: curl https://clickhouse.com/ | sh Run the following command to start the ClickHouse server: ./clickhouse server note If your OS is not supported or for other install options, view the installation details in the technical reference guide. "},{"title":"2. Connect to ClickHouse​","type":1,"pageTitle":"Quick Start","url":"docs/quick-start#2-connect-to-clickhouse","content":"The ClickHouse server listens for HTTP clients on port 8123 by default. There is a built-in UI for running SQL queries at http://127.0.0.1:8123/play (change the hostname accordingly). note The default username for accessing ClickHouse is default and there is no password defined initially for thedefault user. Notice in your Play UI that the username was populated with default and the password text field was left empty. Try running a query. For example, the following returns the names of the predefined databases: SHOW databases Click the RUN button and the response is displayed in the lower portion of the Play UI: "},{"title":"3. Create a Table​","type":1,"pageTitle":"Quick Start","url":"docs/quick-start#3-create-a-table","content":"As in most databases management systems, ClickHouse logically groups tables into databases. Use the CREATE DATABASE command to create a new database in ClickHouse: CREATE DATABASE IF NOT EXISTS helloworld Even the simplest of tables in ClickHouse must specify a table engine. The engine determines details about the table like: how and where the data is stored, which queries are supported, and whether or not the data is replicated. There are many engines to choose from, but for a simple table on a single-node ClickHouse server,MergeTree is your likely choice. Run the following command to create a table named my_first_table in the helloworld database: CREATE TABLE helloworld.my_first_table ( user_id UInt32, message String, timestamp DateTime, metric Float32 ) ENGINE = MergeTree() PRIMARY KEY (user_id, timestamp)  "},{"title":"A Brief Intro to Primary Keys​","type":1,"pageTitle":"Quick Start","url":"docs/quick-start#a-brief-intro-to-primary-keys","content":"Before you go any further, it is important to understand how primary keys work in ClickHouse (the implementation of primary keys might seem unexpected!): primary keys in ClickHouse are not unique for each row in a table The primary key of a ClickHouse table determines how the data is sorted when written to disk. Every 8,192 rows or 10MB of data (referred to as the index granularity) creates an entry in the primary key index file. This granularity concept creates a sparse index that can easily fit in memory, and the granules represent a stripe of the smallest amount of column data that gets processed during SELECT queries. The primary key can be defined using the PRIMARY KEY command. If you define a table without a PRIMARY KEY specified, then the key becomes the tuple specified in the ORDER BY clause. If you specify both a PRIMARY KEY and an ORDER BY, the primary key must be a subset of the sort order. In the example above, my_first_table is a MergeTree table with four columns: user_id: a 32-bit unsigned integermessage: a String data type, which replaces types like VARCHAR, BLOB, CLOB and others from other database systemstimestamp: a DateTime value, which represents an instant in timemetric: a 32-bit floating point number The primary key is also the sorting key, which is a tuple of (user_id, timestamp). Therefore, the data stored in each column file will be sorted by user_id, then timestamp. "},{"title":"4. Insert Data​","type":1,"pageTitle":"Quick Start","url":"docs/quick-start#4-insert-data","content":"You can use the familiar INSERT INTO TABLE command with ClickHouse, but it is important to understand that each insert into a MergeTree table causes a part to be created in storage. The best practice with ClickHouse is to insert a large number of rows per batch - tens of thousands or even millions of rows at once. (Don't worry - ClickHouse can easily handle that type of volume!) Even for a simple example, let's insert more than one row at a time: INSERT INTO helloworld.my_first_table (user_id, message, timestamp, metric) VALUES (101, 'Hello, ClickHouse!', now(), -1.0 ), (102, 'Insert a lot of rows per batch', yesterday(), 1.41421 ), (102, 'Sort your data based on your commonly-used queries', today(), 2.718 ), (101, 'Granules are the smallest chunks of data read', now() + 5, 3.14159 ) note Notice the timestamp column is populated using various Date and DateTime functions. ClickHouse has hundreds of useful functions that you can view in the Functions section of the docs. Let's verify it worked: SELECT * FROM helloworld.my_first_table You should see the four rows of data that were inserted: "},{"title":"5. The ClickHouse Client​","type":1,"pageTitle":"Quick Start","url":"docs/quick-start#5-the-clickhouse-client","content":"You can also connect to your ClickHouse server using a command-line tool named clickhouse client. Open a new terminal and change directories to where you downloaded the clickhouse binary in step 1 above, then run the following command: ./clickhouse client If you get the smiley face prompt, you are ready to run queries! :) Give it a try by running the following query: SELECT * FROM helloworld.my_first_table ORDER BY timestamp Notice the response comes back in a nice table format: SELECT * FROM helloworld.my_first_table ORDER BY timestamp ASC Query id: f7a33012-bc8c-4f0f-9641-260ee1ffe4b8 ┌─user_id─┬─message────────────────────────────────────────────┬───────────timestamp─┬──metric─┐ │ 102 │ Insert a lot of rows per batch │ 2022-03-21 00:00:00 │ 1.41421 │ │ 102 │ Sort your data based on your commonly-used queries │ 2022-03-22 00:00:00 │ 2.718 │ │ 101 │ Hello, ClickHouse! │ 2022-03-22 14:04:09 │ -1 │ │ 101 │ Granules are the smallest chunks of data read │ 2022-03-22 14:04:14 │ 3.14159 │ └─────────┴────────────────────────────────────────────────────┴─────────────────────┴─────────┘ 4 rows in set. Elapsed: 0.008 sec. Add a FORMAT clause to specify one of the many supported output formats of ClickHouse: SELECT * FROM helloworld.my_first_table ORDER BY timestamp FORMAT TabSeparated In the above query, the output is returned as tab-separated: Query id: 3604df1c-acfd-4117-9c56-f86c69721121 102 Insert a lot of rows per batch 2022-03-21 00:00:00 1.41421 102 Sort your data based on your commonly-used queries 2022-03-22 00:00:00 2.718 101 Hello, ClickHouse! 2022-03-22 14:04:09 -1 101 Granules are the smallest chunks of data read 2022-03-22 14:04:14 3.14159 4 rows in set. Elapsed: 0.005 sec. To exit the clickhouse client, enter the exit command: :) exit Bye.  "},{"title":"6. Insert a CSV file​","type":1,"pageTitle":"Quick Start","url":"docs/quick-start#6-insert-a-csv-file","content":"A common task when getting started with a database is to insert some data that you already have in files. We have some sample data online that you can insert that represents clickstream data - it includes a user ID, a URL that was visited, and the timestamp of the event. Suppose we have the following text in a CSV file named data.csv: 102,This is data in a file,2022-02-22 10:43:28,123.45 101,It is comma-separated,2022-02-23 00:00:00,456.78 103,Use FORMAT to specify the format,2022-02-21 10:43:30,678.90 The following command inserts the data into my_first_table: ./clickhouse client --query='INSERT INTO helloworld.my_first_table FORMAT CSV' &lt; data.csv Notice the new rows appear in the table now: "},{"title":"What's Next?​","type":1,"pageTitle":"Quick Start","url":"docs/quick-start#whats-next","content":"Check out our 25-minute video on Getting Started with ClickHouse,If you want to play around with larger data, we have a list of example datasets with instructions on how to insert themIf your data is coming from an external source, view our collection of integration guides for connecting to message queues, databases, pipelines and moreIf you are using a UI/BI visualization tool, view the user guides for connecting a UI to ClickHouse "},{"title":"S3 Table Functions","type":0,"sectionRef":"#","url":"docs/integrations/s3/s3-table-functions","content":"","keywords":""},{"title":"Preparation​","type":1,"pageTitle":"S3 Table Functions","url":"docs/integrations/s3/s3-table-functions#preparation","content":"To interact with our s3 based dataset, we prepare a standard merge tree table as our destination. The statement below creates this table under the default database. CREATE TABLE trips ( `trip_id` UInt32, `vendor_id` Enum8('1' = 1, '2' = 2, '3' = 3, '4' = 4, 'CMT' = 5, 'VTS' = 6, 'DDS' = 7, 'B02512' = 10, 'B02598' = 11, 'B02617' = 12, 'B02682' = 13, 'B02764' = 14, '' = 15), `pickup_date` Date, `pickup_datetime` DateTime, `dropoff_date` Date, `dropoff_datetime` DateTime, `store_and_fwd_flag` UInt8, `rate_code_id` UInt8, `pickup_longitude` Float64, `pickup_latitude` Float64, `dropoff_longitude` Float64, `dropoff_latitude` Float64, `passenger_count` UInt8, `trip_distance` Float64, `fare_amount` Float32, `extra` Float32, `mta_tax` Float32, `tip_amount` Float32, `tolls_amount` Float32, `ehail_fee` Float32, `improvement_surcharge` Float32, `total_amount` Float32, `payment_type` Enum8('UNK' = 0, 'CSH' = 1, 'CRE' = 2, 'NOC' = 3, 'DIS' = 4), `trip_type` UInt8, `pickup` FixedString(25), `dropoff` FixedString(25), `cab_type` Enum8('yellow' = 1, 'green' = 2, 'uber' = 3), `pickup_nyct2010_gid` Int8, `pickup_ctlabel` Float32, `pickup_borocode` Int8, `pickup_ct2010` String, `pickup_boroct2010` FixedString(7), `pickup_cdeligibil` String, `pickup_ntacode` FixedString(4), `pickup_ntaname` String, `pickup_puma` UInt16, `dropoff_nyct2010_gid` UInt8, `dropoff_ctlabel` Float32, `dropoff_borocode` UInt8, `dropoff_ct2010` String, `dropoff_boroct2010` FixedString(7), `dropoff_cdeligibil` String, `dropoff_ntacode` FixedString(4), `dropoff_ntaname` String, `dropoff_puma` UInt16 ) ENGINE = MergeTree PARTITION BY toYYYYMM(pickup_date) ORDER BY pickup_datetime SETTINGS index_granularity = 8192  Note the use of partitioning on the pickup_date field. Whilst usually a technique to assist with data management, we can later use this key to parallelize writes to s3. Each entry in our taxi dataset contains a taxi trip. This anonymized data consists of 20m compressed in the s3 bucket https://datasets-documentation.s3.eu-west-3.amazonaws.com/ under the folder nyc-taxi. We offer this data in tsv format with approximately 1m rows per file. "},{"title":"Reading Data from s3​","type":1,"pageTitle":"S3 Table Functions","url":"docs/integrations/s3/s3-table-functions#reading-data-from-s3","content":"We can query s3 data as a source without requiring persistence in ClickHouse. In the following query, we sample 10 rows. Note the absence of credentials here as the bucket is publicly accessible: SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_*.gz', 'TabSeparatedWithNames') LIMIT 10;  Note that we are not required to list the columns since the TabSeparatedWithNames format encodes the column names in the first row. Other formats, such as plain CSV or TSV, will return auto-generated columns for this query, e.g., c1, c2, c3 etc. Queries additionally support the virtual columns _path and _file that provide information regards the bucket path and filename respectively e.g. SELECT _path, _file, trip_id FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_0.gz', 'TabSeparatedWithNames') LIMIT 5;  _path\t_file\ttrip_iddatasets-documentation/nyc-taxi/trips_0.gz\ttrips_0.gz\t1199999902 datasets-documentation/nyc-taxi/trips_0.gz\ttrips_0.gz\t1199999919 datasets-documentation/nyc-taxi/trips_0.gz\ttrips_0.gz\t1199999944 datasets-documentation/nyc-taxi/trips_0.gz\ttrips_0.gz\t1199999969 datasets-documentation/nyc-taxi/trips_0.gz\ttrips_0.gz\t1199999990 Confirm the number of rows in this sample dataset. Note the use of wildcards for file expansion, so we consider all twenty files. This query will take around 10s depending on the number of cores on the ClickHouse instance: SELECT count() as count FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_*.gz', 'TabSeparatedWithNames');  count20000000 Whilst useful for sampling data and executing one-off exploratory queries, reading data directly from s3 is unlikely to perform on larger datasets. "},{"title":"Using clickHouse-local​","type":1,"pageTitle":"S3 Table Functions","url":"docs/integrations/s3/s3-table-functions#using-clickhouse-local","content":"The clickhouse-local program enables you to perform fast processing on local files without deploying and configuring the ClickHouse server. Any queries using the s3 table function can be performed with this utility. For example, clickhouse-local --query &quot;SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_*.gz', 'TabSeparatedWithNames') LIMIT 10&quot;  "},{"title":"Inserting Data from s3​","type":1,"pageTitle":"S3 Table Functions","url":"docs/integrations/s3/s3-table-functions#inserting-data-from-s3","content":"To exploit the full capabilities of ClickHouse, we next read and insert the data into our instance. We combine our s3 function with a simple INSERT statement to achieve this. Note that we aren’t required to list our columns as our target table provides the required structure. This requires the columns to appear in the order specified in the table DDL statement: columns are mapped according to their position in the SELECT clause. The insertion of all 10m rows can take a few minutes depending on the ClickHouse instance. Below we insert 1m to ensure a prompt response. Adjust the LIMIT clause or column selection to import subsets as required: INSERT INTO trips SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_*.gz', 'TabSeparatedWithNames') LIMIT 1000000;  "},{"title":"Remote Insert using ClickHouse Local​","type":1,"pageTitle":"S3 Table Functions","url":"docs/integrations/s3/s3-table-functions#remote-insert-using-clickhouse-local","content":"If network security policies prevent your ClickHouse cluster from making outbound connections, you can potentially insert s3 data using the ClickHouse local. In the example below, we read from an s3 bucket and insert to ClickHouse using the remote function. clickhouse-local --query &quot;INSERT INTO TABLE FUNCTION remote('localhost:9000', 'default.trips', 'username', 'password') (*) SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_*.gz', 'TabSeparatedWithNames') LIMIT 10&quot;  To execute this over a secure SSL connection, utilize the remoteSecure function. This approach offers inferior performance to direct inserts on the cluster and is for use in restricted scenarios only. "},{"title":"Exporting Data​","type":1,"pageTitle":"S3 Table Functions","url":"docs/integrations/s3/s3-table-functions#exporting-data","content":"We assume you have a bucket to write data in the following examples. This will require appropriate permissions. We pass the credentials needed in the request. For further options, see Managing Credentials. In the simple example below, we use the table function as a destination instead of a source. Here we stream 10k rows from the trips table to a bucket, specifying lz4 compression and output type of CSV. INSERT INTO FUNCTION s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/csv/trips.csv.lz4', 's3_key', 's3_secret', 'CSV') SELECT * FROM trips LIMIT 10000;  note This query requires write access to the bucket. Note here how the format of the file is inferred from the extension. We also don’t need to specify the columns in the s3 function - this can be inferred from the SELECT. "},{"title":"Splitting Large Files​","type":1,"pageTitle":"S3 Table Functions","url":"docs/integrations/s3/s3-table-functions#splitting-large-files","content":"It is unlikely you will want to export your data as a single file. Most tools, including ClickHouse, will achieve higher throughput performance when reading and writing to multiple files due to the possibility of parallelism. We could execute our INSERT command multiple times, targeting a subset of the data. ClickHouse offers a means of automatic splitting files using a PARTITION key. In the example below, we create ten files using a modulus of the rand() function. Notice how the resulting partition id is referenced in the filename. This results in ten files with a numerical suffix, e.g. trips_0.csv.lz4, trips_1.csv.lz4 etc...: INSERT INTO FUNCTION s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/csv/trips_{_partition_id}.csv.lz4', 's3_key', 's3_secret', 'CSV') PARTITION BY rand() % 10 SELECT * FROM trips LIMIT 100000;  note This query requires write access to the bucket. Alternatively, we can reference a field in the data. For this dataset, the payment_type provides a natural partitioning key with a cardinality of 5. INSERT INTO FUNCTION s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/csv/trips_{_partition_id}.csv.lz4', 's3_key', 's3_secret', 'CSV') PARTITION BY payment_type SELECT * FROM trips LIMIT 100000;  note This query requires write access to the bucket. "},{"title":"Utilizing Clusters​","type":1,"pageTitle":"S3 Table Functions","url":"docs/integrations/s3/s3-table-functions#utilizing-clusters","content":"The above functions are all limited to execution on a single node. Read speeds will scale linearly with CPU cores until other resources (typically network) are saturated, allowing users to vertically scale. However, this approach has its limitations. While users can alleviate some resource pressure by inserting into a distributed table when performing an INSERT INTO SELECT query, this still leaves a single node reading, parsing, and processing the data. To address this challenge and allow us to scale reads horizontally, we have the s3Cluster function. The node which receives the query, known as the initiator, creates a connection to every node in the cluster. The glob pattern determining which files need to be read is resolved to a set of files. The initiator distributes files to the nodes in the cluster, which act as workers. These workers, in turn, request files to process as they complete reads. This process ensures that we can scale reads horizontally. The s3Cluster function takes the same format as the single node variants, except that a target cluster is required to denote the worker nodes. s3Cluster(cluster_name, source, [access_key_id, secret_access_key,] format, structure)  where, cluster_name — Name of a cluster that is used to build a set of addresses and connection parameters to remote and local servers.source — URL to a file or a bunch of files. Supports following wildcards in read-only mode: *, ?, {'abc','def'} and {N..M} where N, M — numbers, abc, def — strings. For more information see Wildcards In Path.access_key_id and secret_access_key — Keys that specify credentials to use with the given endpoint. Optional.format — The format of the file.structure — Structure of the table. Format 'column1_name column1_type, column2_name column2_type, ...'. Like any s3 functions, the credentials are optional if the bucket is insecure or you define security through the environment, e.g., IAM roles. Unlike the s3 function, however, the structure must be specified in the request as of 22.3.1, i.e., the schema is not inferred. This function will be used as part of an INSERT INTO SELECT in most cases. In this case, you will often be inserting a distributed table. We illustrate a simple example below where trips_all is a distributed table. Whilst this table uses the events cluster, the consistency of the nodes used for reads and writes is not a requirement: INSERT INTO default.trips_all SELECT * FROM s3Cluster('events', 'https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_*.gz', 'TabSeparatedWithNames')  note This query requires fixes to support schema inference present in 21.3.1 and later. Note that as of 22.3.1, inserts will occur against the initiator node. This means that whilst reads will occur on each node, the resulting rows will be routed to the initiator for distribution. In high throughput scenarios, this may prove a bottleneck. To address this, the s3Cluster function will work with the parameter parallel_distributed_insert_select in future versions. See Optimizing for Performance for further details on ensuring the s3cluster function achieves optimal performance. "},{"title":"Other Formats & Increasing Throughput​","type":1,"pageTitle":"S3 Table Functions","url":"docs/integrations/s3/s3-table-functions#other-formats--increasing-throughput","content":"See Optimizing for Performance. "},{"title":"What Is ClickHouse?","type":0,"sectionRef":"#","url":"docs/intro","content":"","keywords":""},{"title":"Key Properties of OLAP Scenario​","type":1,"pageTitle":"What Is ClickHouse?","url":"docs/intro#key-properties-of-olap-scenario","content":"The vast majority of requests are for read access.Data is updated in fairly large batches (&gt; 1000 rows), not by single rows; or it is not updated at all.Data is added to the DB but is not modified.For reads, quite a large number of rows are extracted from the DB, but only a small subset of columns.Tables are “wide,” meaning they contain a large number of columns.Queries are relatively rare (usually hundreds of queries per server or less per second).For simple queries, latencies around 50 ms are allowed.Column values are fairly small: numbers and short strings (for example, 60 bytes per URL).Requires high throughput when processing a single query (up to billions of rows per second per server).Transactions are not necessary.Low requirements for data consistency.There is one large table per query. All tables are small, except for one.A query result is significantly smaller than the source data. In other words, data is filtered or aggregated, so the result fits in a single server’s RAM. It is easy to see that the OLAP scenario is very different from other popular scenarios (such as OLTP or Key-Value access). So it does not make sense to try to use OLTP or a Key-Value DB for processing analytical queries if you want to get decent performance. For example, if you try to use MongoDB or Redis for analytics, you will get very poor performance compared to OLAP databases. "},{"title":"Why Column-Oriented Databases Work Better in the OLAP Scenario​","type":1,"pageTitle":"What Is ClickHouse?","url":"docs/intro#why-column-oriented-databases-work-better-in-the-olap-scenario","content":"Column-oriented databases are better suited to OLAP scenarios: they are at least 100 times faster in processing most queries. The reasons are explained in detail below, but the fact is easier to demonstrate visually: Row-oriented DBMS  Column-oriented DBMS  See the difference? "},{"title":"Input/output​","type":1,"pageTitle":"What Is ClickHouse?","url":"docs/intro#inputoutput","content":"For an analytical query, only a small number of table columns need to be read. In a column-oriented database, you can read just the data you need. For example, if you need 5 columns out of 100, you can expect a 20-fold reduction in I/O.Since data is read in packets, it is easier to compress. Data in columns is also easier to compress. This further reduces the I/O volume.Due to the reduced I/O, more data fits in the system cache. For example, the query “count the number of records for each advertising platform” requires reading one “advertising platform ID” column, which takes up 1 byte uncompressed. If most of the traffic was not from advertising platforms, you can expect at least 10-fold compression of this column. When using a quick compression algorithm, data decompression is possible at a speed of at least several gigabytes of uncompressed data per second. In other words, this query can be processed at a speed of approximately several billion rows per second on a single server. This speed is actually achieved in practice. "},{"title":"CPU​","type":1,"pageTitle":"What Is ClickHouse?","url":"docs/intro#cpu","content":"Since executing a query requires processing a large number of rows, it helps to dispatch all operations for entire vectors instead of for separate rows, or to implement the query engine so that there is almost no dispatching cost. If you do not do this, with any half-decent disk subsystem, the query interpreter inevitably stalls the CPU. It makes sense to both store data in columns and process it, when possible, by columns. There are two ways to do this: A vector engine. All operations are written for vectors, instead of for separate values. This means you do not need to call operations very often, and dispatching costs are negligible. Operation code contains an optimized internal cycle. Code generation. The code generated for the query has all the indirect calls in it. This is not done in “normal” databases, because it does not make sense when running simple queries. However, there are exceptions. For example, MemSQL uses code generation to reduce latency when processing SQL queries. (For comparison, analytical DBMSs require optimization of throughput, not latency.) Note that for CPU efficiency, the query language must be declarative (SQL or MDX), or at least a vector (J, K). The query should only contain implicit loops, allowing for optimization. Original article "},{"title":"ClickHouse Tutorial","type":0,"sectionRef":"#","url":"docs/tutorial","content":"","keywords":"clickhouse install tutorial"},{"title":"What to Expect from This Tutorial?​","type":1,"pageTitle":"ClickHouse Tutorial","url":"docs/tutorial#what-to-expect-from-this-tutorial","content":"In this tutorial, you will create a table and insert a large dataset (two million rows of the New York taxi data). Then you will execute queries on the dataset, including an example of how to create a dictionary from an external data source and use it to perform a JOIN. note This tutorial assumes you have already the ClickHouse server up and running as described in the Quick Start. "},{"title":"1. Create a New Table​","type":1,"pageTitle":"ClickHouse Tutorial","url":"docs/tutorial#1-create-a-new-table","content":"The New York City taxi data contains the details of millions of taxi rides, with columns like pickup and dropoff times and locations, cost, tip amount, tolls, payment type and so on. Let's create a table to store this data... Either open your Play UI at http://localhost:8123/play or startup the clickhouse client by running the following command from the folder where your clickhouse binary is stored: ./clickhouse client Create the following trips table in the default database: CREATE TABLE trips ( `trip_id` UInt32, `vendor_id` Enum8('1' = 1, '2' = 2, '3' = 3, '4' = 4, 'CMT' = 5, 'VTS' = 6, 'DDS' = 7, 'B02512' = 10, 'B02598' = 11, 'B02617' = 12, 'B02682' = 13, 'B02764' = 14, '' = 15), `pickup_date` Date, `pickup_datetime` DateTime, `dropoff_date` Date, `dropoff_datetime` DateTime, `store_and_fwd_flag` UInt8, `rate_code_id` UInt8, `pickup_longitude` Float64, `pickup_latitude` Float64, `dropoff_longitude` Float64, `dropoff_latitude` Float64, `passenger_count` UInt8, `trip_distance` Float64, `fare_amount` Float32, `extra` Float32, `mta_tax` Float32, `tip_amount` Float32, `tolls_amount` Float32, `ehail_fee` Float32, `improvement_surcharge` Float32, `total_amount` Float32, `payment_type` Enum8('UNK' = 0, 'CSH' = 1, 'CRE' = 2, 'NOC' = 3, 'DIS' = 4), `trip_type` UInt8, `pickup` FixedString(25), `dropoff` FixedString(25), `cab_type` Enum8('yellow' = 1, 'green' = 2, 'uber' = 3), `pickup_nyct2010_gid` Int8, `pickup_ctlabel` Float32, `pickup_borocode` Int8, `pickup_ct2010` String, `pickup_boroct2010` FixedString(7), `pickup_cdeligibil` String, `pickup_ntacode` FixedString(4), `pickup_ntaname` String, `pickup_puma` UInt16, `dropoff_nyct2010_gid` UInt8, `dropoff_ctlabel` Float32, `dropoff_borocode` UInt8, `dropoff_ct2010` String, `dropoff_boroct2010` FixedString(7), `dropoff_cdeligibil` String, `dropoff_ntacode` FixedString(4), `dropoff_ntaname` String, `dropoff_puma` UInt16 ) ENGINE = MergeTree PARTITION BY toYYYYMM(pickup_date) ORDER BY pickup_datetime  "},{"title":"2. Insert the Dataset​","type":1,"pageTitle":"ClickHouse Tutorial","url":"docs/tutorial#2-insert-the-dataset","content":"Now that you have a table created, let's add the NYC taxi data. It is in CSV files in S3, and you can simply load the data from there. The following command inserts ~2,000,000 rows into your trips table from two different files in S3: trips_1.tsv.gz and trips_2.tsv.gz: INSERT INTO trips SELECT * FROM s3( 'https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_{1..2}.gz', 'TabSeparatedWithNames' ) Wait for the INSERT to execute - it might take a minute or two for the 150MB of data to be downloaded. note The s3 function cleverly knows how to decompress the data, and the TabSeparatedWithNames format tells ClickHouse that the data is tab-separated and also to skip the header row of each file. When the data is finished being inserted, verify it worked: SELECT count() FROM trips You should see about 2M rows (1,999,657 rows, to be precise). note Notice how quickly and how few rows ClickHouse had to process to determine the count. You can get back the count in 0.001 seconds with only 6 rows processed. (6 just happens to be the number of parts that the trips table currently has, and parts know how many rows they have.) If you run a query that needs to hit every row, you will notice considerably more rows need to be processed, but the execution time is still blazing fast: SELECT DISTINCT(pickup_ntaname) FROM trips This query has to process 2M rows and return 190 values, but notice it does this in about 0.05 seconds. The pickup_ntaname column represents the name of the neighborhood in New York City where the taxi ride originated. "},{"title":"3. Analyze the Data​","type":1,"pageTitle":"ClickHouse Tutorial","url":"docs/tutorial#3-analyze-the-data","content":"Let's see how quickly ClickHouse can process 2M rows of data... We will start with some simple and fast calculations, like computing the average tip amount (which is right on $1) SELECT avg(tip_amount) FROM trips The response is almost immediate: ┌────avg(tip_amount)─┐ │ 1.6847585806972212 │ └────────────────────┘ 1 rows in set. Elapsed: 0.113 sec. Processed 2.00 million rows, 8.00 MB (17.67 million rows/s., 70.69 MB/s.) This query computes the average cost based on the number of passengers: SELECT passenger_count, ceil(avg(total_amount),2) AS average_total_amount FROM trips GROUP BY passenger_count The passenger_count ranges from 0 to 9: ┌─passenger_count─┬─average_total_amount─┐ │ 0 │ 22.69 │ │ 1 │ 15.97 │ │ 2 │ 17.15 │ │ 3 │ 16.76 │ │ 4 │ 17.33 │ │ 5 │ 16.35 │ │ 6 │ 16.04 │ │ 7 │ 59.8 │ │ 8 │ 36.41 │ │ 9 │ 9.81 │ └─────────────────┴──────────────────────┘ 10 rows in set. Elapsed: 0.015 sec. Processed 2.00 million rows, 10.00 MB (129.00 million rows/s., 645.01 MB/s.) Here is a query that calculates the daily number of pickups per neighborhood: SELECT pickup_date, pickup_ntaname, SUM(1) AS number_of_trips FROM trips GROUP BY pickup_date, pickup_ntaname ORDER BY pickup_date ASC The result looks like: ┌─pickup_date─┬─pickup_ntaname───────────────────────────────────────────┬─number_of_trips─┐ │ 2015-07-01 │ Brooklyn Heights-Cobble Hill │ 13 │ │ 2015-07-01 │ Old Astoria │ 5 │ │ 2015-07-01 │ Flushing │ 1 │ │ 2015-07-01 │ Yorkville │ 378 │ │ 2015-07-01 │ Gramercy │ 344 │ │ 2015-07-01 │ Fordham South │ 2 │ │ 2015-07-01 │ SoHo-TriBeCa-Civic Center-Little Italy │ 621 │ │ 2015-07-01 │ Park Slope-Gowanus │ 29 │ │ 2015-07-01 │ Bushwick South │ 5 │  This query computes the length of the trip and groups the results by that value: SELECT avg(tip_amount) AS avg_tip, avg(fare_amount) AS avg_fare, avg(passenger_count) AS avg_passenger, count() AS count, truncate(date_diff('second', pickup_datetime, dropoff_datetime)/3600) as trip_minutes FROM trips WHERE trip_minutes &gt; 0 GROUP BY trip_minutes ORDER BY trip_minutes DESC The result looks like: ┌────────────avg_tip─┬───────────avg_fare─┬──────avg_passenger─┬─count─┬─trip_minutes─┐ │ 0.9800000190734863 │ 10 │ 1.5 │ 2 │ 458 │ │ 1.18236789075801 │ 14.493377928590297 │ 2.060200668896321 │ 1495 │ 23 │ │ 2.1159574744549206 │ 23.22872340425532 │ 2.4680851063829787 │ 47 │ 22 │ │ 1.1218181631781838 │ 13.681818181818182 │ 1.9090909090909092 │ 11 │ 21 │ │ 0.3218181837688793 │ 18.045454545454547 │ 2.3636363636363638 │ 11 │ 20 │ │ 2.1490000009536745 │ 17.55 │ 1.5 │ 10 │ 19 │ │ 4.537058907396653 │ 37 │ 1.7647058823529411 │ 17 │ 18 │  This query shows the number of pickups in each neighborhood, broken down by hour of the day: SELECT pickup_ntaname, toHour(pickup_datetime) as pickup_hour, SUM(1) AS pickups FROM trips WHERE pickup_ntaname != '' GROUP BY pickup_ntaname, pickup_hour ORDER BY pickup_ntaname, pickup_hour The result looks like: ┌─pickup_ntaname───────────────────────────────────────────┬─pickup_hour─┬─pickups─┐ │ Airport │ 0 │ 3509 │ │ Airport │ 1 │ 1184 │ │ Airport │ 2 │ 401 │ │ Airport │ 3 │ 152 │ │ Airport │ 4 │ 213 │ │ Airport │ 5 │ 955 │ │ Airport │ 6 │ 2161 │ │ Airport │ 7 │ 3013 │ │ Airport │ 8 │ 3601 │ │ Airport │ 9 │ 3792 │ │ Airport │ 10 │ 4546 │ │ Airport │ 11 │ 4659 │ │ Airport │ 12 │ 4621 │ │ Airport │ 13 │ 5348 │ │ Airport │ 14 │ 5889 │ │ Airport │ 15 │ 6505 │ │ Airport │ 16 │ 6119 │ │ Airport │ 17 │ 6341 │ │ Airport │ 18 │ 6173 │ │ Airport │ 19 │ 6329 │ │ Airport │ 20 │ 6271 │ │ Airport │ 21 │ 6649 │ │ Airport │ 22 │ 6356 │ │ Airport │ 23 │ 6016 │ │ Allerton-Pelham Gardens │ 4 │ 1 │ │ Allerton-Pelham Gardens │ 6 │ 1 │ │ Allerton-Pelham Gardens │ 7 │ 1 │ │ Allerton-Pelham Gardens │ 9 │ 5 │ │ Allerton-Pelham Gardens │ 10 │ 3 │ │ Allerton-Pelham Gardens │ 15 │ 1 │ │ Allerton-Pelham Gardens │ 20 │ 2 │ │ Allerton-Pelham Gardens │ 23 │ 1 │ │ Annadale-Huguenot-Prince's Bay-Eltingville │ 23 │ 1 │ │ Arden Heights │ 11 │ 1 │ Let's look at rides to LaGuardia or JFK airports, which requires all 2M rows to be processed and returns in less than 0.04 seconds: SELECT pickup_datetime, dropoff_datetime, total_amount, pickup_nyct2010_gid, dropoff_nyct2010_gid, CASE WHEN dropoff_nyct2010_gid = 138 THEN 'LGA' WHEN dropoff_nyct2010_gid = 132 THEN 'JFK' END AS airport_code, EXTRACT(YEAR FROM pickup_datetime) AS year, EXTRACT(DAY FROM pickup_datetime) AS day, EXTRACT(HOUR FROM pickup_datetime) AS hour FROM trips WHERE dropoff_nyct2010_gid IN (132, 138) ORDER BY pickup_datetime The response is: ┌─────pickup_datetime─┬────dropoff_datetime─┬─total_amount─┬─pickup_nyct2010_gid─┬─dropoff_nyct2010_gid─┬─airport_code─┬─year─┬─day─┬─hour─┐ │ 2015-07-01 00:04:14 │ 2015-07-01 00:15:29 │ 13.3 │ -34 │ 132 │ JFK │ 2015 │ 1 │ 0 │ │ 2015-07-01 00:09:42 │ 2015-07-01 00:12:55 │ 6.8 │ 50 │ 138 │ LGA │ 2015 │ 1 │ 0 │ │ 2015-07-01 00:23:04 │ 2015-07-01 00:24:39 │ 4.8 │ -125 │ 132 │ JFK │ 2015 │ 1 │ 0 │ │ 2015-07-01 00:27:51 │ 2015-07-01 00:39:02 │ 14.72 │ -101 │ 138 │ LGA │ 2015 │ 1 │ 0 │ │ 2015-07-01 00:32:03 │ 2015-07-01 00:55:39 │ 39.34 │ 48 │ 138 │ LGA │ 2015 │ 1 │ 0 │ │ 2015-07-01 00:34:12 │ 2015-07-01 00:40:48 │ 9.95 │ -93 │ 132 │ JFK │ 2015 │ 1 │ 0 │ │ 2015-07-01 00:38:26 │ 2015-07-01 00:49:00 │ 13.3 │ -11 │ 138 │ LGA │ 2015 │ 1 │ 0 │ │ 2015-07-01 00:41:48 │ 2015-07-01 00:44:45 │ 6.3 │ -94 │ 132 │ JFK │ 2015 │ 1 │ 0 │ │ 2015-07-01 01:06:18 │ 2015-07-01 01:14:43 │ 11.76 │ 37 │ 132 │ JFK │ 2015 │ 1 │ 1 │ note As you can see, it doesn't seem to matter what type of grouping or calculation that is being performed, ClickHouse retrieves the results almost immediately! "},{"title":"4. Create a Dictionary​","type":1,"pageTitle":"ClickHouse Tutorial","url":"docs/tutorial#4-create-a-dictionary","content":"If you are new to ClickHouse, it is important to understand how dictionaries work. A dictionary is a mapping of key-&gt;value pairs that is stored in memory. They often are associated with data in a file or external database (and they can periodically update with their external data source). Let's see how to create a dictionary associated with a file in S3. The file contains 265 rows, one row for each neighborhood in NYC. The neighborhoods are mapped to the names of the NYC boroughs (NYC has 5 boroughs: the Bronx, Booklyn, Manhattan, Queens and Staten Island), and this file counts Newark Airport (EWR) as a borough as well. The LocationID column in the our file maps to the pickup_nyct2010_gid and dropoff_nyct2010_gid columns in your trips table. Here are a few rows from the CSV file: LocationID\tBorough\tZone\tservice_zone1\tEWR\tNewark Airport\tEWR 2\tQueens\tJamaica Bay\tBoro Zone 3\tBronx\tAllerton/Pelham Gardens\tBoro Zone 4\tManhattan\tAlphabet City\tYellow Zone 5\tStaten Island\tArden Heights\tBoro Zone The URL for the file is https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/taxi_zone_lookup.csv. Run the following SQL, which creates a new dictionary named taxi_zone_dictionary that is based on this file in S3: CREATE DICTIONARY taxi_zone_dictionary ( LocationID UInt16 DEFAULT 0, Borough String, Zone String, service_zone String ) PRIMARY KEY LocationID SOURCE(HTTP( url 'https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/taxi_zone_lookup.csv' format 'CSVWithNames' )) LIFETIME(0) LAYOUT(HASHED()) note Setting LIFETIME to 0 means this dictionary will never update with its source. It is used here to not send unnecessary traffic to our S3 bucket, but in general you could specify any lifetime values you prefer. For example: LIFETIME(MIN 1 MAX 10) specifies the dictionary to update after some random time between 1 and 10 seconds. (The random time is necessary in order to distribute the load on the dictionary source when updating on a large number of servers.) Verify it worked - you should get 265 rows (one row for each neighborhood): SELECT * FROM taxi_zone_dictionary Use the dictGet function (or its variations) to retrieve a value from a dictionary. You pass in the name of the dictionary, the value you want, and the key (which in our example is the LocationID column of taxi_zone_dictionary). For exmaple, the following query returns the Borough whose LocationID is 132 (which as we saw above is JFK airport): SELECT dictGet('taxi_zone_dictionary', 'Borough', 132) JFK is in Queens, and notice the time to retrieve the value is essentially 0: ┌─dictGet('taxi_zone_dictionary', 'Borough', 132)─┐ │ Queens │ └─────────────────────────────────────────────────┘ 1 rows in set. Elapsed: 0.004 sec. Use the dictHas function to see if a key is present in the dictionary. For example, the following query returns 1 (which is &quot;true&quot; in ClickHouse): SELECT dictHas('taxi_zone_dictionary', 132) The following query returns 0 because 4567 is not a value of LocationID in the dictionary: SELECT dictHas('taxi_zone_dictionary', 4567) Use the dictGet function to retrieve a borough's name in a query. For example: SELECT count(1) AS total, dictGetOrDefault('taxi_zone_dictionary','Borough', toUInt64(pickup_nyct2010_gid), 'Unknown') AS borough_name FROM trips WHERE dropoff_nyct2010_gid = 132 OR dropoff_nyct2010_gid = 138 GROUP BY borough_name ORDER BY total DESC This query sums up the number of taxi rides per borough that end at either the LaGuardia or JFK airport. The result looks like the following, and notice there are quite a few trips where the dropoff neighborhood is unknown: ┌─total─┬─borough_name──┐ │ 23683 │ Unknown │ │ 7053 │ Manhattan │ │ 6828 │ Brooklyn │ │ 4458 │ Queens │ │ 2670 │ Bronx │ │ 554 │ Staten Island │ │ 53 │ EWR │ └───────┴───────────────┘ 7 rows in set. Elapsed: 0.019 sec. Processed 2.00 million rows, 4.00 MB (105.70 million rows/s., 211.40 MB/s.)  "},{"title":"5. Perform a Join​","type":1,"pageTitle":"ClickHouse Tutorial","url":"docs/tutorial#5-perform-a-join","content":"Let's write some queries that join the taxi_zone_dictionary with your trips table. We can start with a simple JOIN that acts similarly to the previous airport query above: SELECT count(1) AS total, Borough FROM trips JOIN taxi_zone_dictionary ON toUInt64(trips.pickup_nyct2010_gid) = taxi_zone_dictionary.LocationID WHERE dropoff_nyct2010_gid = 132 OR dropoff_nyct2010_gid = 138 GROUP BY Borough ORDER BY total DESC The response looks familiar: ┌─total─┬─Borough───────┐ │ 7053 │ Manhattan │ │ 6828 │ Brooklyn │ │ 4458 │ Queens │ │ 2670 │ Bronx │ │ 554 │ Staten Island │ │ 53 │ EWR │ └───────┴───────────────┘ 6 rows in set. Elapsed: 0.034 sec. Processed 2.00 million rows, 4.00 MB (59.14 million rows/s., 118.29 MB/s.) note Notice the output of the above JOIN query is the same as the query before it that used dictGetOrDefault (except that the Unknown values are not included). Behind the scenes, ClickHouse is actually calling the dictGet function for the taxi_zone_dictionary dictionary, but the JOIN syntax is more familiar for SQL developers. We do not use SELECT * often in ClickHouse - you should only retrieve the columns you actually need! But it is difficult to find a query that takes a long time, so this query purposely selects every column and returns every row (except there is a built-in 10,000 row maximum in the response by default), and also does a right join of every row with the dictionary: SELECT * FROM trips JOIN taxi_zone_dictionary ON trips.dropoff_nyct2010_gid = taxi_zone_dictionary.LocationID WHERE tip_amount &gt; 0 ORDER BY tip_amount DESC It is the slowest query in this tutorial, yet it only takes about 0.8 seconds to process all 2M rows. Nice! Congrats!​ Well done, you made it through the tutorial, and hopefully you have a better understanding of how to use ClickHouse. Here are some options for what to do next: View the Getting Started with ClickHouse video, an excellent introduction to ClickHouseRead how primary keys work in ClickHouse - this knowledge will move you a long ways forward along your journey to becoming a ClickHouse expertIntegrate an external data source like files, Kafka, PostgreSQL, data pipelines, or lots of other data sourcesConnect your favorite UI/BI tool to ClickHouseCheck out the SQL Reference and browse through the various functions. ClickHouse has an amazing collection of functions for transforming, processing and analyzing data Original article "}]